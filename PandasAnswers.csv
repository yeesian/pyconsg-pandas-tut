Id,PostTypeId,ParentId,CreationDate,Score,Body,OwnerUserId,OwnerDisplayName,LastEditorUserId,LastEditorDisplayName,LastEditDate,LastActivityDate,CommentCount
13456818,2,13454588,2012-11-19 15:24:43,2,"Here's an example derived from - http://matplotliborg/examples/pylab_examples/finance_demohtml

Take special note of axannotate method call in the code below

from pylab import *
from matplotlibdates import  DateFormatter  WeekdayLocator  HourLocator  \
     DayLocator  MONDAY
from matplotlibfinance import quotes_historical_yahoo  candlestick \
     plot_day_summary  candlestick2

# (Year  month  day) tuples suffice as args for quotes_historical_yahoo
date1 = ( 2004  2  1)
date2 = ( 2004  4  12 )


mondays = WeekdayLocator(MONDAY)        # major ticks on the mondays
alldays    = DayLocator()              # minor ticks on the days
weekFormatter = DateFormatter('%b %d')  # Eg  Jan 12
dayFormatter = DateFormatter('%d')      # Eg  12

quotes = quotes_historical_yahoo('INTC'  date1  date2)
if len(quotes) == 0:
    raise SystemExit

fig = figure()
figsubplots_adjust(bottom=02)
ax = figadd_subplot(111)
axxaxisset_major_locator(mondays)
axxaxisset_minor_locator(alldays)
axxaxisset_major_formatter(weekFormatter)
#axxaxisset_minor_formatter(dayFormatter)

#plot_day_summary(ax  quotes  ticksize=3)
candlestick(ax  quotes  width=06)

axxaxis_date()
axautoscale_view()
setp( gca()get_xticklabels()  rotation=45  horizontalalignment='right')

import datetime
dt = datetimedatetime(2004  3  8)

# Annotating a specific candle
axannotate('This is my special candle'  xy=(dt  24)  xytext=(dt  25) 
            arrowprops=dict(facecolor='black'  shrink=005) 
           )

show()


The resulting plot if you run this file  should show you:-

",482506.0,,,,,2012-11-19 15:24:43,2.0
13784026,2,13783721,2012-12-09 02:15:26,2,"I don't think there is an inbuilt pandas or numpy method/function to do this 

However  I would favour using a python generator:

def repeats(lst):
    i_0 = None
    n = -1 # will still work if lst starts with None
    for i in lst:
        if i == i_0:
            n += 1
        else:
            n = 0
        yield n
        i_0 = i
# list(repeats([1 1 1 2 2 3])) == [0 1 2 0 1 0]


Then you can put this generator into a numpy array:

import numpy as np
df['rep'] = nparray(list(repeats(df['time'])))


Count up the repeats:

from collections import Counter
count = Counter(df['time'])
df['count'] = df['time']apply(lambda x: count[x])


and do the calculation (this is the most expensive part of the calculation):

df['time2'] = dfapply(lambda row: (row['time'] 
                                 + datetimetimedelta(0  1) # 1s
                                     * row['rep'] 
                                     / row['count']) 
                 axis=1)


Note: to remove the calculation columns  use del df['rep'] and del df['count']



One ""built-in"" way to accomplish it might be accomplished using shift twice  but I think this is going to be somewhat messier",1240268.0,,1240268.0,,2012-12-09 17:05:45,2012-12-09 17:05:45,4.0
14370190,2,14355151,2013-01-17 00:11:12,0,"I am pretty convinced your issue is related to type mapping of the actual types in DataFrames and to how they are stored by PyTables

Simple types (floats/ints/bools) that have a fixed represenation  these are mapped to fixed c-types
Datetimes are handled if they can properly be converted (eg they have a dtype of 'datetime64[ns]'  notably datetimesdate are NOT handled (NaN are a different story and depending on usage can cause the entire column type to be mishandled)
Strings are mapped (in Storer objects to Object type  Table maps them to String types)
Unicode are not handled 
all other types are handled as Object in Storers or an Exception is throw for Tables
What this means is that if you are doing a put to a Storer (a fixed-representation)  then all of the non-mappable types will become Object  see this PyTables pickles these columns See the below reference for ObjectAtom

http://pytablesgithubcom/usersguide/libref/declarative_classeshtml#the-atom-class-and-its-descendants

Table will raise on an invalid type (I should provide a better error message here) I think I will also provide a warning if you try to store a type that is mapped to ObjectAtom (for performance reasons)

To force some types try some of these:

import pandas as pd

# convert None to nan (its currently Object)
# converts to float64 (or type of other objs)
x = pdSeries([None])
x = xwhere(pdnotnull(x))convert_objects()

# convert datetime like with embeded nans to datetime64[ns]
df['foo'] = pdSeries(df['foo']values  dtype = 'M8[ns]')


Heres a sample on 64-bit linux (file is 1M rows  about 1 GB in size on disk)

In [1]: import numpy as np

In [2]: import pandas as pd

In [3]: pd__version__
Out[3]: '0101dev'

In [3]: import tables

In [4]: tables__version__
Out[4]: '231'

In [4]: df = pdDataFrame(nprandomrandn(1000 * 1000  100)  index=range(int(
   : 1000 * 1000))  columns=['E%03d' % i for i in xrange(100)])

In [5]: for x in range(20):
   :     df['String%03d' % x] = 'string%03d' % x

In [6]: df
Out[6]: 
<class 'pandascoreframeDataFrame'>
Int64Index: 1000000 entries  0 to 999999
Columns: 120 entries  E000 to String019
dtypes: float64(100)  object(20)

# storer put (cannot query) 
In [9]: def test_put():
   :     store = pdHDFStore('test_puth5' 'w')
   :     store['df'] = df
   :     storeclose()

In [10]: %timeit test_put()
1 loops  best of 3: 765 s per loop

# table put (can query)
In [7]: def test_put():
      :     store = pdHDFStore('test_puth5' 'w')
      :     storeput('df' df table=True)
      :     storeclose()


In [8]: %timeit test_put()
1 loops  best of 3: 214 s per loop
",644898.0,,644898.0,,2013-01-18 13:29:48,2013-01-18 13:29:48,2.0
14474667,2,14355151,2013-01-23 07:43:24,0,"How to make this faster?

use 'iosqlread_frame' to load data from a sql db to a dataframe Because the 'read_frame' will take care of the columns whose type is 'decimal' by turning them into float
fill the missing data for each columns
call the function 'DataFrameconvert_objects' before putting operation
if having string type columns in dateframe  use 'table' instead of 'storer'
storeput('key'  df  table=True)

After doing these jobs  the performance of putting operation has a big improvement with the same data set:

CPU times: user 4207 s  sys: 2817 s  total: 7024 s
Wall time: 9897 s


Profile logs of the second test:


95984 function calls (95958 primitive calls) in 68688 CPU seconds

   Ordered by: internal time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      445   16757    0038   16757    0038 {numpycoremultiarrayarray}
       19   16250    0855   16250    0855 {method '_append_records' of 'tablestableExtensionTable' objects}
       16    7958    0497    7958    0497 {method 'astype' of 'numpyndarray' objects}
       19    6533    0344    6533    0344 {pandaslibcreate_hdf_rows_2d}
        4    6284    1571    6388    1597 {method '_fillCol' of 'tablestableExtensionRow' objects}
       20    2640    0132    2641    0132 {pandaslibmaybe_convert_objects}
        1    1785    1785    1785    1785 {pandaslibisnullobj}
        7    1619    0231    1619    0231 {method 'flatten' of 'numpyndarray' objects}
       11    1059    0096    1059    0096 {pandaslibinfer_dtype}
        1    0997    0997   41952   41952 pytablespy:2468(write_data)
       19    0985    0052   40590    2136 pytablespy:2504(write_data_chunk)
        1    0827    0827   60617   60617 pytablespy:2433(write)
     1504    0592    0000    0592    0000 {method '_g_readSlice' of 'tableshdf5ExtensionArray' objects}
        4    0534    0133   13676    3419 pytablespy:1038(set_atom)
        1    0528    0528    0528    0528 {pandaslibmax_len_string_array}
        4    0441    0110    0571    0143 internalspy:1409(_stack_arrays)
       35    0358    0010    0358    0010 {method 'copy' of 'numpyndarray' objects}
        1    0276    0276    3135    3135 internalspy:208(fillna)
        5    0263    0053    2054    0411 commonpy:128(_isnull_ndarraylike)
       48    0253    0005    0253    0005 {method '_append' of 'tableshdf5ExtensionArray' objects}
        4    0240    0060    1500    0375 internalspy:1400(_simple_blockify)
        1    0234    0234   12145   12145 pytablespy:1066(set_atom_string)
       28    0225    0008    0225    0008 {method '_createCArray' of 'tableshdf5ExtensionArray' objects}
       36    0218    0006    0218    0006 {method '_g_writeSlice' of 'tableshdf5ExtensionArray' objects}
     6110    0155    0000    0155    0000 {numpycoremultiarrayempty}
        4    0097    0024    0097    0024 {method 'all' of 'numpyndarray' objects}
        6    0084    0014    0084    0014 {tablesindexesExtensionkeysort}
       18    0084    0005    0084    0005 {method '_g_close' of 'tableshdf5ExtensionLeaf' objects}
    11816    0064    0000    0108    0000 filepy:1036(_getNode)
       19    0053    0003    0053    0003 {method '_g_flush' of 'tableshdf5ExtensionLeaf' objects}
     1528    0045    0000    0098    0000 arraypy:342(_interpret_indexing)
    11709    0040    0000    0042    0000 filepy:248(__getitem__)
        2    0027    0013    0383    0192 indexpy:1099(get_neworder)
        1    0018    0018    0018    0018 {numpycoremultiarrayputmask}
        4    0013    0003    0017    0004 indexpy:607(final_idx32)
",807695.0,,807695.0,,2013-01-24 04:46:52,2013-01-24 04:46:52,
14471383,2,14471120,2013-01-23 02:13:36,0,"for i in range(len(tcolumnslevels)):
    if tcolumnslevels[i]dtype == npint64:
        tcolumnslevels[i] = tcolumnslevels[i]astype(npfloat64)
",1479269.0,,,,,2013-01-23 02:13:36,
9551068,2,9550867,2012-03-04 00:10:52,3,"Just replace rows=['Y'] with rows=['X2']

>>> print pivot_table(df rows=['X2']  cols=['X'] aggfunc=npsum)
      Y                                        
X     1    2    3    4    5    6    7    8    9
X2                                             
1   101  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN
2   NaN  102  NaN  NaN  NaN  NaN  NaN  NaN  NaN
3   NaN  NaN  103  NaN  NaN  NaN  NaN  NaN  NaN
4   NaN  NaN  NaN  104  NaN  NaN  NaN  NaN  NaN
5   NaN  NaN  NaN  NaN  105  NaN  NaN  NaN  NaN
6   NaN  NaN  NaN  NaN  NaN  106  NaN  NaN  NaN
7   NaN  NaN  NaN  NaN  NaN  NaN  107  NaN  NaN
8   NaN  NaN  NaN  NaN  NaN  NaN  NaN  108  NaN
9   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  109
",50065.0,,,,,2012-03-04 00:10:52,2.0
9627900,2,9550867,2012-03-09 01:52:25,3,"Try this:

In [3]: dfpivot_table('Y'  rows='X'  cols='X2')
X2         1         2         3         4         5         6         7         8         9
X                                                                                           
1   99999991       NaN       NaN       NaN       NaN       NaN       NaN       NaN       NaN
2        NaN  99999992       NaN       NaN       NaN       NaN       NaN       NaN       NaN
3        NaN       NaN  99999993       NaN       NaN       NaN       NaN       NaN       NaN
4        NaN       NaN       NaN  99999994       NaN       NaN       NaN       NaN       NaN
5        NaN       NaN       NaN       NaN  99999995       NaN       NaN       NaN       NaN
6        NaN       NaN       NaN       NaN       NaN  99999996       NaN       NaN       NaN
7        NaN       NaN       NaN       NaN       NaN       NaN  99999997       NaN       NaN
8        NaN       NaN       NaN       NaN       NaN       NaN       NaN  99999998       NaN
9        NaN       NaN       NaN       NaN       NaN       NaN       NaN       NaN  99999999


This would also work:

pivot_table(df  'Y'  rows='X'  cols='X2')


or

pivot_table(df  rows='X'  cols='X2')['Y']
",776560.0,,,,,2012-03-09 01:52:25,
10300905,2,10214692,2012-04-24 15:18:14,0,"probably what happened is:

it couldn't install ""pandas""  so the numpy was not installed too

pip works this way:

if you try to install more than one package and if one of those packages doesn't really exist it breaks all the installation  so as pandas could not be installed it doesn't install pandas

there's a issue in pip to solve this  I gave a shot trying to solve  but is not that simple

anyway:

try install each package separately

try to install numpy (it will work)  
try to install pandas (it
    won't work  but you'll be able to see why it didn't install)
",1172125.0,,,,,2012-04-24 15:18:14,
10982599,2,10982266,2012-06-11 14:57:10,3,"In [28]: a = pdread_csv('aacsv')

In [29]: a
Out[29]: 
       time contract ticker    expiry  strike quote  price  volume
0  08:01:08        C    PXA  20100101    4000     A   578      60
1  08:01:11        C    PXA  20100101    4000     A   584      60
2  08:01:12        C    PXA  20100101    4000     A   580      60
3  08:01:16        C    PXA  20100101    4000     A   584      60
4  08:01:16        C    PXA  20100101    4000     A   580      60
5  08:01:21        C    PXA  20100101    4000     A   584      60
6  08:01:21        C    PXA  20100101    4000     A   580      60

In [30]: pdDataFrame([{'time': k 
                        'price': (vprice * vvolume)sum() / vvolumesum() 
                        'volume': vvolumemean()}
                       for k v in agroupby(['time'])] 
                      columns=['time'  'price'  'volume'])
Out[30]: 
       time  price  volume
0  08:01:08   578      60
1  08:01:11   584      60
2  08:01:12   580      60
3  08:01:16   582      60
4  08:01:21   582      60
",449449.0,,,,,2012-06-11 14:57:10,4.0
13738471,2,13737992,2012-12-06 06:57:31,1,"Try indexing with a Timestamp object:

>>> import pandas as pd
>>> from pandaslib import Timestamp
>>> url = 'http://ichartfinanceyahoocom/tablecsv?s=SPY&d=12&e=4&f=2012&g=d&a=01&b=01&c=2001&ignore=csv'
>>> df = pdread_csv(url  index_col='Date'  parse_dates=True)
>>> s = df['Close']
>>> s[Timestamp('2012-12-04')]
14125
",393304.0,,,,,2012-12-06 06:57:31,3.0
13743366,2,13737992,2012-12-06 12:11:05,1,"When the time series is not ordered and you give a partial timestamp (eg a date  rather than a datetime) it's not clear which datetime should be selected

It can't be assumed that there is only one datetime object per date  although there are in this example  here there are several options but it seems safer to throw an error here rather than guess a users motives (We could return a series/list similar to ix['2011-01']  but this may be confusing if returning a number in other cases We could try to return a ""closest match"" but this doesn't really make sense either)

In an ordered case it's easier  we pick the first datetime with the selected date

You can see in this behaviour in this simple example:

import pandas as pd
from numpyrandom import randn
from random import shuffle
rng = pddate_range(start='2011-01-01'  end='2011-12-31')
rng2 = list(rng)
shuffle(rng2) # not in order
rng3 = list(rng)
del rng3[20] # in order  but no freq

ts = pdSeries(randn(len(rng))  index=rng)
ts2 = pdSeries(randn(len(rng))  index=rng2)
ts3 = pdSeries(randn(len(rng)-1)  index=rng3)

tsindex
<class 'pandastseriesindexDatetimeIndex'>
[2011-01-01 00:00:00    2011-12-31 00:00:00]
Length: 365  Freq: D  Timezone: None

ts['2011-01-01']
# -11454418070543406

ts2index
<class 'pandastseriesindexDatetimeIndex'>
[2011-04-16 00:00:00    2011-03-10 00:00:00]
Length: 365  Freq: None  Timezone: None

ts2['2011-01-01']
#error which you describe
TimeSeriesError: Partial indexing only valid for ordered time series

ts3index
<class 'pandastseriesindexDatetimeIndex'>
[2011-01-01 00:00:00    2011-12-31 00:00:00]
Length: 364  Freq: None  Timezone: None

ts3['2011-01-01']
17631554507355987


rng4 = pddate_range(start='2011-01-01'  end='2011-01-31'  freq='H')
ts4 = pdSeries(randn(len(rng4))  index=rng4)

ts4['2011-01-01'] == ts4[0]
# True # it picks the first element with that date


I don't think this is a bug  nevertheless I posted it as an issue on github",1240268.0,,1240268.0,,2012-12-06 12:42:38,2012-12-06 12:42:38,1.0
14179212,2,13737992,2013-01-06 04:13:00,0,"While the pandas tutorial was instructive  I think the original question posed deserves a direct answer I ran into the same problem converting Yahoo chart info to a DataFrame that could be sliced  etc I found that the only thing that was required was:

import pandas as pd
import datetime as dt

def dt_parser(date): 
return dtdatetimestrptime(date  '%Y-%m-%d') + dttimedelta(hours=16)

url = 'http://ichartfinanceyahoocom/tablecsvs=SPY&d=12&e=4&f=2012&g=d&a=01&b=01&c=2001&ignore=csv'  
df = pdread_csv(url  index_col=0  parse_dates=True  date_parser=dt_parser)
dfsort_index(inplace=True)
s = df['Close']
s['2012-12-04']     # now should work


The ""trick"" was to include my own date_parser I'm guessing that there is some better way to do this within read_csv  but this at least produced a DataFrame that was indexed and could be sliced",428007.0,,,,,2013-01-06 04:13:00,
14023269,2,14023037,2012-12-24 15:44:23,0,"As indicated by your matrix data col1 cannot be an index because  as you said  it ""does not result in an unique index""

I think your best best is:

grouped = dfgroupby('col3')
pandasmerge(groupedfirst()  groupedlast()  on=['col1' 'col2'])
",1876739.0,,,,,2012-12-24 15:44:23,
13768161,2,13767950,2012-12-07 17:35:02,2,"You are looking for an outer join  here is a simple example:

from pandas import DataFrame
df1 = DataFrame([[1]]  columns=['a'])
df2 = DataFrame([[3] [4]]  columns=['b'])

In [4]: df1
Out[4]: 
   a
0  1

In [5]: df2
Out[5]: 
   b
0  3
1  4

In [6]: df1join(df2)
Out[6]: 
   a  b
0  1  3

In [7]: df1join(df2  how='outer')
Out[7]: 
    a  b
0   1  3
1 NaN  4
",1240268.0,,,,,2012-12-07 17:35:02,1.0
14075432,2,14075337,2012-12-28 20:34:05,1,"It's because i is not yet defined  just like the error message says

In this line:

for i in idfindex[i][0]:


You are telling the Python interpreter to iterate over all the values yielded by the list returning from the expression idfindex[i][0] but you have not yet defined what i is (although you are attempting to set each item in the list to the variable i as well)

The way the Python for  in  loop works is that it takes the right most component and asks for the next item from the iterator  It then assigns the value yielded by the call to the variable name provided on the left hand side",269581.0,,,,,2012-12-28 20:34:05,
14075467,2,14075337,2012-12-28 20:36:10,2,"Try something like this:

for i in range(len(idfindex)):
  value = idfindex[i][0]


Same thing for the iteration with the j index variable As has been pointed  you can't reference the iteration index in the expression to be iterated  and besides you need to perform a very specific iteration (traversing over a column in a matrix)  and Python's default iterators won't work ""out of the box"" for this  so a custom index handling is needed here",201359.0,,201359.0,,2012-12-28 20:43:38,2012-12-28 20:43:38,0.0
14076983,2,14075337,2012-12-28 23:17:11,2,"This is what I think you're trying to accomplish based on your edit: for every date in your CSV file  group the date along with a list of prices for each item with a signal of ""S""

You didn't include any sample data in your question  so I made a test one that I hope matches the format you described:

12/28/2012 1:30 1000 ""foo"" ""S""
12/28/2012 2:15 1100 ""bar"" ""N""
12/28/2012 3:00 1200 ""baz"" ""S""
12/28/2012 4:45 1300 ""fibble"" ""N""
12/28/2012 5:30 1400 ""whatsit"" ""S""
12/28/2012 6:15 1500 ""bobs"" ""N""
12/28/2012 7:00 1600 ""widgets"" ""S""
12/28/2012 7:45 1700 ""weevils"" ""N""
12/28/2012 8:30 1800 ""badger"" ""S""
12/28/2012 9:15 1900 ""moose"" ""S""
11/29/2012 1:30 1000 ""foo"" ""N""
11/29/2012 2:15 1100 ""bar"" ""N""
11/29/2012 3:00 1200 ""baz"" ""S""
11/29/2012 4:45 1300 ""fibble"" ""N""
11/29/2012 5:30 1400 ""whatsit"" ""N""
11/29/2012 6:15 1500 ""bobs"" ""N""
11/29/2012 7:00 1600 ""widgets"" ""S""
11/29/2012 7:45 1700 ""weevils"" ""N""
11/29/2012 8:30 1800 ""badger"" ""N""
11/29/2012 9:15 1900 ""moose"" ""N""
12/29/2012 1:30 1000 ""foo"" ""N""
12/29/2012 2:15 1100 ""bar"" ""N""
12/29/2012 3:00 1200 ""baz"" ""S""
12/29/2012 4:45 1300 ""fibble"" ""N""
12/29/2012 5:30 1400 ""whatsit"" ""N""
12/29/2012 6:15 1500 ""bobs"" ""N""
12/29/2012 7:00 1600 ""widgets"" ""S""
12/29/2012 7:45 1700 ""weevils"" ""N""
12/29/2012 8:30 1800 ""badger"" ""N""
12/29/2012 9:15 1900 ""moose"" ""N""
8/9/2008 1:30 1000 ""foo"" ""N""
8/9/2008 2:15 1100 ""bar"" ""N""
8/9/2008 3:00 1200 ""baz"" ""S""
8/9/2008 4:45 1300 ""fibble"" ""N""
8/9/2008 5:30 1400 ""whatsit"" ""N""
8/9/2008 6:15 1500 ""bobs"" ""N""
8/9/2008 7:00 1600 ""widgets"" ""S""
8/9/2008 7:45 1700 ""weevils"" ""N""
8/9/2008 8:30 1800 ""badger"" ""N""
8/9/2008 9:15 1900 ""moose"" ""N""


And here's a method using Python 27 and built-in libraries to group it in the way it sounds like you want:

import csv
import itertools
import time
from collections import OrderedDict

with open(""samplecsv""  ""r"") as file:
    reader = csvDictReader(file 
                            fieldnames=[""date""  ""time""  ""price""  ""mag""  ""signal""])

    # Reduce the size of the data set by filtering out the non-""S"" rows
    filterFunc = lambda row: row[""signal""] == ""S""
    filteredData = itertoolsifilter(filterFunc  reader)

    # Sort by date so we can use the groupby function
    dateKeyFunc = lambda row: timestrptime(row[""date""]  r""%m/%d/%Y"")
    sortedData = sorted(filteredData  key=dateKeyFunc)

    # Group by date: create a new dictionary of date to a list of prices
    datePrices = OrderedDict((date  [row[""price""] for row in rows])
                             for date  rows
                             in itertoolsgroupby(sortedData  dateKeyFunc))

for date  prices in datePricesiteritems():
    print ""{0}: {1}""format(timestrftime(r""%m/%d/%Y""  date) 
                            ""  ""join(str(price) for price in prices))

>>> 08/09/2008: 1200  1600
>>> 11/29/2012: 1200  1600
>>> 12/28/2012: 1000  1200  1400  1600  1800  1900
>>> 12/29/2012: 1200  1600


The type conversions are up to you  since you may be using other libraries to do your CSV reading  but that should hopefully get you started -- and take careful note of @DSM's comment about import *",1864859.0,,1864859.0,,2012-12-29 23:54:11,2012-12-29 23:54:11,5.0
13597189,2,13596419,2012-11-28 03:13:49,1,"Perhaps try npwhere:

import numpy as np
df['year'] = npwhere(source_years source_years df['year'])
",190597.0,,,,,2012-11-28 03:13:49,1.0
13637005,2,13596419,2012-11-30 00:01:17,2,"In pandas >= 0100 try

df['year'] = df['year']where(source_years!=0 df['year'])


and see:

http://pandaspydataorg/pandas-docs/dev/indexinghtml#where-and-masking

As noted in the comments  this DOES use npwhere under the hood - the difference is that pandas aligns the series with the output (so for example you can only do a partial update)",644898.0,,-1.0,,2012-11-30 13:29:56,2012-11-30 13:29:56,2.0
13953332,2,13953276,2012-12-19 13:21:07,4,"Try using the read_csv() function Ensure that your csv includes a header or pass header=None for correct parsing parse_dates=[[0 1 2]] will facilitate the desired dattime parsing

In [4]: pandasioparsersread_csv(""inputcsv""  parse_dates=[[0 1 2]]  header=None)
Out[4]: 
              X0_X1_X2    X3    X4   X5
0  2012-10-03 00:00:00  AAPL   BUY  200
1  2012-12-05 00:00:00  AAPL  SELL  200
",1331457.0,,1331457.0,,2012-12-19 13:51:04,2012-12-19 13:51:04,7.0
14200289,2,14199718,2013-01-07 16:47:06,2,"In [1]: df = DataFrame({'ID':[1 1 1 1 1 2 2] 'TS':[10 20 25 30 50 10 40] 'Stat
us':['G' 'G' 'B' 'B' 'B' 'B' 'B']}  columns=['ID' 'TS' 'Status'])

In [2]: f = lambda x: xdiff()sum()

In [3]: df['diff'] = df[dfStatus=='B']groupby('ID')['TS']transform(f)

In [4]: df
Out[4]:
   ID  TS Status  diff
0   1  10      G   NaN
1   1  20      G   NaN
2   1  25      B    25
3   1  30      B    25
4   1  50      B    25
5   2  10      B    30
6   2  40      B    30


Explanation:
Subset the dataframe to only those records with the desired Status  Groupby the ID and apply the lambda function diff()sum() to each group  Use transform instead of apply because transform returns an indexed series which you can use to assign to a new column 'diff'

EDIT: New response to account for expanded question scope

In [1]: df
Out[1]:
   ID   TS Status
0   1   10      G
1   1   20      G
2   1   25      B
3   1   30      B
4   1   50      B
5   1  600      G
6   2   40      G

In [2]: df['shift'] = -df['TS']diff(-1)

In [3]: df['diff'] = df[dfStatus=='B']groupby('ID')['shift']transform('sum')
In [4]: df
Out[4]:
   ID   TS Status  shift  diff
0   1   10      G     10   NaN
1   1   20      G      5   NaN
2   1   25      B      5   575
3   1   30      B     20   575
4   1   50      B    550   575
5   1  600      G   -560   NaN
6   2   40      G    NaN   NaN
",919872.0,,919872.0,,2013-01-07 19:52:03,2013-01-07 19:52:03,5.0
14207404,2,14199718,2013-01-08 02:38:44,1,"Here's a solution to separately aggregate each contiguous block of bad status (part 2 of your question?)

In [5]: df = pandasDataFrame({'ID':[1 1 1 1 1 1 1 1 2 2 2] 
                               'TS':[10 20 25 30 50 600 650 670 40 50 60] 
                               'Status':['G' 'G' 'B' 'B' 'B' 'G' 'B' 'B' 'G' 'B' 'B']
                               } 
                               columns=['ID' 'TS' 'Status'])

In [6]: grp = dfgroupby('ID')

In [7]: def status_change(df):
   :         return (dfStatusshift(1) != dfStatus)astype(int)
   : 

In [8]: df['BlockId'] = grpapply(lambda df: status_change(df)cumsum())

In [9]: df['Duration'] = grpTSdiff()shift(-1)

In [10]: df
Out[10]: 
    ID   TS Status  BlockId  Duration
0    1   10      G        1        10
1    1   20      G        1         5
2    1   25      B        2         5
3    1   30      B        2        20
4    1   50      B        2       550
5    1  600      G        3        50
6    1  650      B        4        20
7    1  670      B        4       NaN
8    2   40      G        1        10
9    2   50      B        2        10
10   2   60      B        2       NaN

In [11]: df[dfStatus == 'B']groupby(['ID'  'BlockId'])Durationsum()
Out[11]: 
ID  BlockId
1   2          575
    4           20
2   2           10
Name: Duration
",243434.0,,243434.0,,2013-01-08 02:47:17,2013-01-08 02:47:17,
13635082,2,13628860,2012-11-29 21:29:18,0,"It looks like this is not a bug and the subtle difference is due to the usage of the reindex_like() method  The call to reindex_like() inserts some NaN data into the series so the dtype of that series changes from bool to object

>>> series = pandasSeries([1  2  3])
>>> x = pandasSeries([True])
>>> xdtype
dtype('bool')
>>> x = pandasSeries([True])reindex_like(series)
>>> xdtype
dtype('object')


I posted an issue about this anomaly on the Pandas github page

The full explanation/discussion is here  It looks like this behavior could potentially change in the future so watch the issue for more on-going details ",1108031.0,,,,,2012-11-29 21:29:18,
13984352,2,13982250,2012-12-21 04:40:14,1,"First create a new column to distinguish first_babies:

live['first_babies'] = live['birthord']lambda(x: 'first_babies' if x==1 else 'others')


You can unstack the groupby:

grouped = livegroupby(by=['prglength'  'first_babies'])size()
unstacked_count = groupedsize()unstack()


Now you can plot a stacked bar-plot directly:

unstacked_countplot(kind='bar'  stacked=True)
",1240268.0,,1240268.0,,2012-12-21 05:34:44,2012-12-21 05:34:44,4.0
14225838,2,14225676,2013-01-08 23:27:40,0,"You should be using pandas own ExcelWriter class:

from pandasioparsers import ExcelWriter


Then the save_xls function works as expected:

def save_xls(list_dfs  xls_path):
    writer = ExcelWriter(xls_path)
    for n  df in enumerate(list_dfs):
        dfto_excel(writer 'sheet%s' % n)
    writersave()
",1240268.0,,1240268.0,,2013-01-08 23:40:36,2013-01-08 23:40:36,5.0
14430418,2,14430263,2013-01-20 23:10:13,2,"How about just call myClass__init__ inside myClassAv__init__:

def __init__(self  frameType='All'):
    myClass__init__(self  frameType)
    def addingCol(cObject): 
        
    addingCol(selfcObject)


For concreteness 

import pandas as pd
import pylab as pl
import numpy as np


class myClass(object):
    def __init__(self  frameType='All'):
        def method1():
            myFrame = pdDataFrame(
                {'c1': [1  2  3]  'c2': [4  5  6]  'c3': [7  8  9]})
            return myFrame

        def method2():
            myFrame = pdDataFrame(
                {'c1': [1  2  3]  'c2': [4  5  6]  'c3': [7  8  9]})
            return myFrame

        def makingChoice(self):
            if selfframeType == 'All':
                variable = method1() + method2()
            elif selfframeType == 'a':
                variable = method1()
            elif selfframeType == 'b':
                variable = method2()
            else:
                variable = pdDataFrame(
                    {'c1': [0  0  0]  'c2': [0  0  0]  'c3': [0  0  0]})
            # print 'FROM __init__ : %s' % variable
            return variable
        selfframeType = frameType
        selfcObject = makingChoice(self)  # object created by the class

    def __str__(self):
        return str(selfcObject)

    def plotData(self):
        selffig1 = plplot(selfcObject['c1']  selfcObject['c2'])
        selffig2 = plplot(selfcObject['c1']  selfcObject['c3'])
        plshow()


class myClassAv(myClass):
    def __init__(self  frameType='All'):
        myClass__init__(self  frameType)

        def addingCol(cObject):
            print 'CURRENT cObject \n%s' % cObject  # the object is visible
            cObject['average'] = cObjectmean(axis=1)
            # creating new column works
            print 'THIS WORKS IN GENERAL\n%s' % str(cObject['average'])
            return cObject

        addingCol(selfcObject)

    def plotData(self):
        # Function to add new plot to already existing plots
        selffig3 = plplot(selfcObject['c1']  selfcObject['average'])

if __name__ == '__main__':
    myObject1 = myClass()
    print 'myObject1 =\n%s' % myObject1
    myObject1plotData()
    myObject2 = myClass('a')
    print 'myObject2 =\n%s' % myObject2
    myObject3 = myClass('b')
    print 'myObject3 =\n%s' % myObject3
    myObject4 = myClass('c')
    print 'myObject4 =\n%s' % myObject4

    myObject5 = myClassAv('a')
    print 'myObject5 =\n%s' % myObject5
    myObject5plotData()


By the way  instead of 

selfcObject['avarage'] = (selfcObject['c1']+selfcObject['c2']+selfcObject['c3'])/3


you can use mean(axis = 1):

selfcObject['average'] = selfcObjectmean(axis=1)
",190597.0,,190597.0,,2013-01-20 23:25:42,2013-01-20 23:25:42,1.0
14024968,2,14023037,2012-12-24 19:17:12,0,"What do you want col1 and col2 to look like after you pivot?  Your example output shows A1 and B1 for the final row yet neither of those values are associated with the 18 and 22 I have a couple of options:

In [234]: tmp = DataFrame(
    {'id':[1 2 3 4 5 6]  
     'col1':['A1' 'A1' 'A1' 'A1' 'A2' 'A2'] 
     'col2':['B1' 'B1' 'B2' 'B2' 'B1' 'B2'] 
     'col3':['before' 'after' 'before' 'after' 'before' 'after'] 
     'value':[20 13 11 21 18 22]} 
    columns=['id' 'col1' 'col2' 'col3' 'value'])


Option 1:

In [236]: pivoted = pdpivot_table(tmp  values='value' 
                                        rows=['col1' 'col2'] 
                                        cols=['col3'])
In [237]: pivoted
Out[237]:
col3       after  before
col1 col2
A1   B1       13      20
     B2       21      11
A2   B1      NaN      18
     B2       22     NaN


This doesn't sound like the kind of behavior you want

Option 2:

In [238]: pivoted = pivotedfillna(method='bfill')dropna()
Out[238]:
col3       after  before
col1 col2
A1   B1       13      20
     B2       21      11
A2   B1       22      18

In [245]: pivotedreset_index()
Out[245]:
col3 col1 col2  after  before
0      A1   B1     13      20
1      A1   B2     21      11
2      A2   B1     22      18


This gets you pretty close  Again  I'm not sure how you want col1 and col2 to behave  but this has the right values in the before and after columns",919872.0,,,,,2012-12-24 19:17:12,1.0
14298443,2,14297959,2013-01-12 22:09:51,1,"I don't know if its a bug or a feature but you have to specify names for all columns present even if you specify just a subset of columns to usecols

df = pdread_csv(StringIO(raw) 
                 parse_dates=True 
                 header=None 
                 index_col=0 
                 usecols=[0 1 2 3 4 5] 
                 names='0 1 2 3 4 5 6 7 8 9'split())


which gives

                1      2      3      4        5
0                                              
2000-01-03  1285  1311  1274  1311   976500
2000-01-04  1354  1360  1256  1333  2493000
2000-01-05  1268  1334  1237  1268  1680000


I figured this by trying the edge case where you specify a full list to both names and usecols and tried then to gradually reduce and see what happens

What is weired is the error message you get when you try for instance usecols=[1 2 3] and names=['1' '2' '3']:

ValueError: Passed header names mismatches usecols


which does not make sense",733291.0,,,,,2013-01-12 22:09:51,
8915588,2,8914992,2012-01-18 18:45:57,6,"If you're using pandas >= 070 (currently only available in the GitHub repository  though I'll be making a release imminently!)  you can concatenate your dict of DataFrames:

http://pandassourceforgenet/merginghtml#more-concatenating-with-group-keys

In [6]: data
Out[6]: 
{'file1csv':    A       B     
0  10914 -13538
1  05775 -02392
2 -02157 -02253
3 -24924  10896
4  06910  08992
5 -16196  03009
6 -15500  01360
7 -02156  04530
8  17018  11169
9 -17378 -03373 
 'file2csv':    A       B      
0 -04948 -015551
1  06987  085838
2 -13949  025995
3  15314  125364
4  18582  009912
5 -11717 -021276
6 -02603 -178605
7 -33247  126865
8  07741 -225362
9 -06956  108774}


In [10]: cdf = concat(data  axis=1)

In [11]: cdf
O    ut[11]: 
   file1csv          file2csv         
   A          B       A          B      
0  10914    -13538 -04948    -015551
1  05775    -02392  06987     085838
2 -02157    -02253 -13949     025995
3 -24924     10896  15314     125364
4      06910     08992  18582     009912
5 -16196     03009 -11717    -021276
6 -15500     01360 -02603    -178605
7 -02156     04530 -33247     126865
8  17018     11169  07741    -225362
9 -17378    -03373 -06956     108774


Then if you wish to switch the order of the column indexes  you can do:

In [14]: cdfswaplevel(0  1  axis=1)
Out[14]: 
   A          B          A          B        
   file1csv  file1csv  file2csv  file2csv
0  10914    -13538    -04948    -015551  
1  05775    -02392     06987     085838  
2 -02157    -02253    -13949     025995  
3 -24924     10896     15314     125364  
4  06910     08992     18582     009912  
5 -16196     03009    -11717    -021276  
6 -15500     01360    -02603    -178605  
7 -02156     04530    -33247     126865  
8  17018     11169     07741    -225362  
9 -17378    -03373    -06956     108774  


Alternately  and perhaps a bit straightforwardly  you can use a Panel:

In [16]: p = Panel(data)

In [17]: p
Out[17]: 
<class 'pandascorepanelPanel'>
Dimensions: 2 (items) x 10 (major) x 2 (minor)
Items: file1csv to file2csv
Major axis: 0 to 9
Minor axis: A to B

In [18]: p = pswapaxes(0  2)

In [19]: p
Out[19]: 
<class 'pandascorepanelPanel'>
Dimensions: 2 (items) x 10 (major) x 2 (minor)
Items: A to B
Major axis: 0 to 9
Minor axis: file1csv to file2csv

In [20]: p['A']
Out[20]: 
   file1csv  file2csv
0  10914    -04948   
1  05775     06987   
2 -02157    -13949   
3 -24924     15314   
4  06910     18582   
5 -16196    -11717   
6 -15500    -02603   
7 -02156    -33247   
8  17018     07741   
9 -17378    -06956   
",776560.0,,,,,2012-01-18 18:45:57,0.0
13689607,2,13689512,2012-12-03 18:41:47,4,"Pandas implements diff like so: 

In [3]: s = pdSeries(nparange(10))

In [4]: sdiff()
Out[4]:
0   NaN
1     1
2     1
3     1
4     1
5     1
6     1
7     1
8     1
9     1


Using npdiff directly: 

In [7]: npdiff(svalues)
Out[7]: array([1  1  1  1  1  1  1  1  1])

In [8]: npdiff(nparray(s))
Out[8]: array([1  1  1  1  1  1  1  1  1])


So why doesn't npdiff(s) work? Because np is taking npasanyarray() of the series before finding the diff Like so: 

In [25]: a = npasanyarray(s)

In [26]: a 
Out[26]:
0    0
1    1
2    2
3    3
4    4
5    5
6    6
7    7
8    8
9    9

In [27]: npdiff(a)
Out[27]:
0   NaN
1     0
2     0
3     0
4     0
5     0
6     0
7     0
8     0
9   NaN
",484596.0,,484596.0,,2012-12-03 18:57:49,2012-12-03 18:57:49,
14016595,2,14012235,2012-12-24 03:03:07,0,If you take the first and last duplicate value of each year and shift the data in-between by an hour  that should be the easiest way of correcting the issue You'll obviously have to take into account that the first data points start in daylight savings,1410646.0,,,,,2012-12-24 03:03:07,1.0
14292553,2,14287903,2013-01-12 10:46:56,1,"I'm not sure how Wes did it in the video  but when using plot(dfindex  dfcolumn) and do that several times with different columns  then the graphs have different colors 

(I'm using the plot command from ipython --pylab) for this )",680232.0,,,,,2013-01-12 10:46:56,
14296342,2,14287903,2013-01-12 18:08:05,1,"You just need to pass a couple of kwargs to the call to plot

import numpy as np
import pandas as pd
import matplotlibpyplot

fig  ax = pltsubplots()
pdDataFrame(nprandomrandn(100  2))cumsum()plot(ax=ax)
",1552748.0,,1552748.0,,2013-01-13 01:27:41,2013-01-13 01:27:41,4.0
14443106,2,14432059,2013-01-21 16:34:02,2,No  Pandas is pretty well tied to CPython Like you said  your best bet is to do the analysis in CPython with Pandas and export the result to CSV,129592.0,,,,,2013-01-21 16:34:02,
14632215,2,14631139,2013-01-31 18:02:15,1,"Perhaps it makes more sense to use rolling_sum:

pdrolling_sum(ts  window=1  freq='1ms')
",1240268.0,,,,,2013-01-31 18:02:15,1.0
14638497,2,14631139,2013-02-01 01:53:05,0,"How about something like this:

Create an offset for 1 ms:

In [1]: ms = tseriesoffsetsMilli()


Create a series of index positions the same length as your timeseries:

In [2]: s = Series(range(len(ts)))


Apply a lambda function that indexes the current time from the ts series  The function returns the sum of all ts entries between x - ms and x

In [3]: sapply(lambda x: tsbetween_time(start_time=tsindex[x]-ms  end_time=tsindex[x])sum())

In [4]: tshead()
Out[4]:
2013-02-01 09:00:00000558    348
2013-02-01 09:00:00000647    361
2013-02-01 09:00:00000726    312
2013-02-01 09:00:00001012    550
2013-02-01 09:00:00002208    758


Results of the above function:

0     348
1     709
2    1021
3    1571
4     758
",919872.0,,,,,2013-02-01 01:53:05,1.0
10149110,2,10145025,2012-04-13 22:27:10,2,"Yes  use the map Series method:

In [16]: seriesmap(freq['count'])
Out[16]: 
0     012
1     006
2     020
3     011
4     002
5     013
6     014
7     011
8     012
9     016
10    020
<snip>


you can then do:

In [22]: series[seriesmap(freq['count']) > 016]
Out[22]: 
2     4
10    4
11    4
22    4
27    4
31    4
34    4
56    4
64    4
71    4
73    4
76    4
77    4
79    4
80    4
86    4
88    4
89    4
91    4
99    4
",776560.0,,,,,2012-04-13 22:27:10,1.0
14087161,2,14075337,2012-12-30 00:18:13,2,"Using @Max Fellows' handy example data  we can have a look at it in pandas  [BTW  you should always try to provide a short  self-contained  correct example (see here for more details)  so that the people trying to help you don't have to spend time coming up with one]

First  import pandas as pd  Then:

In [23]: df = pdread_csv(""samplecsv""  names=""date time price mag signal""split())

In [24]: dfset_index([""date""  ""time""]  inplace=True)


which gives me

In [25]: df
Out[25]: 
                 price      mag signal
date       time                       
12/28/2012 1:30     10      foo      S
           2:15     11      bar      N
           3:00     12      baz      S
           4:45     13   fibble      N
           5:30     14  whatsit      S
           6:15     15     bobs      N
           7:00     16  widgets      S
           7:45     17  weevils      N
           8:30     18   badger      S
           9:15     19    moose      S
11/29/2012 1:30     10      foo      N
           2:15     11      bar      N
           3:00     12      baz      S
           4:45     13   fibble      N
           5:30     14  whatsit      N
           6:15     15     bobs      N
           7:00     16  widgets      S
           7:45     17  weevils      N
           8:30     18   badger      N
           9:15     19    moose      N
[etc]


We can see which rows have a signal of S easily:

In [26]: df[""signal""] == ""S""
Out[26]: 
date        time
12/28/2012  1:30     True
            2:15    False
            3:00     True
            4:45    False
            5:30     True
            6:15    False
[etc]


and we can select using this too:

In [27]: df[""price""][df[""signal""] == ""S""]
Out[27]: 
date        time
12/28/2012  1:30    10
            3:00    12
            5:30    14
            7:00    16
            8:30    18
            9:15    19
11/29/2012  3:00    12
            7:00    16
12/29/2012  3:00    12
            7:00    16
8/9/2008    3:00    12
            7:00    16
Name: price


This is a DataFrame with every date  time  and price where there's an S  And if you simply want a list:

In [28]: list(df[""price""][df[""signal""] == ""S""])
Out[28]: [100  120  140  160  180  190  120  160  120  160  120  160]


Update:

v=[df[""signal""]==""S""] makes v a Python list containing a Series  That's not what you want  df[""price""][[v and (u and t)]] doesn't make much sense to me either --: v and u are mutually exclusive  so if you and them together  you'll get nothing  For these logical vector ops you can use & and | instead of and and or  Using the reference data again:

In [85]: import pandas as pd

In [86]: df = pdread_csv(""samplecsv""  names=""date time price mag signal""split())

In [87]: v=df[""signal""]==""S""

In [88]: t=df[""time""]==""4:45""

In [89]: u=df[""signal""]!=""S""

In [90]: df[t]
Out[90]: 
          date  time  price     mag signal
3   12/28/2012  4:45     13  fibble      N
13  11/29/2012  4:45     13  fibble      N
23  12/29/2012  4:45     13  fibble      N
33    8/9/2008  4:45     13  fibble      N

In [91]: df[""price""][t]
Out[91]: 
3     13
13    13
23    13
33    13
Name: price

In [92]: df[""price""][v | (u & t)]
Out[92]: 
0     10
2     12
3     13
4     14
6     16
8     18
9     19
12    12
13    13
16    16
22    12
23    13
26    16
32    12
33    13
36    16
Name: price


[Note: this question has now become too long and meandering  I suggest spending some time working through the examples in the pandas documentation at the console to get a feel for it]",487339.0,,487339.0,,2012-12-30 18:44:58,2012-12-30 18:44:58,2.0
14342825,2,14341805,2013-01-15 17:00:28,2,"Reset the indices and then merge on multiple (column-)keys:

dfLeftreset_index(inplace=True)
dfRightreset_index(inplace=True)
dfMerged = pdmerge(dfLeft  dfRight 
              left_on=['date'  'cusip'] 
              right_on=['date'  'idc__id'] 
              how='inner')


You can then reset 'date' as an index:

dfMergedset_index('date'  inplace=True)


Here's an example:

raw1 = '''
2012-01-03    XXXX      45
2012-01-03    YYYY      62
2012-01-04    XXXX      47
2012-01-04    YYYY      61
'''

raw2 = '''
2012-01-03    XYXX      45
2012-01-03    YYYY      62
2012-01-04    XXXX      -47
2012-01-05    YYYY      61
'''

import pandas as pd
from StringIO import StringIO


df1 = pdread_table(StringIO(raw1)  header=None 
                    delim_whitespace=True  parse_dates=[0]  skiprows=1)
df2 = pdread_table(StringIO(raw2)  header=None 
                    delim_whitespace=True  parse_dates=[0]  skiprows=1)

df1columns = ['date'  'cusip'  'factorL']
df2columns = ['date'  'idc__id'  'factorL']

print pdmerge(df1  df2 
         left_on=['date'  'cusip'] 
         right_on=['date'  'idc__id'] 
         how='inner')


which gives

                  date cusip  factorL_x idc__id  factorL_y
0  2012-01-03 00:00:00  YYYY        62    YYYY         62
1  2012-01-04 00:00:00  XXXX        47    XXXX        -47
",733291.0,,,,,2013-01-15 17:00:28,1.0
14342919,2,14341805,2013-01-15 17:06:12,3,"You could append 'cuspin' and 'idc_id' as a indices to your DataFrames before you join (here's how it would work on the first couple of rows):

In [10]: dfL
Out[10]: 
           cuspin  factorL
date                      
2012-01-03   XXXX      45
2012-01-03   YYYY      62

In [11]: dfL1 = dfLeftset_index('cuspin'  append=True)

In [12]: dfR1 = dfRightset_index('idc_id'  append=True)

In [13]: dfL1
Out[13]: 
                   factorL
date       cuspin         
2012-01-03 XXXX        45
           YYYY        62

In [14]: dfL1join(dfR1)
Out[14]: 
                   factorL  factorR
date       cuspin                  
2012-01-03 XXXX        45        5
           YYYY        62        6
",1240268.0,,,,,2013-01-15 17:06:12,
14450336,2,14450020,2013-01-22 01:50:48,4,"You have some strings in your DenFrameHper and DenFrameVper

You can see this by checking the dtype or type of each element:

In [11]: dfHperdtype
Out[11]: dtype('object')


Means that the numpy array could contain various types  we can see what these types are:

In [12]: DenFrameHpermap(type)unique()
Out[12]: [<type 'float'> <type 'str'>]


And you could inspect which entries are strings:

DenFrame[DenFrameHpermap(type) == str]


Perhaps it makes sense to only include those which are floats:

DenFrame_floats = DenFrame[(DenFrameHpermap(type) == float) & 
                           (DenFrameVpermap(type) == float)]


or you could (if it's possible) convert them to floats:

DenFrameHper = DenFrameHperapply(float)
",1240268.0,,1240268.0,,2013-01-22 02:11:22,2013-01-22 02:11:22,1.0
9339501,2,9339184,2012-02-18 07:36:49,3,">>> dfpivot('A'  'B'  'C')
  B  11  12  13
A              
10   a   b   c 
20   d   e   f 


Where df is:

>>> df = DataFrame(dict(A=[10]*3+[20]*3  B=range(11  14)*2  C=list('abcdef')))
>>> df
   A   B   C
0  10  11  a
1  10  12  b
2  10  13  c
3  20  11  d
4  20  12  e
5  20  13  f


See Reshaping and Pivot Tables",4279.0,,,,,2012-02-18 07:36:49,
14408712,2,14408634,2013-01-18 22:29:12,2,"Don't code your own Newton-Raphson method in Python You'll get better performance using one of the root finders in scipyoptimize such as brentq or newton
(Presumably  if you have pandas  you'd also install scipy)

Back of the envelope calculation:

Making 600M calls to brentq should be manageable on standard hardware:

import scipyoptimize as optimize
def f(x):
    return x**2 - 2

In [28]: %timeit optimizebrentq(f  0  10)
100000 loops  best of 3: 486 us per loop


So if each call to optimizebrentq takes 486 microseconds  600M calls will take about 486 * 600  ~ 3000 seconds ~ 1 hour

newton may be slower  but still manageable:

def f(x):
    return x**2 - 2
def fprime(x):
    return 2*x

In [40]: %timeit optimizenewton(f  10  fprime)
100000 loops  best of 3: 822 us per loop
",190597.0,,190597.0,,2013-01-18 22:59:08,2013-01-18 22:59:08,3.0
14531449,2,14515239,2013-01-25 22:36:43,2,"I suspect you are trying to set the date as the index too early My suggestion would be to first set_index as date and company name  then you can unstack the company name and resample

Something like this:

In [11]: df1
Out[11]: 
  ticker_symbol  monthly_return                date
0          AAPL           0112 1992-02-28 00:00:00
1            GS           0130 1981-11-30 00:00:00
2            GS          -0230 1981-12-22 00:00:00

df2 = df2set_index(['date' 'ticker_symbol'])
df3 = df2unstack(level=1)
df4 = dfresample('M')

In [14]: df2
Out[14]: 
                          monthly_return
date       ticker_symbol                
1992-02-28 AAPL                    0112
1981-11-30 GS                      0130
1981-12-22 GS                     -0230

In [15]: df3
Out[15]: 
               monthly_return      
ticker_symbol            AAPL    GS
date                               
1981-11-30                NaN  013
1981-12-22                NaN -023
1992-02-28              0112   NaN

In [16]: df4
Out[16]: 
<class 'pandascoreframeDataFrame'>
DatetimeIndex: 124 entries  1981-11-30 00:00:00 to 1992-02-29 00:00:00
Freq: M
Data columns:
(monthly_return  AAPL)    1  non-null values
(monthly_return  GS)      2  non-null values
dtypes: float64(2)
",1240268.0,,,,,2013-01-25 22:36:43,6.0
9712271,2,9695668,2012-03-15 00:13:16,1,Simply put: no  not yet More work (read: more active developers) is needed on this particular area If you could post how you're using read_csv it might help I suspect that the whitespace between the bars may be the problem,776560.0,,,,,2012-03-15 00:13:16,1.0
10666301,2,10665889,2012-05-19 15:02:43,1,"You can slice along the columns of a DataFrame by referring to the names of each column in a list  like so:

data = pandasDataFrame(nprandomrand(10 5)  columns = list('abcde'))
data_ab = data[list('ab')]
data_cde = data[list('cde')]
",126751.0,,,,,2012-05-19 15:02:43,4.0
10677896,2,10665889,2012-05-20 22:51:30,5,"The DataFrameix index is what you want to be accessing It's a little confusing (I agree that Pandas indexing is perplexing at times!)  but the following seems to do what you want:

>>> df = DataFrame(nprandomrand(4 5)  columns = list('abcde'))
>>> dfix[: 'b':]
      b         c         d         e
0  0418762  0042369  0869203  0972314
1  0991058  0510228  0594784  0534366
2  0407472  0259811  0396664  0894202
3  0726168  0139531  0324932  0906575


where ix[row slice  column slice] is what is being interpreted More on Pandas indexing here: http://pandaspydataorg/pandas-docs/stable/indexinghtml#indexing-advanced",1284636.0,,,,,2012-05-20 22:51:30,1.0
11565749,2,11515290,2012-07-19 16:58:35,0,"The Hour DateOffset is ""every hour"" and not ""every hour on the hour""
You can try subclassing Hour to override onOffset
You can also checkout the TimeSeriesbetween_time method",1306530.0,,,,,2012-07-19 16:58:35,
11982843,2,11976503,2012-08-16 07:53:01,3,"In [5]: areset_index()merge(b  how=""left"")set_index('index')
Out[5]:
       col1  to_merge_on  col2
index
a         1            1     1
b         2            3     2
c         3            4   NaN
",1548051.0,,,,,2012-08-16 07:53:01,3.0
12117333,2,12101113,2012-08-24 22:15:41,2,"I think your best bet is to read the data in as a record array first using numpy

# what you described:
In [15]: import numpy as np
In [16]: import pandas
In [17]: x = pandasread_csv('weirdcsv')

In [19]: xdtypes
Out[19]: 
int_field            int64
floatlike_field    float64  # what you don't want?
str_field           object

In [20]: datatypes = [('int_field' 'i4') ('floatlike' 'S10') ('strfield' 'S10')]

In [21]: y_np = nploadtxt('weirdcsv'  dtype=datatypes  delimiter=' '  skiprows=1)

In [22]: y_np
Out[22]: 
array([(1  '231'  'one')  (2  '312'  'two')  (3  '132'  'three ')]  
      dtype=[('int_field'  '<i4')  ('floatlike'  '|S10')  ('strfield'  '|S10')])

In [23]: y_pandas = pandasDataFramefrom_records(y_np)

In [25]: y_pandasdtypes
Out[25]: 
int_field     int64
floatlike    object  # better?
strfield     object
",1552748.0,,,,,2012-08-24 22:15:41,
12992260,2,12101113,2012-10-20 20:12:00,1,I'm planning to add explicit column dtypes in the upcoming file parser engine overhaul in pandas 010 Can't commit myself 100% to it but it should be pretty simple with the new infrastructure coming together (http://wesmckinneycom/blog/?p=543),776560.0,,,,,2012-10-20 20:12:00,1.0
12940387,2,12939093,2012-10-17 17:55:54,2,"Index constructor tries to be clever when the inputs are special (all ints or datetimes for example) and skips to calls to view at the end So you need to put that in explicitly:

In [150]: class InfoIndex(pdIndex):
   :     def __new__(cls  data  info=None):
   :         obj = pdIndex__new__(cls  data)
   :         objinfo = info
   :         obj = objview(cls)
   :         return obj
   :     

In [151]: I = InfoIndex((3 ))

In [152]: I
Out[152]: InfoIndex([3])


Caveat emptor: be careful subclassing pandas objects as many methods will explicitly return Index as opposed to the subclass And there are also features in sub-classes of Index that you'll lose if you're not careful ",1306530.0,,,,,2012-10-17 17:55:54,
12990320,2,12939093,2012-10-20 16:17:24,2,If you implement the __array_finalize__ method you can ensure that metadata is preserved in many operations For some index methods you'll need to provide implementations in your subclass See http://docsscipyorg/doc/numpy/user/basicssubclassinghtml for a bit more help,776560.0,,,,,2012-10-20 16:17:24,
13114736,2,13114512,2012-10-29 01:04:32,1,"I don't know pandas  and I'm pretty sure it has something specific for this; however  I'll give you the pure-Python solution  that might be of some help even if you need to use pandas:

import csv
import urllib

# This basically retrieves the CSV files and loads it in a list  converting
# All numeric values to floats
url='http://ichartfinanceyahoocom/tablecsv?s=IBM&a=00&b=1&c=2011&d=11&e=31&f=2011&g=d&ignore=csv'
reader = csvreader(urlliburlopen(url)  delimiter=' ')
# We sort the output list so the records are ordered by date
cleaned = sorted([[r[0]] + map(float  r[1:]) for r in list(reader)[1:]])

for i  row in enumerate(cleaned): ## enumerate() yields two-tuples: (<id>  <item>)
    ## The tryexcept here is to skip the IndexError for line 0
    try:
        ## This will calculate difference of each numeric field with the same field
        ## in the row before this one
        print row[0]  [(row[j] - cleaned[i-1][j]) for j in range(1  7)]
    except IndexError:
        pass
",148845.0,,,,,2012-10-29 01:04:32,
13226764,2,9339184,2012-11-05 06:03:43,0,"You can also use panels to help you do this pivot Like this:-

In [86]: panel = dfset_index(['A'  'B'])sortlevel(0)to_panel()

In [87]: panel[""C""]
Out[87]: 
B  11 12 13
A          
10  a  b  c
20  d  e  f


Which gives you the same result as Sebastian's answer above",482506.0,,,,,2012-11-05 06:03:43,
10195347,2,10194482,2012-04-17 16:38:52,7,"Basically  you can just use imshow or matshow

However  I'm not quite clear what you mean

If you want a chessboard with every ""white"" cell colored by some other vector  you could do something like this:

import matplotlibpyplot as plt
import numpy as np

# Make a 9x9 grid
nrows  ncols = 9 9
image = npzeros(nrows*ncols)

# Set every other cell to a random number (this would be your data)
image[::2] = nprandomrandom(nrows*ncols //2 + 1)

# Reshape things into a 9x9 grid
image = imagereshape((nrows  ncols))

row_labels = range(nrows)
col_labels = ['A'  'B'  'C'  'D'  'E'  'F'  'G'  'H'  'I']
pltmatshow(image)
pltxticks(range(ncols)  col_labels)
pltyticks(range(nrows)  row_labels)
pltshow()




Obviously  this only works for things with and odd number of rows and columns  You can iterate over each row for datasets with an even number of rows and columns

Eg:

for i  (image_row  data_row) in enumerate(zip(image  data)):
    image_row[i%2::2] = data_row


However  the number of ""data"" cells in each row is going to be different  which is where I get confused by your problem definition  

By definition  a checkerboard pattern has a different number of ""white"" cells in each row
Your data presumably (?) has the same number of values in each row   You need to define what you want to do  You can either truncate the data  or add an extra column

Edit: I just realized that that's true only for odd-length numbers of columns

Regardless  I'm still confused by your question  

Do you want have a ""full"" grid of data and want to set a ""checkerboard"" pattern of values in the data grid to a different color  or do you want to ""intersperse"" your data with a ""checkerboard"" pattern of values plotted as some constant color?

Update

It sounds like you want something more like a spreasheet?  Matplotlib isn't ideal for this  but you can do it

Ideally  you'd just use plttable  but in this case  it's easier to use matplotlibtableTable directly:

import matplotlibpyplot as plt
import numpy as np
import pandas

from matplotlibtable import Table

def main():
    data = pandasDataFrame(nprandomrandom((12 8))  
                columns=['A' 'B' 'C' 'D' 'E' 'F' 'G' 'H'])
    checkerboard_table(data)
    pltshow()

def checkerboard_table(data  fmt='{:2f}'  bkg_colors=['yellow'  'white']):
    fig  ax = pltsubplots()
    axset_axis_off()
    tb = Table(ax  bbox=[0 0 1 1])

    nrows  ncols = datashape
    width  height = 10 / ncols  10 / nrows

    # Add cells
    for (i j)  val in npndenumerate(data):
        # Index either the first or second item of bkg_colors based on
        # a checker board pattern
        idx = [j % 2  (j + 1) % 2][i % 2]
        color = bkg_colors[idx]

        tbadd_cell(i  j  width  height  text=fmtformat(val)  
                    loc='center'  facecolor=color)

    # Row Labels
    for i  label in enumerate(dataindex):
        tbadd_cell(i  -1  width  height  text=label  loc='right'  
                    edgecolor='none'  facecolor='none')
    # Column Labels
    for j  label in enumerate(datacolumns):
        tbadd_cell(-1  j  width  height/2  text=label  loc='center'  
                           edgecolor='none'  facecolor='none')
    axadd_table(tb)
    return fig

if __name__ == '__main__':
    main()


",325565.0,,325565.0,,2012-04-17 21:21:13,2012-04-17 21:21:13,2.0
13498331,2,13492226,2012-11-21 17:03:47,0,I finally solved my issue by checking the new version's documentation  where it is explained how index are now treated as timestamp here thus using to_pydatetime() method to convert the index values in proper datetime objects  as required by my function,1529852.0,,,,,2012-11-21 17:03:47,
13921674,2,13921647,2012-12-17 20:29:56,3,dfshape  where df is your DataFrame,1427416.0,,,,,2012-12-17 20:29:56,1.0
14179954,2,14178194,2013-01-06 06:35:53,3,"You're so close!

You can specify the colors in the styles list:

import numpy as np
import matplotlibpyplot as plt
import pandas as pd

testdataframe = pdDataFrame(nparange(12)reshape(4 3)  columns=['A'  'B'  'C'])
styles = ['bs-' 'ro-' 'y^-']
linewidths = [2  1  4]
fig  ax = pltsubplots()
for col  style  lw in zip(testdataframecolumns  styles  linewidths):
    testdataframe[col]plot(style=style  lw=lw  ax=ax)


Also note that the plot method can take a matplotlibaxes object  so you can make multiple calls like this (if you want to):

import numpy as np
import matplotlibpyplot as plt
import pandas as pd

testdataframe1 = pdDataFrame(nparange(12)reshape(4 3)  columns=['A'  'B'  'C'])
testdataframe2 = pdDataFrame(nprandomnormal(size=(4 3))  columns=['D'  'E'  'F'])
styles1 = ['bs-' 'ro-' 'y^-']
styles2 = ['rs-' 'go-' 'b^-']
fig  ax = pltsubplots()
testdataframe1plot(style=styles1  ax=ax)
testdataframe2plot(style=styles2  ax=ax)


Not really practical in this case  but the concept might come in handy later",1552748.0,,1552748.0,,2013-01-06 21:37:22,2013-01-06 21:37:22,3.0
14399463,2,14395678,2013-01-18 12:56:24,1,"One way would be using drop and indexget_duplicates:

In [43]: df
Out[43]: 
                      String
STK_ID RPT_Date             
600809 20061231  demo_string
       20070331  demo_string
       20070630  demo_string
       20070930  demo_string
       20071231  demo_string
       20060331  demo_string
       20060630  demo_string
       20060930  demo_string
       20061231  demo_string
       20070331  demo_string
       20070630  demo_string

In [44]: dfdrop(dfindexget_duplicates())
Out[44]: 
                      String
STK_ID RPT_Date             
600809 20070930  demo_string
       20071231  demo_string
       20060331  demo_string
       20060630  demo_string
       20060930  demo_string
",1301710.0,,,,,2013-01-18 12:56:24,3.0
14400659,2,14395678,2013-01-18 14:10:30,3,"You can groupby the index and apply a function that returns one value per index group  Here  I take the first value:

In [1]: s = Series(range(10)  index=[1 2 2 2 5 6 7 7 7 8])

In [2]: s
Out[2]:
1    0
2    1
2    2
2    3
5    4
6    5
7    6
7    7
7    8
8    9

In [3]: sgroupby(sindex)first()
Out[3]:
1    0
2    1
5    4
6    5
7    6
8    9


UPDATE

Addressing BigBug's comment about crashing when passing a MultiIndex to Seriesgroupby():

In [1]: s
Out[1]:
STK_ID  RPT_Date
600809  20061231    demo
        20070331    demo
        20070630    demo
        20070331    demo

In [2]: sreset_index()groupby(sindexnames)first()
Out[2]:
                    0
STK_ID RPT_Date
600809 20061231  demo
       20070331  demo
       20070630  demo
",919872.0,,919872.0,,2013-01-19 03:25:20,2013-01-19 03:25:20,8.0
14513693,2,14513638,2013-01-25 01:03:18,1,"You can use groupby  Start from a csv with duplicates:

>>> !cat tomergecsv
date  cola  colb  colc
1 10  
2 11  
1  14 
2  15 
1  24 
2  40 
1   17
2   18


Read it in:

>>> df = pdread_csv(""tomergecsv"")
>>> df
   date   cola   colb   colc
0     1     10    NaN    NaN
1     2     11    NaN    NaN
2     1    NaN     14    NaN
3     2    NaN     15    NaN
4     1    NaN     24    NaN
5     2    NaN     40    NaN
6     1    NaN    NaN     17
7     2    NaN    NaN     18


And then the magic happens:

>>> dfgroupby(""date"")mean()
       cola   colb   colc
date                     
1        10   190     17
2        11   275     18
>>> dfgroupby(""date"")max()
       cola   colb   colc
date                     
1        10     24     17
2        11     40     18
",487339.0,,,,,2013-01-25 01:03:18,2.0
11265413,2,11265116,2012-06-29 16:18:18,2,"I don't know about efficient  but I might do something like this:

~/coding$ cat colgroupdat
A_1 A_2 A_3 B_1 B_2 B_3
1 2 3 4 5 6
7 8 9 10 11 12
13 14 15 16 17 18
~/coding$ python
Python 273 (default  Apr 20 2012  22:44:07) 
[GCC 463] on linux2
Type ""help""  ""copyright""  ""credits"" or ""license"" for more information
>>> import pandas
>>> df = pandasread_csv(""colgroupdat"")
>>> df
   A_1  A_2  A_3  B_1  B_2  B_3
0    1    2    3    4    5    6
1    7    8    9   10   11   12
2   13   14   15   16   17   18
>>> grouped = dfgroupby(lambda x: x[0]  axis=1)
>>> for i  group in grouped:
     print i  group
 
A    A_1  A_2  A_3
0    1    2    3
1    7    8    9
2   13   14   15
B    B_1  B_2  B_3
0    4    5    6
1   10   11   12
2   16   17   18
>>> groupedmean()
key_0   A   B
0       2   5
1       8  11
2      14  17


I suppose lambda x: xsplit('_')[0] would be a little more robust",487339.0,,,,,2012-06-29 16:18:18,2.0
11287350,2,11265116,2012-07-02 02:59:34,2,"You want to make use of the built-in mean() function that accepts an axis argument to specify row-wise means Since you know your specific column name convention for the different means that you want  you can use the example code below to do it very efficiently Here I chose to just make two additional columns rather than to actually destroy the existing data I could have also put these new columns into a new data frame; it just depends on what your needs are and what's convenient for you The same basic idea will work in either case

In [1]: import pandas

In [2]: dfrm = pandasDataFrame([[1 2 3 4 5 6] [7 8 9 10 11 12] [13 14 15 16 17 18]]  columns = ['A_1'  'A_2'  'A_3'  'B_1'  'B_2'  'B_3'])

In [3]: dfrm
Out[3]: 
   A_1  A_2  A_3  B_1  B_2  B_3
0    1    2    3    4    5    6
1    7    8    9   10   11   12
2   13   14   15   16   17   18

In [4]: dfrm[""A_mean""] = dfrm[[elem for elem in dfrmcolumns if elem[0]=='A']]mean(axis=1)

In [5]: dfrm
Out[5]: 
   A_1  A_2  A_3  B_1  B_2  B_3  A_mean
0    1    2    3    4    5    6       2
1    7    8    9   10   11   12       8
2   13   14   15   16   17   18      14

In [6]: dfrm[""B_mean""] = dfrm[[elem for elem in dfrmcolumns if elem[0]=='B']]mean(axis=1)

In [7]: dfrm
Out[7]: 
   A_1  A_2  A_3  B_1  B_2  B_3  A_mean  B_mean
0    1    2    3    4    5    6       2       5
1    7    8    9   10   11   12       8      11
2   13   14   15   16   17   18      14      17
",567620.0,,567620.0,,2012-07-02 13:07:24,2012-07-02 13:07:24,1.0
11673682,2,11673453,2012-07-26 16:20:19,0,"parse_dates doesn't take the index values

Try something like:

pdread_csv('data/EURUSD15csv'   parse_dates = [['YYYYMMDD'  'HH:MM']]  index_col = 0  
        date_parser=parse)
",1523170.0,,,,,2012-07-26 16:20:19,2.0
11684184,2,11673453,2012-07-27 08:29:34,0,"The columns are zero indexed  so you need to do parse_dates=[[0 1]]
This is on latest version of pandas but should work with 080+:

In [74]: data = """"""\
20111208 22:45 133434 133465 133415 133419 265
20111208 23:00 133419 133542 133419 133472 391
20111208 23:15 133470 133483 133383 133411 420
20111208 23:30 133413 133451 133389 133400 285
""""""

In [75]: pdread_csv(StringIO(data)  
                     names=['Date'  'Time'  'Open'  'High'  'Low'  'Close'  'Volume']  
                     index_col='Date_Time'  parse_dates=[[0  1]])
Out[75]: 
                        Open     High      Low    Close  Volume
Date_Time                                                      
2011-12-08 22:45:00  133434  133465  133415  133419     265
2011-12-08 23:00:00  133419  133542  133419  133472     391
2011-12-08 23:15:00  133470  133483  133383  133411     420
2011-12-08 23:30:00  133413  133451  133389  133400     285


Note the index_col=0 will also work Complex date parsing prepends resulting columns so parse_dates will refer to pre-date processing column indices (ie  0 is Date and 1 is Time) and index_col refers to post-date processing column indices Thus  using column names are recommended since it allows you to not have to think about pre-vs-post processing columns indices",1306530.0,,,,,2012-07-27 08:29:34,1.0
10182172,2,10175068,2012-04-16 21:47:06,2,"The problem lies in my assumption(incorrect) that I was in the dev version while in reality I had 161  one can check the current installed version with:

import pandas
print pandas__version__


in the current version dfxs() with the level parameter works ok",1330293.0,,,,,2012-04-16 21:47:06,
10816150,2,10797734,2012-05-30 12:19:09,0,"Don't know if this is the optimal way  but this is simpler and should be more efficient as it uses vectorized functions for the calculations:

def func(x  y):
    return x ** y

data = pdread_csv('datadat'  sep=';'  index_col=0  parse_dates=True 
                    header=None  names='abc')
para = pdread_csv('parameterdat'  sep=';'  index_col=0  parse_dates=True 
                    header=None  names=['para'])

for col in data:
    data['%s_result' % col] = func(data[col]  parapara)

print data


results in 

                     a  b  c  a_result  b_result  c_result
2010-01-03 00:00:00  9  5  7        81        25        49
2010-01-03 00:10:00  9  1  4     59049         1      1024
2010-01-03 00:20:00  5  3  8        25         9        64
2010-01-03 00:30:00  7  7  1       343       343         1
2010-01-03 00:40:00  8  2  3         1         1         1
2010-01-03 00:50:00  0  3  4         0         9        16
2010-01-03 01:00:00  4  3  2       256        81        16
2010-01-03 01:10:00  6  2  2       216         8         8
2010-01-03 01:20:00  6  8  5       216       512       125
2010-01-03 01:30:00  7  7  0         7         7         0


If your real function is more complex you should even try to vectorize it or use numpyvectorize() as the next best solution",1301710.0,,1301710.0,,2012-06-04 14:01:14,2012-06-04 14:01:14,7.0
11436825,2,11435668,2012-07-11 16:08:26,1,"Try using itertools to generate the powerset of column names: 

In [23]: import itertools as iter

In [24]: def pset(lst):
   :     comb = (itercombinations(lst  l) for l in range(len(lst) + 1))
   :     return list(iterchainfrom_iterable(comb))
   : 


In [25]: pset(lst)
Out[25]: 
[() 
 ('A' ) 
 ('B' ) 
 ('C' ) 
 ('D' ) 
 ('A'  'B') 
 ('A'  'C') 
 ('A'  'D') 
 ('B'  'C') 
 ('B'  'D') 
 ('C'  'D') 
 ('A'  'B'  'C') 
 ('A'  'B'  'D') 
 ('A'  'C'  'D') 
 ('B'  'C'  'D') 
 ('A'  'B'  'C'  'D')]
",1306530.0,,,,,2012-07-11 16:08:26,1.0
11447841,2,11435668,2012-07-12 08:27:39,0,"If you are looking for combination of columns to regression against each other    

df = DataFrame(numpyrandomrandn(3 6)  columns=['a' 'b' 'c' 'd' 'e' 'g'])
df2 =[df[list(pair)] for pair in list(itercombinations(dfcolumns  2))]
",1377107.0,,,,,2012-07-12 08:27:39,
11941772,2,11941492,2012-08-13 20:26:38,1,"Syntax like the following will work:

dfix['a']
dfix['a']ix['c']


since group1 and group2 are indices Please forgive my previous attempt!

To get at the second index only  I think you have to swap indices:

dfswaplevel(0 1)ix['c']


But I'm sure Wes will correct me if I'm wrong",567186.0,,567186.0,,2012-08-13 20:49:22,2012-08-13 20:49:22,10.0
11942697,2,11941492,2012-08-13 21:37:08,9,"Try using xs to be very precise:

In [5]: dfxs('a'  level=0)
Out[5]: 
        value1  value2
group2                
c          11     71
c          20     80
d          30     90

In [6]: dfxs('c'  level='group2')
Out[6]: 
        value1  value2
group1                
a          11     71
a          20     80
",776560.0,,,,,2012-08-13 21:37:08,9.0
12065904,2,12065885,2012-08-22 03:21:12,3,Use the isin method  rpt[rpt['STK_ID']isin(stk_list)],1427416.0,,,,,2012-08-22 03:21:12,1.0
10948591,2,10947968,2012-06-08 12:09:32,1,"Try this:

DataFramefrom_records([(int(word['x1'])  int(word['x2']))
                        for word in souppagefindAll('word')] 
                       columns=('x1'  'x2'))
",449449.0,,,,,2012-06-08 12:09:32,1.0
11495984,2,11470105,2012-07-15 22:00:05,1,"The issue is that using ix[] returns a view to the actual memory objects for that subset of the DataFrame  rather than a new DataFrame made out of its contents

Instead use 

# The left-hand-side does not use ix  since we're assigning into it
df['b' 'c']] = dfix[: 'e':'f']copy()


Note that you will need copy() if you are intent on using ix to do the slicing  otherwise it would set columns 'b' and 'c' as the same objects in memory as the columns 'e' and 'f'  which does not seem like what you want to do here

Alternatively  to avoid worrying about the copying you  you can just do:

df[['b' 'c']] = df[['e' 'f']]


If the convenience of indexing matters to you  one way to simulate this effect is to write your own function:

def col_range(df  col1  col2): 
    return list(dfrmix[dfrmindexvalues[0] col1:col2]index)


Now you could do the following:

df[col_range(df 'b' 'd')] = dfix[: 'e':'g']copy()


Note: in the definition of col_range I used the first index which will select the first row of the data frame I did this because making a view of the whole data frame just to select a range of columns seems wasteful  whereas one row probably won't matter Since slicing this way produces a Series  the way to extract the columns is to actually grab the index  and I return them as a list

Added for additional row slice request:

To specify a set of rows in the assignment  you can use ix  but you need to specify just a matrix of values on the right-hand side Having the structure of a sub-DataFrame on the right-hand side will cause problems

dfix[0:4 col_range(df 'b' 'd')] = dfix[0:4 'e':'g']values


You can replace the [0:4] with [dfindexvalues[i]:dfindexvalues[j]] or [dfindexvalues[i] for i in range(N)] or even with logical values such as [df['a']>5] to only get rows where the 'a' column exceeds 5  for example 

The full slice for and example logical indexing where you want column 'a' bigger than 5 and column 'e' less than 10 might look like this:

import numpy as np
my_rows = nplogical_and(df['a'] > 5)  df['e'] < 10)
dfix[my_rows col_range(df 'b' 'd')] = dfix[my_rows 'e':'g']values


In many cases  you will not need to use the ix on the left-hand side (I recommend against it because it only works in some cases and not in others) For instance  something like:

df[""A""] = nprepeat(False  len(df))
df[""A""][df[""B""] > 0] = True


will work as is  no special ix needed for identifying the rows where the condition is true The ix seems to be needed on the left when the thing on the right is complicated",567620.0,,567620.0,,2012-07-17 12:45:57,2012-07-17 12:45:57,3.0
11977593,2,11945897,2012-08-15 21:26:42,0,"You're encountering an bug in pandas 081 in parsing timezones: https://githubcom/pydata/pandas/issues/1693

Marked as an enhancement  so maybe I'm not understanding and there could be a workaround  but I'm pretty sure it's just a bug Already fixed but will have to wait for 082
This works in 080 (I ran into the same error only after upgrading) You can try falling back to 080:

pip install -I pandas==080


As long as 080 still satisfies requirements of anything else that is using pandas in your app

Or you might try to get the lastest from https://githubcom/pydata/pandas and build it  but might be less stable",128508.0,,,,,2012-08-15 21:26:42,2.0
14553291,2,14531544,2013-01-27 22:55:19,0,"I found a solution via this thread: ipython reads wrong python version

@minrk's point about the non-matching between which python and which ipython led me to rewrite the first line of my /usr/local/bin/ipython file",1321181.0,,,,,2013-01-27 22:55:19,
9762583,2,9762193,2012-03-18 21:43:52,2,"There's nothing really to stop you right now:

In [17]: idf = dfset_index(['tag'  'epochTimeMS']  verify_integrity=False)sort_index()

In [18]: idf
Out[18]: 
                     event  timeTakenMS
tag  epochTimeMS                       
tag1 1331782842381  event2          436
     1331782842801  event1           16
     1331782842801  event1           17
tag2 1331782841535  event1         1278

In [20]: idfix['tag1']
Out[20]: 
                event  timeTakenMS
epochTimeMS                       
1331782842381  event2          436
1331782842801  event1           16
1331782842801  event1           17


Accessing specific values by timestamp will cause an exception (this is going to be improved  as you mention)  but you can certainly work with the data Now  if you want a fixed-length (in time space) window  that's not supported very well yet but I created an issue here:

https://githubcom/pydata/pandas/issues/936

If you could speak up on the mailing list about your API requirements in your application it would be helpful for me and the guys since we're actively working on the time series capabilities right now",776560.0,,,,,2012-03-18 21:43:52,2.0
10701821,2,10697423,2012-05-22 12:12:45,0,"You can't print a plot object (or you can  but you only see the Axes() text)

IPython should make a display() function available that you can use on each plot to show it",434217.0,,,,,2012-05-22 12:12:45,
10708261,2,10697423,2012-05-22 18:55:02,2,Each of those plots appears on the same subplot; pandas creates a figure in the first plot call but leaves it to you to create further figures and subplots after that Try inserting pltfigure() (cf import matplotlibpyplot as plt) before each plot command,776560.0,,,,,2012-05-22 18:55:02,1.0
11395193,2,11391969,2012-07-09 12:39:29,3,"This should work:

datagroupby(lambda x: data['date'][x]year)
",567292.0,,,,,2012-07-09 12:39:29,
11397052,2,11391969,2012-07-09 14:25:27,3,"ecatmur's solution will work fine This will be better performance on large datasets  though:

datagroupby(data['date']map(lambda x: xyear))
",776560.0,,,,,2012-07-09 14:25:27,
11876415,2,11871152,2012-08-09 03:34:22,0,"The dates are originally read in as the column names and pandas currently does not parse the column names into dates For feature requests  please create a new issue on github: https://githubcom/pydata/pandas/issues

For now you can do some post-processing:

eur3mindex = [datetimedatetimestrptime(x  '%d/%m/%Y') for x in eur3mindex]
",1306530.0,,,,,2012-08-09 03:34:22,1.0
12035069,2,12034727,2012-08-20 09:19:56,1,stack and unstack add level(s) to the end of the MultiIndex  this is not controllable You can change the order of the levels in a MultiIndex with reorder_levels(): stackedreorder_levels([2  1  0]) will give you the same MultiIndex levels order as in df,1548051.0,,,,,2012-08-20 09:19:56,2.0
13032992,2,13032834,2012-10-23 14:39:21,3,"I would create a dictionary of keys which are tuples that hold column1 and column2 data  The values would be a list which holds column3 and column4 data

from collections import defaultdict
with open('testdat') as f:
    data = defaultdict( lambda:([] []))
    header = freadline()
    for line in f:
        col1 col2 col3 col4 = linesplit()
        col3_data col4_data = data[(col1 col2)]  #data[frozenset((col1 col2))] if order doesn't matter
        col3_dataappend(col3)
        col4_dataappend(col4)


Now sort and write the output (joining column3 and column4 lists with a ' '  making unique with set and sorted to order properly)

with open('outfiledat' 'w') as f:
   fwrite(header)
   #If you used a frozenset in the first part  you might want to do something like:
   #for k in sorted(map(sorted datakeys())):
   for k in sorted(datakeys()):
       col1 col2 = k
       col3_data col4_data = data[k]
       col3_data = ' 'join(col3_data) #join the list
       col3_data = set(int(x) for x in col3_datasplit(' ')) #make unique integers
       col3_str = ' 'join(map(str sorted(col3_data)))       #sort  convert to strings and join with ' '
       col4_data = ' 'join(col4_data)  #join the list
       col4_data = sorted(set(col4_datasplit(' ')))  #make unique and sort
       fwrite('{0}\t{1}\t{2}\t{3}\n'format(col1 col2 col3_str ' 'join(col4_data)))
",748858.0,,748858.0,,2012-10-23 15:19:03,2012-10-23 15:19:03,7.0
10739432,2,10729210,2012-05-24 14:24:52,3,You should use dfiterrows() Though iterating row-by-row is not especially efficient since Series objects have to be created,776560.0,,,,,2012-05-24 14:24:52,2.0
11435721,2,11422552,2012-07-11 15:09:16,3,"You can use DataFrameirow:

In [18]: df2
Out[18]: 
          0         1         2
1  2279885 -0414938 -2230296
2 -0237980 -0219556  1231576

In [19]: df2irow(0)
Out[19]: 
0    2279885
1   -0414938
2   -2230296
Name: 1

In [20]: df2irow([0  1])
Out[20]: 
          0         1         2
1  2279885 -0414938 -2230296
2 -0237980 -0219556  1231576
",1306530.0,,,,,2012-07-11 15:09:16,
11888068,2,11887504,2012-08-09 16:41:40,3,"The dateutil package should be helpful  eg 

from dateutilrrule import *    

def add_weekday_seconds(start  x):
    rr = rrule(SECONDLY  byweekday=(MO  TU  WE  TH  FR)  dtstart=start  interval=x)
    return rrafter(start)


This (1) uses the rrule class to creates a ""repeating date rule"" that includes all seconds within Monday through Friday starting on start (which is a datetime)  skipping every x seconds; and then (2) executes this rule using the after method which returns the first datetime matching the the rule after the start time -- which should be your answer!

Tests of adding 5  10  and 15 seconds to a start time of 10 seconds before midnight on Friday night  resulting in  respectively  5 seconds before midnight on Friday  midnight Monday morning  and 5 seconds after midnight Monday morning:

In [131]: friday_night = datetimedatetime(2012  8  10  23  59  50)

In [132]: add_weekday_seconds(friday_night  5)
Out[132]: datetimedatetime(2012  8  10  23  59  55)

In [133]: add_weekday_seconds(friday_night  10)
Out[133]: datetimedatetime(2012  8  13  0  0)

In [134]: add_weekday_seconds(friday_night  15)
Out[134]: datetimedatetime(2012  8  13  0  0  5)
",848235.0,,848235.0,,2012-08-09 18:12:29,2012-08-09 18:12:29,1.0
12054685,2,12047418,2012-08-21 12:18:26,4,If you want to select rows/columns based on MultiIndex level values i suggest using the 'xs()' method See also Selecting rows from a Pandas dataframe with a compound (hierarchical) index,1548051.0,,,,,2012-08-21 12:18:26,1.0
12059336,2,12047418,2012-08-21 16:36:14,1,"@lodagro has the solution I need:

#short hand:
metalsxs('BI'  level=0)xs('Arsenic  Dissolved'  level=0)xs(1  level=1)

# more verbose so I can read it later
metalsxs('BI'  level='bmp_category')xs('Arsenic  Dissolved'  level='parameter')xs(1  level='storm')

# along the same lines as my original thought:
metalsix['BI'  'Arsenic  Dissolved']ix[:  1]
",1552748.0,,,,,2012-08-21 16:36:14,3.0
12619626,2,12619215,2012-09-27 10:47:00,1,"How about just creating the Panel out of a dict of DataFrame?

In [10]: dd = {}

In [11]: for i in range(1  3):
   :     name = 'X' + str(i)
   :     dd[name] = DataFrame(nprandomrandn(3 3))
   :     

In [12]: Panel(dd)
Out[12]: 
<class 'pandascorepanelPanel'>
Dimensions: 2 (items) x 3 (major) x 3 (minor)
Items: X1 to X2
Major axis: 0 to 2
Minor axis: 0 to 2


So something like:

def panelCreation():
    dd = {}
    for i in range(1 3):
        name = 'X' + str(i)
        dd[name] = createNewDf(i)
    return Panel(dd)
",1306530.0,,,,,2012-09-27 10:47:00,2.0
13040570,2,13040312,2012-10-23 22:54:33,0,"Me not knowing panda  a general answer:

You can overload anything in python  and they must have done that there If you define a special method __getitem__ on your class  it is called when you use obj[key] or obj[start:stop] (With just key as argument in the former case  with a special slice object in the latter) You can then return anything you want

Here's an example to show how __getitem__ works:

class Foo(object):
    def __getitem__(self  k):
        if isinstance(k  slice):
            return kstart + kstop # properties of the slice object
        else:
            return k


This gives you:

>>> f = rangeFoo()
>>> f[42]
42
>>> f[23:42]
65


I assume that in your example  the __getitem__ method returns some special object  which contains the datetime objects plus a reference to the original ts object That special object can then use that information to fetch the desired information later on  when the first_valid_index method or a similar one is called (It does not even have to modify the original object  like your question suggested)

TL;DR: Learn not to worry :-)

Addition: I got curious  so I implemented a minimal example of the behavior you described above myself:

class FilterableList(list):
    def __init__(self  *args):
        list__init__(self  *args)
        selffilter = FilterProxy(self)

class FilterProxy(object):
    def __init__(self  parent):
        selfparent = parent

    def __getitem__(self  sl):
        if isinstance(sl  slice):
            return Filter(selfparent  sl)

class Filter(object):
    def __init__(self  parent  sl):
        selfparent = parent
        selfsl = sl

    def eval(self):
        return [e for e in selfparent if selfslstart <= e <= selfslstop]


>>> l = FilterableList([4 5 6 7])
>>> f = lfilter[6:10]
>>> feval()
[6  7]
>>> lappend(8)
>>> feval()
[6  7  8]
",196244.0,,196244.0,,2012-10-23 23:13:55,2012-10-23 23:13:55,
13115473,2,13114512,2012-10-29 03:17:30,5,"I think you want to do something like this:

In [26]: data
Out[26]: 
           Date   Close  Adj Close
251  2011-01-03  14748     14325
250  2011-01-04  14764     14341
249  2011-01-05  14705     14283
248  2011-01-06  14866     14440
247  2011-01-07  14793     14369

In [27]: dataset_index('Date')diff()
Out[27]: 
            Close  Adj Close
Date                        
2011-01-03    NaN        NaN
2011-01-04   016       016
2011-01-05  -059      -058
2011-01-06   161       157
2011-01-07  -073      -071
",1306530.0,,,,,2012-10-29 03:17:30,
13257199,2,13256917,2012-11-06 18:36:37,0,"I found a way using join:

In [101]:
aggregated = dfgroupby('A')sum()['values']
aggregatedname = 'sum_values_A'
dfjoin(aggregated on='A')

Out[101]:
   A  B  values  sum_values_A
0  1  1      10            25
1  1  2      15            25
2  2  1      20            45
3  2  2      25            45


Anyone has a simpler way to do it?",189418.0,,,,,2012-11-06 18:36:37,
13257396,2,13256917,2012-11-06 18:49:45,2,"This is not so direct but I found it very intuitive (the use of map to create new columns from another column) and can be applied to many other cases:

gb = dfgroupby('A')sum()['values']

def getvalue(x):
    return gb[x]

df['sum'] = df['A']map(getvalue)
df
",308903.0,,,,,2012-11-06 18:49:45,1.0
13257677,2,13256917,2012-11-06 19:07:38,2,"In [20]: df = pdDataFrame({'A':[1 1 2 2] 'B':[1 2 1 2] 'values':nparange(10 30 5)})

In [21]: df
Out[21]:
   A  B  values
0  1  1      10
1  1  2      15
2  2  1      20
3  2  2      25

In [22]: df['sum_values_A'] = dfgroupby('A')['values']transform(npsum)

In [23]: df
Out[23]:
   A  B  values  sum_values_A
0  1  1      10            25
1  1  2      15            25
2  2  1      20            45
3  2  2      25            45
",1548051.0,,,,,2012-11-06 19:07:38,1.0
13259701,2,13256917,2012-11-06 21:26:44,1,"In [15]: def sum_col(df  col  new_col):
   :     df[new_col] = df[col]sum()
   :     return df

In [16]: dfgroupby(""A"")apply(sum_col  'values'  'sum_values_A')
Out[16]: 
   A  B  values  sum_values_A
0  1  1      10            25
1  1  2      15            25
2  2  1      20            45
3  2  2      25            45
",243434.0,,,,,2012-11-06 21:26:44,
13574627,2,13572550,2012-11-26 22:59:22,1,"Why not try the concat function:

In [25]: df1
Out[25]: 
     City  Population
0  Zagreb      700000
1  Rijeka      142000

In [26]: df2
Out[26]: 
        City    Area
0      Split  20000
1     Osijek  17100
2  Dubrovnik  14335

In [27]: pdconcat([df1 df2])
Out[27]: 
     Area       City  Population
0     NaN     Zagreb      700000
1     NaN     Rijeka      142000
0  20000      Split         NaN
1  17100     Osijek         NaN
2  14335  Dubrovnik         NaN

In [28]: pdconcat([df1 df2]  ignore_index=True)
Out[28]: 
     Area       City  Population
0     NaN     Zagreb      700000
1     NaN     Rijeka      142000
2  20000      Split         NaN
3  17100     Osijek         NaN
4  14335  Dubrovnik         NaN


Note: the concat does have some additional options if you have slightly different requirements",1240268.0,,,,,2012-11-26 22:59:22,1.0
13886304,2,13886019,2012-12-14 21:12:08,0,"ues unique method to get every different year:

DataFrame1plot(legend=False)
patch labels=axget_legend_handels_labels()
DateFrame1legend(str(unique(DataFrame1['Year']value)) loc='best')
pltshow()


so it's work correctly",1843099.0,,1843099.0,,2012-12-15 03:27:15,2012-12-15 03:27:15,0.0
13888672,2,13886019,2012-12-15 02:12:15,0,"You were very close  you just need to update your legend with the years:

ax = dfplot()

years = [2005  2007  2008  2009  2011  2012]
# you can get years from you dataframe (but without seeing the dataframe I can't say exactly how)
# legend also accepts a Series or numpy array
axlegend(years  loc='best')
pltshow()
",1240268.0,,,,,2012-12-15 02:12:15,1.0
12022047,2,12021730,2012-08-18 20:35:00,2,"I think there's just a missing \ in the docs (maybe because it was interpreted as an escape marker at some point?)  It's a regexp  after all:

In [68]: data = read_table('sampletxt'  skiprows=3  header=None  sep=r""\s*"")

In [69]: data
Out[69]: 
<class 'pandascoreframeDataFrame'>
Int64Index: 7 entries  0 to 6
Data columns:
X1     7  non-null values
X2     7  non-null values
X3     7  non-null values
X4     7  non-null values
X5     7  non-null values
X6     7  non-null values
[]
X23    7  non-null values
X24    7  non-null values
X25    5  non-null values
X26    3  non-null values
dtypes: float64(8)  int64(10)  object(8)


Because of the delimiter problem noted by @MRAB  it has some trouble with the last few columns:

In [73]: dataix[: 20:]
Out[73]: 
   X21  X22           X23                   X24            X25    X26
0   315  095            ABC            transporter   transmembrane  region
1   527  093            ABC            transporter            None    None
2   408  086  RecF/RecN/SMC                      N        terminal  domain
3   575  085  RecF/RecN/SMC                      N        terminal  domain
4   556  072            AAA                 ATPase          domain    None
5   275  085      YceG-like                 family            None    None
6   200  085       Pyridine  nucleotide-disulphide  oxidoreductase    None


but that can be patched up at the end",487339.0,,,,,2012-08-18 20:35:00,0.0
12252958,2,12251483,2012-09-03 19:13:02,4,"You can use a converter in combination with read_csv

In [423]: d = """"""\
timestamp data
1283346000062 a
1283346000062 b
1283346000062 c
1283346000062 d
1283346000300 e
""""""

In [424]: fromtimestamp = lambda x:datetimefromtimestamp(int(x) / 1000)

In [425]: df = pandasread_csv(StringIO(d)  sep='\s+'  converters={'timestamp': fromtimestamp})set_index('timestamp')

In [426]: dfindex
Out[426]:
<class 'pandastseriesindexDatetimeIndex'>
[2010-09-01 15:00:00062000    2010-09-01 15:00:00300000]
Length: 5  Freq: None  Timezone: None

In [427]: df
Out[427]:
                           data
timestamp
2010-09-01 15:00:00062000    a
2010-09-01 15:00:00062000    b
2010-09-01 15:00:00062000    c
2010-09-01 15:00:00062000    d
2010-09-01 15:00:00300000    e
",1548051.0,,,,,2012-09-03 19:13:02,2.0
12270200,2,12251483,2012-09-04 19:37:20,1,"Internally  Timestamps are stored in int representing nanoseconds They use the numpy datetime/timedelta The issue with your timestamps is that they are in ms precision  which you already know since you're dividing by 1000 In this case  it's easier to astype('M8[ms]') It's essentially saying view these ints as datetime-ints with ms precision 

In [21]: int_arr
Out[21]: 
array([1283346000062  1283346000062  1283346000062  1283346000062 
       1283346000300])

In [22]: int_arrastype('M8[ms]')
Out[22]: 
array(['2010-09-01T09:00:00062-0400'  '2010-09-01T09:00:00062-0400' 
       '2010-09-01T09:00:00062-0400'  '2010-09-01T09:00:00062-0400' 
       '2010-09-01T09:00:00300-0400']  dtype='datetime64[ms]')


Pandas will assume any regular int array is in M8[ns] An array with a datetime64 dtype will be correctly interpreted You can view the M8[ns] representation of a DatetimeIndex by access ing it's asi8 attribute 

[EDIT] I realize that this won't help you directly with read_csv Just thought I'd throw out how to quickly convert between timestamp arrays",1297165.0,,,,,2012-09-04 19:37:20,3.0
13031591,2,13030245,2012-10-23 13:31:16,1,"ts_miunstack()shift(2  freq='D')stack()


One can see that the output is correct:

utilsside_by_side(ts_mi  ts_miunstack()shift(2  freq='D')stack())
2012-01-01  0    0481555    2012-01-03  0    0481555
            1    0000628                1    0000628
            2    2509266                2    2509266
            3    0021571                3    0021571
            4   -0539981                4   -0539981
2012-01-02  0   -1465450    2012-01-04  0   -1465450
            1    0815251                1    0815251
            2   -1489051                2   -1489051
            3    0639746                3    0639746
            4   -0176939                4   -0176939
2012-01-03  0   -0441842    2012-01-05  0   -0441842
            1   -0792810                1   -0792810
            2   -0802665                2   -0802665
            3    1922190                3    1922190
            4    0165446                4    0165446
                                                


How it works: dfunstack() moves the values in the nested 0  1  2  3  4 to the homonym columns  and dfstack() recovers the original nested index

EDIT: here's the side_by_side function from @Wes_McKinney

def side_by_side(*objs  **kwds):
    from pandascorecommon import adjoin
    space = kwdsget('space'  4)
    reprs = [repr(obj)split('\n') for obj in objs]
    print adjoin(space  *reprs)
",54567.0,,54567.0,,2012-10-23 16:02:11,2012-10-23 16:02:11,5.0
9652858,2,9652832,2012-03-11 06:06:56,5,"The documentation lists a from_csv function that appears to do what you want:

DataFramefrom_csv('c:/~/trainSetRel3txt'  sep='\t')


If you have a header  you can pass header=1",1256624.0,,,,,2012-03-11 06:06:56,1.0
9656288,2,9652832,2012-03-11 15:34:23,3,Use read_table(filepath) The default separator is tab,776560.0,,,,,2012-03-11 15:34:23,
10781413,2,10591000,2012-05-28 08:16:28,4,"It looks like you can't avoid pandas from trying to convert numeric/boolean values in the CSV file Take a look at the source code of pandas for the IO parsers  in particular functions _convert_to_ndarrays  and _convert_types
https://githubcom/pydata/pandas/blob/master/pandas/io/parserspy

You can always assign the type you want after you have read the file:

dfphone = dfphoneastype(str)
",1063605.0,,,,,2012-05-28 08:16:28,1.0
12335136,2,11194610,2012-09-08 22:59:04,1,"I don't know of anything off-hand Created an enhancement ticket about it:

http://githubcom/pydata/pandas/issues/1864",776560.0,,,,,2012-09-08 22:59:04,
11651153,2,11651048,2012-07-25 13:43:57,3,"b = dfpivot('USERNAME'  'REQUEST_TYPE')
bcolumns = ['{0}_{1}'format(*col) for col in bcolumns]


b is now:

         LATENCY_1 LATENCY_2 STATUS_1 STATUS_2
USERNAME                                      
bar             10        12  SUCCESS  FAILURE
foo              7        17  SUCCESS  SUCCESS
",449449.0,,,,,2012-07-25 13:43:57,1.0
12007574,2,12007406,2012-08-17 14:20:05,6,"In [193]: df
Out[193]:
   A  B  C  D
a  1  8  9  1
b  5  4  3  6
c  4  6  1  3
d  1  0  2  9

In [194]: dfdivide(dfix[0] / 100)
Out[194]:
     A    B           C    D
a  100  100  100000000  100
b  500   50   33333333  600
c  400   75   11111111  300
d  100    0   22222222  900
",1548051.0,,,,,2012-08-17 14:20:05,
12192290,2,12190716,2012-08-30 07:51:48,4,"Assuming here that RPT_Data is a string  any reason why not to use Datetime?

It is possible to groupby using functions  but only on a non MultiIndex-index Working around this by resetting the index  and set 'RPT_Date' as index to extract the year (note: pandas toggles between object and int as dtype for 'RPT_Date')

In [135]: year = lambda x : datetimestrptime(str(x)  '%Y%m%d')year

In [136]: grouped = RPTreset_index()set_index('RPT_Date')groupby(['STK_ID'  year])

In [137]: for key  df in grouped:
   :     print key
   :     print df
   :
(876  2006)
          STK_ID       sales
RPT_Date
20060331     876   798627000
20060630     876  1656110000
20060930     876  2719700000
20061231     876  3573660000
(876  2007)
          STK_ID       sales
RPT_Date
20070331     876   878415000
20070630     876  2024660000
20070930     876  3352630000
20071231     876  4791770000
(600141  2006)
          STK_ID       sales
RPT_Date
20060331  600141   270912000
20060630  600141   658981000
20060930  600141  1010270000
20061231  600141  1591500000
(600141  2007)
          STK_ID       sales
RPT_Date
20070331  600141   319602000
20070630  600141   790670000
20070930  600141  1250530000
20071231  600141  1711240000


Other option is to use a tmp column

In [153]: RPT_tmp = RPTreset_index()

In [154]: RPT_tmp['year'] = RPT_tmp['RPT_Date']apply(year)

In [155]: grouped = RPT_tmpgroupby(['STK_ID'  'year'])


EDIT
Reorganising your frame make it much easier

In [48]: RPT
Out[48]: 
                                  sales
STK_ID RPT_Year RPT_Quarter            
876    2006     0             798627000
                1            1656110000
                2            2719700000
                3            3573660000
       2007     0             878415000
                1            2024660000
                2            3352630000
                3            4791770000
600141 2006     0             270912000
                1             658981000
                2            1010270000
                3            1591500000
       2007     0             319602000
                1             790670000
                2            1250530000
                3            1711240000

In [49]: RPTgroupby(level=['STK_ID'  'RPT_Year'])['sales']apply(sale_per_q)
Out[49]: 
STK_ID  RPT_Year  RPT_Quarter
876     2006      0               798627000
                  1               857483000
                  2              1063590000
                  3               853960000
        2007      0               878415000
                  1              1146245000
                  2              1327970000
                  3              1439140000
600141  2006      0               270912000
                  1               388069000
                  2               351289000
                  3               581230000
        2007      0               319602000
                  1               471068000
                  2               459860000
                  3               460710000
",1548051.0,,1548051.0,,2012-08-30 10:50:54,2012-08-30 10:50:54,5.0
13034790,2,13032834,2012-10-23 16:09:21,1,"@mgilson gave a good no-extra-parts required solution (+1)  I see that pandas is tagged too  so for completeness I'll give a pandas equivalent:

import pandas as pd

df = pdread_csv(""mergecsv"" delimiter=r""\s*"")

key_cols = [""col1""  ""col2""]
df[key_cols] = df[key_cols]apply(sorted  axis=1)

def join_strings(seq  key):
    vals = [term for entry in seq for term in entrysplit(' ')]
    return ' 'join(sorted(set(vals)  key=key))

new_df = dfgroupby(key_cols)agg({""col3"": lambda x: join_strings(x  int) 
                                   ""col4"": lambda x: join_strings(x  str)})
new_dfto_csv(""postmergedcsv"")


which produces

In [173]: !cat postmergedcsv
col1 col2 col3 col4
a a 4 genetic
a c ""1 2 3"" ""genetic physical""
b c 22 physical
b d ""33 44"" genetic
c e ""1 2"" ""genetic physical xxxxx""
c f ""3 33 44"" ""genetic physical""


All this does is (1) sort the first two columns so that e c becomes c e  (2) group the terms by col and col 2  and then aggregate (agg) col3 and col4 by comma-joining the sorted set of the flattened terms

groupby is really handy for things like this  There might be a built-in replacement for the join_strings function lurking somewhere too  but I'm not sure",487339.0,,,,,2012-10-23 16:09:21,
13180215,2,13169723,2012-11-01 15:23:39,1,"I didn't find any mentions about deltatime in pandas  and datetime module has one  so to convert D_Time to seconds is not bad idea:

def seconds(time_str):
    end_time = datetimedatetimestrptime(time_str '%H:%M:%S')
    delta = end_time - datetimedatetimestrptime('0:0:0' '%H:%M:%S')
    return deltatotal_seconds()


df_SOF1D_Time = df_SOF1D_Timeapply(seconds)


result :

>>> df_SOF1groupby('CostCentre')sum()
            Location  D_Time  count
CostCentre                         
0                 45   27180     69
2073             420  594660     21


moving datetimedatetimestrptime('0:0:0' '%H:%M:%S') to global namespace can reduce exec time:

timeittimeit(""sec('01:01:01')""  setup=""from __main__ import sec"" 
              number=10000)
1025843858718872

timeittimeit(""seconds('01:01:01')""  setup=""from __main__ import seconds"" 
              number=10000)
06128969192504883 
",1777562.0,,,,,2012-11-01 15:23:39,7.0
13386916,2,13384795,2012-11-14 20:39:03,0,"You could use a combination of the two Seriesresample() parameters loffset= and closed=

For example:

In [1]: import numpy as np  pandas as pd

In [2]: data = nparange(1  13)

In [3]: s = pdSeries(data  pddate_range(start='1/31/2007'  periods=len(data)  freq='M'))

In [4]: sresample('6M'  how='sum'  closed='left'  loffset='-1M')
Out[4]: 
2007-06-30    21
2007-12-31    57


I used loffset='-1M' to tell pandas to aggregate one period earlier than its default (moved us to Jan-Jun) 

I used closed='left' to make the aggregator include the 'left' end of the sample window and exclude the 'right' end (closed='right' is the default behavior)

NOTE: I used how='sum' just to make sure it was doing what I thought You can use any of the appropriate how's ",1742701.0,,,,,2012-11-14 20:39:03,4.0
13655271,2,13654699,2012-12-01 00:51:12,2,"You could use pdto_datetime:

In [1]: import pandas as pd

In [2]: pdto_datetime('2008-02-27')
Out[2]: datetimedatetime(2008  2  27  0  0)


This allows you to ""clean"" the index (or similarly a column) by applying it to the Series:

dfindex = pdto_datetime(dfindex)


or

df['date_col'] = df['date_col']apply(pdto_datetime)
",1240268.0,,,,,2012-12-01 00:51:12,0.0
13062357,2,13062300,2012-10-25 05:49:36,4,"You cannot sort a dict because dictionary has no ordering 

Instead  use collectionsOrderedDict:

>>> from collections import OrderedDict
>>> d = {'Gears of war 3': 6  'Batman': 5  'gears of war 3': 4  'Rocksmith': 5  'Madden': 3}

>>> od = OrderedDict(sorted(ditems()  key=lambda x:x[1]  reverse=True))
>>> od
OrderedDict([('Gears of war 3'  6)  ('Batman'  5)  ('gears of war 3'  4)  ('Rocksmith'  5)  ('Madden'  3)])

>>> odkeys()
['Gears of war 3'  'Batman'  'gears of war 3'  'Rocksmith'  'Madden']
>>> odvalues()
[6  5  4  5  3]
>>> od['Batman']
5


The ""order"" you see in an JSON object is not meaningful  as JSON object is unordered[RFC4267] 

If you want meaningful ordering in your JSON  you need to use a list (that's sorted the way you wanted) Something like this is what you'd want:

{
  ""count"": 24 
  ""top 5"": [
    {""Gears of war 3"": 6} 
    {""Batman"": 5} 
    {""Rocksmith"": 5} 
    {""gears of war 3"": 4} 
    {""Madden"": 3}
  ]
}


Given the same dict d  you can generate a sorted list (which is what you want) by:

>>> l = sorted(ditems()  key=lambda x:x[1]  reverse=True)
>>> l
[('Gears of war 3'  6)  ('Batman'  5)  ('Rocksmith'  5)  ('gears of war 3'  4)  ('Madden'  3)]


Now you just pass l to m['top5'] and dump it:

m[""Top 5""]= l
k = jsondumps(m)
",853611.0,,853611.0,,2012-10-25 07:08:36,2012-10-25 07:08:36,5.0
13228149,2,13227865,2012-11-05 08:05:55,1,"I would do this:-

new_array = nparray(dfindexto_pydatetime()  dtype=numpydatetime64)


using the to_pydatetime() method",482506.0,,,,,2012-11-05 08:05:55,
13319626,2,13227865,2012-11-10 05:42:40,1,The data inside is of datetime64 dtype (datetime64[ns] to be precise) Just take the values attribute of the index Note it will be nanosecond unit ,776560.0,,,,,2012-11-10 05:42:40,
13446880,2,13446791,2012-11-19 02:25:26,3,"I believe this will do it:

from scipy import stats
dgroupby('City')agg(lambda x: statsmode(x['Borough'])[0])


This gives you a DataFrame with the City as the index and the most frequent borough in the Borough column:

>>> d
         City      Borough
0    Brooklyn     Brooklyn
1     Astoria       Queens
2     Astoria       Queens
3     Astoria     Brooklyn
4     Astoria  Unspecified
5   Ridgewood  Unspecified
6   Ridgewood       Queens
7   Ridgewood       Queens
8   Ridgewood     Brooklyn
9   Ridgewood     Brooklyn
10  Ridgewood     Brooklyn
>>> dgroupby('City')agg(lambda x: statsmode(x['Borough'])[0])
             Borough
City               
Astoria      Queens
Brooklyn   Brooklyn
Ridgewood  Brooklyn


(If you don't have scipy installed you'll have to make your own ""mode"" function  which I guess you could do using collectionsCounter  But if you're using pandas it's a good bet you've got Scipy as well)",1427416.0,,,,,2012-11-19 02:25:26,6.0
13777063,2,13773777,2012-12-08 11:34:11,1,"If you use a pandas Series rather than a list  you can use its diff method:

s = Series(uni_index)
sdiff()


For example:

In [45]: s
Out[45]: 
0    2012-02-01 10:00:00
1    2012-02-01 10:01:00
2    2012-02-01 10:02:00
3    2012-02-01 10:03:00

In [46]: sdiff()
Out[46]: 
0        NaN
1    0:01:00
2    0:01:00
3    0:01:00
",1240268.0,,,,,2012-12-08 11:34:11,1.0
13984585,2,13984461,2012-12-21 05:08:00,1,Prior to 0100  pandas labeled resample bins with the right-most edge  which for daily resampling  is the next day  Starting with 0100  the default binning behavior for daily and higher frequencies changed to label='left'  closed='left' to minimize this confusion  See http://pandaspydataorg/pandas-docs/stable/whatsnewhtml#api-changes for more information,243434.0,,243434.0,,2012-12-21 05:16:00,2012-12-21 05:16:00,
12188163,2,11945897,2012-08-30 00:02:38,0,I've created an issue here: http://githubcom/pydata/pandas/issues/1825 I was just working on this stuff recently and apparently broke it (unless it works on git master now),776560.0,,,,,2012-08-30 00:02:38,1.0
12098586,2,12096252,2012-08-23 19:20:12,2,"This is indeed a duplicate of how to filter the dataframe rows of pandas by ""within""/""in""?  translating the response to your example gives:

In [5]: df = DataFrame({'A' : [5 6 3 4]  'B' : [1 2 3  5]})

In [6]: df
Out[6]:
   A  B
0  5  1
1  6  2
2  3  3
3  4  5

In [7]: df[df['A']isin([3  6])]
Out[7]:
   A  B
1  6  2
2  3  3
",1548051.0,,,,,2012-08-23 19:20:12,
12772652,2,12772498,2012-10-07 20:35:37,0,Arg  DataMatrix is the same as DataFrame There is a 'load' and a 'save' function that seems to work  I guess the best way to search the docs is using the interactive prompt with ipython ,287238.0,,,,,2012-10-07 20:35:37,
12783604,2,12772498,2012-10-08 14:18:01,1,"Can you instead get PyTables (installation) working on all platforms relevant to you?

In that case you can use directly the HDFStore class in Pandas to serialize Panels and DataFrames  potentially with compression  to HDF5 ",54567.0,,,,,2012-10-08 14:18:01,2.0
13119196,2,13105505,2012-10-29 09:49:48,2,"You need to ""flatten"" the dictionaries contained in resultsrows In your case  results[n] ( where n is a zero based index representing an individual ""record"" ) is a dict that contains nested dicts ( for keys name and headshot )

Flattening of dicts has been discussed in detail in this question and its linked questions

One possible approach:

import collections

def flatten(d  parent_key=''):
    items = []
    for k  v in ditems():
        new_key = parent_key + '_' + k if parent_key else k
        if isinstance(v  collectionsMutableMapping):
            itemsextend(flatten(v  new_key)items())
        else:
            itemsappend((new_key  v))
    return dict(items)

flattened_records = [flatten(record) for record in resultsrows]
df = DataFrame(flattened_records)


Note that  with this approach  the keys of the nested columns will be derived by concatenating the ""parent"" key with the key in the nested dict eg ""name_first""  ""name_last"" You can customize the flatten method to change that

More than one approach can be used here The key insight is that you need to flatten the dictionaries contained in resultsrows",886608.0,,886608.0,,2012-10-30 08:14:41,2012-10-30 08:14:41,1.0
13250472,2,13250046,2012-11-06 11:53:58,0,"I don't think you can specify a column type the way you want (if there haven't been changes reciently and if the 6 digit number is not a date that you can convert to datetime) You could try using npgenfromtxt() and create the DataFrame from there

EDIT: Take a look at  Wes Mckinney's blog  there might be something for you It seems to be that there is a new parser from pandas 010 coming in November",1199589.0,,1199589.0,,2012-11-06 12:29:29,2012-11-06 12:29:29,5.0
13475127,2,13462802,2012-11-20 14:15:25,0,"Currently I think you need to create a custom subclass You'd need to override the apply and onOffset methods to take into account your holiday calendar

We should add an optional holiday calendar parameter in the business-X frequencies eventually though I made a github issue to keep track of it: https://githubcom/pydata/pandas/issues/2301",1306530.0,,,,,2012-11-20 14:15:25,1.0
13482165,2,13478597,2012-11-20 21:07:57,1,"Your error was the syntax  which although one might hope it would work  it doesn't:

dataCreated_Dateday - dataCreated_Dateday
AttributeError: 'Series' object has no attribute 'day'


With more complicated selections like this one you can use apply:

In [111]: df['sub'] = dfapply(lambda x: x['Created_Date']day - x['Closed_Date']day  axis=1)

In [112]: df[['Created_Date' 'Closed_Date' 'sub']]
Out[112]: 
         Created_Date         Closed_Date  sub
0 2009-01-07 00:00:00 2009-01-01 00:00:00    6
1                 NaT 2009-01-01 00:00:00    9
2 2009-01-01 00:00:00 2009-01-01 00:00:00    0
3 2009-01-01 00:00:00 2009-01-01 00:00:00    0
4 2009-01-01 00:00:00 2009-01-01 00:00:00    0
5 2009-01-12 00:00:00 2009-01-01 00:00:00   11
6 2009-01-12 00:00:00 2009-01-01 00:00:00   11
7 2009-01-07 00:00:00 2009-01-01 00:00:00    6
8 2009-01-10 00:00:00 2009-01-01 00:00:00    9
9 2009-01-07 00:00:00 2009-01-01 00:00:00    6


Be wary  you'll probably ought to do something separately with these NaTs:

In [114]: dfix[1][1]day # NaTday
Out[114]: -1


 

Note: there is similarly strange behaviour using days on a timedelta with NaT:

In [115]: df['sub2'] = dfapply(lambda x: (x['a'] - x['b'])days  axis=1)

In [116]: df['sub2'][1]
Out[116]: 92505
",1240268.0,,,,,2012-11-20 21:07:57,1.0
13851602,2,13851535,2012-12-13 01:37:01,4,"When you do len(df['column name']) you are just getting one number  namely the number of rows in the DataFrame (ie  the length of the column itself)  If you want to apply len to each element in the column  use df['column name']map(len)  So try

df[df['column name']map(len) < 2]
",1427416.0,,,,,2012-12-13 01:37:01,1.0
14125409,2,14125172,2013-01-02 16:19:22,4,"You can use the reindex() method and pass the padded index which you created
The default fill_value is NaN

In [1]: ix1 = [0  1  2  3  4  9]

In [2]: ix1
Out[2]: [0  1  2  3  4  9]

In [3]: ix2 = range(10)

In [4]: ix2
Out[4]: [0  1  2  3  4  5  6  7  8  9]

In [5]: s = Series(ix1  index=ix1)

In [6]: s
Out[6]:
0    0
1    1
2    2
3    3
4    4
9    9

In [7]: sreindex(ix2)
Out[7]:
0     0
1     1
2     2
3     3
4     4
5   NaN
6   NaN
7   NaN
8   NaN
9     9

In [8]: Seriesreindex()?

Docstring:
Conform Series to new index with optional filling logic  placing
NA/NaN in locations having no value in the previous index A new object
is produced unless the new index is equivalent to the current one and
copy=False
",919872.0,,1240268.0,,2013-01-02 18:47:59,2013-01-02 18:47:59,
14363721,2,14363640,2013-01-16 17:01:31,5,"You can use the DataFrame drop function to remove columns  You have to pass the axis=1 option for it to work on columns and not rows Note that it returns a copy so you have to assign the result to a new DataFrame:

In [1]: from pandas import *

In [2]: df = DataFrame(dict(x=[0 0 1 0 1]  y=[1 0 1 1 0]  z=[0 0 1 0 1]))

In [3]: df
Out[3]:
   x  y  z
0  0  1  0
1  0  0  0
2  1  1  1
3  0  1  0
4  1  0  1

In [4]: df = dfdrop(['x' 'y']  axis=1)

In [5]: df
Out[5]:
   z
0  0
1  0
2  1
3  0
4  1
",919872.0,,,,,2013-01-16 17:01:31,1.0
14363758,2,14363640,2013-01-16 17:03:39,4,"Basically the same as Zelazny7's answer -- just specifying what to keep:

In [68]: df
Out[68]: 
   x  y  z
0  0  1  0
1  0  0  0
2  1  1  1
3  0  1  0
4  1  0  1

In [70]: df = df[['x' 'z']]                                                                

In [71]: df
Out[71]: 
   x  z
0  0  0
1  0  0
2  1  1
3  0  0
4  1  1
",733291.0,,,,,2013-01-16 17:03:39,2.0
13040754,2,13040312,2012-10-23 23:16:18,1,"Some setup: 

In [1]: import numpy as np
In [2]: import pandas as pd
In [3]: from datetime import datetime
In [4]: dates = [datetime(2011  1  2)  datetime(2011  1  5)  datetime(2011  1  7)  datetime(2011  1  8)  datetime(2011  1  10)  datetime(2011  1  12)]

In [5]: ts = pdSeries(nprandomrandn(6)  index=dates)

In [6]: ts
Out[6]: 
2011-01-02   -0412335
2011-01-05   -0809092
2011-01-07   -0442320
2011-01-08   -0337281
2011-01-10    0522765
2011-01-12    1559876


Okay  now to answer your first question  a) yes  there are less clunky ways  depending on your intention This is pretty simple: 

In [9]: ts[datetime(2011  1  8):]
Out[9]: 
2011-01-08   -0337281
2011-01-10    0522765
2011-01-12    1559876


This is a slice containing all the values after your chosen date You can select just the first one  as you wanted  by: 

In [10]: ts[datetime(2011  1  8):][0]
Out[10]: -033728079849770815


To your second question  (b) -- this type of indexing is a slice of the original  just as other numpy arrays It is NOT a copy of the original See this question  or many similar:
Bug or feature: cloning a numpy array w/ slicing

To demonstrate  let's modify the slice:

In [21]: ts2 = ts[datetime(2011  1  8):]
In [23]: ts2[0] = 99


This changes the original timeseries object ts  since ts2 is a slice and not a copy 

In [24]: ts
Out[24]: 
2011-01-02    -0412335
2011-01-05    -0809092
2011-01-07    -0442320
2011-01-08    99000000
2011-01-10     0522765
2011-01-12     1559876


If you DO want a copy  you can (in general) use the copy method or  (in this case) use truncate: 

In [25]: ts3 = tstruncate(before='2011-01-08')

In [26]: ts3  
Out[26]: 
2011-01-08    99000000
2011-01-10     0522765
2011-01-12     1559876


Changing this copy will not change the original 

In [27]: ts3[1] = 99

In [28]: ts3
Out[28]: 
2011-01-08    99000000
2011-01-10    99000000
2011-01-12     1559876

In [29]: ts                #The january 10th value will be unchanged 
Out[29]: 
2011-01-02    -0412335
2011-01-05    -0809092
2011-01-07    -0442320
2011-01-08    99000000
2011-01-10     0522765
2011-01-12     1559876


This example is straight out of ""Python for Data Analysis"" by Wes Check it out It's great ",484596.0,,,,,2012-10-23 23:16:18,
13190930,2,13188114,2012-11-02 07:15:40,0,"Im not aware of a specific Panda function for this  but you could consider the nproll() function:

myindex = nparange(1 367)
myindex = nproll(myindex  int(len(myindex)/2))
",1755432.0,,,,,2012-11-02 07:15:40,
13197520,2,13188114,2012-11-02 14:46:13,1,"Why not just reindex the result?

In [7]: myminreindex(myindex)
Out[7]: 
184   -0788140
185   -2206314
186    0284884
187   -2197727
188   -0714634
189   -1082745
190   -0789286
191   -1489837
192   -1278941
193   -0795507
194   -0661476
195    0582994
196   -1634310
197    0104332
198   -0602378

169   -1150616
170   -0315325
171   -2233139
172   -1081528
173   -1316668
174   -0963783
175   -0215260
176   -2723446
177   -0493480
178   -0706771
179   -2082051
180   -1066649
181   -1455419
182   -0332383
183   -1277424
",776560.0,,,,,2012-11-02 14:46:13,1.0
13389659,2,13389203,2012-11-15 00:16:53,0,"not sure if this is ideal but it works by creating a mask

In [59]: sindex
Out[59]: 
MultiIndex
[('a'  0) ('a'  1) ('a'  5) ('b'  0) ('b'  1) ('b'  2) ('b'  4)
 ('b'  50) ('c'  0)]
In [77]: s[(tpl for tpl in sindex if 2<=tpl[1]<=10 and tpl[0]=='b')]                                                               
Out[77]: 
b  2   -0586568
   4    1559988


EDIT : hayden's solution is the way to go",239007.0,,239007.0,,2012-11-15 00:39:37,2012-11-15 00:39:37,2.0
14066692,2,14043958,2012-12-28 08:24:23,0,"One way to vectorize is access the values in Series P1 and P2 by indexing with an array of labels

In [20]: df = Xreset_index()

In [21]: mP1 = P1[dfP1]values

In [22]: mP2 = P2[dfP2]values

In [23]: mP1
Out[23]: array([ 04   04   06   06])

In [24]: mP2
Out[24]: array([ 07   03   07   03])

In [25]: mp = mP1 * mP2

In [26]: mp
Out[26]: array([ 028   012   042   018])

In [27]: Xmul(mp  axis=0)
Out[27]: 
       A      B
P1 P2              
1  1   0056  0224
   2   0060  0060
2  1   0378  0042
   2   0162  0018

In [28]: Xmul(mp  axis=0)sum()
Out[28]: 
A    0656
B    0344

In [29]: sum(
    sum(
    Xxs(i  level=""P1"")*P1[i]
    for i in P1index
    )xs(j)*P2[j]
    for j in P2index
    )
Out[29]: 
A    0656
B    0344


(Alternately  access the values of a MultiIndex 
without resetting the index as follows)

In [38]: P1[Xindexget_level_values(""P1"")]values
Out[38]: array([ 04   04   06   06])
",243434.0,,,,,2012-12-28 08:24:23,
14207893,2,14206217,2013-01-08 03:44:25,0,You should build a list of the pieces and concatenate them all in one shot at the end ,776560.0,,,,,2013-01-08 03:44:25,
14345875,2,14345739,2013-01-15 20:02:32,3,"strip only removes the specified characters at the beginning and end of the string  If you want to remove all \n  you need to use replace

misc['product_desc'] = misc['product_desc']strreplace('\n'  '')
",1427416.0,,,,,2013-01-15 20:02:32,
14658413,2,14650341,2013-02-02 03:55:38,1,"I think this will work (and what Panelwhere should do  but its a bit non-trivial because it
has to handle a bunch of cases)

# construct the mask in 2-d (a frame)
In [36]: mask = (pn['close']>0) & (pn['rate']>0)

In [37]: mask
Out[37]: 
ticker                AAPL   GOOG     GS
2009-03-01 06:29:59  False  False  False
2009-03-02 06:29:59  False  False   True


# here's the key  this broadcasts  setting the values which 
# don't meet the condition to nan
In [38]: masked_values = npwhere(mask pnvalues npnan)

# reconstruct the panel (the _construct_axes_dict is an internal function that returns
# dict of the axes  eg items -> the items  major_axis -> 
In [42]: x = pdPanel(masked_values **pn_construct_axes_dict())
Out[42]: 
<class 'pandascorepanelPanel'>
Dimensions: 2 (items) x 12 (major_axis) x 3 (minor_axis)
Items axis: close to rate
Major_axis axis: 2009-03-01 06:29:59 to 2009-03-12 06:29:59
Minor_axis axis: AAPL to GS

# the values
In [43]: x
Out[43]: 
array([[[        nan          nan          nan] 
    [        nan          nan   009575723] 
    [        nan          nan          nan] 
    [        nan          nan          nan] 
    [        nan   207229823   004347515] 
    [        nan          nan          nan] 
    [        nan          nan   218342239] 
    [        nan          nan   173674381] 
    [        nan   201173087          nan] 
    [ 024109645   094583072          nan] 
    [ 036953467          nan   018044432] 
    [ 174164222   102314752   173736033]] 

   [[        nan          nan          nan] 
    [        nan          nan   006960387] 
    [        nan          nan          nan] 
    [        nan          nan          nan] 
    [        nan   063202199   056724391] 
    [        nan          nan          nan] 
    [        nan          nan   071964824] 
    [        nan          nan   103482927] 
    [        nan   018256148          nan] 
    [ 129451667   049804327          nan] 
    [ 204726538          nan   012883128] 
    [ 070647885   07277734    077844475]]])
",644898.0,,,,,2013-02-02 03:55:38,2.0
9918868,2,9877391,2012-03-29 03:54:52,4,"You might want to go directly do the index:

i = frameindexsearchsorted(date)
frameix[frameindex[i]]


A touch verbose but you could put it in a function About as good as you'll get (O(log n))",776560.0,,,,,2012-03-29 03:54:52,
13181960,2,13167391,2012-11-01 17:00:54,2,"If you still need a workaround:

In [49]: pdconcat([group for _  group in grouped if len(group) > 1])
Out[49]: 
     A  B
0  foo  0
2  foo  2
3  foo  3
",1306530.0,,,,,2012-11-01 17:00:54,2.0
13486429,2,13378490,2012-11-21 04:37:55,1,I'd recommend starting with the free EPDFree (does not include pandas  but can be easy_install'd from source) or Anaconda CE (which includes pandas) distributions if you can't get set up with MacPorts,776560.0,,,,,2012-11-21 04:37:55,
13606898,2,13606487,2012-11-28 14:04:03,2,"You can use groupby (I have called your dataframe df):

dfgroupby(['p1'  'p2'])mean()


This results in a MultiIndex DataFrame To get the layout in your question  select only the columns you want and reset the index:

dfgroupby(['p1'  'p2'])mean()['result']reset_index()
",1452002.0,,,,,2012-11-28 14:04:03,
13977244,2,13926089,2012-12-20 17:16:15,5,"You can store the dataframe with an index of the columns as follows:

import pandas as pd
import numpy as np
from pandasiopytables import Term

index = pddate_range('1/1/2000'  periods=8)
df = pdDataFrame( nprandomrandn(8 3)  index=index  columns=list('ABC'))  

store = pdHDFStore('mydatah5')
storeappend('df_cols'  df  axes='columns')


and then select as you might hope:

In [8]: storeselect('df_cols'  [Term('columns'  '='  'A')])
Out[8]: 
2000-01-01    0347644
2000-01-02    0477167
2000-01-03    1419741
2000-01-04    0641400
2000-01-05   -1313405
2000-01-06   -0137357
2000-01-07   -1208429
2000-01-08   -0539854


Where:

In [9]: df
Out[9]: 
                   A         B         C
2000-01-01  0347644  0895084 -1457772
2000-01-02  0477167  0464013 -1974695
2000-01-03  1419741  0470735 -0309796
2000-01-04  0641400  0838864 -0112582
2000-01-05 -1313405 -0678250 -0306318
2000-01-06 -0137357 -0723145  0982987
2000-01-07 -1208429 -0672240  1331291
2000-01-08 -0539854 -0184864 -1056217




To me this isn't an ideal solution  as we can only indexing the DataFrame by one thing! Worryingly the docs seem to suggest you can only index a DataFrame by one thing  at least using axes:


  Pass the axes keyword with a list of dimension (currently must by exactly 1 less than the total dimensions of the object)


I may be reading this incorrectly  in which case hopefully someone can prove me wrong!



Note: One way I have found to index a DataFrame by two things (index and columns)  is to convert it to a Panel  which can then retrieve using two indices However then we have to convert to the selected subpanel to a DataFrame each time items are retrieved again  not ideal",1240268.0,,,,,2012-12-20 17:16:15,2.0
13999234,2,13926089,2012-12-22 01:27:49,3,"The way HDFStore records tables  the columns are stored by type as single numpy arrays You always get back all of the columns  you can filter on them  so you will be returned for what you ask In 0100 you can pass a Term that involves columns

storeselect('df'  [ Term('index'  '>'  Timestamp('20010105'))  
                     Term('columns'  '='  ['A' 'B']) ])


or you can reindex afterwards

df = storeselect('df'  [ Term('index'  '>'  Timestamp('20010105') ])
dfreindex(columns = ['A' 'B'])


The axes is not really the solution here (what you actually created was in effect storing a transposed frame) This parameter allows you to re-order the storage of axes to enable data alignment in different ways For a dataframe it really doesn't mean much; for 3d or 4d structures  on-disk data alignment is crucial for really fast queries 

0101 will allow a more elegant solution  namely data columns  that is  you can elect certain columns to be represented as there own columns in the table store  so you really can select just them Here is a taste what is coming

 storeappend('df'  columns = ['A' 'B' 'C'])
 storeselect('df'  [ 'A > 0'  Term('index'  '>'  Timestamp(2000105)) ])


Another way to do go about this is to store separate tables in different nodes of the file  then you can select only what you need

In general  I recommend again really wide tables hayden offers up the Panel solution  which might be a benefit for you now  as the actual data arangement should reflect how you want to query the data",644898.0,,,,,2012-12-22 01:27:49,
13965098,2,13956564,2012-12-20 04:01:03,2,"How about interleaving inbound and outbound events into a single frame?

In [15]: df
Out[15]: 
                      date_in     aet                    date_out
0  2012-12-05 10:08:59318600  Z2XG17  2012-12-05 10:09:37172300
1  2012-12-05 10:08:59451300  Z2XG17  2012-12-05 10:09:38048800
2  2012-12-05 10:08:59587400  Z2XG17  2012-12-05 10:09:39044100

In [16]: inbnd = pdDataFrame({'event': 1}  index=dfdate_in)

In [17]: outbnd = pdDataFrame({'event': -1}  index=dfdate_out)

In [18]: real_stream = pdconcat([inbnd  outbnd])sort()

In [19]: real_stream
Out[19]: 
                            event
date                             
2012-12-05 10:08:59318600      1
2012-12-05 10:08:59451300      1
2012-12-05 10:08:59587400      1
2012-12-05 10:09:37172300     -1
2012-12-05 10:09:38048800     -1
2012-12-05 10:09:39044100     -1


In this format (one decrement for every increment)  queue depth
can easily be computed with cumsum()

In [20]: real_stream['depth'] = real_streameventcumsum()

In [21]: real_stream
Out[21]: 
                            event  depth
date                                    
2012-12-05 10:08:59318600      1      1
2012-12-05 10:08:59451300      1      2
2012-12-05 10:08:59587400      1      3
2012-12-05 10:09:37172300     -1      2
2012-12-05 10:09:38048800     -1      1
2012-12-05 10:09:39044100     -1      0


To simulate different consumption rates  replace all real outbound timestamps
with a manufactured series of outbound timestamps at a fixed frequency  Since cumsum() function won't work in this case  I created a counting function
that takes a floor value

In [53]: outbnd_1s = pdDataFrame({'event': -1} 
   :                          index=real_streameventresample(""S"")index)

In [54]: fixed_stream = pdconcat([inbnd  outbnd_1s])sort()

In [55]: def make_floor_counter(floor):
   :     count = [0]
   :     def process(n):
   :         count[0] += n
   :         if count[0] < floor
   :             count[0] = floor
   :         return count[0]
   :     return process
   : 

In [56]: fixed_stream['depth'] = fixed_streameventmap(make_floor_counter(0))

In [57]: fixed_streamhead(8)
Out[57]: 
                            event  depth
2012-12-05 10:08:59            -1      0
2012-12-05 10:08:59318600      1      1
2012-12-05 10:08:59451300      1      2
2012-12-05 10:08:59587400      1      3
2012-12-05 10:09:00            -1      2
2012-12-05 10:09:01            -1      1
2012-12-05 10:09:02            -1      0
2012-12-05 10:09:03            -1      0
",243434.0,,,,,2012-12-20 04:01:03,1.0
14089046,2,14089039,2012-12-30 07:27:43,1,"I don't have the book  but it looks like you left out a comma:

frame = pdDataFrame(nparange(12)reshape((4  3))  index = list('aabb')  columns = [['Ohio'  'Ohio'  'Colorado']  ['Green'  'Red'  'Green']])


Note the comma between 'Colorado'] and ['Green'",1427416.0,,,,,2012-12-30 07:27:43,1.0
14309050,2,14298401,2013-01-13 22:41:47,0,"Currently  you can do this in a few steps with the built-in pandasmerge() and boolean indexing

merged = dfmerge(df2  on='key')

valid = (mergeddate >= mergedvalid_from) & \
        (mergeddate <= mergedvalid_to)

df['joined_value'] = merged[valid]value_y


(Note: the value column of df2 is accessed as value_y after the merge because it conflicts with a column of the same name in df and the default merge-conflict suffixes are _x  _y for the left and right frames  respectively)

Here's an example  with a different setup to show how invalid dates are handled

n = 8
dates = pddate_range('1/1/2013'  freq='D'  periods=n)
df = DataFrame({'key': nparange(n) 
                'date': dates 
                'value': nparange(n) * 10})
df2 = DataFrame({'key': nparange(n) 
                 'valid_from': dates[[1 1 1 1 5 5 5 5]] 
                 'valid_to': dates[[4 4 4 4 6 6 6 6]] 
                 'value': nparange(n) * 100})


Input df2:

   key          valid_from            valid_to  value
0    0 2013-01-02 00:00:00 2013-01-05 00:00:00      0
1    1 2013-01-02 00:00:00 2013-01-05 00:00:00    100
2    2 2013-01-02 00:00:00 2013-01-05 00:00:00    200
3    3 2013-01-02 00:00:00 2013-01-05 00:00:00    300
4    4 2013-01-06 00:00:00 2013-01-07 00:00:00    400
5    5 2013-01-06 00:00:00 2013-01-07 00:00:00    500
6    6 2013-01-06 00:00:00 2013-01-07 00:00:00    600
7    7 2013-01-06 00:00:00 2013-01-07 00:00:00    700


Intermediate frame merged:

                 date  key  value_x          valid_from            valid_to  value_y
0 2013-01-01 00:00:00    0        0 2013-01-02 00:00:00 2013-01-05 00:00:00        0
1 2013-01-02 00:00:00    1       10 2013-01-02 00:00:00 2013-01-05 00:00:00      100
2 2013-01-03 00:00:00    2       20 2013-01-02 00:00:00 2013-01-05 00:00:00      200
3 2013-01-04 00:00:00    3       30 2013-01-02 00:00:00 2013-01-05 00:00:00      300
4 2013-01-05 00:00:00    4       40 2013-01-06 00:00:00 2013-01-07 00:00:00      400
5 2013-01-06 00:00:00    5       50 2013-01-06 00:00:00 2013-01-07 00:00:00      500
6 2013-01-07 00:00:00    6       60 2013-01-06 00:00:00 2013-01-07 00:00:00      600
7 2013-01-08 00:00:00    7       70 2013-01-06 00:00:00 2013-01-07 00:00:00      700


Final value of df after adding column joined_value:

                 date  key  value  joined_value
0 2013-01-01 00:00:00    0      0           NaN
1 2013-01-02 00:00:00    1     10           100
2 2013-01-03 00:00:00    2     20           200
3 2013-01-04 00:00:00    3     30           300
4 2013-01-05 00:00:00    4     40           NaN
5 2013-01-06 00:00:00    5     50           500
6 2013-01-07 00:00:00    6     60           600
7 2013-01-08 00:00:00    7     70           NaN
",243434.0,,243434.0,,2013-01-13 23:25:31,2013-01-13 23:25:31,1.0
12998887,2,12995222,2012-10-21 14:46:13,1,"so Seriesadd returns the summed seriesI thought it just added it to the already existing Series object So I did this:

selfshares = selfsharesadd(shareSer  fill_value=0)


instead of 

selfsharesadd(shareSer  fill_value=0)


and it works",1513413.0,,,,,2012-10-21 14:46:13,
13158109,2,13156660,2012-10-31 12:36:22,2,"df = DataFrame({'event': range(1 4)  'unixtime': [1346617885925  1346961625305 1347214217566]})
df['day'] = df['unixtime']apply(lambda x: datetimefromtimestamp(x/1000)date())
df['arbday']=df['day']map(lambda x: (x-dfget_value(df[dfevent == 1]first_valid_index()  'day'))days+1)
print df


output:

   event       unixtime         day  arbday
0      1  1346617885925  2012-09-02       1
1      2  1346961625305  2012-09-06       5
2      3  1347214217566  2012-09-09       8
",1199589.0,,1199589.0,,2012-10-31 15:22:24,2012-10-31 15:22:24,6.0
13370512,2,13369684,2012-11-13 23:13:27,2,Here's a good explanation and simple comparison between pandas and numpy record arrays - Normalize/Standardize a numpy recarray,482506.0,,,,,2012-11-13 23:13:27,
13581730,2,13575090,2012-11-27 10:05:17,3,"Up to two level nesting you can use pdDataFramefrom_dict()  for three level nesting i used two steps

In [57]: user_dict
Out[57]:
{12: {'Category 1': {'att_1': 1  'att_2': 'whatever'} 
  'Category 2': {'att_1': 23  'att_2': 'another'}} 
 15: {'Category 1': {'att_1': 10  'att_2': 'foo'} 
  'Category 2': {'att_1': 30  'att_2': 'bar'}}}

In [58]: user_ids = []

In [59]: frames = []

In [60]: for user_id  d in user_dictiteritems():
   :     user_idsappend(user_id)
   :     framesappend(pdDataFramefrom_dict(d  orient='index'))

In [61]: pdconcat(frames  keys=user_ids)
Out[61]:
               att_1     att_2
12 Category 1      1  whatever
   Category 2     23   another
15 Category 1     10       foo
   Category 2     30       bar
",1548051.0,,,,,2012-11-27 10:05:17,1.0
13893632,2,13893227,2012-12-15 15:47:22,15,"Use our friend lookup  designed precisely for this purpose:

In [17]: prices
Out[17]: 
              AAPL    GOOG     IBM    XOM
2011-01-10  33944  61421  14278  7157
2011-01-13  34264  61669  14392  7308
2011-01-26  34082  61650  15574  7589
2011-02-02  34129  61200  15793  7946
2011-02-10  35142  61644  15932  7968
2011-03-03  35640  60956  15873  8219
2011-05-03  34514  53389  16784  8200
2011-06-03  34042  52308  16097  7819
2011-06-10  32303  50951  15914  7684
2011-08-01  39326  60677  17628  7667
2011-12-20  39246  63037  18414  7997

In [18]: orders
Out[18]: 
                  Date direction  size ticker  prices
0  2011-01-10 00:00:00       Buy  1500   AAPL  33944
1  2011-01-13 00:00:00      Sell  1500   AAPL  34264
2  2011-01-13 00:00:00       Buy  4000    IBM  14392
3  2011-01-26 00:00:00       Buy  1000   GOOG  61650
4  2011-02-02 00:00:00      Sell  4000    XOM   7946
5  2011-02-10 00:00:00       Buy  4000    XOM   7968
6  2011-03-03 00:00:00      Sell  1000   GOOG  60956
7  2011-03-03 00:00:00      Sell  2200    IBM  15873
8  2011-06-03 00:00:00      Sell  3300    IBM  16097
9  2011-05-03 00:00:00       Buy  1500    IBM  16784
10 2011-06-10 00:00:00       Buy  1200   AAPL  32303
11 2011-08-01 00:00:00       Buy    55   GOOG  60677
12 2011-08-01 00:00:00      Sell    55   GOOG  60677
13 2011-12-20 00:00:00      Sell  1200   AAPL  39246

In [19]: priceslookup(ordersDate  ordersticker)
Out[19]: 
array([ 33944   34264   14392   6165     7946    7968   60956 
        15873   16097   16784   32303   60677   60677   39246])
",776560.0,,,,,2012-12-15 15:47:22,2.0
14076064,2,14075326,2012-12-28 21:31:00,0,"One workaround is to sort before plotting:

dfsort()plot()


It looks like a bug  so I posted it on github!

Note: this seems to plot ticks better if you use datetime rather than date:

df1 = DataFrame(randn(3 1)  index=[datetime(2012 10 1)  datetime(2012 9 1)  datetime(2012 8 1)]  columns=['test'])
",1240268.0,,1240268.0,,2012-12-28 21:40:55,2012-12-28 21:40:55,1.0
13793980,2,13793805,2012-12-10 01:56:35,1,"Pandas read_csv has a parse_dates argument  which will process date strings into datetimes (and hence they will sort properly):

pdread_csv('__csv'  parse_dates={'datetime': [0 1]})set_index('datetime')
           # where 0 and 1 are the columns for the date and time


The for loops and appends will not be efficient and should be avoided Try rewrting these using numpy functions and/or the DataFrame apply method",1240268.0,,,,,2012-12-10 01:56:35,1.0
14039589,2,14036397,2012-12-26 10:54:56,1,"You can resample the data to business month If you don't want the mean price (which is the default in resample) you can use a custom resample method using the keyword argument how:

In [31]: from pandasio import data as web

# read some example data  note that this is not exactly your data!
In [32]: s = webget_data_yahoo('AAPL'  start='2009-01-02' 
                             end='2009-12-31')['Adj Close']

# resample to business month and return the last value in the period
In [34]: monthly = sresample('BM'  how=lambda x: x[-1])

In [35]: monthly
Out[35]: 
Date
2009-01-30     8934
2009-02-27     8852
2009-03-31    10419

2009-10-30    18684
2009-11-30    19815
2009-12-31    20888
Freq: BM

In [36]: monthlypct_change()
Out[36]: 
Date
2009-01-30         NaN
2009-02-27   -0009178
2009-03-31    0177022

2009-10-30    0016982
2009-11-30    0060533
2009-12-31    0054151
Freq: BM
",1301710.0,,,,,2012-12-26 10:54:56,5.0
14169558,2,14167487,2013-01-05 07:10:49,3,"There doesn't seem to be a pandas method to compute time differences between entries of an irregular time series  though there is a convenience method to convert a time series index to an array of datetimedatetime objects  which can be converted to datetimetimedelta objects through subtraction

In [6]: start_end = pdDataFrame({'status': [0  0]} 
                                 index=[pddatetoolsparse('1/1/2012') 
                                        pddatetoolsparse('12/31/2012')])

In [7]: df = dfappend(start_end)sort()

In [8]: df
Out[8]: 
                     status
2012-01-01 00:00:00       0
2012-01-01 12:43:35       1
2012-03-12 15:46:43       0
2012-09-26 18:35:11       1
2012-11-11 02:34:59       0
2012-12-31 00:00:00       0

In [9]: pydatetime = pdSeries(dfindexto_pydatetime()  index=dfindex)

In [11]: df['duration'] = pydatetimediff()shift(-1)\
              map(datetimetimedeltatotal_seconds  na_action='ignore')

In [16]: df
Out[16]: 
                     status  duration
2012-01-01 00:00:00       0     45815
2012-01-01 12:43:35       1   6145388
2012-03-12 15:46:43       0  17117308
2012-09-26 18:35:11       1   3916788
2012-11-11 02:34:59       0   4310701
2012-12-31 00:00:00       0       NaN

In [17]: (dfstatus * dfduration)sum() / dfdurationsum()
Out[17]: 031906950786402843


Note:

Our answers seem to differ because I set status before the first timestamp to zero  while those entries are NA in your df1 as there's no start value to forward fill and NA values are excluded by pandas mean()
timedeltatotal_seconds() is new in Python 27
Timing comparison of this method versus reindexing:

In [8]: timeit delta_method(df)
1000 loops  best of 3: 13 ms per loop

In [9]: timeit redindexing(df)
1 loops  best of 3: 278 s per loop

",243434.0,,243434.0,,2013-01-06 23:33:16,2013-01-06 23:33:16,
14345393,2,14341584,2013-01-15 19:29:30,1,"This looks like a bug in to_excel  for the moment as a workaround I would recommend using to_csv (which seems not to show this issue)

I added this as an issue on github

To answer the second question  if you really need to use to_excel

You can use filter to select only those columns which include '-ba':

In [21]: filter(lambda x: '-ba' in x  tab2columns)
Out[21]: ['east-ba'  'north-ba'  'south-ba']

In [22]: tab2[filter(lambda x: '-ba' in x  tab2columns)]
Out[22]: 
        east-ba  north-ba  south-ba
Gender                             
     f        1         0         1
     m        1         1         0
",1240268.0,,1240268.0,,2013-01-15 19:40:56,2013-01-15 19:40:56,1.0
13984636,2,13984461,2012-12-21 05:14:38,2,"What you are doing looks correct  it's just that pandas gives NaN for the mean of an empty array

In [1]: Series()mean()
Out[1]: nan


resample converts to a regular time interval  so if there are no samples that day you get NaN

Most of the time having NaN isn't a problem If it is we can either use fill_method (for example 'ffill') or if you really wanted to remove them you could use dropna (not recommended):

data_mresample('D'  how = mean  fill_method='ffill')
data_mresample('D'  how = mean)dropna()
",1240268.0,,,,,2012-12-21 05:14:38,
14148511,2,14144867,2013-01-03 22:47:21,4,"As of today  Google App Engine Python 27 runtime doesn't currently allow running arbitrary C extensions

But it does allow inclusion of third party library that have been hand selected by the engineering team based on their popularity and developer demand

You can request for a specific library to be included by filling a feature request on the public issue tracker If it gets enough  it is likely to be considered for inclusion in an upcoming release",656408.0,,656408.0,,2013-01-03 23:04:30,2013-01-03 23:04:30,2.0
14298829,2,14298447,2013-01-12 22:54:41,3,"Combine date and time into a single datetime index

df = pdread_csv(ioBytesIO(text)  parse_dates = [[0 1]]  header = None  index_col = 0)
print(df)
#                      2
# 0_1                   
# 2012-01-05 08:00:00  1
# 2012-01-05 08:00:30  2
# 2012-01-05 08:01:00  3
# 2012-01-05 08:01:30  4
# 2012-01-05 08:02:00  5

Use date as the primary index and time as the secondary index in a
multiindex dataframe

df2 = pdread_csv(ioBytesIO(text)  parse_dates = True  header = None  index_col = [0 1])
print(df2)
#                   2
# 0          1       
# 2012-01-05 80000  1
#            80030  2
#            80100  3
#            80130  4
#            80200  5

My naive inclination would be to prefer a single index over the multiindex 

As the Zen of Python asserts  ""Flat is better than nested""
The datetime is one conceptual object Treat it as such (It is better to have one datetime object than multiple columns for the year  month  day  hour  minute  etc Similarly  it is better to have one index rather than two)
However  I am not very experienced with Pandas  and there could be some advantage to having the multiindex when doing time-of-day analysis

I would try coding up some typical calculations both ways  and then see which one I liked better on the basis of ease of coding  readability  and performance

This was my setup to produce the results above 

import io
import pandas as pd

text = '''\
20120105  080000    1
20120105  080030    2
20120105  080100    3
20120105  080130    4
20120105  080200    5'''


You can of course use 

pdread_csv(filename  )


instead of 

pdread_csv(ioBytesIO(text)  )
",190597.0,,190597.0,,2013-01-13 01:47:40,2013-01-13 01:47:40,
14513351,2,14513006,2013-01-25 00:27:10,4,"You could do something like:

axlines[-1]set_linewidth(8)
pltshow()
",1240268.0,,,,,2013-01-25 00:27:10,1.0
8916746,2,8916302,2012-01-18 20:12:06,8,"I encourage you to pose these questions on the mailing list  but in any case  it's still a very much low level affair working with the underlying NumPy arrays For example  to select rows where the value in any column exceed  say  15 in this example:

In [11]: df
Out[11]: 
            A        B        C        D      
2000-01-03 -059885 -018141 -068828 -077572
2000-01-04  083935  015993  095911 -112959
2000-01-05  280215 -010858 -162114 -020170
2000-01-06  071670 -026707  136029  174254
2000-01-07 -045749  022750  046291 -058431
2000-01-10 -078702  044006 -036881 -013884
2000-01-11  079577 -009198  014119  002668
2000-01-12 -032297  062332  193595  078024
2000-01-13  174683 -157738 -002134  011596
2000-01-14 -055613  092145 -022832  156631
2000-01-17 -055233 -028859 -118190 -080723
2000-01-18  073274  024387  088146 -094490
2000-01-19  056644 -049321  117584 -017585
2000-01-20  156441  062331 -026904  011952
2000-01-21  061834  017463 -162439  099103
2000-01-24  086378 -068111 -015788 -016670
2000-01-25 -112230 -016128  120401  108945
2000-01-26 -063115  076077 -092795 -217118
2000-01-27  137620 -110618 -037411  073780
2000-01-28 -140276  198372  147096 -138043
2000-01-31  054769  044100 -052775  084497
2000-02-01  012443  032880 -071361  131778
2000-02-02 -028986 -063931  088333 -258943
2000-02-03  054408  117928 -026795 -051681
2000-02-04 -007068 -129168 -059877 -145639
2000-02-07 -065483 -029584 -002722  031270
2000-02-08 -018529 -018701 -059132 -115239
2000-02-09 -228496  036352  111596  002293
2000-02-10  051054  097249  174501  020525
2000-02-11  010100  027722  065843  173591

In [12]: df[(dfvalues > 15)any(1)]
Out[12]: 
            A       B       C        D     
2000-01-05  28021 -01086 -162114 -02017
2000-01-06  07167 -02671  136029  17425
2000-01-12 -03230  06233  193595  07802
2000-01-13  17468 -15774 -002134  01160
2000-01-14 -05561  09215 -022832  15663
2000-01-20  15644  06233 -026904  01195
2000-01-28 -14028  19837  147096 -13804
2000-02-10  05105  09725  174501  02052
2000-02-11  01010  02772  065843  17359


Multiple conditions have to be combined using & or | (and parentheses!):

In [13]: df[(df['A'] > 1) | (df['B'] < -1)]
Out[13]: 
            A        B       C        D     
2000-01-05  280215 -01086 -162114 -02017
2000-01-13  174683 -15774 -002134  01160
2000-01-20  156441  06233 -026904  01195
2000-01-27  137620 -11062 -037411  07378
2000-02-04 -007068 -12917 -059877 -14564


I'd be very interested to have some kind of query API to make these kinds of things easier",776560.0,,,,,2012-01-18 20:12:06,2.0
13389808,2,13389203,2012-11-15 00:30:57,5,"Use ix:

s['b']ix[1:10]
# 1   -0713173
# 2    1280302
# 4   -0667083


The docs note:


  The most robust and consistent way of slicing ranges along arbitrary axes is described in the Advanced indexing section detailing the ix method
",1240268.0,,,,,2012-11-15 00:30:57,
13389951,2,13389203,2012-11-15 00:47:54,3,"The best way I can think of is to use 'select' in this case Although it even says in the docs that ""This method should be used only when there is no more direct way""

Indexing and selecting data

In [116]: s
Out[116]: 
a  0     1724372
   1     0305923
   5     1780811
b  0    -0556650
   1     0207783
   4    -0177901
   50    0289365
   0     1168115

In [117]: sselect(lambda x: x[0] == 'b' and 2 <= x[1] <= 10)
Out[117]: b  4   -0177901
",738785.0,,,,,2012-11-15 00:47:54,1.0
13758846,2,13757090,2012-12-07 07:36:50,1,"I think the default way is to use:

df1 = pandasDataFrame({'key': ['b'  'b'  'a'  'c'  'a'  'a'  'b'] 
             'dat a1': range(7)})

df1['dat a1']


The other methods  like exposing it as an attribute are more for convenience",1755432.0,,,,,2012-12-07 07:36:50,1.0
13982029,2,13965036,2012-12-20 23:13:55,1,"This does seem like a bug with pandas 073 or numpy 16 This only happens if the column being merged on is a date (internally converted to numpydatetime64) My solution was to convert date into a string-

def _DatetimeToString(datetime64):
  timestamp = datetime64astype(long)/1000000000
  return datetimefromtimestamp(timestamp)strftime('%Y-%m-%d')

i = pdDataFramefrom_csv('icsv')
i = ireset_index()
i['date'] = i['date']map(_DatetimeToString)
e = pdDataFramefrom_csv('ecsv')
e = ereset_index()
i['date'] = i['date']map(_DatetimeToString)

total_df = pdmerge(i  e  right_index=False  left_index=False 
                    right_on=['date']  left_on=['date']  how='left')
total_df = total_dfsort(column='date')
",1917577.0,,,,,2012-12-20 23:13:55,
14125503,2,14125428,2013-01-02 16:25:28,2,"It looks like quantile() doesn't ignore the nuisance columns and is trying to find quantiles for your text columns  Here's a trivial example:

In [75]: df = DataFrame({'col1':['A' 'A' 'B' 'B']  'col2':[1 2 3 4]})

In [76]: df
Out[76]:
  col1  col2
0    A     1
1    A     2
2    B     3
3    B     4

In [77]: dfgroupby('col1')quantile()
ValueError: ('could not convert string to float: A'  u'occurred at index col1')


However  when I subset out only the numeric columns  I get:

In [78]: dfgroupby('col1')['col2']quantile()
Out[78]:
col1
A       15
B       35
",919872.0,,,,,2013-01-02 16:25:28,2.0
14298661,2,14298433,2013-01-12 22:34:53,2,"I think using the labels stored inside the Categorical object returned by qcut can make this a lot simpler  For example:

>>> import pandas as pd
>>> import numpy as np
>>> nprandomseed(1001)
>>> df = pdDataFrame(nprandomrandn(10  2)  columns=['A'  'B'])
>>> df
          A         B
0 -1086446 -0896065
1 -0306299 -1339934
2 -1206586 -0641727
3  1307946  1845460
4  0829115 -0023299
5 -0208564 -0916620
6 -1074743 -0086143
7  1175839 -1635092
8  1228194  1076386
9  0394773 -0387701
>>> q = pdqcut(df[""A""]  5)
>>> q
Categorical: A
array([[-1207  -10771]  (-10771  -0248]  [-1207  -10771] 
       (1186  1308]  (0569  1186]  (-0248  0569]  (-10771  -0248] 
       (0569  1186]  (1186  1308]  (-0248  0569]]  dtype=object)
Levels (5): Index([[-1207  -10771]  (-10771  -0248] 
                   (-0248  0569]  (0569  1186]  (1186  1308]]  dtype=object)
>>> qlabels
array([0  1  0  4  3  2  1  3  4  2])


or to match your code:

>>> len(qlevels) - qlabels
array([5  4  5  1  2  3  4  2  1  3])
>>> quintile(df  ""A"")
>>> nparray(df[""A""])
array([5  4  5  1  2  3  4  2  1  3])
",487339.0,,,,,2013-01-12 22:34:53,1.0
10726275,2,10715519,2012-05-23 19:05:57,4,"You probably want to do 

df['Normalized'] = npwhere(df['Currency'] == '$'  df['Budget'] * 078125  df['Budget'])
",776560.0,,,,,2012-05-23 19:05:57,
11401559,2,11362376,2012-07-09 19:14:55,0,"Right now there is not an easy way to maintain metadata on pandas objects across computations

Maintaining metadata has been an open discussion on github for some time now but we haven't had to time code it up

We'd welcome any additional feedback you have (see pandas on github) and would love to accept a pull-request if you're interested in rolling your own",1306530.0,,,,,2012-07-09 19:14:55,
12090798,2,11925827,2012-08-23 11:44:36,1,"When you load your workbook with use_iterators=True  it then _set_optimized_read() on the Workbook object  which cause it to be loaded read-only

Thus  with the following code :

from openpyxlreaderexcel import load_workbook

book = load_workbook('txlsx'  use_iterators=False) # Assume txlsx contains ['Data'  'Feuil2'  'Feuil3']
print bookget_sheet_names()


class temp_excel_writer():
    def __init__(self  path  book):
        selfbook=book
        test_sheet=selfbookcreate_sheet(title='Test') # No exception here now
        selfbooksave(path)
        selfuse_xlsx = True
        selfsheet_names=selfbookget_sheet_names()
        print selfsheet_names
        selfactual_sheets=selfbookworksheets
        selfsheets={}
        for i j in enumerate(selfsheet_names):
            selfsheets[j] = (selfactual_sheets[i] 1)
        selfcur_sheet = None
        selfpath = path # I had to modify this line also

my_temp_writer = temp_excel_writer('my_excel_filexlsx'  book)


It create a file named my_excel_filexlsx and the following output :

 ['Data'  'Feuil2'  'Feuil3']
 ['Data'  'Feuil2'  'Feuil3'  'Test']


Hope it helps",667433.0,,,,,2012-08-23 11:44:36,
12326113,2,12322289,2012-09-07 22:16:09,4,"As you mentioned in the question  looping through each column should work for you:

df1apply(lambda x: xasof(df2index))


We could potentially create a faster NaN-naive version of DataFrameasof to do all the columns in one shot But for now  I think this is the most straightforward way",1306530.0,,,,,2012-09-07 22:16:09,2.0
12336039,2,12322289,2012-09-09 02:22:04,5,"I wrote an under-advertised ordered_merge function some time ago:

In [27]: quotes
Out[27]: 
                        time    bid    ask  bsize  asize
0 2012-09-06 09:30:00026000  1334  1344      3     16
1 2012-09-06 09:30:00043000  1334  1344      3     17
2 2012-09-06 09:30:00121000  1336  1365      1     10
3 2012-09-06 09:30:00386000  1336  1352     21      1
4 2012-09-06 09:30:00440000  1340  1344     15     17

In [28]: trades
Out[28]: 
                        time  price   size
0 2012-09-06 09:30:00439000  1342  60511
1 2012-09-06 09:30:00439000  1342  60511
2 2012-09-06 09:30:02332000  1342    100
3 2012-09-06 09:30:02332000  1342    100
4 2012-09-06 09:30:02333000  1341    100

In [29]: ordered_merge(quotes  trades)
Out[29]: 
                        time    bid    ask  bsize  asize  price   size
0 2012-09-06 09:30:00026000  1334  1344      3     16    NaN    NaN
1 2012-09-06 09:30:00043000  1334  1344      3     17    NaN    NaN
2 2012-09-06 09:30:00121000  1336  1365      1     10    NaN    NaN
3 2012-09-06 09:30:00386000  1336  1352     21      1    NaN    NaN
4 2012-09-06 09:30:00439000    NaN    NaN    NaN    NaN  1342  60511
5 2012-09-06 09:30:00439000    NaN    NaN    NaN    NaN  1342  60511
6 2012-09-06 09:30:00440000  1340  1344     15     17    NaN    NaN
7 2012-09-06 09:30:02332000    NaN    NaN    NaN    NaN  1342    100
8 2012-09-06 09:30:02332000    NaN    NaN    NaN    NaN  1342    100
9 2012-09-06 09:30:02333000    NaN    NaN    NaN    NaN  1341    100

In [32]: ordered_merge(quotes  trades  fill_method='ffill')
Out[32]: 
                        time    bid    ask  bsize  asize  price   size
0 2012-09-06 09:30:00026000  1334  1344      3     16    NaN    NaN
1 2012-09-06 09:30:00043000  1334  1344      3     17    NaN    NaN
2 2012-09-06 09:30:00121000  1336  1365      1     10    NaN    NaN
3 2012-09-06 09:30:00386000  1336  1352     21      1    NaN    NaN
4 2012-09-06 09:30:00439000  1336  1352     21      1  1342  60511
5 2012-09-06 09:30:00439000  1336  1352     21      1  1342  60511
6 2012-09-06 09:30:00440000  1340  1344     15     17  1342  60511
7 2012-09-06 09:30:02332000  1340  1344     15     17  1342    100
8 2012-09-06 09:30:02332000  1340  1344     15     17  1342    100
9 2012-09-06 09:30:02333000  1340  1344     15     17  1341    100


It could be easily (well  for someone who is familiar with the code) extended to be a ""left join"" mimicking KDB I realize in this case that forward-filling the trade data is not appropriate; just illustrating the function",776560.0,,,,,2012-09-09 02:22:04,1.0
14077469,2,14077355,2012-12-29 00:21:11,2,"I think you are looking for set_index:

In [11]: dfset_index('date')
Out[11]: 
                     AAPL  GOOG   IBM   XOM
date                                  
2011-01-13 16:00:00     0     0  4000     0
2011-01-26 16:00:00     0  1000  4000     0
2011-02-02 16:00:00     0  1000  4000     0
2011-02-10 16:00:00     0  1000  4000  4000
2011-03-03 16:00:00     0     0  1800  4000
2011-06-03 16:00:00     0     0  3300  4000
2011-05-03 16:00:00     0     0     0  4000
2011-06-10 16:00:00  1200     0     0  4000
2011-08-01 16:00:00  1200     0     0  4000
2011-12-20 16:00:00     0     0     0  4000
",1240268.0,,1240268.0,,2012-12-29 11:47:38,2012-12-29 11:47:38,1.0
14268919,2,14268794,2013-01-10 23:06:24,1,"Use the round function()

x = round(number to round   number of decimal places to round the number to )
",1925847.0,,,,,2013-01-10 23:06:24,1.0
14269168,2,14268794,2013-01-10 23:28:33,2,"It could be slightly faster to add a half and cast astype int:

df = pdread_csv('datadat'  header=None  sep='\s+')

In [2]: df
Out[2]: 
           0           1  2  3     4  5  6        7
0  56499115  737127789  0  1  1530  1  1  160225
1  56499115  737127789  0  1  8250  1  1  144405
2  56499115  737127789  0  2  1530  1  1  148637
3  56499115  737127789  0  2  8250  1  1  148918
4  56499117  737127789  0  3  1530  1  1  160002
5  56499117  737127789  0  3  8250  1  1  154333
6  56499104  737127676  0  4  1530  1  1  147300
7  56499104  737127676  0  4  8250  1  1  156138
8  56499104  737127676  0  5  1530  1  1  162453
9  56499104  737127676  0  5  8250  1  1  156138

df1 = dfgroupby([0  1  4])[7]mean()reset_index()
df1['ints'] = (df1[7] + 05)astype(int)

In [5]: df1
Out[5]: 
           0           1     4         7  ints
0  56499104  737127676  1530  1548765    15
1  56499104  737127676  8250  1561380    16
2  56499115  737127789  1530  1544310    15
3  56499115  737127789  8250  1466615    15
4  56499117  737127789  1530  1600020    16
5  56499117  737127789  8250  1543330    15


Note: you can save a DataFrame using the DataFrame method to_csv",1240268.0,,,,,2013-01-10 23:28:33,2.0
14282372,2,14268794,2013-01-11 16:25:33,0,"This piece of code does exactly what I want:

import os
import numpy as np
import pandas as pd 

datadirectory = '/media/DATA'
oschdir( datadirectory)

df = pdread_csv('/media/DATA/datadat'  sep=""\\s+""  header=None)
df1 = dfgroupby([""X1"" ""X2"" ""X5""])[""X8""]mean()reset_index() 
df1['X3'] = df[""X3""]
df1['X4']=df[""X4""]
df1['X6']=df[""X6""]
df1['X7']=df[""X7""]
sorted_data = df1reindex_axis(sorted(df1columns)  axis=1)
tuple_data = [tuple(x) for x in sorted_datavalues]
datas = npasarray(tuple_data)

dfround = df
dfround['X1'] = df[""X1""]astype(int)
dfround['X2'] = df[""X2""]astype(int)
df2 = dfroundgroupby([""X1"" ""X2"" ""X5""])[""X8""]mean()reset_index()
df2['X3'] = df[""X3""] #add extra columns
df2['X4']=df[""X4""]
df2['X6']=df[""X6""]
df2['X7']=df[""X7""]
sorted_data2 = df2sort_index(axis=1) #rearragne data - method 2
tuple_data2 = [tuple(x) for x in sorted_data2values]
datas2 = npasarray(tuple_data2)

npsavetxt('sorted_datadat'  datas  fmt='%s'  delimiter='\t') #Save the data
npsavetxt('sorted_rounded_datadat'  datas2  fmt='%s'  delimiter='\t') #Save   the data
print ('DONE')
",1804537.0,,,,,2013-01-11 16:25:33,
14433084,2,14433039,2013-01-21 05:38:38,1,"You are looking for indexlevels:

In [10]: df1indexlevels
Out[10]: 
[Index(['000568'  '000596'  '000799']  dtype=object) 
 Int64Index([20120630  20120930]  dtype=int64)]

In [11]: df1indexlevels[0]
Out[11]: Index(['000568' '000596' '000799']  dtype=object)


Note you can see the index names:

In [12]: df1indexnames
Out[12]: ['STK_ID'  'RPT_Date']


These are discussed in the docs here",1240268.0,,1240268.0,,2013-01-21 06:57:57,2013-01-21 06:57:57,3.0
14540509,2,14539992,2013-01-26 19:08:54,4,"You can use the indexer_between_time Index method

For example  to include those times between 9am and 6pm (inclusive):

tsix[tsindexindexer_between_time(datetimetime(9)  datetimetime(18))]


to do the opposite and exclude those times between 6pm and 9am (exclusive):

tsix[tsindexindexer_between_time(datetimetime(18)  datetimetime(9) 
                                    include_start=False  include_end=False)]


Note: indexer_between_time's arguments include_start and include_end are by default True  setting include_start to False means that datetimes whose time-part is precisely start_time (the first argument)  in this case 6pm  will not be included

Example:

In [1]: rng = pddate_range('1/1/2000'  periods=24  freq='H')

In [2]: ts = pdSeries(pdnprandomrandn(len(rng))  index=rng)

In [3]: tsix[tsindexindexer_between_time(datetimetime(10)  datetimetime(14))] 
Out[3]: 
2000-01-01 10:00:00    1312561
2000-01-01 11:00:00   -1308502
2000-01-01 12:00:00   -0515339
2000-01-01 13:00:00    1536540
2000-01-01 14:00:00    0108617


Note: the same syntax (using ix) works for a DataFrame:

In [4]: df = pdDataFrame(ts)

In [5]: dfix[dfindexindexer_between_time(datetimetime(10)  datetimetime(14))]
Out[5]: 
                            0
2000-01-03 10:00:00  1312561
2000-01-03 11:00:00 -1308502
2000-01-03 12:00:00 -0515339
2000-01-03 13:00:00  1536540
2000-01-03 14:00:00  0108617
",1240268.0,,1240268.0,,2013-01-26 20:09:52,2013-01-26 20:09:52,6.0
9729741,2,9721429,2012-03-15 23:28:51,0,"What do you mean by display? Doesn't df['gvkey'] give you the data in the gvkey column?

If what you do is print the whole data frame to the console  then take a look at dfto_string()  but it'll be hard to read if you have too many columns Pandas won't print the whole thing by default if you have too many columns:

import pandas
import numpy 

df1 = pandasDataFrame(numpyrandomrandn(10  3)  columns=['col%d' % d for d in range(3)] )
df2 = pandasDataFrame(numpyrandomrandn(10  30)  columns=['col%d' % d for d in range(30)] )

print df1   # <--- substitute by df2 to see the difference
print
print df1['col1']
print
print df1to_string()
",890108.0,,,,,2012-03-15 23:28:51,2.0
9730620,2,9721429,2012-03-16 01:26:47,5,"Wes answered me in an email Cheers 

This is a fixed-width-format file (not delimited by commas or tabs as
usual) I realize that pandas does not have a fixed-width reader like
R does  though one can be fashioned very easily I'll see what I can
do In the meantime if you can export the data in another format (like
csv--truly comma separated) you'll be able to read it with read_csv I
suspect with some unix magic you can transform a FWF file into a CSV
file

I recommend following the issue on github as your e-mail is about to
disappear from my inbox :)

https://githubcom/pydata/pandas/issues/920

best 
Wes",1610626.0,,,,,2012-03-16 01:26:47,1.0
9753188,2,9721429,2012-03-17 19:53:59,0,"user  if you need to deal with the fixed format right now  you can use something like the following:

def fixed_width_to_items(filename  fields  first_column_is_index=False  ignore_first_rows=0):
    reader = open(filename  'r')
    # skip first rows 
    for i in xrange(ignore_first_rows):
        readernext()
    if first_column_is_index:
        index = slice(0  fields[1])
        fields = [slice(*x) for x  in zip(fields[1:-1]  fields[2:])]
        return ((line[index]  [line[x]strip() for x in fields]) for line in reader)
    else:
        fields = [slice(*x) for x  in zip(fields[:-1]  fields[1:])]
        return ((i  [line[x]strip() for x in fields]) for i line in enumerate(reader)) 


Here's a test program:

import pandas
import numpy
import tempfile

# create a data frame
df = pandasDataFrame(numpyrandomrandn(100  5))
file_ = tempfileNamedTemporaryFile(delete=True)
file_write(dfto_string())
file_flush()

# specify fields
fields = [0  3  12  22  32  42  52]
df2 = pandasDataFramefrom_items( fixed_width_to_items(file_name  fields  first_column_is_index=True  ignore_first_rows=1) )T

# need to specify the datatypes  otherwise everything is a string
df2 = pandasDataFrame(df2  dtype=float)
df2index = [int(x) for x in df2index]

# check
assert (df - df2)abs()max()max() < 1E-6


This should do the trick if you need it right now  but bear in mind that the function above is very simple  in particular it doesn't do anything about data types",890108.0,,,,,2012-03-17 19:53:59,
14283423,2,14263560,2013-01-11 17:24:09,1,"I don't think so The HTMLFormatter used by DataFrameto_html helps to pretty render a DataFrame in a IPython HTML Notebooks I think

The method does not parse each element of your DataFrame  ie recognizes an URI pattern to write <a href=""URI"">Content</a> or something else

I don't think that (1) it's planned and (2) it's not the purpose of this method Maybe you can add an issue to the GitHub pandas issues page",956765.0,,,,,2013-01-11 17:24:09,1.0
14390487,2,14390224,2013-01-18 00:15:30,5,"You can call reshape on the values array of the Series:

In [4]: avaluesreshape(2 2)
Out[4]: 
array([[1  2] 
       [3  4]]  dtype=int64)


I actually think it won't always make sense to apply reshape to a Series (do you ignore the index?)  and that you're correct in thinking it's just numpy's reshape:


  areshape?Docstring:  See numpyndarrayreshape


that said  I agree the fact that it let's you try to do this looks like a bug",1240268.0,,1240268.0,,2013-01-18 01:16:06,2013-01-18 01:16:06,5.0
10054720,2,10009468,2012-04-07 13:02:28,0,"How about directly group the result of xa*xb  for example:

from pandas import *
from numpyrandom import randn
df = DataFrame({'A' : ['foo'  'bar'  'foo'  'bar' 
               'foo'  'bar'  'foo'  'foo'] 
        'B' : ['one'  'one'  'two'  'three' 
               'two'  'two'  'one'  'three'] 
        'C' : randn(8)  'D' : randn(8)})

print (dfC*dfD)groupby(dfA)sum()
",772649.0,,,,,2012-04-07 13:02:28,1.0
10058000,2,10009468,2012-04-07 20:18:37,2,"I want to eventually build an embedded array expression evaluator (Numexpr on steroids) to do things like this Right now we're working with the limitations of Python-- if you implemented a Cython aggregator to do (x * y)sum() then it could be connected with groupby  but ideally you could write the Python expression as a function:

def weight_sum(x  y):
    return (x * y)sum()


and that would get ""JIT-compiled"" and be about as fast as groupby()sum() What I'm describing is a pretty significant (many month) project If there were a BSD-compatible APL implementation I might be able to do something like the above quite a bit sooner (just thinking out loud) ",776560.0,,,,,2012-04-07 20:18:37,
11037996,2,11018120,2012-06-14 17:05:15,3,"This is indeed a bug  I have two issues for it:

https://githubcom/pydata/pandas/issues/1458

https://githubcom/pydata/pandas/issues/1471

This should be fixed before pandas 080 is released Note that this works correctly:

In [15]: dfresample('M'  kind='period')
Out[15]: 
          0
Jan-2012  1
Feb-2012  2
Mar-2012  3


EDIT: Just fixed this in git master (both of the above reference issues have been closed)",776560.0,,776560.0,,2012-06-14 19:08:55,2012-06-14 19:08:55,1.0
11610785,2,11607387,2012-07-23 10:31:10,2,"All the pandas classes (TimeSeries  DataFrame  DatetimeIndex etc) have pure-Python definitions so there isn't a C API  You might be best off passing numpy ndarrays from C to your Python code and letting your Python code construct pandas objects from them

If necessary you could use PyObject_CallFunction etc to call the pandas constructors  but you'd have to take care of accessing the names from module imports and checking for errors",567292.0,,,,,2012-07-23 10:31:10,1.0
12053335,2,12052067,2012-08-21 10:52:15,1,"I never took the time to dig into pandas  but I use structured array quite often in numpy Here are a few considerations:

structured arrays are as convenient as recarrays with less overhead  if you don't mind losing the possibility to access fields by attributes But then  have you ever tried to use min or max as field name in a recarray ?
NumPy has been developed over a far longer period than pandas  with a larger crew  and it becomes ubiquitous enough that a lot of third party packages rely on it You can expect structured arrays to be more portable than pandas dataframes
Are pandas dataframes easily pickable ? Can they be sent back and forth with PyTables  for example ?
Unless you're 100% percent that you'll never have to share your code with non-pandas users  you might want to keep some structured arrays around",1491200.0,,,,,2012-08-21 10:52:15,1.0
7813756,2,7813132,2011-10-18 21:08:15,1,"You can create a ordered list and get index using listindex or may be just create a mapping of type and index eg

labels =  {'single':0  'touching':1  'nuclei':2  'dusts':3}

alist = [ ['dusts'  'touching']  ['single'  'nuclei'] ]
for l in alist:
    for i  v in enumerate(l):
        l[i] = labels[v]

print alist


or

labels =  ['single'  'touching' 'nuclei' 'dusts']

alist = [ ['dusts'  'touching']  ['single'  'nuclei'] ]
for l in alist:
    for i  v in enumerate(l):
        l[i] = labelsindex(v)

print alist


output:

[[3  1]  [0  2]]
",6946.0,,,,,2011-10-18 21:08:15,6.0
7821507,2,7813132,2011-10-19 12:43:50,3,"If you have a vector of strings or other objects and you want to give it categorical labels  you can use the Factor class (available in the pandas namespace):

In [1]: s = Series(['single'  'touching'  'nuclei'  'dusts'  'touching'  'single'  'nuclei'])

In [2]: s
Out[2]: 
0    single
1    touching
2    nuclei
3    dusts
4    touching
5    single
6    nuclei
Name: None  Length: 7

In [4]: Factor(s)
Out[4]: 
Factor:
array([single  touching  nuclei  dusts  touching  single  nuclei]  dtype=object)
Levels (4): [dusts nuclei single touching]


The factor has attributes labels and levels:

In [7]: f = Factor(s)

In [8]: flabels
Out[8]: array([2  3  1  0  3  2  1]  dtype=int32)

In [9]: flevels
Out[9]: Index([dusts  nuclei  single  touching]  dtype=object)


This is intended for 1D vectors so not sure if it can be instantly applied to your problem  but have a look

BTW I recommend that you ask these questions on the statsmodels and / or scikit-learn mailing list since most of us are not frequent SO users",776560.0,,,,,2011-10-19 12:43:50,1.0
10264895,2,10264739,2012-04-22 03:06:47,1,"You can try numpyfromfile 

http://docsscipyorg/doc/numpy/reference/generated/numpyfromfilehtml",1078084.0,,,,,2012-04-22 03:06:47,
10269848,2,10264739,2012-04-22 16:38:34,4,"import pandas  re  numpy as np

def load_file(filename  num_cols  delimiter='\t'):
    data = None
    try:
        data = npload(filename + 'npy')
    except:
        splitter = recompile(delimiter)

        def items(infile):
            for line in infile:
                for item in splittersplit(line):
                    yield item

        with open(filename  'r') as infile:
            data = npfromiter(items(infile)  float64  -1)
            data = datareshape((-1  num_cols))
            npsave(filename  data)

    return pandasDataFrame(data)


This reads in the 25GB file  and serializes the output matrix The input file is read in ""lazily""  so no intermediate data-structures are built and minimal memory is used The initial load takes a long time  but each subsequent load (of the serialized file) is fast Please let me if you have tips!",190894.0,,190894.0,,2012-04-26 00:26:27,2012-04-26 00:26:27,2.0
10272449,2,10264739,2012-04-22 21:53:55,1,Try out recfile for now: http://codegooglecom/p/recfile/  There are a couple of efforts I know of to make a fast C/C++ file reader for NumPy; it's on my short todo list for pandas because it causes problems like these Warren Weckesser also has a project here: https://githubcom/WarrenWeckesser/textreader  I don't know which one is better  try them both?,776560.0,,,,,2012-04-22 21:53:55,
11067072,2,11067027,2012-06-16 21:12:21,7,"dfreindex_axis(sorted(dfcolumns)  axis=1)


This assumes that sorting the column names will give the order you want  If your column names won't sort lexicographically (eg  if you want column Q103 to appear after Q91)  you'll need to sort differently  but that has nothing to do with pandas",1427416.0,,,,,2012-06-16 21:12:21,1.0
10484726,2,10484106,2012-05-07 15:26:13,0,"I've never used pandas for csv processing I just use the standard Python lib csv functions as these use iterators

import csv
myCSVfile=r""c:/Documents and Settings/Jason/Desktop/hist_EURIBOR_2012csv""
f=open(myCSVfile ""r"")
reader=csvreader(f delimiter=' ')
data=[]
for l in reader:
    if l[0]strip()==""3m"":
        dataappend(l)

fclose()
",1364242.0,,,,,2012-05-07 15:26:13,1.0
10484737,2,10484106,2012-05-07 15:26:52,3,"A pandas dataframe has a transpose() method  but it doesn't like all the empty rows in this file Here's how to get it cleaned up:

df = pandasread_csv(""hist_EURIBOR_2012csv"")  # Read the file
df = df[:15]    # Chop off the empty rows beyond 12m
df2 = dftranspose()
df2 = df2[:88]  # Chop off what were empty columns (I guess you should increase 88 as more data is added


Of course  you can chain these together:

df2 = pandasread_csv(""hist_EURIBOR_2012csv"")[:15]transpose()[:88]


Then df2['3m'] is the data you want  but the dates are still stored as strings I'm not quite sure how to convert it to a DateIndex",434217.0,,434217.0,,2012-05-07 18:34:52,2012-05-07 18:34:52,2.0
11366706,2,11350770,2012-07-06 17:08:46,1,"Here's what I ended up doing for partial string matches  If anyone has a more efficient way of doing this please let me know

def stringSearchColumn_DataFrame(df  colName  regex):
    newdf = DataFrame()
    for idx  record in df[colName]iteritems():

        if research(regex  record):
            newdf = concat([df[df[colName] == record]  newdf]  ignore_index=True)

    return newdf
",1170342.0,,,,,2012-07-06 17:08:46,
11531402,2,11350770,2012-07-17 21:52:18,6,"Based on github issue #620  it looks like you'll soon be able to do the following:

df[df['A']strcontains(""hello"")]


Update: vectorized string methods (ie  Seriesstr) are available in pandas 081 and up ",243434.0,,243434.0,,2012-09-30 17:52:13,2012-09-30 17:52:13,1.0
11919200,2,11918342,2012-08-12 02:14:37,1,"The solution mentioned in the link above works  so I will just repost the snippet here Thanks!

import datetime

def minimalist_xldate_as_datetime(xldate  datemode):
    # datemode: 0 for 1900-based  1 for 1904-based
    return (
        datetimedatetime(1899  12  30)
        + datetimetimedelta(days=xldate + 1462 * datemode)
    )
",190894.0,,,,,2012-08-12 02:14:37,
12325135,2,12315810,2012-09-07 20:44:58,1,"I don't know how to do exactly what you want  but perhaps this is close enough:

In [23]: df = pdDataFrame({'A' : ['a1'  'a1'  'a2'  'a2'] 
                            'B' : ['b11'  'b12'  'b21'  'b22'] 
                            'C' : ['c11'  'c12'  'c21'  'c22']})

In [24]: grpA  = dfgroupby('A')
In [25]: a1 = grpAget_group('a1')


Using that I then get:

In [26]: a1['B']  # or a1B
Out[26]: 
0    b11
1    b12
Name: B


also:

In [39]: import numpy as np

In [40]: nparray(a1B)
Out[40]: array([b11  b12]  dtype=object)


and finally:

In [41]: grpdAget_group('a1')Btolist()  # leave off `tolist()` to get a series
Out[41]: ['b11'  'b12']


Hope that helps",1552748.0,,1552748.0,,2012-09-07 21:12:04,2012-09-07 21:12:04,
12335404,2,12315810,2012-09-08 23:50:09,0,"In [46]: dfgroupby('A')agg(lambda g: dict([(k g[k]tolist()) for k in g]))
Out[46]: 
                 B               C
A                                 
a1  ['b11'  'b12']  ['c11'  'c12']
a2  ['b21'  'b22']  ['c21'  'c22']
",243434.0,,,,,2012-09-08 23:50:09,
12549298,2,12544361,2012-09-23 02:38:01,3,"How about something like this:

In [111]: df
Out[111]: 
                mat  strike  vol
date     tenor                  
20120903 3m      1y    025   52
         3m      1y    050   51
         3m      1y    100   49
         3m      5y    025   32
         3m      5y    050   55
         3m      5y    100   23
         3m     10y    025   65
         3m     10y    050   55
         3m     10y    100   19
20120904 3m      1y    025   32
         3m      1y    050   57
         3m      1y    100   44
         3m      5y    025   54
         3m      5y    050   50
         3m      5y    100   69
         3m     10y    025   42
         3m     10y    050   81
         3m     10y    100   99

In [112]: def agg_func(x):
    mats = list(xmatunique())
    strikes = list(xstrikeunique())
    vols = xpivot('mat'  'strike'  'vol')reindex(mats  columns=strikes)
    return [mats  strikes  volsvaluestolist()]
   : 

In [113]: rs = dfgroupby(level=['date'  'tenor'])apply(agg_func)

In [114]: rs
Out[114]: 
date      tenor
20120903  3m       [['1y'  '5y'  '10y']  [025  05  10]  [[520
20120904  3m       [['1y'  '5y'  '10y']  [025  05  10]  [[320

In [115]: rsvalues[0]
Out[115]: 
[['1y'  '5y'  '10y'] 
 [025  05  10] 
 [[520  510  490]  [320  550  230]  [650  550  190]]]
",1306530.0,,,,,2012-09-23 02:38:01,2.0
13419324,2,8916302,2012-11-16 15:12:56,0,"There are at least a few approaches to shortening the syntax for this in Pandas  until it gets a full query API down the road (perhaps I'll try to join the github project and do this is time permits and if no one else already has started)

One method to shorten the syntax a little is given below:

inds = dfapply(lambda x: x[""A""]>10 and x[""B""]<5  axis=1) 
print df[inds]to_string()


To fully solve this  one would need to build something like the SQL select and where clauses into Pandas This is not trivial at all  but one stab that I think might work for this is to use the Python operator built-in module This allows you to treat things like greater-than as functions instead of symbols So you could do the following:

def pandas_select(dataframe  select_dict):

    inds = dataframeapply(lambda x: reduce(lambda v1 v2: v1 and v2  
                           [elem[0](x[key]  elem[1]) 
                           for key elem in select_dictiteritems()])  axis=1)
    return dataframe[inds]


Then a test example like yours would be to do the following:

import operator
select_dict = {
               ""A"":(operatorgt 10) 
               ""B"":(operatorlt 5)                  
              }

print pandas_select(df  select_dict)to_string()


You can shorten the syntax even further by either building in more arguments to pandas_select to handle the different common logical operators automatically  or by importing them into the namespace with shorter names

Note that the pandas_select function above only works with logical-and chains of constraints You'd have to modify it to get different logical behavior Or use not and DeMorgan's Laws",567620.0,,,,,2012-11-16 15:12:56,
10465162,2,10464738,2012-05-05 19:16:19,4,"You can use DataFrameapply with Seriesinterpolate to get a linear interpolation 

In : df = pandasDataFrame(numpyrandomrandn(5 3)  index=['a' 'c' 'd' 'e' 'g'])

In : df
Out:
          0         1         2
a -1987879 -2028572  0024493
c  2092605 -1429537  0204811
d  0767215  1077814  0565666
e -1027733  1330702 -0490780
g -1632493  0938456  0492695

In : df2 = dfreindex(['a' 'b' 'c' 'd' 'e' 'f' 'g'])

In : df2
Out:
          0         1         2
a -1987879 -2028572  0024493
b       NaN       NaN       NaN
c  2092605 -1429537  0204811
d  0767215  1077814  0565666
e -1027733  1330702 -0490780
f       NaN       NaN       NaN
g -1632493  0938456  0492695

In : df2apply(pandasSeriesinterpolate)
Out:
          0         1         2
a -1987879 -2028572  0024493
b  0052363 -1729055  0114652
c  2092605 -1429537  0204811
d  0767215  1077814  0565666
e -1027733  1330702 -0490780
f -1330113  1134579  0000958
g -1632493  0938456  0492695


For anything more complex  you need to roll-out your own function that will deal with a Series object and fill NaN values as you like and return another Series object",843822.0,,,,,2012-05-05 19:16:19,2.0
11346337,2,11346283,2012-07-05 14:23:27,6,"Just assign it to the columns attribute:

>>> df = pdDataFrame({'$a':[1 2]  '$b': [10 20]})
>>> dfcolumns = ['a'  'b']
>>> df
   a   b
0  1  10
1  2  20
",449449.0,,,,,2012-07-05 14:23:27,1.0
11354850,2,11346283,2012-07-06 01:48:15,6,"df = dfrename(columns={'$a': 'a'  '$b': 'b'})


http://pandaspydataorg/pandas-docs/stable/generated/pandasDataFramerenamehtml",1505540.0,,1274613.0,,2012-11-28 12:25:20,2012-11-28 12:25:20,3.0
11889488,2,11889474,2012-08-09 18:13:34,3,Just do from numpy import nan  (You will have to convert your DataTable to float type  because you can't use NaN in integer arrays),1427416.0,,1427416.0,,2012-08-09 18:55:51,2012-08-09 18:55:51,3.0
12593342,2,12589481,2012-09-26 01:30:08,0,"Would something like this work:

In [7]: dfgroupby('dummy')returnsagg({'func1' : lambda x: xsum()  'func2' : lambda x: xprod()})
Out[7]: 
              func2     func1
dummy                        
1     -4263768e-16 -0188565
",1306530.0,,,,,2012-09-26 01:30:08,6.0
13592901,2,12589481,2012-11-27 20:57:33,0,"You can simply pass the functions as a list:

In [323]: dfgroupby(""dummy"")agg({""returns"": [npmean  npsum]})
Out[323]: 
        returns          
           mean       sum
dummy                    
1     -0003467 -0034669
",1301710.0,,,,,2012-11-27 20:57:33,
13003524,2,13003051,2012-10-22 00:15:38,0,"We can potentially make this easier (I created a github issue)  but for now you can select out the columns you want to plot:

dfix[:  dfcolumns - to_excl]hist()
",1306530.0,,,,,2012-10-22 00:15:38,1.0
13574802,2,13089789,2012-11-26 23:13:18,0,"For simple column vs coloumn you can try the first code snippet under ""Gui neutral animation in pylab"" here http://wwwscipyorg/Cookbook/Matplotlib/Animations

This will just animate (very efficiently) by updating the line  no option to save to file",948652.0,,,,,2012-11-26 23:13:18,
13358139,2,13353233,2012-11-13 09:21:03,0,"An alternative is:

In [36]: dff
Out[36]:
   a         b
0  A  0689785
1  A -0374623
2  A  0517337
3  B  1549259
4  A  0576892
5  A -0833309
6  B -0209827
7  A -0150917
8  A -1296696

In [37]: dff['grpb'] = npNaN

In [38]: breaks = dff[dffa == 'B']index

In [39]: dff['grpb'][breaks] = range(len(breaks))

In [40]: dfffillna(method='bfill')fillna(len(breaks))
Out[40]:
   a         b  grpb
0  A  0689785     0
1  A -0374623     0
2  A  0517337     0
3  B  1549259     0
4  A  0576892     1
5  A -0833309     1
6  B -0209827     1
7  A -0150917     2
8  A -1296696     2


Or using itertools to create 'grpb' is an option too",1548051.0,,,,,2012-11-13 09:21:03,
13838744,2,13838405,2012-12-12 11:28:39,0,"import pandas as pd
custom_dict = {'March':0 'April':1 'Dec':3}

df = pdDataFrame() # with columns April  March  Dec (probably alphabetically)

df = pdDataFrame(df  columns=sorted(custom_dict  key=custom_dictget))


returns a DataFrame with columns March  April  Dec",449449.0,,,,,2012-12-12 11:28:39,
13839029,2,13838405,2012-12-12 11:44:10,0,"You could create an intermediary series  and set_index on that:

df = pdDataFrame([[1  2  'March'] [5  6  'Dec'] [3  4  'April']]  columns=['a' 'b' 'm'])
s = df['m']apply(lambda x: {'March':0  'April':1  'Dec':3}[x])
ssort()

In [4]: dfset_index(sindex)sort()
Out[4]: 
   a  b      m
0  1  2  March
1  3  4  April
2  5  6    Dec
",1240268.0,,,,,2012-12-12 11:44:10,
14057225,2,14057007,2012-12-27 15:40:35,1,"You can use the select DataFrame method:

In [1]: df = pdDataFrame([[1 2] [3 4]]  index=['A' 'B'])

In [2]: df
Out[2]: 
   0  1
A  1  2
B  3  4

In [3]: L = ['A']

In [4]: dfselect(lambda x: x in L)
Out[4]: 
   0  1
A  1  2
",1240268.0,,,,,2012-12-27 15:40:35,2.0
14058892,2,14057007,2012-12-27 17:46:47,1,"You can use numpylogical_not to invert the boolean array returned by isin:

In [63]: s = pdSeries(nparange(100))

In [64]: x = range(4  8)

In [65]: mask = nplogical_not(sisin(x))

In [66]: s[mask]
Out[66]: 
0    0
1    1
2    2
3    3
8    8
9    9
",1301710.0,,1301710.0,,2012-12-27 17:53:47,2012-12-27 17:53:47,1.0
12980313,2,12979568,2012-10-19 18:30:15,0,"dtype of the stats_value was not correct plotting works again

-A",369541.0,,,,,2012-10-19 18:30:15,
13083259,2,13081030,2012-10-26 08:18:26,2,"If i understand your question correctly  thats how you can do it:

from pandas import *


Read in the data  index by MatchDate:

frame=read_csv(""datescsv""   parse_dates = True  index_col = 4)
print frame

                Country            Player  Runs  ScoreRate Weekday
MatchDate                                                         
2010-02-16  Afghanistan  Mohammad Shahzad   118      9752     Tue
2010-03-16        india             schin   112      9802     wed


Define two datetime object's that define the range that you want slice:

x=datetime(2010  1  5)
y=datetime(2010  2  25)


And slice it (get all rows  that have a MatchDate between x and y):

print frameix[x:y]
                Country            Player  Runs  ScoreRate Weekday
MatchDate                                                         
2010-02-16  Afghanistan  Mohammad Shahzad   118      9752     Tue


If you just want to get a certain month or year  you can just do this:

frameix['2010-2']

            Country            Player  Runs  ScoreRate Weekday
MatchDate                                                         
2010-02-16  Afghanistan  Mohammad Shahzad   118      9752     Tue
",1199589.0,,1199589.0,,2012-10-26 08:32:46,2012-10-26 08:32:46,3.0
13100681,2,13081030,2012-10-27 13:41:51,0,I'm planning to add a usecols option to the file readers for reading out individual columns Probably for pandas 010 (later this month),776560.0,,,,,2012-10-27 13:41:51,
13770992,2,13769600,2012-12-07 21:06:42,2,"The pandas implementation uses a rolling window of the previous n values  which is how it's usually done in finance (see this Wikipedia entry for simple moving average) 

I guess it would be nice to have the option to specify whether the values should be taken from either side or just use previous values - you can raise an issue on GitHub

len(nparange(12)) and len(pdstatsmomentsrolling_mean(nparange(12) 6)) both equal 12 as I would have expected - what result were you expecting?",1452002.0,,,,,2012-12-07 21:06:42,3.0
14012374,2,14004545,2012-12-23 15:55:17,1,"Here's one way (depending  if tz is already set it might be a tz_convert rather than tz_localize):

In [11]: from pandaslib import Timestamp

In [12]: sapply(lambda x: Timestamp(x)tz_localize('UTC')) 
Out[12]: 
0    2012-01-01 06:30:00+00:00
1    2012-01-01 06:31:00+00:00
2    2012-01-01 06:32:00+00:00


Note: just using xtz_localize fails  I've posted this as an issue on github",1240268.0,,1240268.0,,2012-12-23 16:11:07,2012-12-23 16:11:07,1.0
14193085,2,14192741,2013-01-07 09:27:17,3,"I am pretty sure that your 1st way is returning a copy  instead of a view  and so assigning to it does not change the original data I am not sure why this is happening though

It seems to be related to the order in which you select rows and columns  NOT the syntax for getting columns These both work:

dfD[dfkey == 1] = 1
df['D'][dfkey == 1] = 1


And neither of these works:

df[dfkey == 1]['D'] = 1
df[dfkey == 1]D = 1


From this evidence  I would assume that the slice df[dfkey == 1] is returning a copy But this is not the case! df[dfkey == 1] = 0 will actually change the original data  as if it were a view

So  I'm not sure My sense is that this behavior has changed with the version of pandas I seem to remember that dfD used to return a copy and df['D'] used to return a view  but this doesn't appear to be true anymore (pandas 0100)

If you want a more complete answer  you should post in the pystatsmodels forum:
https://groupsgooglecom/forum/?fromgroups#!forum/pystatsmodels",1676378.0,,,,,2013-01-07 09:27:17,1.0
14455831,2,14455746,2013-01-22 10:00:06,3,"Replace the line

b = npsort(a)


with 

b = pdSeries(npsort(a)  index=aindex)


This will sort the values  but keep the index

EDIT:

To get the fourth value in the sorted Series:

npsort(a)values[3]
",449449.0,,449449.0,,2013-01-22 10:17:27,2013-01-22 10:17:27,2.0
14466665,2,14455746,2013-01-22 19:38:47,0,"You can use iget to retrieve by position:(In fact  this method was created especially to overcome this ambiguity)

In [1]: s = pdSeries([0  2  1])

In [2]: ssort()

In [3]: s
Out[3]: 
0    0
2    1
1    2

In [4]: siget(1)
Out[4]: 1




The behaviour of ix with an integer index is noted in the pandas ""gotchas"":


  In pandas  our general viewpoint is that labels matter more than integer locations Therefore  with an integer axis index only label-based indexing is possible with the standard tools like ix
  
  This deliberate decision was made to prevent ambiguities and subtle bugs (many users reported finding bugs when the API change was made to stop falling back on position-based indexing)


Note: this would work if you were using a non-integer index  where ix is not ambiguous

For example:

In [11]: s1 = pdSeries([0  2  1]  list('abc'))

In [12]: s1
Out[12]: 
a    0
b    2
c    1

In [13]: s1sort()

In [14]: s1
Out[14]: 
a    0
c    1
b    2

In [15]: s1ix[1]
Out[15]: 1
",1240268.0,,1240268.0,,2013-01-22 21:59:02,2013-01-22 21:59:02,1.0
7837947,2,7837722,2011-10-20 15:02:16,8,"Pandas is based on NumPy arrays
The key to speed with NumPy arrays is to perform your operations on the whole array at once  never row-by-row or item-by-item

For example  if close is a 1-d array  and you want the day-over-day percent change 

pct_change = close[1:]/close[:-1]


This computes the entire array of percent changes as one statement  instead of 

pct_change = []
for row in close:
    pct_changeappend()


So try to avoid the Python loop for i  row in enumerate() entirely  and
think about how to perform your calculations with operations on the entire array (or dataframe) as a whole  rather than row-by-row",190597.0,,,,,2011-10-20 15:02:16,5.0
7849789,2,7837722,2011-10-21 13:04:53,21,"You can loop through the rows by transposing and then calling iteritems:

for date  row in dfTiteritems():
   # do some logic here


I am not certain about efficiency in that case To get the best possible performance in an iterative algorithm  you might want to explore writing it in Cython  so you could do something like:

def my_algo(ndarray[object] dates  ndarray[float64_t] open 
            ndarray[float64_t] low  ndarray[float64_t] high 
            ndarray[float64_t] close  ndarray[float64_t] volume):
    cdef:
        Py_ssize_t i  n
        float64_t foo
    n = len(dates)

    for i from 0 <= i < n:
        foo = close[i] - open[i] # will be extremely fast


I would recommend writing the algorithm in pure Python first  make sure it works and see how fast it is-- if it's not fast enough  convert things to Cython like this with minimal work to get something that's about as fast as hand-coded C/C++",776560.0,,,,,2011-10-21 13:04:53,1.0
11617194,2,7837722,2012-07-23 17:09:39,5,"The newest versions of pandas now include a built-in function for iterating over rows 

for row in dfiterrows():
    # do some logic here


But  unutbu's suggestion to use numpy functions to avoid iterating over rows will produce the fastest code ",475872.0,,,,,2012-07-23 17:09:39,
11067079,2,11067027,2012-06-16 21:14:20,0,"The sort method and sorted function allow you to provide a custom function to extract the key used for comparison:

>>> ls = ['Q13'  'Q61'  'Q12']
>>> sorted(ls  key=lambda x: float(x[1:]))
['Q12'  'Q13'  'Q61']
",1460341.0,,,,,2012-06-16 21:14:20,2.0
11385780,2,11067027,2012-07-08 18:56:47,5,"You can also do more succinctly:

dfsort_index(axis=1)",776560.0,,,,,2012-07-08 18:56:47,
11622769,2,11622652,2012-07-24 01:10:50,3,"In principle it shouldn't run out of memory  but there are currently memory problems with read_csv on large files caused by some complex Python internal issues (this is vague but it's been known for a long time: http://githubcom/pydata/pandas/issues/407) 

At the moment there isn't a perfect solution (here's a tedious one: you could transcribe the file row-by-row into a pre-allocated NumPy array or memory-mapped file--npmmap)  but it's one I'll be working on in the near future Another solution is to read the file in smaller pieces (use iterator=True  chunksize=1000) then concatenate then with pdconcat The problem comes in when you pull the entire text file into memory in one big slurp",776560.0,,776560.0,,2012-09-09 02:46:50,2012-09-09 02:46:50,2.0
12193309,2,11622652,2012-08-30 08:57:26,3,"Wes in of course right! I'm just chiming in to provide a little more complete example code I had the same issue with a 129 Mb file  which was solved by

from pandas import *

tp = read_csv('exp4326csv'  iterator=True  chunksize=1000)
df = concat([chunk for chunk in tp]  ignore_index=True)
",709479.0,,709479.0,,2012-08-30 09:04:54,2012-08-30 09:04:54,
12086260,2,12085393,2012-08-23 06:53:27,0,"If you know a few specific values that are off  you might just examine those lines specifically in your csv file  You should also check out the docs on csv  particularly in terms of dialects and the Sniffer class You might be able to find some setting that will correctly detect how the the file is delimited

If you find that the errors go away when you look only at specific lines  that probably means that there is an erroneous/missing line break somewhere that is throwing things off

Finally  if you can't seem to find patterns of correct/incorrect lines  you might try (randomly or otherwise) selecting a subset of the lines in your csv file and see whether the error is occurring because of the size of the file (I'd guess this would be unlikely  but I'm not sure)",1316786.0,,,,,2012-08-23 06:53:27,1.0
13597747,2,12085393,2012-11-28 04:16:54,0,Some bugs have been fixed in the DataFrame to Panel code Please try with the latest pandas version (preferably upcoming 010) and let us know if you're still having issues,776560.0,,,,,2012-11-28 04:16:54,1.0
12411852,2,12410438,2012-09-13 17:41:05,2,"You're producing an aggregate r and s value per group  so you should be using Series here:

In [26]: dfgroupby('x')apply(lambda x: 
             Series({'r': (xy + xz)sum() / xzsum()  
                     's': (xy + xz ** 2)sum() / xzsum()}))
Out[26]: 
           r           s
x                       
a  -0338590   -0916635
b  66655533  102566146
",1306530.0,,,,,2012-09-13 17:41:05,1.0
12648343,2,12647811,2012-09-28 22:50:31,0,"One way to do this is to use apply:

def my_fun(row):
    if row['col1'] == 'rs640249':
        return row['col2']  row['col1']
    else:
        return row['col1']  row['col2']

df = dfapply(my_fun  axis=1)


If you want to change the values in only one column you can still use apply:

def my_fun2(row  colID):
    if row[colID][0] == 'rs640249':
        return row[colID][::-1] #reverse the tuple
    else:
        return row[colID]

df[colID] = dfapply(lambda x: my_fun2(x  colID)  axis=1)


Note: since my_fun2 returns a single value  this time apply return a Series  so we need to slightly change the way we apply apply

Example:

df
#                             0
# 0    ('rs649071'  'rs640249')
# 1  ('rs640249'  'rs11073074')

df[0] = dfapply(lambda x: my_fun2(x 0)  axis=1)
#                             0
# 0    ('rs649071'  'rs640249')
# 1  ('rs11073074'  'rs640249')
",1240268.0,,1240268.0,,2012-10-02 12:43:22,2012-10-02 12:43:22,3.0
14509574,2,14509517,2013-01-24 19:53:18,8,"The correct answer is PostgreSQL For most platforms it's just as easy to install as MySQL  but it is a better database  and it's especially an improvement on MySQL when it comes to handling large amounts of data  which you are doing

I wouldn't even begin to consider handling a few hundred GB of data in SQLite",126214.0,,,,,2013-01-24 19:53:18,6.0
14509945,2,14509517,2013-01-24 20:16:09,0,"SQLite is great for embedded databases  but it's not really great for anything that requires access by more than one process at a time For this reason it cannot be taken seriously for your application

MySQL is a much better alternative I'm also in agreement that Postgres would be an even better option",87189.0,,,,,2013-01-24 20:16:09,
14514661,2,14509517,2013-01-25 03:00:58,0,"For many 'research' oriented time series database loads  it is far faster to do as much analysis in the database than to copy the data to a client and analyze it using a regular programming language  Copying 10G across the network is far slower than reading it from disk

Relational databases do not natively support time series operations  so generating something as simple as security returns from security prices is either impossible or very difficult in both MySQL and SQLite

Postgres has windowing operations  as do several other relational-like databases; the trade-off is that that they don't do as many transactions per second  Many others use K or Q

The financial services web apps that I've seen used multiple databases; the raw data was stored in 'research' databases that were multiply indexed and designed for flexibility  while the web-apps interacted directly with in-memory caches and higher-speed RDBs; the tradeoff was that data had to be copied from the 'research' databases to the 'production' databases",1984745.0,,,,,2013-01-25 03:00:58,
8842419,2,8842114,2012-01-12 21:16:35,2,"apply first tries to apply the function to the whole series Only if that fails it maps the given function to each element [:2] is a valid function on a series  + 'qwerty' apparently isn't  that's why you do get the implicit mapping on the latter If you always want to do the mapping you can use smap

apply's source code for reference:

    try:
        result = func(self)
        if not isinstance(result  Series):
            result = Series(result  index=selfindex  name=selfname)
        return result
    except Exception:
        mapped = libmap_infer(selfvalues  func)
        return Series(mapped  index=selfindex  name=selfname)
",617711.0,,617711.0,,2012-01-12 21:23:57,2012-01-12 21:23:57,
8842485,2,8842114,2012-01-12 21:22:26,4,"You're on the right track:

In [3]: s = Series(data=['abcdef']*20)

In [4]: s
Out[4]: 
0     abcdef
1     abcdef
2     abcdef
3     abcdef
4     abcdef
5     abcdef
6     abcdef
7     abcdef
8     abcdef
9     abcdef
10    abcdef
11    abcdef
12    abcdef
13    abcdef
14    abcdef
15    abcdef
16    abcdef
17    abcdef
18    abcdef
19    abcdef

In [5]: smap(lambda x: x[:2])
Out[5]: 
0     ab
1     ab
2     ab
3     ab
4     ab
5     ab
6     ab
7     ab
8     ab
9     ab
10    ab
11    ab
12    ab
13    ab
14    ab
15    ab
16    ab
17    ab
18    ab
19    ab


I would really like to add a bunch of vectorized  NA-friendly string processing tools in pandas (See here) Always appreciate any development help also",776560.0,,,,,2012-01-12 21:22:26,
10458386,2,10457584,2012-05-05 02:44:12,8,"Why don't you simply use set_index method?

In : col = ['a' 'b' 'c']

In : data = DataFrame([[1 2 3] [10 11 12] [20 21 22]] columns=col)

In : data
Out:
    a   b   c
0   1   2   3
1  10  11  12
2  20  21  22

In : data2 = dataset_index('a')

In : data2
Out:
     b   c
a
1    2   3
10  11  12
20  21  22
",843822.0,,,,,2012-05-05 02:44:12,
12680776,2,12647811,2012-10-01 20:43:43,0,"For future refereces  here goes a possible solution:

    for row_index  row in dfiterrows():
        if row['L1'] == 'rs640249':
            dfset_value(row_index  'L1'   row['L2'])
            dfset_value(row_index  'L2'   row['L1'])


Best ",1289107.0,,,,,2012-10-01 20:43:43,3.0
12992015,2,12647811,2012-10-20 19:42:29,0,"Why don't you try something like this  with array operations:

condition = df['L1'] == 'rs640249'
tmp = df['L1']copy()
df['L1'][condition] = df['L2'][condition]
df['L2'][condition] = tmp[condition]
",776560.0,,,,,2012-10-20 19:42:29,
13730903,2,13041180,2012-12-05 19:27:24,0,"You can use datetimedatetimestrptime to parse your 'Date' strings and then construct numpydatetime64 values from the datetimedatetime types:

data = rsreindex(numpyarray([(lambda x : datetimedatetimestrptime(x '%m/%d/%Y'))(x) for x in rs['Date']] dtype='datetime64[us]')
",1876739.0,,1876739.0,,2012-12-05 19:40:03,2012-12-05 19:40:03,
13218891,2,13218461,2012-11-04 13:23:20,3,"For statsmodels >=04  if I remember correctly

modelpredict doesn't know about the parameters  and requires them in the call
see http://statsmodelssourceforgenet/stable/generated/statsmodelsregressionlinear_modelOLSpredicthtml

What should work in your case is to fit the model and then use the predict method of the results instance

model = OLS(labels[:half]  data[:half])
results = modelfit()
predictions = resultspredict(data[half:])


or shorter

results = OLS(labels[:half]  data[:half])fit()
predictions = resultspredict(data[half:])


http://statsmodelssourceforgenet/stable/generated/statsmodelsregressionlinear_modelRegressionResultspredicthtml  with missing docstring

Note: this has been changed in the development version (backwards compatible)  that can take advantage of ""formula"" information in predict
http://statsmodelssourceforgenet/devel/generated/statsmodelsregressionlinear_modelRegressionResultspredicthtml",333700.0,,,,,2012-11-04 13:23:20,
13447678,2,13439098,2012-11-19 04:26:44,4,"How about using the level parameter in DataFramereindex?

In [14]: df
Out[14]: 
            0         1
a 0  0007288 -0840392
  1  0652740  0597250
b 0 -1197735  0822150
  1 -0242030 -0655058

In [15]: stk_list = ['a']

In [16]: dfreindex(stk_list  level=0)
Out[16]: 
            0         1
a 0  0007288 -0840392
  1  0652740  0597250
",1306530.0,,1240268.0,,2012-11-19 12:28:33,2012-11-19 12:28:33,
13453153,2,13439098,2012-11-19 11:48:31,1,"You can try:

df[dfindexmap(lambda x: x[0] in stk_list)]


Example:

In : stk_list
Out: ['600106'  '300204'  '300113']

In : df
Out:
                STK_Name   ROIC   mg_r
STK_ID RPT_Date
002410 20111231      ???  0401  0956
300204 20111231      ???  0375  0881
300295 20111231     ????  2370  0867
300288 20111231     ????  1195  0861
600106 20111231     ????  1214  0857
300113 20111231     ????  0837  0852

In : df[dfindexmap(lambda x: x[0] in stk_list)]
Out:
                STK_Name   ROIC   mg_r
STK_ID RPT_Date
300204 20111231      ???  0375  0881
600106 20111231     ????  1214  0857
300113 20111231     ????  0837  0852
",843822.0,,,,,2012-11-19 11:48:31,
13953154,2,13952977,2012-12-19 13:10:10,3,"If all you want to do is override the index i would use:

df1 = df
df1index = c
",1755432.0,,,,,2012-12-19 13:10:10,
14110955,2,14110721,2013-01-01 13:40:39,2,"With this setup:

import pandas as pd
import io

text = '''\
STK_ID RPT_Date sales cash
000568 20120930 80093 57488
000596 20120930 32585 26177
000799 20120930 14784 8157
'''

df = pdread_csv(ioBytesIO(text)  delimiter = ' '  
                 converters = {0:str})
dfset_index(['STK_ID' 'RPT_Date']  inplace = True)


The index  dfindex can be reassigned to a new MultiIndex like this:

index = dfindex
names = indexnames
index = [('000999' '20121231')] + dfindextolist()[1:]
dfindex = pdMultiIndexfrom_tuples(index  names = names)
print(df)
#                   sales    cash
# STK_ID RPT_Date                
# 000999 20121231  80093  57488
# 000596 20120930  32585  26177
# 000799 20120930  14784   8157


Or  the index could be made into columns  the values in the columns could be then reassigned  and then the columns returned to indices:

dfreset_index(inplace = True)
dfix[0  ['STK_ID'  'RPT_Date']] = ('000999' '20121231')
dfset_index(['STK_ID' 'RPT_Date']  inplace = True)
print(df)

#                   sales    cash
# STK_ID RPT_Date                
# 000999 20121231  80093  57488
# 000596 20120930  32585  26177
# 000799 20120930  14784   8157


Benchmarking with IPython %timeit suggests reassigning the index (the first method  above) is significantly faster than resetting the index  modifying column values  and then setting the index again (the second method  above):

In [2]: %timeit reassign_index(df)
10000 loops  best of 3: 158 us per loop

In [3]: %timeit reassign_columns(df)
1000 loops  best of 3: 843 us per loop
",190597.0,,190597.0,,2013-01-01 13:58:15,2013-01-01 13:58:15,1.0
14224489,2,14224172,2013-01-08 21:38:58,1,"You could sort the columns using sort:

df1sort(axis=1) == df2sort(axis=1)


This will evaluate to a dataframe of all True values",1240268.0,,1240268.0,,2013-01-08 21:58:47,2013-01-08 21:58:47,
14440972,2,14440187,2013-01-21 14:40:55,1,"You can write a custom function for a rolling_window in Pandas Using numpy's argsort() in that function can give you the rank within the window:

import pandas as pd
import StringIO

testdata = StringIOStringIO(""""""
Date A
01-01-2013 100
02-01-2013 85
03-01-2013 110
04-01-2013 60
05-01-2013 20
06-01-2013 40"""""")

df = pdread_csv(testdata  header=True  index_col=['Date'])

rollrank = lambda data: datasize - dataargsort()argsort()[-1]

df['rank'] = pdrolling_apply(df  3  rollrank)

print df


results in:

              A  rank
Date                 
01-01-2013  100   NaN
02-01-2013   85   NaN
03-01-2013  110     1
04-01-2013   60     3
05-01-2013   20     3
06-01-2013   40     2
",1755432.0,,,,,2013-01-21 14:40:55,
14442343,2,14440187,2013-01-21 15:52:58,1,"If you want to use the Pandas built-in rank method (with some additional semantics  such as the ascending option)  you can create a simple function wrapper for it

def rank(array):
    s = pdSeries(array)
    return srank(ascending=False)[len(s)-1]


that can then be used as a custom rolling-window function

pdrolling_apply(df['A']  3  rank)


which outputs

Date
01-01-2013   NaN
02-01-2013   NaN
03-01-2013     1
04-01-2013     3
05-01-2013     3
06-01-2013     2


(I'm assuming the df data structure from Rutger's answer)",544059.0,,,,,2013-01-21 15:52:58,1.0
9788516,2,9788299,2012-03-20 14:14:40,2,"Try the truncate method:

dftruncate(before=d1  after=d2)


It won't modify your original df and will return a truncated one

From docs:

Function truncate a sorted DataFrame / Series before and/or after
some particular dates

Parameters
----------
before : date
    Truncate before date
after : date
    Truncate after date

Returns
-------
truncated : type of caller
",449449.0,,128508.0,,2012-12-19 03:07:09,2012-12-19 03:07:09,5.0
12077782,2,12052067,2012-08-22 16:48:03,4,"I'm currently in the middle of transition to Pandas DataFrames from the various Numpy arrays This has been relatively painless since Pandas  AFAIK  if built largely on top of Numpy What I mean by that is that mean()  sum() etc all work as you would hope On top of that  the ability to add a hierarchical index and use the ix[] (index) attribute and xs() (cross-section) method to pull out arbitray pieces of the code has greatly improved the readability and performance of my code (namely by reducing the number of round-trips to my database)

One thing I haven't fully investigated yet is Pandas compatibility with the more advanced functionality of Scipy and Matplotlib However  in case of any issues  it's easy enough to pull out a single column that behaves enough like an array for those libraries to work  or even convert to an array on the fly A DataFrame's plotting methods  for instance  rely on matplotlib and take care of any conversion for you

Also  if you're like me and your main use of Scipy is the statistics module  pystatsmodels is quickly maturing and relies heavily on pandas 

That's my two cents' worth",1552748.0,,,,,2012-08-22 16:48:03,
12184679,2,12052067,2012-08-29 18:53:08,4,"pandas's DataFrame is a high level tool while structured arrays are a very low-level tool  enabling you to interpret a binary blob of data as a table-like structure One thing that is hard to do in pandas is nested data types with the same semantics as structured arrays  though this can be imitated with hierarchical indexing (structured arrays can't do most things you can do with hierarchical indexing)

Structured arrays are also amenable to working with massive tabular data sets loaded via memory maps (npmemmap) This is a limitation that will be addressed in pandas eventually  though",776560.0,,,,,2012-08-29 18:53:08,
12394122,2,12389898,2012-09-12 18:19:35,2,"What problems are you running into with apply? It works for this toy example here and the group lengths are different:

In [82]: df
Out[82]: 
   X         Y
0  0 -0631214
1  0  0783142
2  0  0526045
3  1 -1750058
4  1  1163868
5  1  1625538
6  1  0076105
7  2  0183492
8  2  0541400
9  2 -0672809

In [83]: def func(x):
   :     x['NewCol'] = npnan
   :     return x
   : 

In [84]: dfgroupby('X')apply(func)
Out[84]: 
   X         Y  NewCol
0  0 -0631214     NaN
1  0  0783142     NaN
2  0  0526045     NaN
3  1 -1750058     NaN
4  1  1163868     NaN
5  1  1625538     NaN
6  1  0076105     NaN
7  2  0183492     NaN
8  2  0541400     NaN
9  2 -0672809     NaN
",1306530.0,,,,,2012-09-12 18:19:35,4.0
13031471,2,13030488,2012-10-23 13:25:25,2,"What is your data shape ?

For an n-by-1 data vector  you need a n-by-2 error vector (positive error and negative error ) : 

import pandas as pd 
import numpy as np
import matplotlibpyplot as plt


df2 = pdDataFrame( [  04   19 ] )
df2plot(kind='bar'  yerr = [ [01 30]   [30 01]]  )

pltshow()
",1741450.0,,,,,2012-10-23 13:25:25,3.0
13443473,2,13432213,2012-11-18 19:01:30,2,"It's a bug As a temporary workaround  you can do:

subset['Month'] = pdPeriodIndex(subset['Created On'] freq='M')asobject


http://githubcom/pydata/pandas/issues/2281",776560.0,,,,,2012-11-18 19:01:30,2.0
13888519,2,13887013,2012-12-15 01:39:41,0,"Pandas has an ordinary least squares (ols) function  there is a very detailed example in the docs of how to plot the result  here's a snippet:

model = ols(y=rets['AAPL']  x=retsix[:  ['GOOG']]  window=250)
# just plot the coefficient for GOOG
modelbeta['GOOG']plot()
",1240268.0,,,,,2012-12-15 01:39:41,
14193170,2,14192741,2013-01-07 09:32:27,4,"The pandas documentation says:


  Returning a view versus a copy
  
  The rules about when a view on the data is returned are entirely
  dependent on NumPy Whenever an array of labels or a boolean vector
  are involved in the indexing operation  the result will be a copy
  With single label / scalar indexing and slicing  eg dfix[3:6] or
  dfix[:  'A']  a view will be returned


In df[dfkey==1]['D'] you first do boolean slicing (leading to a copy of the Dataframe)  then you choose a column ['D']

In dfD[dfkey==1] = 34  you first choose a column  then do boolean slicing on the resulting Series 

This seems to make the difference  although I must admit that it is a little counterintuitive

Edit: The difference was identified by Dougal  see his comment: With version 1  the copy is made as the __getitem__ method is called for the boolean slicing For version 2  only the __setitem__ method is accessed - thus not returning a copy but just assigning",1156006.0,,344821.0,,2013-01-07 10:32:39,2013-01-07 10:32:39,4.0
14306366,2,14301004,2013-01-13 17:52:51,2,"In case date_time is not your index  a date_time-indexed DataFrame could be created with:

dfts = dfset_index('date_time')


From there you can group by intervals using

dftsgroupby(lambda x : xmonth)mean()


to see mean values for each month Similarly  you can do

dftsgroupby(lambda x : xyear)std()


for standard deviations across the years

If I understood the example task you would like to achieve  you could simply split the data into years using xs  group them and concatenate the results and store this in a new DataFrame

years = range(2012  2015)
yearly_month_stats = [dftsxs(str(year))groupby(lambda x : xmonth)mean() for year in years]
df2 = pdconcat(yearly_month_stats  axis=1  keys = years)


From which you get something like

        2012       2013       2014
       value      value      value
1        NaN   5324165  15747767
2        NaN -23193429   9193217
3        NaN -14144287  23896030
4        NaN -21877975  16310195
5        NaN  -3079910  -6093905
6        NaN  -2106847 -23253183
7        NaN  10644636   6542562
8        NaN  -9763087  14335956
9        NaN  -3529646   2607973
10       NaN -18633832   0083575
11       NaN  10297902  14059286
12  3395442  13692435  22293245
",544059.0,,544059.0,,2013-01-13 18:11:16,2013-01-13 18:11:16,
14489293,2,14488697,2013-01-23 21:00:42,3,"Yes  I could reproduce the problem  but don't know how to fix it with pdread_csv Here is a workaround:

In [46]: import numpy as np
In [47]: arr = npgenfromtxt('test3csv'  delimiter = ' '  
                             dtype = None  names = True)

In [48]: df = pdDataFrame(arr)

In [49]: df
Out[49]: 
   x    y
0     Reg
1     Reg
2  I  Swp
3  I  Swp


Note that with names = True the first valid line of the csv is interpreted as column names (and therefore does not affect the dtype of the values on the subsequent lines) Thus  if the csv file contains numerical data such as 

In [22]: with open('/tmp/testcsv' 'r') as f:
   :     print(repr(fread()))
   :     
'x y z\n \x00\x00\x00 Reg 1\n \x00\x00\x00 Reg 2\nI Swp 3\nI Swp 4\n'


Then genfromtxt will assign a numerical dtype to the third column (<i4 in this case)

In [19]: arr = npgenfromtxt('/tmp/testcsv'  delimiter = ' '  dtype = None  names = True)

In [20]: arr
Out[20]: 
array([(''  'Reg'  1)  (''  'Reg'  2)  ('I'  'Swp'  3)  ('I'  'Swp'  4)]  
      dtype=[('x'  '|S3')  ('y'  '|S3')  ('z'  '<i4')])


However  if the numerical data is intermingled with bytes such as '\x00' then genfromtxt will be unable to recognize this column as numerical and will therefore resort to assigning a string dtype Nevertheless  you can force the dtype of the columns by manually assigning the dtype parameter For example 

In [11]: arr = npgenfromtxt('/tmp/testcsv'  delimiter = ' '  dtype = [('x'  '|i4')  ('y'  '|S3')]  names = True)


sets the first column x to have dtype |i4 (4-byte integers) and the second column y to have dtype |S3 (3-byte string) See this doc page for more information on available dtypes",190597.0,,190597.0,,2013-01-24 22:47:02,2013-01-24 22:47:02,2.0
12204428,2,12203901,2012-08-30 19:52:44,0,"Inspecting framepy  it looks like pandas tries to insert a column 'index' or 'level_0'  If either/both(??) of them are already taken  then it throws the error

Fortunately  there's a ""drop"" option  AFAICT  this drops an existing index with the same name and replaces it with the new  reset index  This might get you in trouble if you have a column named ""index "" but I think otherwise you're okay

""Fixed"" code:

import pandas
A = pandasDataFrame({
    'val' :  ['aaaaa'  'acaca'  'ddddd'  'zzzzz'] 
    'extra' : range(10 14) 
})
A = Areset_index(drop=True)
A = Areset_index(drop=True)
A = Areset_index(drop=True)
",660664.0,,,,,2012-08-30 19:52:44,1.0
12463255,2,12461470,2012-09-17 16:16:28,1,"This should work:

df['salary'] = seriesreindex(dfindex  level=0)
",1306530.0,,,,,2012-09-17 16:16:28,4.0
12850453,2,12850345,2012-10-12 00:07:38,4,"I believe you can use the append

bigdata=data1append(data2 ignore_index = True)


to keep their indexes just dont use the ignore_index keyword ",541038.0,,,,,2012-10-12 00:07:38,
13072686,2,13072259,2012-10-25 16:06:06,6,"You can use the pivot function:

dfpivot(index='date'  columns='variable'  values='value')


For more info see: http://pandaspydataorg/pandas-docs/stable/reshapinghtml",653364.0,,,,,2012-10-25 16:06:06,1.0
13314300,2,13298633,2012-11-09 19:03:23,0,"You can filter duplicate rows with drop_duplicates  and select to join only columns that are not yet present

import pandas as pd
from StringIO import StringIO

data0 = """"""\
index A B
a 1 2
a 1 2
b 3 4
c 5 6
""""""

data1 = """"""\
index A C
a 7 8
d 9 10
""""""

df = pdDataFrame()
columns = []
for data in [data0  data1]:
    frame= pdread_csv(StringIO(data)  index_col=0)drop_duplicates()
    frame = frameix[:  framecolumns - columns]
    if len(frame):
        df = dfjoin(frame  how='outer') if len(df) else frame

print df


results in:

        A   B   C
index
a       1   2   8
b       3   4 NaN
c       5   6 NaN
d     NaN NaN  10
",1548051.0,,,,,2012-11-09 19:03:23,1.0
13652027,2,13651117,2012-11-30 19:43:40,1,"I didn't find a straight-forward way to do it within context of read_csv However  read_csv returns a DataFrame  which can be filtered using this:

Select rows by boolean vector   df[bool_vec]    DataFrame

(eg):

filtered = df[df['timestamp'] > targettime]]

This is selecting all rows in df (assuming df is any DataFrame  such as the result of a read_csv call  that at least contains a datetime column ""timestamp"") for which the values in the ""timestamp"" column are greater than the value of targettime

UPDATE:
This question is similar to pandas: filter rows of DataFrame with operator chaining",812933.0,,812933.0,,2012-11-30 21:29:31,2012-11-30 21:29:31,
13652102,2,13651117,2012-11-30 19:49:58,0,"you can only skip rows using argument skiprows

as Griffin mentioned i would just load all and filter the DataFrame if the rows you want to filter are not consecutive",239007.0,,,,,2012-11-30 19:49:58,
13653490,2,13651117,2012-11-30 21:31:28,1,"There isn't an option to filter before the file is loaded into a pandas object  you can either load the file and then filter as normal after reading like df[df['field'] > constant]  or if you have a very large file  and you are worried about memory running out  then use an iterator and apply the filter as you concatenate chunks of your file eg:

iter_csv = pandasread_csv('filecsv'  iterator=True  chunksize=1000)
df = pdconcat([chunk[chunk['field'] > constant] for chunk in iter_csv])


You can naturally vary the chunksize to suit your available memory See here for more details",1452002.0,,,,,2012-11-30 21:31:28,
11330122,2,11329611,2012-07-04 13:31:44,0,"time_effects is a valid parameter only for a panel OLS  not for a simple OLS
From pandas docs:

def ols(**kwargs):
""""""Returns the appropriate OLS object depending on whether you need
simple or panel OLS  and a full-sample or rolling/expanding OLS

Will be a normal linear regression or a (pooled) panel regression depending
on the type of the inputs:

y : Series  x : DataFrame -> OLS
y : Series  x : dict of DataFrame -> OLS
y : DataFrame  x : DataFrame -> PanelOLS
y : DataFrame  x : dict of DataFrame/Panel -> PanelOLS
y : Series with MultiIndex  x : Panel/DataFrame + MultiIndex -> PanelOLS


Can you post the CSV file you are using  to find out what OLS you are trying to do?

EDIT: from the CSV you posted it's clear that you are passing to pandasols two Series objects From the above docstring  to do a PanelOLS you need to pass two DataFrames instead",1063605.0,,1063605.0,,2012-07-05 09:29:09,2012-07-05 09:29:09,2.0
11384848,2,11329611,2012-07-08 16:45:17,0,"I reported a bug here I've seen one other bug report on to_panel  some additional info posted on GitHub (eg link to data file) would be useful:

https://githubcom/pydata/pandas/issues/1582",776560.0,,,,,2012-07-08 16:45:17,
11707706,2,11707586,2012-07-29 08:03:35,1,"You can use print dfdescribe()to_string() to force it to show the whole table  (You can use to_string() like this for any DataFrame  The result of describe is just a DataFrame itself)

The 8 is the number of rows in the DataFrame holding the ""description"" (because describe computes 8 statistics  min  max  mean  etc)",1427416.0,,,,,2012-07-29 08:03:35,1.0
11708664,2,11707586,2012-07-29 10:56:01,2,"You can adjust pandas print options with set_printoptions

In [3]: dfdescribe()
Out[3]: 
<class 'pandascoreframeDataFrame'>
Index: 8 entries  count to max
Data columns:
x1    8  non-null values
x2    8  non-null values
x3    8  non-null values
x4    8  non-null values
x5    8  non-null values
x6    8  non-null values
x7    8  non-null values
dtypes: float64(7)

In [4]: pdset_printoptions(precision=2)

In [5]: dfdescribe()
Out[5]: 
            x1       x2       x3       x4       x5       x6       x7
count      80      80      80      80      80      80      80
mean   690245  690255  690265  690275  690285  690295  690305
std       171     171     171     171     171     171     171
min    690000  690010  690020  690030  690040  690050  690060
25%    690122  690132  690142  690152  690162  690172  690182
50%    690245  690255  690265  690275  690285  690295  690305
75%    690368  690378  690388  690398  690408  690418  690428
max    690490  690500  690510  690520  690530  690540  690550


However this will not work in all cases as pandas detects your console width and it will only use to_string if the output fits in the console (see the docstring of set_printoptions) 
In this case you can explicitly call to_string as answered by BrenBarn

Update

With version 010 the way wide dataframes are printed changed:

In [3]: dfdescribe()
Out[3]: 
                 x1            x2            x3            x4            x5  \
count      8000000      8000000      8000000      8000000      8000000   
mean   59832361578  27356711336  49317281222  51214837838  51254839690   
std    22600723536  26867192716  28071737509  21012422793  33831515761   
min    31906695474   1648359160     56378115  16278322271     43745574   
25%    45264625201  12799540572  41429628749  40374273582  29789643875   
50%    56340214856  18666456293  51995661512  54894562656  47667684422   
75%    75587003417  31375610322  61069190523  67811893435  76014884048   
max    98136474782  84544484627  91743983895  75154587156  99012695717   

                 x6            x7  
count      8000000      8000000  
mean   41863000717  33950235126  
std    38709468281  29075745673  
min     3590990740   1833464154  
25%    15145759625   6879523949  
50%    22139243042  33706029946  
75%    72038983496  51449893980  
max    98601190488  83309051963  


Further more the API for setting pandas options changed:

In [4]: pdset_option('displayprecision'  2)

In [5]: dfdescribe()
Out[5]: 
            x1       x2       x3       x4       x5       x6       x7
count      80      80      80      80      80      80      80
mean   598324  273567  493173  512148  512548  418630  339502
std    226007  268672  280717  210124  338315  387095  290757
min    319067   16484     564  162783     437   35910   18335
25%    452646  127995  414296  403743  297896  151458   68795
50%    563402  186665  519957  548946  476677  221392  337060
75%    755870  313756  610692  678119  760149  720390  514499
max    981365  845445  917440  751546  990127  986012  833091
",1301710.0,,1301710.0,,2013-01-10 10:06:59,2013-01-10 10:06:59,1.0
11706782,2,7837722,2012-07-29 04:53:26,3,"I checked out iterrows after noticing Nick Crawford's answer  but found that it yields (index  Series) tuples Not sure which would work best for you  but I ended up using the itertuples method for my problem  which yields (index  row_value1) tuples

There's also iterkv  which iterates through (column  series) tuples",386279.0,,,,,2012-07-29 04:53:26,
10458522,2,10445549,2012-05-05 03:16:40,0,"It would be much easier if the following worked  but no:

dfix[[1  0  2]]


The following is more of a workaround Maybe there is a better way but I couldn't figure out any This merely creates a list of DataFrame 'slices' in the correct order and concatenates them with pandasconcat

In : df
Out:
            A         B         C
0 0  1202098 -0031121  1417629
  1 -0895862  0697531 -0572411
  2  1179101 -0008602  1583385
1 0  1969477 -0968004 -0567695
  1 -1504443 -0002264 -0413091
  2 -1412457  0310518  0267475
2 0 -0385933 -0471800 -0598141
  1 -0105032  0443437 -0615566
  2 -1035326 -0282289 -0042762

In : shuffled = [2 0 1]

In : df2 = pandasconcat([dfix[i:i] for i in shuffled])

In : df2
Out:
            A         B         C
2 0 -0385933 -0471800 -0598141
  1 -0105032  0443437 -0615566
  2 -1035326 -0282289 -0042762
0 0  1202098 -0031121  1417629
  1 -0895862  0697531 -0572411
  2  1179101 -0008602  1583385
1 0  1969477 -0968004 -0567695
  1 -1504443 -0002264 -0413091
  2 -1412457  0310518  0267475
",843822.0,,,,,2012-05-05 03:16:40,
10462487,2,10445549,2012-05-05 13:49:37,1,"The reindex method can accomplish this when passed a reordered array of tuples matching the desired order  At which point  reordering can be done as best fits your problem  For example:

In [38]: df
Out[38]: 
            A         B         C
0 0 -1725337  0111493  0178294
  1 -1809003 -0614219 -0931909
  2  0621427 -0186233  0254727
1 0 -1322863  1242415  1375579
  1  0249738 -1280204  0356491
  2 -0743671  0325841 -0167772
2 0 -0070937  0401172 -1790801
  1  1433794  2257198  1848435
  2 -1021557 -1054363 -1485536

In [39]: neworder = [1  0  2]

In [41]: newindex = sorted(dfindex  key=lambda x: neworderindex(x[0]))

In [42]: newindex
Out[42]: 
[(1L  0L) 
 (1L  1L) 
 (1L  2L) 
 (0L  0L) 
 (0L  1L) 
 (0L  2L) 
 (2L  0L) 
 (2L  1L) 
 (2L  2L)]

In [43]: dfreindex(newindex)
Out[43]: 
            A         B         C
1 0 -1322863  1242415  1375579
  1  0249738 -1280204  0356491
  2 -0743671  0325841 -0167772
0 0 -1725337  0111493  0178294
  1 -1809003 -0614219 -0931909
  2  0621427 -0186233  0254727
2 0 -0070937  0401172 -1790801
  1  1433794  2257198  1848435
  2 -1021557 -1054363 -1485536
",243434.0,,,,,2012-05-05 13:49:37,2.0
11385324,2,11289670,2012-07-08 17:53:34,1,It may be specific to your machine If you keep having problems  can you post an issue on http://githubcom/pydata/pandas/issues? ,776560.0,,,,,2012-07-08 17:53:34,
11616216,2,11289670,2012-07-23 16:02:57,0,This happened to me too It works if you right click and 'Run As Administrator',1546337.0,,,,,2012-07-23 16:02:57,
11684081,2,11679716,2012-07-27 08:22:32,1,"pandas DateOffsets does not inherit from timedelta It's possible for some DateOffsets to be compared  but for offsets like MonthEnd  MonthStart  etc  the span of time to the next offset is non-uniform and depends on the starting date

Please feel free to start a github issue on this at https://githubcom/pydata/pandas  we can continue the discussion there and it'll serve as a reminder

Thanks",1306530.0,,,,,2012-07-27 08:22:32,1.0
14197381,2,14197088,2013-01-07 13:53:26,0,"Something like this should do it  if I understand you correctly This is very quick and dirty  untested Reads the named file  prints to stdout:

for l in open(""filetxt"")readlines():
  l = lstrip()
  fields = lsplit("" "")
  if len(fields) != 4: continue
  if fields[3][0] == ""<"":
    fields[2] = ""<""
    fields[3] = fields[3][1:]
  print ""\t""join(fields)
",28169.0,,,,,2013-01-07 13:53:26,
14197643,2,14197088,2013-01-07 14:10:57,0,"You could create a function which takes an entry in dfD columns and returns a Series Then you can use Series apply with this function:

def f(d):
    try:
        if d[0] == '<':
            return pdSeries(['<='  float(d[1:])])
    except TypeError:
        return pdSeries(['=='  d])

In [9]: df = DataFrame({'A': {0: 'e'  1: 'e'  2: 'e'  3: 'e'  4: 'e'  5: 'e'}  'B': {0: 2  1: 2  2: 2  3: 2  4: 2  5: 2}  'C': {0: '='  1: '='  2: '='  3: '='  4: '='  5: '='}  'D': {0: '<01'  1: '<011'  2: 01  3: 01  4: 01  5: '<014'}})

In [10]: df
Out[10]: 
   A  B  C      D
0  e  2  =   <01
1  e  2  =  <011
2  e  2  =    01
3  e  2  =    01
4  e  2  =    01
5  e  2  =  <014

In [11]: df[['C'  'D']] = dfDapply(f)

In [12]: df
Out[12]: 
   A  B   C     D
0  e  2  <=   01
1  e  2  <=  011
2  e  2  ==   01
3  e  2  ==   01
4  e  2  ==   01
5  e  2  <=  014
",1240268.0,,1240268.0,,2013-01-07 23:19:17,2013-01-07 23:19:17,9.0
14371545,2,14371476,2013-01-17 03:01:48,0,"Convert local datetime to GMT datetime like this:

gmtDatetime = localdatetime - datetimetimedelta(hours=8)


The time zone is +08 (China)

Or using 'datetimeutcfromtimestamp':

classmethod datetimeutcfromtimestamp(timestamp)
classmethod datetimefromtimestamp(timestamp  tz=None)



  Return the UTC datetime corresponding to the POSIX timestamp  with
  tzinfo None This may raise OverflowError  if the timestamp is out of
  the range of values supported by the platform C gmtime() function  and
  OSError on gmtime() failure Its common for this to be restricted to
  years in 1970 through 2038 See also fromtimestamp()
",1120333.0,,1120333.0,,2013-01-17 03:07:21,2013-01-17 03:07:21,
14508355,2,14507794,2013-01-24 18:37:10,3,"I think the easiest way to do this would be to set the columns to the top level:

dfcolumns = dfcolumnsget_level_values(0)


Note: if the to level has a name you can also access it by this  rather than 0



If you want to combine/join your MultiIndex into one Index (assuming you have just string entries in your columns) you could:

dfcolumns = map(strstrip  map(' 'join  dfcolumnsvalues))


Note: we must strstrip the whitespace for when there is no second index

In [11]: map(strstrip  map(' 'join  dfcolumnsvalues))
Out[11]: 
['USAF' 
 'WBAN' 
 'day' 
 'month' 
 's_CD sum' 
 's_CL sum' 
 's_CNT sum' 
 's_PC sum' 
 'tempf amax' 
 'tempf amin' 
 'year']
",1240268.0,,1240268.0,,2013-01-24 19:26:28,2013-01-24 19:26:28,9.0
14508633,2,14507794,2013-01-24 18:53:56,1,"And if you want to retain any of the aggregation info from the second level of the multiindex you can try this:

In [1]: [''join(t) for t in dfcolumns]
Out[1]:
['USAF' 
 'WBAN' 
 'day' 
 'month' 
 's_CDsum' 
 's_CLsum' 
 's_CNTsum' 
 's_PCsum' 
 'tempfamax' 
 'tempfamin' 
 'year']

In [2]: dfcolumns = new_cols
",919872.0,,,,,2013-01-24 18:53:56,
14508639,2,14507794,2013-01-24 18:54:14,2,"Andy Hayden's answer is certainly the easiest way -- if you want to avoid duplicate column labels you need to tweak a bit

In [34]: df
Out[34]: 
     USAF   WBAN  day  month  s_CD  s_CL  s_CNT  s_PC  tempf         year
                               sum   sum    sum   sum   amax   amin      
0  702730  26451    1      1    12     0     13     1  3092  2498  1993
1  702730  26451    2      1    13     0     13     0  3200  2498  1993
2  702730  26451    3      1     2    10     13     1  2300   698  1993
3  702730  26451    4      1    12     0     13     1  1004   392  1993
4  702730  26451    5      1    10     0     13     3  1994  1094  1993


In [35]: mi = dfcolumns

In [36]: mi
Out[36]: 
MultiIndex
[(USAF  )  (WBAN  )  (day  )  (month  )  (s_CD  sum)  (s_CL  sum)  (s_CNT  sum)  (s_PC  sum)  (tempf  amax)  (tempf  amin)  (year  )]


In [37]: mitolist()
Out[37]: 
[('USAF'  '') 
 ('WBAN'  '') 
 ('day'  '') 
 ('month'  '') 
 ('s_CD'  'sum') 
 ('s_CL'  'sum') 
 ('s_CNT'  'sum') 
 ('s_PC'  'sum') 
 ('tempf'  'amax') 
 ('tempf'  'amin') 
 ('year'  '')]

In [38]: ind = pdIndex([e[0] + e[1] for e in mitolist()])

In [39]: ind
Out[39]: Index([USAF  WBAN  day  month  s_CDsum  s_CLsum  s_CNTsum  s_PCsum  tempfamax  tempfamin  year]  dtype=object)

In [40]: dfcolumns = ind




In [46]: df
Out[46]:
  USAF  WBAN day month s_CDsum s_CLsum s_CNTsum s_PCsum tempfamax tempfamin \
0 702730 26451  1   1    12    0    13    1   3092   2498 
1 702730 26451  2   1    13    0    13    0   3200   2498 
2 702730 26451  3   1    2    10    13    1   2300    698 
3 702730 26451  4   1    12    0    13    1   1004    392 
4 702730 26451  5   1    10    0    13    3   1994   1094 




 year 
0 1993 
1 1993 
2 1993 
3 1993 
4 1993
",733291.0,,,,,2013-01-24 18:54:14,
14095645,2,14095463,2012-12-30 23:09:45,1,"Here's one workaround to do it using apply  not ideal but it works:

In [11]: from pandaslib import Timestamp

In [12]: dfDatesapply(lambda x: Timestamp(x)month)
Out[12]: 
0      1
1      1
2      1
3      1
4      1
5      1
6      1
7      1
8      1
9      1
10    12
Name: Dates


It seems like a bug (that you can't do apply(lambda x: xmonth))  perhaps worth adding as an issue on github As Wes would say: ""welcome to hell""",1240268.0,,,,,2012-12-30 23:09:45,
14095663,2,14095463,2012-12-30 23:13:23,1,For now I would suggest doing pdDatetimeIndex(datesDates)month I've been debating whether to add a bunch of data type-specific attributes to Series that will only work for timestamps  but haven't done it yet,776560.0,,,,,2012-12-30 23:13:23,1.0
14224444,2,14224068,2013-01-08 21:35:53,2,"Confirmed that there's some kind of memory leak going on in the indexing infrastructure It's not caused by the above reference graph Let's move the discussion to GitHub (SO is for Q&A):

https://githubcom/pydata/pandas/issues/2659

EDIT: this actually appears to not be a memory leak at all  but has to do with the OS memory allocation issues perhaps Please have a look at the github issue for more information",776560.0,,776560.0,,2013-01-08 23:01:20,2013-01-08 23:01:20,2.0
14417036,2,14416660,2013-01-19 17:46:39,3,"You are just printing these and not apply-ing them to the DataFrame  here's one way to do it:

Create a function to do the striping (if unicode) or leave it if already a number:

def no_comma_or_dollar(num):
    if isinstance(num  unicode):
        return float(numlstrip('$')replace(' ' ''))
    else:
        return num

table[col_name] = table[col_name]apply(no_comma_or_dollar)


For example:

df = pdDataFrame([[u'$1 000']  [200]])

In [3]: df[0]apply(no_comma_or_dollar)
Out[3]: 
0    1000
1     200
Name: 0


Update:

With the thread which you give  I would be tempted to give a slightly lazier version of no_comma_or_dollar and applymap:

def no_comma_or_dollar2(num):
    try:
        return float(numlstrip('$')replace(' ' ''))
    except: # if you can't strip/replace/convert just leave it
        return num

In [5]: threadapplymap(no_comma_or_dollar2)
Out[5]: 
        Consolidated Balance Sheet (USD $)  Dec 31  2011  Sep 30  2012
0  In Millions  unless otherwise specified            NaN            NaN
1                           Current assets            NaN            NaN
2                Cash and cash equivalents           2219           3029
3          Marketable securities - current           1461           1989
4                Accounts receivable - net           3867           4409
",1240268.0,,1240268.0,,2013-01-20 02:10:00,2013-01-20 02:10:00,
14417039,2,14416660,2013-01-19 17:46:54,3,"If I understand you right  you're looking for the apply method:

In [33]: import pandas as pd

In [34]: table = pdSeries([None  u'$ 3 12'  u'$ 4 5'])

In [35]: table
Out[35]: 
0      None
1    $ 3 12
2     $ 4 5

In [36]: def f(cell):
   :     if isinstance(cell  unicode):
   :         return float(celllstrip('$')replace(' ' ''))
   :     else:
   :         return cell
   :     

In [37]: tableapply(f)
Out[37]: 
0    NaN
1    312
2     45


This does create a new object In order to store the new object instead of the old  do:

In [42]: table = tableapply(f)

In [43]: table
Out[43]: 
0    NaN
1    312
2     45
",733291.0,,,,,2013-01-19 17:46:54,
12279674,2,12110812,2012-09-05 10:44:34,0,"Is this what you are intending to do?

for i in range(1 6):
    df['super_'+str(i)] = df['place']map(lambda x: xcount(str(i)) )
",1240268.0,,,,,2012-09-05 10:44:34,
12417434,2,12417129,2012-09-14 02:40:04,1,"They are not zero pandas probably does some formatting while printing DataFrame/Series so they look like zero

By the way  you don't need converters read_table correctly identifies them as float64:

In [117]: df = pandasread_table('gradStat_mmntdf')

In [118]: dfix[0:10]
Out[118]:
    Subject Group Local Global  Attn  mean
0         1  DSub     S      S  Attn     0
1         1  DSub     S      S  Dist     0
2         1  DSub     D      S  Attn     0
3         1  DSub     D      S  Dist     0
4         1  DSub     S      D  Attn     0
5         1  DSub     S      D  Dist     0
6         1  DSub     D      D  Attn     0
7         1  DSub     D      D  Dist     0
8         2  ASub     S      S  Attn     0
9         2  ASub     S      S  Dist     0
10        2  ASub     D      S  Attn     0

In [119]: df['mean']dtype
Out[119]: dtype('float64')

In [120]: df['mean'][0]
Out[120]: 32529000000000002e-22
",843822.0,,,,,2012-09-14 02:40:04,
12514640,2,12417129,2012-09-20 14:19:21,2,"This has been fixed with version 09 of pandas:

In [4]: df = pandasread_table('http://dldropboxcom/u/6160029/gradStat_mmntdf')

In [5]: dfhead()
Out[5]: 
   Subject Group Local Global  Attn          mean
0        1  DSub     S      S  Attn  3252900e-22
1        1  DSub     S      S  Dist  6010100e-22
2        1  DSub     D      S  Attn  4215700e-22
3        1  DSub     D      S  Dist  8308100e-22
4        1  DSub     S      D  Attn  2983500e-22
",776560.0,,1301710.0,,2012-10-13 08:39:12,2012-10-13 08:39:12,1.0
12700373,2,12698764,2012-10-03 00:30:35,0,"I believe the levels are ordered by the labels:

In [38]: alevels
Out[38]: Index([mixed  read  write]  dtype=object)

In [39]: alabels
Out[39]: array([1  0  2])
",1306530.0,,,,,2012-10-03 00:30:35,2.0
13039399,2,12698764,2012-10-23 21:17:53,0,"Categorical expects an array of integers and an array of levels:

In [14]: Categorical([0  1  2]  Index(['read'  'write'  'mixed']))
Out[14]: 
Categorical: 
array([read  write  mixed]  dtype=object)
Levels (3): Index([read  write  mixed]  dtype=object)


I do not believe it does much error checking (since typically they're created by some other function) but this could be changed",776560.0,,,,,2012-10-23 21:17:53,
13051376,2,13050003,2012-10-24 14:31:58,0,"repeated_items = [list(row[1]*row[2]) for row in dfitertuples()]


will create a nested list:

[['A']  []  ['C'  'C']]


which you can then iterate over with list comprehensions to create a new data frame:

new_df = pdDataFrame({""class"":[j for i in repeated_items for j in i]})


Of course  you can do it in a single line as well if you want:

new_df = pdDataFrame({""class"":[j for i in [list(row[1]*row[2]) for row in dfitertuples()] for j in i]})
",1452002.0,,,,,2012-10-24 14:31:58,
13052373,2,13050003,2012-10-24 15:25:40,2,"You could use groupby:

def f(group):
    row = groupirow(0)
    return DataFrame({'class': [row['class']] * row['count']})
dfgroupby('class'  group_keys=False)apply(f)


so you get

In [25]: dfgroupby('class'  group_keys=False)apply(f)
Out[25]: 
  class
0     A
0     C
1     C


You can fix the index of the result however you like",776560.0,,,,,2012-10-24 15:25:40,1.0
13251056,2,13250499,2012-11-06 12:28:06,1,"This is unrelated to subclassing Pandas objects' attributes do not serialize 

You can read this thread for a discussion and a workaround The topic has resurfaced again in this other recent thread",54567.0,,,,,2012-11-06 12:28:06,1.0
14600682,2,14580684,2013-01-30 09:37:19,0,"
  Ok  if you go that route  this answer stackoverflowcom/a/5314808/243434 on how to capture >matplotlib figures as inline PNGs may help  @crewbum
  
  To prevent duplication of plots  try running with pylab disabled (double-check your config >files and the command line)  @crewbum


--> this last requires a restart of the notebook: ipython notebook --pylab (NB no inline)",1583083.0,,,,,2013-01-30 09:37:19,1.0
9772031,2,9762935,2012-03-19 14:30:06,7,"Do dfjoin(df2):

http://pandaspydataorg/pandas-docs/stable/merginghtml#joining-on-index",776560.0,,,,,2012-03-19 14:30:06,1.0
10533776,2,10532501,2012-05-10 12:16:19,0,"import pandas as pd

def stretch(start_date  end_date  value  freq):
    freq_dict = {'d': pddatetoolsday 
                 'h': pddatetoolsHour(1)}
    dr = pdDateRange(start_date  end_date  offset=freq_dict[freq])
    return pdTimeSeries(value / drsize  index=dr)


print stretch('2011-01-01 00:00'  '2011-01-20 00:00'  200  'd')


prints

2011-01-01    10
2011-01-02    10
2011-01-03    10
2011-01-04    10
2011-01-05    10
2011-01-06    10
2011-01-07    10
2011-01-08    10
2011-01-09    10
2011-01-10    10
2011-01-11    10
2011-01-12    10
2011-01-13    10
2011-01-14    10
2011-01-15    10
2011-01-16    10
2011-01-17    10
2011-01-18    10
2011-01-19    10
2011-01-20    10
",449449.0,,,,,2012-05-10 12:16:19,3.0
11018542,2,11017862,2012-06-13 15:42:31,1,"If you wrote the query I'd recommend just using CAST to control the data type I always explicitly cast computed columns in SQL Server

SELECT CAST(ColumnA/ColumnB AS FLOAT) AS 'FieldA' FROM Table
",1454120.0,,458741.0,,2012-06-13 20:34:22,2012-06-13 20:34:22,1.0
11083005,2,11017862,2012-06-18 12:34:23,2,"Here are conversion tables (under the Results heading for Python 2x and 3x) detailing the conversions between ODBC and Python data types

Either CAST the result as mentioned in @Aaron's answer  or change the source data type in the database

With some more detail (specific column data type  sample data  flexibility of database changes)  we could help decide which option is best",366335.0,,,,,2012-06-18 12:34:23,1.0
11589000,2,11587782,2012-07-21 02:29:57,2,"It's hard to infer what you're looking for from the question  but my best guess is as follows

If we assume you have a DataFrame where some column is 'Category' and contains integers (or otherwise unique identifiers) for categories  then we can do the following

Call the DataFrame dfrm  and assume that for each row  dfrm['Category'] is some value in the set of integers from 1 to N Then 

for elem in dfrm['Category']unique():
    dfrm[str(elem)] = dfrm['Category'] == elem


Now there will be a new indicator column for each category that is True/False depending on whether the data in that row are in that category

If you want to control the category names  you could make a dictionary  such as

cat_names = {1:'Some_Treatment'  2:'Full_Treatment'  3:'Control'}
for elem in dfrm['Category']unique():
    dfrm[cat_names[elem]] = dfrm['Category'] == elem


to result in having columns with specified names  rather than just string conversion of the category values In fact  for some types  str() may not produce anything useful for you",567620.0,,,,,2012-07-21 02:29:57,
11991366,2,11987270,2012-08-16 16:11:34,1,It seems to be an issue with using run filenamepy in ipython  it for some reason used alot more RAM then the ordinary python from the command prompt,905596.0,,,,,2012-08-16 16:11:34,
12343023,2,11987270,2012-09-09 21:11:52,0,There are some places where pandas is not as careful as it could be about memory usage when it comes to MultiIndex-- if you do find a case that reproduces the issue please do post it on the issue tracker,776560.0,,,,,2012-09-09 21:11:52,
14657647,2,14656852,2013-02-02 01:23:46,2,"use rpy the conversion is part of pandas so you don't need to do it yoursef
http://pandaspydataorg/pandas-docs/dev/r_interfacehtml

In [1217]: from pandas import DataFrame

In [1218]: df = DataFrame({'A': [1  2  3]  'B': [4  5  6]  'C':[7 8 9]} 
   :                index=[""one""  ""two""  ""three""])
   :

In [1219]: r_dataframe = comconvert_to_r_dataframe(df)

In [1220]: print type(r_dataframe)
<class 'rpy2robjectsvectorsDataFrame'>
",239007.0,,,,,2013-02-02 01:23:46,1.0
10067924,2,10065051,2012-04-09 01:13:24,8,Have you looked at pandas/io/sqlpy  in particular the read_frame function? That would be the best place to start,776560.0,,,,,2012-04-09 01:13:24,3.0
11138275,2,10065051,2012-06-21 12:34:29,11,"As Wes says  io/sql's read_frame will do it  once you've gotten a database connection using a DBI compatible library  Here's a short example using the MySQLdb and cx_Oracle libraries to connect to Oracle and MySQL and query their data dictionaries:

import pandasiosql as psql
import cx_Oracle

ora_conn = cx_Oracleconnect('your_connection_string')
df_ora = psqlframe_query('select * from user_objects'  con=ora_conn)    
print 'loaded dataframe from Oracle # Records: '  len(df_ora)
ora_connclose()


import MySQLdb
mysql_cn= MySQLdbconnect(host='myhost'  
                port=3306 user='myusername'  passwd='mypassword'  
                db='information_schema')
df_mysql = psqlframe_query('select * from VIEWS;'  con=mysql_cn)    
print 'loaded dataframe from MySQL records:'  len(df_mysql)
mysql_cnclose()
",1472080.0,,1472080.0,,2012-06-21 12:45:38,2012-06-21 12:45:38,
12834193,2,10065051,2012-10-11 07:18:29,3,"The same syntax works for Ms SQL server using podbc also 

import pyodbc
import pandasiosql as psql

cnxn = pyodbcconnect('DRIVER={SQL Server};SERVER=servername;DATABASE=mydb;UID=username;PWD=password') 
cursor = cnxncursor()
sql = (""""""select * from mytable"""""")

df = psqlframe_query(sql  cnxn)
cnxnclose()
",573491.0,,,,,2012-10-11 07:18:29,
14247741,2,10065051,2013-01-09 22:36:02,0,"For Sybase the following works (with http://python-sybasesourceforgenet)

import pandasiosql as psql
import Sybase

df = psqlframe_query(""<Query>""  con=Sybaseconnect(""<dsn>""  ""<user>""  ""<pwd>""))
",1827356.0,,1827356.0,,2013-01-09 22:48:13,2013-01-09 22:48:13,
10856106,2,10839701,2012-06-01 19:15:41,1,"You can convert dfindex to integers and use that to compute the average There is a shortcut asi8 property that returns an array of int64 values:

npaverage(dfy - dfx  weights=dfindexasi8)
",776560.0,,,,,2012-06-01 19:15:41,1.0
11214168,2,11197690,2012-06-26 18:56:50,0,Try setting min_periods to be less than the window size (like 70  say) That means the minimum number of non-NA periods-- whenever you have an NA in the window the result will be NA,776560.0,,,,,2012-06-26 18:56:50,
11850848,2,11848578,2012-08-07 17:10:48,0,"I ran the snippet you pasted and it seems fine to me What version of pandas/numpy are you using? Can you post all/more of the data?

In [26]: paste
{u'_id': u'770000000049' 
 u'value': {u'timestamps': [datetimedatetime(2012  7  25  10  16  1  270000) 
                            datetimedatetime(2012  7  25  10  18  29  745000) 
                            datetimedatetime(2012  7  25  10  21  54  931000) 
                            datetimedatetime(2012  7  25  10  23  18  896000)] 
            u'values': [2040  16788  1392  116004]}}
## -- End pasted text --
Out[26]: 
{u'_id': u'770000000049' 
 u'value': {u'timestamps': [datetimedatetime(2012  7  25  10  16  1  270000) 
   datetimedatetime(2012  7  25  10  18  29  745000) 
   datetimedatetime(2012  7  25  10  21  54  931000) 
   datetimedatetime(2012  7  25  10  23  18  896000)] 
  u'values': [2040  16788  1392  116004]}}

In [27]: data = [_]

In [28]: paste
df_data = dict()

for x in data:
    series = pandasSeries(x['value']['values']  index=x['value']['timestamps'])

    df_data[x['_id']] = series

df = pandasDataFrame(df_data)
## -- End pasted text --

In [29]: print df['770000000049']
2012-07-25 10:16:01270000    204000
2012-07-25 10:18:29745000     16788
2012-07-25 10:21:54931000    139200
2012-07-25 10:23:18896000    116004
Name: 770000000049
",1306530.0,,,,,2012-08-07 17:10:48,2.0
9794823,2,9788299,2012-03-20 20:58:26,4,"This works:

In [25]: dfix[d1:d2]
Out[25]: 
                   A         B         C         D
2000-01-10  1149815  0686696 -1230991 -1610557
2000-01-11 -1296118 -0172950 -0603887  0383690
2000-01-12 -1034574 -0523238  0626968  0471755
2000-01-13 -0193280  1857499 -0046383  0849935
2000-01-14 -1043492 -0820525  0868685 -0773050
2000-01-17 -1622019 -0363992  1207590  0577290


cf http://pandaspydataorg/pandas-docs/stable/indexinghtml#advanced-indexing-with-labels

On first principles df[d1:d2] should work as it does for Series:

In [27]: df['A'][d1:d2]
Out[27]: 
2000-01-10    1149815
2000-01-11   -1296118
2000-01-12   -1034574
2000-01-13   -0193280
2000-01-14   -1043492
2000-01-17   -1622019
Name: A


Creating an issue here: https://githubcom/pydata/pandas/issues/946",776560.0,,,,,2012-03-20 20:58:26,
10602831,2,10601041,2012-05-15 14:22:04,4,"A couple things to point out:

In ""Check memory after changing size""  you haven't deleted the original DataFrame yet  so          this will be using strictly more memory
The Python interpreter is a bit greedy about holding onto OS memory
I looked into this and can assure you that pandas is not leaking memory I'm using the memory_profiler (http://pypipythonorg/pypi/memory_profiler) package:

import time  string  pandas  numpy  gc
from memory_profiler import LineProfiler  show_results
import memory_profiler as mprof

prof = LineProfiler()

@prof
def test(nrow=1000000  ncol = 4  timetest = 5):
    from_ = nrow // 10
    to_ = 9 * nrow // 10
    df = pandasDataFrame(numpyrandomrandn(nrow  ncol) 
                          index = numpyrandomrandn(nrow) 
                          columns = list(stringletters[0:ncol]))
    df_new = df[from_:to_]copy()
    del df
    del df_new
    gccollect()

test()
# for _ in xrange(10):
#     print mprofmemory_usage()

show_results(prof)


And here's the output

10:15 ~/tmp $ python profmempy 
Line #    Mem usage  Increment   Line Contents
==============================================
     7                           @prof
     8     2877 MB    000 MB   def test(nrow=1000000  ncol = 4  timetest = 5):
     9     2877 MB    000 MB       from_ = nrow // 10
    10     2877 MB    000 MB       to_ = 9 * nrow // 10
    11     5919 MB   3042 MB       df = pandasDataFrame(numpyrandomrandn(nrow  ncol) 
    12     6677 MB    758 MB                             index = numpyrandomrandn(nrow) 
    13     9046 MB   2370 MB                             columns = list(stringletters[0:ncol]))
    14    11496 MB   2449 MB       df_new = df[from_:to_]copy()
    15    11496 MB    000 MB       del df
    16     9054 MB  -2442 MB       del df_new
    17     5239 MB  -3815 MB       gccollect()


So indeed  there is more memory in use than when we started But is it leaking?

for _ in xrange(20):
    test()
    print mprofmemory_usage()


And output:

10:19 ~/tmp $ python profmempy 
[523984375]
[12259375]
[12259375]
[12259375]
[12259375]
[12259375]
[12259375]
[12259375]
[12259375]
[12259375]
[12259375]
[12259375]
[12259375]
[12259375]
[12259375]
[12259375]
[12259375]
[12259765625]
[12259765625]
[12259765625]


So actually what's gone on is that the Python process is holding on to a pool of memory given what it's been using to avoid having to keep requesting more memory (and then freeing it) from the host OS I don't know all the technical details behind this  but that is at least what is going on",776560.0,,,,,2012-05-15 14:22:04,
11131297,2,11129478,2012-06-21 03:57:18,0,"If I understand your problem correctly  you need to find all node pairs that are not bidirectional In the example above  the only such pair of nodes is 2 and 3 Given this  you could do the following:

In [1]: df['is_bi'] = dfindexmap(lambda x: npany(map(lambda y: npall(y)  dfix[x][['E'  'S']]values == dfvalues)))
In [2]: df
Out[2]:
   S  E  is_bi
0  1  2   True
1  1  3   True
2  2  1   True
3  2  3  False
4  3  1   True


So df[-dfis_bi] will give you all pairs of nodes that are not bidirectional:

In [3]: df[-dfis_bi][['S'  'E']]
Out[3]: 
   S  E
3  2  3


I have the feeling that I overly complicated this and there must be a way to do this with pandas-native functions  but the above solution does the trick",696023.0,,,,,2012-06-21 03:57:18,
11711637,2,11707586,2012-07-29 18:02:02,6,"As mentioned above  pandas auto detects (by default) the size of the display area  a summary view will be used when an object repr does not fit on the display You mentioned resizing the IDLE window  to no effect If you do print dfdescribe()to_string() does it fit on the IDLE window?

The terminal size is determined by pandasutilterminalget_terminal_size()  this returns a tuple containing the (width  height) of the display Does the output match the size of your IDLE window? There might be an issue (there was one before when running a terminal in emacs)

Note that it is possible to bypass the autodetect  pandasset_printoptions(max_rows=200  max_columns=10) will never switch to summary view if number of rows  columns does not exceed the given limits",1548051.0,,,,,2012-07-29 18:02:02,3.0
12201215,2,12201099,2012-08-30 16:14:27,3,"Sure  you can use the rename method:

In [11]: df = DataFrame({""A"": [1 2]  ""B"": [3 4]})

In [12]: dfrename(columns={""A"": ""series formerly known as A""})
Out[12]: 
   series formerly known as A  B
0                           1  3
1                           2  4


This doesn't change df  though:

In [13]: df
Out[13]: 
   A  B
0  1  3
1  2  4


You can get that behaviour if you want using inplace:

In [14]: dfrename(columns={""A"": ""series formerly known as A""}  inplace=True)
Out[14]: 
   series formerly known as A  B
0                           1  3
1                           2  4

In [15]: df
Out[15]: 
   series formerly known as A  B
0                           1  3
1                           2  4
",487339.0,,,,,2012-08-30 16:14:27,
12834717,2,12834568,2012-10-11 07:51:56,1,">>> date_rng = pddate_range('20060101' '20060105' freq='D')
>>> pdSeries(date_rngformat())

0    2006-01-01
1    2006-01-02
2    2006-01-03
3    2006-01-04
4    2006-01-05
",449449.0,,449449.0,,2012-10-11 07:58:58,2012-10-11 07:58:58,1.0
13068991,2,13068551,2012-10-25 12:49:13,3,"In recent version of pandas help(dataxs) shows how to get a view on the data 

It can be done by using xs arg copy=False",1548051.0,,1792889.0,,2012-11-01 23:59:15,2012-11-01 23:59:15,5.0
13313756,2,13289368,2012-11-09 18:20:05,0,This was discussed and resolved on GitHub: https://githubcom/pydata/pandas/issues/2188 The issue had to due with Clang's C99 behavior wrt inline C functions,776560.0,,,,,2012-11-09 18:20:05,3.0
13665037,2,13289368,2012-12-01 23:57:20,0,"Fixed the root problem (pip install pandas failing) on my MBP  and then tested it on another box -- the problem seems to be a conflict between the old version of numpy that comes with the mac install  and the newer one subsequently installed using pip Pandas sees the old numpy  and fails 

To fix this  cd to the location of the default packages Yours is probably the same as mine: 

$ cd /System/Library/Frameworks/Pythonframework/Versions/27/Extras/lib/python/ 
$ sudo rm -r numpy

$ sudo pip install pandas


With that out of the way  sudo pip install pandas worked for me on both boxes ",1453172.0,,,,,2012-12-01 23:57:20,
13615440,2,13614273,2012-11-28 22:20:31,0,"Well  the data isn't lying about the observed 3-tuples:

In [9]: df[['cuepos'  'targetpos'  'soa']]drop_duplicates()
Out[9]: 
     cuepos  targetpos        soa
0         2          2   0400000
1         2          1   0400000
2         1          1  -0100000
3         1          1   0400000
4         1          2  -0100000
5         1          1   0100000
8         2          2  -0100000
12        1          2   0400000
18        2          2   0100000
24        2          1  -0100000
52       85         85  85689698
77       -3         -3  -3265563
117     -83        -83 -83869535
133      11         11  11225720
26      -88        -88 -88206486
31      -48        -48 -48634430
34       63         63  63157160
55       85         85  85841413
80      -61        -61 -61812236
86      -61        -61 -61891543
89       87         87  87215989
92       80         80  80226447
94       58         58  58204967
126      71         71  71912378
128      60         60  60154749
132       8          8   8476819
139      65         65  65626850
141      54         54  54554612
11      -61        -61 -61800551
39      -33        -33 -33771084
46       76         76  76373127
52      -37        -37 -37064064
55      -44        -44 -44847510
60      -70        -70 -70332081
62       61         61  61735423
63       75         75  75201113
69       58         58  58845140
94      -79        -79 -79815161
109     -29        -29 -29062383
111     -51        -51 -51356056
117     -83        -83 -83879358
123      21         21  21075946
135     -31        -31 -31183030
143       6          6   6928400
4       -17        -17 -17511389
11       57         57  57490227
18      -88        -88 -88447288
36       78         78  78397278
39      -14        -14 -14673170
42       52         52  52107845
49      -87        -87 -87773396
50       60         60  60495264
71       33         33  33756600
74      -61        -61 -61969217
84       18         18  18548979
85       -8         -8  -8475269
98      -59        -59 -59815514
101     -80        -80 -80384548
114     -39        -39 -39031321
119      71         71  71873576
121     -86        -86 -86863720
128      68         68  68729972
130     -34        -34 -34233129
140      82         82  82759221
0       -75        -75 -75839334
15       67         67  67326223
34      -57        -57 -57679345
35      -74        -74 -74535290
42      -48        -48 -48026567
67       85         85  85191812
75       72         72  72877565
80       -7         -7  -7484698
99       -9         -9  -9389620
118     -44        -44 -44208168
130      73         73  73235472
143      58         58  58442172
56       22         22  22364050
67      -85        -85 -85505145
95       60         60  60621240
109      54         54  54220262
111      87         87  87809107
112     -81        -81 -81435704
114      71         71  71640124
119     -22        -22 -22176304
120      27         27  27977155
121      56         56  56923978
128      57         57  57820305
133      22         22  22101345
11       61         61  61882729
13       58         58  58591805
28       57         57  57156343
78      -80        -80 -80706399
80       49         49  49592969
81      -37        -37 -37092464
101     -36        -36 -36895517
124      17         17  17272663
128      51         51  51094356
137     -89        -89 -89411393
140     -64        -64 -64180244
36       -8         -8  -8871026
44      -73        -73 -73103972
47       -9         -9  -9281091
49       -2         -2  -2842961
51       87         87  87608103
85       24         24  24658481
90      -53        -53 -53910388
98       82         82  82385620
120      79         79  79276744
127     -43        -43 -43971857
130      13         13  13949059
",776560.0,,,,,2012-11-28 22:20:31,1.0
13984485,2,13983876,2012-12-21 04:56:45,3,"pdconcat() performs an 'outer' join on the indexes by default and holes can be filled by padding forwards and/or backwards in time

In [17]: pdconcat([DataFrame({'s1': s1})  DataFrame({'s2': s2})])ffill()bfill()
Out[17]: 
                 s1   s2
2012-12-21  90e-01 -03
2012-12-22  50e-03 -03
2012-12-23 -29e-01 -03
2012-12-23 -29e-01 -03
2012-12-24 -29e-01 -18
2012-12-25 -29e-01 -14


I should add that ffill() and bfill() are new in pandas 0100  Prior to that  you can use fillna(method='ffill') and fillna(method='bfill')",243434.0,,243434.0,,2012-12-21 05:25:46,2012-12-21 05:25:46,1.0
14189912,2,14189695,2013-01-07 04:32:47,3,"How about simply reassigning dfcolumns:

levels = dfcolumnslevels
labels = dfcolumnslabels
dfcolumns = levels[1][labels[1]]


For example:

import pandas as pd

columns = pdMultiIndexfrom_arrays([['basic_amt']*4 
                                     ['NSW' 'QLD' 'VIC' 'All']])
index = pdIndex(['All'  'Full Time'  'Part Time']  name = 'Faculty')
df = pdDataFrame([(1 1 2 4) 
                   (0 01 0 1) 
                   (1 0 2 3)])
dfcolumns = columns
dfindex = index


Before:

print(df)

           basic_amt               
                 NSW  QLD  VIC  All
Faculty                            
All                1    1    2    4
Full Time          0    1    0    1
Part Time          1    0    2    3


After:

levels = dfcolumnslevels
labels = dfcolumnslabels
dfcolumns = levels[1][labels[1]]
print(df)

           NSW  QLD  VIC  All
Faculty                      
All          1    1    2    4
Full Time    0    1    0    1
Part Time    1    0    2    3
",190597.0,,190597.0,,2013-01-10 22:57:29,2013-01-10 22:57:29,8.0
14321532,2,14284266,2013-01-14 15:35:58,2,"Toy example  First make a datetime index  Here I make an index using two days repeated 10 times each  I then make some dummy data using randn

In [1]: date_index = [datetime(2012 01 01)] * 10 + [datetime(2013 01 01)] * 10

In [2]: df = DataFrame({'A':randn(20) 'B':randn(20)}  index=date_index)

In [3]: df
Out[3]:
                   A         B
2012-01-01 -1155124  1018059
2012-01-01 -0312090 -1083568
2012-01-01  0688247 -1296995
2012-01-01 -0205218  0837194
2012-01-01  0700611 -0001015
2012-01-01  1996796 -0914564
2012-01-01 -2268237  0517232
2012-01-01 -0170778 -0143245
2012-01-01 -0826039  0581035
2012-01-01 -0351097 -0013259
2013-01-01 -0767911 -0009232
2013-01-01 -0322831 -1384785
2013-01-01  0300160  0334018
2013-01-01 -1406878 -2275123
2013-01-01  1722454  0873262
2013-01-01  0635711 -1763352
2013-01-01 -0816891 -0451424
2013-01-01 -0808629 -0092290
2013-01-01  0386046 -1297096
2013-01-01  0261837  0562373


If I understand your question correctly  you want to decile within each date  To do that  you can first move the index into the dataframe as a column  Then  you can groupby by the new column (here it's called index)  and use transform with a lambda function  The lambda function below  applies pandasqcut to the grouped series and returns the labels attribute

In [4]: dfreset_index()groupby('index')transform(lambda x: qcut(x 10)labels)
Out[4]:
    A  B
0   1  9
1   4  1
2   7  0
3   5  8
4   8  5
5   9  2
6   0  6
7   6  3
8   2  7
9   3  4
10  3  6
11  4  2
12  6  7
13  0  0
14  9  9
15  8  1
16  1  4
17  2  5
18  7  3
19  5  8
",919872.0,,,,,2013-01-14 15:35:58,12.0
14655778,2,14655172,2013-02-01 22:03:17,3,"Assuming that Date is the index rather than a column then you can do an ""outer"" join:

df1join([df2  df3    df7000]  how='outer')


Note: it may be more efficient to pass in a generator of DataFrames rather than a list

For example:

df1 = pdDataFrame([[1  2]]  columns=['a'  'b'])
df2 = pdDataFrame([[3  4]]  index=[1]  columns=['c'  'd'])
df3 = pdDataFrame([[5  6]  [7  8]]  columns=['e'  'f'])

In [4]: df1join([df2  df3]  how='outer')
Out[4]: 
    a   b   c   d  e  f
0   1   2 NaN NaN  5  6
1 NaN NaN   3   4  7  8




If 'Date' is a column you can use set_index first:

df1set_index('Date'  inplace=True)
",1240268.0,,,,,2013-02-01 22:03:17,
11904292,2,11848578,2012-08-10 14:58:29,1,"EDIT: It is a bug I (Wes) fixed it here: https://githubcom/pydata/pandas/commit/aea7c4522bd7beffd0df80efee818873110609fa

It turns out it's not a bug


  While pandas does not force you to have a sorted date index  some of these methods may have unexpected or incorrect behavior if the dates are unsorted So please be careful


Sorting the dates at the DB level fixed the issue for me",1569050.0,,776560.0,,2012-08-10 15:30:34,2012-08-10 15:30:34,
12322877,2,12322779,2012-09-07 17:38:49,1,"Figured out one way to do it by reading the split-apply-combine documentation examples 

df = pandasDataFrame({'b':[2 2 4 5]  'c': [3 3 0 9]}  index=[1 1 3 7])
df_unique = dfgroupby(level=0)first()

df
   b  c
1  2  3
1  2  3
3  4  0
7  5  9

df_unique
   b  c
1  2  3
3  4  0
7  5  9
",656188.0,,656188.0,,2012-09-07 20:17:54,2012-09-07 20:17:54,1.0
12323599,2,12322779,2012-09-07 18:37:39,2,"In [29]: dfdrop_duplicates()
Out[29]: 
   b  c
1  2  3
3  4  0
7  5  9
",1548051.0,,,,,2012-09-07 18:37:39,1.0
12645835,2,12645216,2012-09-28 18:59:47,1,"If I understand you right  you can get this effect just by indexing with an ""isin"" on the index:

>>> df
         A         B         C
0  0754956 -0597896  0245254
1 -0987808  0162506 -0131674
2 -1064639 -2193629  1814078
3 -0483950 -1290789  1776827
4 -0191055 -0461204  0412220
>>> df[dfindexisin([0  2  3])]   # Drop rows whose label is not in the set [0  2  3]
         A         B         C
0  0754956 -0597896  0245254
2 -1064639 -2193629  1814078
3 -0483950 -1290789  1776827
",1427416.0,,,,,2012-09-28 18:59:47,
12658729,2,12645216,2012-09-30 04:49:45,1,"Generally  I find myself using boolean indexing and the tilde operator when obtaining the inverse of a selection  rather than dfdrop()  though the same concept applies to dfdrop when boolean indexing is used to form the array of labels to drop  Hope that helps

In [44]: df
Out[44]: 
          A         B
0  0642010  0116227
1  0848426  0710739
2  0563803  0416422

In [45]: cond = (dfA > 6) & (dfB > 3)

In [46]: df[cond]
Out[46]: 
          A         B
1  0848426  0710739

In [47]: df[~cond]
Out[47]: 
          A         B
0  0642010  0116227
2  0563803  0416422
",243434.0,,243434.0,,2012-09-30 04:59:55,2012-09-30 04:59:55,1.0
13092937,2,13089359,2012-10-26 19:08:56,3,"It seems that may be the reason: http://wwwhdfgrouporg/hdf5-questhtml#del

That's one big gotcha HDF5  wtf",645212.0,,,,,2012-10-26 19:08:56,
13100648,2,13089359,2012-10-27 13:37:15,3,"Yeah: ""HDF5 is not a database"" Folks often use ptrepack (part of PyTables) to ""repack"" the HDF5 file without any dead bytes",776560.0,,,,,2012-10-27 13:37:15,
13581784,2,13581517,2012-11-27 10:08:09,0,"You can select only the 'AA' column and use it as a filter on the entire df

Like:

df[df[('AA' '[m]')]>00]

parameter         AA        BB        CC        DD
unit             [m]       [m]       [s]       [s]
2000-01-01  0600748 -1163793 -0982248 -0397988
2000-01-03  1045428  0365353  0049152  1902942
2000-01-06  0891202  0021921  1215515 -1624741
2000-01-08  0999217 -1110213  0257718 -0096018
",1755432.0,,,,,2012-11-27 10:08:09,1.0
13793390,2,13751926,2012-12-10 00:21:51,0,"Take a look at the new PyTables docs in 010 (coming soon) or you can get from master http://pandaspydataorg/pandas-docs/dev/whatsnewhtml

PyTables is actually pretty good at appending  and writing to a HDFStore every second will work You want to store a DataFrame table You can then select data in a query like fashion  eg

storeappend('df'  the_latest_df)
storeappend('df'  the_latest_df)

storeselect('df'  [ 'index>12:00:01' ])


If this is all from the same process  then this will work great If you have a writer process and then another process is reading  this is a little tricky (but will work correctly depending on what you are doing)

Another option is to use messaging to transmit from one process to another (and then append in memory)  this avoids the serialization issue",644898.0,,,,,2012-12-10 00:21:51,
5516153,2,5515021,2011-04-01 16:26:40,5,"There is a fantastic module called pandas that was written by a guy at AQR (a hedge fund) that excels at calculations like this what you need is a way to handle ""missing data"" as someone mentioned above  the basics are using the nan (not a number) capabilities of scipy or numpy; however  even those libraries don't make financial calculations that much easier  if you use pandas  you can mark the data you don't want to consider as nan  and then any future calculations will reject it  while performing normal operations on other data

I have been using pandas on my trading platform for about 8 months I wish I had started using it sooner

Wes (the author) gave a talk at pyCon 2010 about the capabilities of the module see the slides and video on the pyCon 2010 webpage  In that video  he demonstrates how to get daily returns  run 1000s of linear regressions on a matrix of returns (in a fraction of a second)  timestamp / graph data all done with this module  Combined with psyco  this is a beast of a financial analysis tool

The other great thing it handles is cross-sectional data so you could grab daily close prices  their rolling means  etc then timestamp every  calculation  and get all this stored in something similar to a python dictionary (see the pandasDataFrame class) then you access slices of the data as simply as:

close_prices['stdev_5d']


See the pandas rolling moments doc for more information on to calculate the rolling stdev (it's a one-liner)

Wes has gone out of his way to speed the module up with cython  although I'll concede that I'm considering upgrading my server (an older Xeon)  due to my analysis requirements

EDIT FOR STRIMP's QUESTION:
After you converted your code to use pandas data structures  it's still unclear to me how you're indexing your data in a pandas dataframe and the compounding function's requirements for handling missing data (or for that matter the conditions for a 00 return or if you are using NaN in pandas)  I will demonstrate using my data indexing a day was picked at random df is a dataframe with ES Futures quotes in it indexed per second missing quotes are filled in with numpynan  DataFrame indexes are datetime objects  offset by the pytz module's timezone objects

>>> dfinfo
<bound method DataFrameinfo of <class 'pandascoreframeDataFrame'>
Index: 86400 entries   2011-03-21 00:00:00-04:00 to 2011-03-21 23:59:59-04:00
etf                                         18390  non-null values
etfvol                                      18390  non-null values
fut                                         29446  non-null values
futvol                                      23446  non-null values

>>> # ET is a pytz object
>>> et
<DstTzInfo 'US/Eastern' EST-1 day  19:00:00 STD>
>>> # To get the futures quote at 9:45  eastern time
>>> dfxs(etlocalize(dtdatetime(2011 3 21 9 45 0)))['fut']
129175
>>>


To give a simple example of how to calculate a column of continuous returns (in a pandasTimeSeries)  which reference the quote 10 minutes ago (and filling in for missing ticks)  I would do this: 

>>> df['fut']fill(method='pad')/df['fut']fill(method='pad')shift(600)


No lambda is required in that case  just dividing the column of values by itself 600 seconds ago  That shift(600) part is because my data is indexed per-second

HTH 
\mike",667301.0,,667301.0,,2011-04-03 11:19:53,2011-04-03 11:19:53,9.0
5546233,2,5515021,2011-04-05 01:36:44,1,"imagine I have a DataMatrix with closing prices  some indicator value  and a trade signal like this:

 >>> data_matrix
                        close          dvi            signal
 2008-01-02 00:00:00    1449          06504         -1             
 2008-01-03 00:00:00    1449          06603         -1             
 2008-01-04 00:00:00    1413          07528         -1             
 2008-01-07 00:00:00    1412          08226         -1             
 2008-01-08 00:00:00    1389          08548         -1             
 2008-01-09 00:00:00    1404          08552         -1             
 2008-01-10 00:00:00    1413          0846          -1             
 2008-01-11 00:00:00    1402          07988         -1             
 2008-01-14 00:00:00    1413          06151         -1             
 2008-01-15 00:00:00    1382          03714         1   


I use the signal to create a DataMatrix of returns based on the trade signal:

>>> get_indicator_returns()

                   indicator_returns    
2008-01-02 00:00:00    NaN            
2008-01-03 00:00:00    0000483       
2008-01-04 00:00:00    002451        
2008-01-07 00:00:00    00008492      
2008-01-08 00:00:00    001615        
2008-01-09 00:00:00    -001051       
2008-01-10 00:00:00    -0006554      
2008-01-11 00:00:00    0008069       
2008-01-14 00:00:00    -0008063      
2008-01-15 00:00:00    002201 


What I ended up doing is this:

def get_compounded_indicator_cumulative(self):

    indicator_dm = selfget_indicator_returns()
    dates = indicator_dmindex

    indicator_returns = indicator_dm['indicator_returns']
    compounded = array(zeros(size(indicator_returns)))

    compounded[1] = indicator_returns[1]

    for i in range(2  len(indicator_returns)):

        compounded[i] = (1 + compounded[i-1]) * (1 + indicator_returns[i]) - 1

    data = {
        'compounded_returns': compounded
    }

    return DataMatrix(data  index=dates)


For some reason I really struggled with this one

I'm in the process of converting all my price series to PyTables Looks promising so far",687739.0,,,,,2011-04-05 01:36:44,6.0
10202789,2,10202570,2012-04-18 04:31:30,3,"You just need the argmax() function It's straightforward:

>>> import pandas
>>> import numpy as np
>>> df = pandasDataFrame(nprandomrandn(5 3) columns=['A' 'B' 'C'])
>>> df
          A         B         C
0  1232853 -1979459 -0573626
1  0140767  0394940  1068890
2  0742023  1343977 -0579745
3  2125299 -0649328 -0211692
4 -0187253  1908618 -1862934
>>> df['A']argmax()
3
>>> df['B']argmax()
4
>>> df['C']argmax()
1
",567620.0,,,,,2012-04-18 04:31:30,
10213167,2,10202570,2012-04-18 15:51:32,7,"You might also try idxmax:

In [5]: df = pandasDataFrame(nprandomrandn(10 3) columns=['A' 'B' 'C'])

In [6]: df
Out[6]: 
          A         B         C
0  2001289  0482561  1579985
1 -0991646 -0387835  1320236
2  0143826 -1096889  1486508
3 -0193056 -0499020  1536540
4 -2083647 -3074591  0175772
5 -0186138 -1949731  0287432
6 -0480790 -1771560 -0930234
7  0227383 -0278253  2102004
8 -0002592  1434192 -1624915
9  0404911 -2167599 -0452900

In [7]: dfidxmax()
Out[7]: 
A    0
B    8
C    7


eg

In [8]: dfix[df['A']idxmax()]
Out[8]: 
A    2001289
B    0482561
C    1579985
",776560.0,,,,,2012-04-18 15:51:32,
10844760,2,10844493,2012-06-01 05:13:47,2,"This is not a pandas-specific issue  In Python  assignment never copies anything:

>>> a = [1 2 3]
>>> b = a
>>> b[0] = 'WHOA!'
>>> a
['WHOA!'  2  3]


If you want a new DataFrame  make a copy with e = dcopy()

Edit: I should clarify that assignment to a bare name never copies anything  Assignment to an item or attribute (eg  a[1] = x or afoo = bar) is converted into method calls under the hood and may do copying depending on what kind of object a is",1427416.0,,1427416.0,,2012-06-01 05:27:19,2012-06-01 05:27:19,3.0
13518146,2,13492530,2012-11-22 18:15:40,3,"If you unstack STK_ID  you can create side by side plots per RPT_Date

 In [55]: dfu = dfunstack(""STK_ID"")

 In [56]: fig  axes = subplots(2 1)

 In [57]: dfuplot(ax=axes[0]  kind=""bar"")
 Out[57]: <matplotlibaxesAxesSubplot at 0xb53070c>

 In [58]: dfuplot(ax=axes[1])
 Out[58]: <matplotlibaxesAxesSubplot at 0xb60e8cc>


",1548051.0,,,,,2012-11-22 18:15:40,1.0
13962218,2,13962133,2012-12-19 22:19:10,2,"Try:

prep_bcgps['lati'] = prep_bcgps['lat']interpolate()


For example:

df = pdDataFrame({'X' : [1  None  3  None  5]})
print(df)
#     X
# 0   1
# 1 NaN
# 2   3
# 3 NaN
# 4   5

df['X'] = df['X']interpolate()
print(df)
#    X
# 0  1
# 1  2
# 2  3
# 3  4
# 4  5
",190597.0,,,,,2012-12-19 22:19:10,1.0
14182195,2,14181732,2013-01-06 12:39:00,1,"The problem here is that the configure script is finding gcc-40 instead of gcc Since Apple hasn't shipped a gcc-40 in a very long time (Xcode 31 for 105  if I remember right)  this can't possibly be an Apple build

So  you have at least one gcc toolchain in addition to the on that came with Xcode/Command Line Tools And  wherever it came from  it doesn't have the Apple extensionsin particular  the -arch flag

And that's why you get:

gcc-40: error: unrecognized command line option '-arch'


The easiest solution is to figure out where you installed gcc-40 from and uninstall it

If you don't remember installing gcc-40  there are a few possibilities:

If you're using Homebrew  MacPorts  or Fink  it may have been installed as a prerequisite for some other package you installed In this case  it should almost certainly be in  respectively  /usr/local/bin  /opt/local/bin  or /sw/bin  rather than /usr/bin
Some other package you installed may have installed gcc-40 as well for some reason
In general  having multiple gcc toolchains will lead to confusion And the same goes for having multiple Python 27 installations In both cases  just use Apple's  unless you need another one for some reason  or you really know what you're doing

Since you said this is a new laptop you just started setting up  if you're hopelessly lost  it might be better to start over: migrate your user account and settings to a clean install  then re-install the things you need

(While we're at it  you're generally better off using pip instead of easy_install Do a sudo easy_install pip  then you can sudo pip install pandas and Cython and so on Except for pip itself  and readline)",908494.0,,,,,2013-01-06 12:39:00,1.0
14493836,2,14181732,2013-01-24 04:11:57,0,"Since you've got Xcode already  i would just install through Macports That will take care of all your dependencies and compilation issues Get Macports and its as simple as:

sudo port install py27-pandas
",983191.0,,,,,2013-01-24 04:11:57,
14237942,2,14237749,2013-01-09 14:54:11,3,"Your CSV file is malformed I get the same answer as you in Pandas 010  and while I admit that it is indeed very  very strange  you shouldn't be feeding it malformed data

Date Currenncy Symbol Type Units UnitPrice Cost Tax
2012-03-14 USD AAPL BUY 1000   
2012-05-12 USD SBUX SELL 500   


returns the expected

>>> import pandas as pd
>>> t = pdread_csv('pandas_test'  parse_dates=True  index_col=0)
>>> t
           Currenncy Symbol  Type  Units  UnitPrice  Cost  Tax
Date                                                          
2012-03-14       USD   AAPL   BUY   1000        NaN   NaN  NaN
2012-05-12       USD   SBUX  SELL    500        NaN   NaN  NaN
",1901658.0,,1901658.0,,2013-01-09 15:10:57,2013-01-09 15:10:57,8.0
11138040,2,11129478,2012-06-21 12:22:08,0,"Pandas is great  but not sure you need it here  Something like the following should give you the links that aren't bidirectional:

x = [1 1 2 2 3]
y = [2 3 1 3 1]

fwd = set( zip(x y) )
rev = set( zip(y x) )
print ' not bi: '  fwddifference(rev)


This returns:


  
    
      not bi:  set([(2  3)])
    
  
",1472080.0,,,,,2012-06-21 12:22:08,1.0
11622565,2,11621165,2012-07-24 00:38:29,2,"You get an error with df['A']ix[0] because your indexing doesn't start at 0  it starts at 10 You can get the value you want with either of the following

df['A']ix[10]
df['A']irow(0)


The first uses by the correct index The second command  which I suspect is what you want  finds the value by the row number  rather than by index value  and is technically only two characters longer than if df['A']ix[0] worked

Alternatively  you can reset the indices so that they will respond the way you expect for df['A']ix[0]:

df2=dfreset_index()


This will preserve your old indices (10  20  etc) by moving them into a column called ""index"" in the df2 data frame Then df2['A']ix[0] will return 'a' If you want to remove the old 10-based indices  you can insert the flag drop=True into the parenthesis of the reset_index function",943044.0,,943044.0,,2012-07-24 03:14:22,2012-07-24 03:14:22,
11624117,2,11621165,2012-07-24 04:21:16,2,Just FYI  we're working on a purely positional version of ix  probably to be released in v082 in about 3 weeks,1306530.0,,,,,2012-07-24 04:21:16,3.0
12218209,2,12217960,2012-08-31 15:16:22,1,"Try this:

df_c = df_areindex(df_aindex | df_bindex)
df_cix[df_bindex] = df_b
",1427416.0,,,,,2012-08-31 15:16:22,
12218559,2,12217960,2012-08-31 15:37:37,0,"Similar to BrenBarn's answer  but with more flexibility:

# reindex both to union of indices
df_a = df_areindex(df_aindex | df_bindex)
df_b = df_breindex(df_aindex | df_bindex)

# replacement criteria can be put in this lambda function
combiner = lambda: x  y: npwhere(y < x  y  x)
df_c = df_acombine(dfb  combiner)
",1571826.0,,,,,2012-08-31 15:37:37,
12253163,2,12217960,2012-09-03 19:33:32,1,"To fill gaps in one dataframe with values (or even full rows) from another  take a look at the dfcombine_first() built-in method

In [34]: df_bcombine_first(df_a)
Out[34]: 
           sales  cogs
STK_ID QT             
000876 1     100   100
       2     100   100
       3     100   100
       4     100   100
       5      50    50
       6      50    50
       7      50    50
       8      50    50
       9      50    50
       10     50    50
",243434.0,,243434.0,,2012-09-03 19:49:30,2012-09-03 19:49:30,
12384077,2,12383436,2012-09-12 08:30:24,0,Not possible You could convert the Series to a one-column DataFrame,1571826.0,,,,,2012-09-12 08:30:24,2.0
12384507,2,12383436,2012-09-12 08:55:04,2,"I get the behaviour by overloading the Series__get_attr__ method :

def my__getattr__(self  key):
    # If attribute is in the self Series instance 
    if key in self:
        #  return is as an attribute
        return self[key]
    else:
        #  raise the usual exception
        raise AttributeError(""'Series' object has no attribute '%s'"" % key)

# Overwrite current Series attributes 'else' case
pandasSeries__getattr__ = my__getattr__


Then I can access Seriee items with attributes :

xx = pandasSeries(dict(a=44  b=55))
xxa
",31335.0,,,,,2012-09-12 08:55:04,
13146832,2,13020346,2012-10-30 20:07:49,0,"In [188]: from dateutil import parser

In [189]: from StringIO import StringIO

In [190]: data = """"""\
TIMESTAMP;MILLISECONDS;PRICE
15102012 08:00:06;350;246
""""""

In [191]: def date_parser(s):
    return parserparse(s[:-4])replace(microsecond=int(s[-3:])*1000)
   :

In [192]: df = pdread_csv(StringIO(data)  sep=';'  parse_dates=[[0  1]]  date_parser=date_parser)

In [193]: df
Out[193]:
       TIMESTAMP_MILLISECONDS  PRICE
0  2012-10-15 08:00:06350000   246

In [194]: dfset_index('TIMESTAMP_MILLISECONDS'  inplace=True)
Out[194]:
                            PRICE
TIMESTAMP_MILLISECONDS
2012-10-15 08:00:06350000   246

In [195]: dfindex
Out[195]:
<class 'pandastseriesindexDatetimeIndex'>
[2012-10-15 08:00:06350000]
Length: 1  Freq: None  Timezone: None
",1548051.0,,,,,2012-10-30 20:07:49,
13977632,2,13976491,2012-12-20 17:42:15,2,"Assuming Y is a column in your dataframe  one way is to use diff and cumsum:

df = DataFrame(Y)
df[1] = df[0]diff() > 6000000000000 #nanoseconds in ten minutes
df[1] = df[1]apply(lambda x: 1 if x else 0)cumsum()
dfgroupby(1)


Note: If you use the number of nanoseconds in 72 hours it'll split into two groups",1240268.0,,,,,2012-12-20 17:42:15,7.0
14184944,2,14184115,2013-01-06 17:50:49,1,"As hayden commented  I did try the function separately  but not on a large enough sample size  

Apparently for some values of those variables  it will approximate to zero  I changed my calcvol function to try using mibian to get the volatility  and if a ZeroDivisionError is caught then set impvol to NaN  That'll help me figure out which ones are causing such a ruckus",1850663.0,,1332690.0,,2013-01-06 17:52:44,2013-01-06 17:52:44,1.0
14511960,2,14241993,2013-01-24 22:26:34,0,"frame_query = read_frame is just a function assignement The read_frame function takes at least two arguments: (sql  con) -- where sql is a string

In the version 0101 of pandas  there are a few contents in the documentation about SQL Queries It's not a full 'SQL with pandas' doc but it's a beginning

sqlread_frame(""SELECT * FROM table_name;""  cnx)


I suppose you can use frame_query as you use read_frame In the git log  I saw that the file io/sqlpy changed one year ago The function frame_query was renamed I think the it was just for compatibility It should be removed if the name of this function is not used

execute returns the result of the query read_frame returns the result into a DataFrame You can also specify the index_col parameter as in the read_csv function",956765.0,,,,,2013-01-24 22:26:34,
14451264,2,14451185,2013-01-22 03:46:12,5,"Perhaps you are looking for pandascut:

import pandas as pd
import numpy as np

df = pdDataFrame(range(50)  columns  = ['filtercol'])
w = 0
x = 5
y = 17
z = 33
filter_values = [w  x  y  z]
out = pdcut(dffiltercol  bins = filter_values)
counts = pdvalue_counts(out)
# counts is a Series
print(counts)


yields

(17  33]    16
(5  17]     12
(0  5]       5


print(countsorder())


yields

(0  5]       5
(5  17]     12
(17  33]    16
",190597.0,,190597.0,,2013-01-22 12:05:44,2013-01-22 12:05:44,2.0
14647083,2,14646336,2013-02-01 13:12:35,2,"dfgroupby([dfindexyear  dfindexmonth  dfindexday])transform(npcumsum)resample('B'  how='ohlc')


I think this might be what I want but I have to test

EDIT:
After zelazny7's repsonse:

dfgroupby(pdTimeGrouper('D'))transform(npcumsum)resample('D'  how='ohlc')


works and is also more efficient than my previous solution",220120.0,,220120.0,,2013-02-01 15:17:24,2013-02-01 15:17:24,4.0
14648211,2,14646336,2013-02-01 14:14:16,1,"I wasn't able to get your resample suggestion to work  Did you have any luck?  Here's a way to aggregate the data at the business day level and compute the OHLC stats in one pass:

from io import BytesIO
from pandas import *

text = """"""1999-08-09 12:30:00-04:00   -0000486
1999-08-09 12:31:00-04:00   -0000606
1999-08-09 12:32:00-04:00   -0000120
1999-08-09 12:33:00-04:00   -0000037
1999-08-09 12:34:00-04:00   -0000337
1999-08-09 12:35:00-04:00    0000100
1999-08-09 12:36:00-04:00    0000219
1999-08-09 12:37:00-04:00    0000285
1999-08-09 12:38:00-04:00   -0000981
1999-08-09 12:39:00-04:00   -0000487
1999-08-09 12:40:00-04:00    0000476
1999-08-09 12:41:00-04:00    0000362
1999-08-09 12:42:00-04:00   -0000038
1999-08-09 12:43:00-04:00   -0000310
1999-08-09 12:44:00-04:00   -0000337""""""

df = read_csv(BytesIO(text)  sep='\s+'  parse_dates=[[0 1]]  index_col=[0]  header=None)


Here I create a dictionary of dictionaries  The outer key references the columns you want to apply the functions to  The inner key contains the names of your aggregation functions and the inner values are the functions you want to apply:

f = {2: {'O':'first' 
         'H':'max' 
         'L':'min' 
         'C':'last'}}

dfgroupby(TimeGrouper(freq='B'))agg(f)

Out:
                   2
                   H         C         L         O
1999-08-09  0000476 -0000337 -0000981 -0000486
",919872.0,,919872.0,,2013-02-01 15:14:17,2013-02-01 15:14:17,
11402371,2,11360675,2012-07-09 20:12:49,1,"It looks like you've hit a couple of known bugs in non-unique index handling:

https://githubcom/pydata/pandas/issues/1201/

https://githubcom/pydata/pandas/issues/1587/

A bug fix release is coming out very soon so please check the pandas website or PyPI in a week or so

Thanks",1306530.0,,,,,2012-07-09 20:12:49,1.0
11416734,2,11360675,2012-07-10 15:28:07,1,"I've managed to circumvent some of the issues highlighted above  here is a ""solution"" until the bugs mentioned by Chang She are resolved

I start with my original TimeSeries indexed DataFrame as before I sort the df  this sorts records by date (using the TimeSeries index)

df = dfsort()


once sorted I replace the dfindex with a numerical index

dfindex = range(len(df))


I subsequently extract my milestones as before with the difference now these will have an index which is a positive int  and create a list of that index:

milestones_list = milestones_dfindextolist()


and extract the data between my milestones from the original df using the numeric index like so:

datasets = {}
    for milestone in milestones_list:
        milestone_index = milestones_listindex(milestone)
        print ""milestone {0} index {1}""format(milestone  milestone_index)
        if milestone_index < len(milestones_list) -1:
            x = df[milestones_dfindex[milestone_index]:milestones_dfindex[milestone_index+1]]
        else:
            x = df[milestones_dfindex[milestone_index]:dfindexmax()]

        n = str(int(xindexmin())) +'-'+  str(int(xindexmax()))
        datasets[n] = x


this creates a dict with DataFrames for each milestone time interval named as the index intervals they represent

print datasetskeys()
['592-650'  '448-527'  '382-447'  '264-318'  '319-381'  '118-198'  '528-591'  '728-798'  '54-117'  '199-263'  '651-727']


Although admittedly not the ideal solution I hope it helps someone with similar issues",240068.0,,,,,2012-07-10 15:28:07,
11888784,2,11888599,2012-08-09 17:26:24,4,"Just use the aggregate method of the groupby object:

In [156]: df
Out[156]: 
   CHROM   POS     GENE  DESC  JOE  FRED  BILLY  SUSAN  TONY
0     10  1442    LOXL4   bad    1     0      0      1     0
1     10   335    LOXL4   bad    1     0      0      0     0
2     10  3438    LOXL4  good    0     0      1      0     0
3     10  4819  PYROXD2   bad    0     1      0      0     0
4     10  4829  PYROXD2   bad    0     1      0      1     0
5     10  9851     HPS1  good    1     0      0      0     0

In [157]: grouped = dfgroupby(['GENE'  'DESC'])

In [158]: groupedagg(npsum) # agg is a shortcut for aggregate
Out[158]: 
              CHROM   POS  JOE  FRED  BILLY  SUSAN  TONY
GENE    DESC                                            
HPS1    good     10  9851    1     0      0      0     0
LOXL4   bad      20  1777    2     0      0      1     0
        good     10  3438    0     0      1      0     0
PYROXD2 bad      20  9648    0     2      0      1     0


As mentioned by Daniel Velkow in the comment  the groupby object has some ""build in"" methods for simple aggregations like sum  mean   (something like ufuncs in numpy which are available as methods for numpy arrays) So the last step could be further simplified to 

In [159]: groupedsum()
Out[159]: 
              CHROM   POS  JOE  FRED  BILLY  SUSAN  TONY
GENE    DESC                                            
HPS1    good     10  9851    1     0      0      0     0
LOXL4   bad      20  1777    2     0      0      1     0
        good     10  3438    0     0      1      0     0
PYROXD2 bad      20  9648    0     2      0      1     0


If you want different operations on each column  according to the docs you can pass a dict to aggregate

However I found no way to specify a function for a single column and use a default for others So one way would be to define a custom aggregation function:

def custom_agg(s  default=npsum  other={}):
    if sname in otherkeys():
        return other[sname](s)
    else:
        return default(s)


and than apply it by passing the function and the args to agg:

In [59]: groupedagg(custom_agg  default=npsum  other={'CHROM': npmean})
Out[59]: 
              CHROM   POS  JOE  FRED  BILLY  SUSAN  TONY
GENE    DESC                                            
HPS1    good     10  9851    1     0      0      0     0
LOXL4   bad      10  1777    2     0      0      1     0
        good     10  3438    0     0      1      0     0
PYROXD2 bad      10  9648    0     2      0      1     0
",1301710.0,,1301710.0,,2012-08-14 05:39:22,2012-08-14 05:39:22,5.0
14238424,2,14237749,2013-01-09 15:15:59,2,"Here's a method which can handle some more cases (when there is some data in UnitCost  Cost  etc)

In [1]: df = pdread_csv('transcsv'  header=None)

In [2]: dfcolumns = dfix[0]

In [3]: df[1:]set_index('Date')
Out[3]: 
           Currenncy Symbol  Type Units UnitPrice Cost  Tax
Date                                                       
2012-03-14       USD   AAPL   BUY  1000       NaN  NaN  NaN
2012-05-12       USD   SBUX  SELL   500       NaN  NaN  NaN
2012-05-12       USD   SBUX  SELL   500       NaN  NaN  NaN


It's worth noting that the dtype of the these columns will be object

However  I think this should be caught by to_csv so I posted as an issue on github",1240268.0,,1240268.0,,2013-01-09 15:27:48,2013-01-09 15:27:48,1.0
14449538,2,14449367,2013-01-22 00:19:48,2,"These settings are controlled in the options (I suspect you're looking for max_rows or max_columns  but there are many options seen in the set_options docstring):

In [11]: pdoptionsdisplaymax_columns
Out[11]: 20


And change them using set_option:

In [12]: pdset_option('displaymax_columns'  10)


If the DataFrame either has more columns or more rows than these settings it will abbreviate

For example:

In [17]: df = pdDataFrame(pdnparange(10)reshape(5 2))

In [18]: pdset_option('displaymax_rows'  4)

In [19]: df
Out[19]: 
<class 'pandascoreframeDataFrame'>
Int64Index: 5 entries  0 to 4
Data columns:
0    5  non-null values
1    5  non-null values
dtypes: int64(2
",1240268.0,,1240268.0,,2013-01-22 00:28:27,2013-01-22 00:28:27,2.0
14639627,2,14639551,2013-02-01 04:16:02,2,"You could use the add method with a fill_value:

>>> s = pdSeries(index=[-9999  240  13899]  data=[26371  1755  2])
>>> s2 = pdSeries(index=[-9999  240  11303  110]  data=[26371  1755  6  4])
>>> sadd(s2  fill_value=0)
-999900    52742
 11000         4
 11303         6
 13899         2
 24000      3510


Or you could align the two first  and then simply add them with +:

>>> salign(s2  fill_value=0)
(-999900    26371
 11000         0
 11303         0
 13899         2
 24000      1755  -999900    26371
 11000         4
 11303         6
 13899         0
 24000      1755)
>>> s  s2 = salign(s2  fill_value=0)
>>> s + s2
-999900    52742
 11000         4
 11303         6
 13899         2
 24000      3510
",487339.0,,,,,2013-02-01 04:16:02,
9927769,2,9927711,2012-03-29 14:45:40,1,"NumPy provides the function genfromtxt() specifically for this purpose  The first sentence from the linked documentation:


  Load data from a text file  with missing values handled as specified
",279627.0,,,,,2012-03-29 14:45:40,3.0
9927957,2,9927711,2012-03-29 14:55:21,3,You can pass a custom list of values to be treated as missing using pandasread_csv  Alternately you can pass functions to the converters argument ,776560.0,,,,,2012-03-29 14:55:21,
9941582,2,9927711,2012-03-30 10:54:02,1,"pandasread_csv has the parameter na_values:

na_values : list-like  default None
    List of additional strings to recognize as NA/NaN


where you can define these bad values",449449.0,,,,,2012-03-30 10:54:02,1.0
13941980,2,13941472,2012-12-18 21:44:00,4,"The problem is that those fitpack routines that are used underneath require floats So  at some point there has to be a conversion from datetime to floats This conversion is easy If bb2tempindexvalues is your datetime array  just do:

In [1]: bb2tempindexvaluesastype('d')
Out[1]: 
array([  122403588e+12    122405867e+12    122408299e+12 
         122410577e+12    122413010e+12    122415288e+12 
         122417720e+12    122419998e+12])


You just need to pass that to your spline And to convert the results back to datetime objects  you do resultsastype('datetime64')",1580351.0,,,,,2012-12-18 21:44:00,5.0
14106517,2,14106439,2012-12-31 22:26:06,0,"You have multiple versions of Python 27 You installed pandas for one version  and then tried to import it into the other  and you can't do that  because they have separate site libraries

If you need multiple versions of Python 27 for some reason  you have to learn how to manage multiple versions of Python For example  always be sure whether you're using /usr/bin/easy_install or /usr/local/bin/easy_install  and use the one that goes with the python you plan to run

But you probably don't need multiple versions If you just uninstall the non-Apple one  everything will be a lot easier

You can figure out the details from the paths in your logs The manual install went to /Library/Python/27/site-packages  which is where Apple's /usr/bin/python looks But the easy_install went to /usr/local/lib/python/27/site-packages  which is where the third-party (presumably Homebrew  from the brew tag?) /usr/local/bin/python So clearly  the first python on your path is /usr/bin/python  while the first easy_install is /usr/local/bin/easy_install That's going to lead to confusion  as it did here

Even worse  if you install ipython into both Pythons  whichever one you install second is going to end up as /usr/local/bin/ipython  which is going to lead to even more confusion

If you do sudo /usr/bin/easy_install pandas  you can use pandas in the Apple Python To make sure that's the one you run  always do /usr/bin/python or /usr/bin/python /usr/local/bin/ipython If you do sudo /usr/local/bin/easy_install pandas  you can use pandas in the third-party Python To make sure that's the one you run  always do /usr/local/bin/python or /usr/local/bin/python /usr/local/bin/ipython

Looking at your comments  and your more detailed edit  it's possible that you actually have two third-party Pythons here  which makes things even more confusing If both of them prefer /usr/local/bin (and unless you're using MacPorts or Fink  they do)  you've probably got one of them half-overwritten by the other  and there's just no way you're going to get this working If that's the case  I would recommend that you do something radical If you're not willing to do an install-from-scratch-with-settings-import of OS X  at least rm -rf /usr/local /Library/Python ~/Library/Python  then reinstall brew and any other third-party stuff you need  and this time make sure to only install one extra Python (although zero would still be better!)

Meanwhile  two minor side notes: 

It's almost always better to use pip than easy_install If you don't have it  sudo easy_install pip  and now you do (The only common exceptions to that ""almost"" are for pip itself  and for readline)
Don't use sudo with Homebrew Homebrew goes through a lot of trouble to set up all of the directories it touches so you never need sudo Once you start doing sudo brew  sudo /usr/local/bin/easy_install  etc  you end up breaking that  so later installations get permissions errors  and it takes a lot of work with brew doctor to fix everything
",908494.0,,908494.0,,2012-12-31 23:24:56,2012-12-31 23:24:56,4.0
14295623,2,14295531,2013-01-12 17:02:22,3,"Use DataFrameix[]:

In [21]: d
Out[21]: 
   x   y
0  0  11
1  1  12
2  2  13

In [22]: dix[dx % 2 == 0  'y'] = -5

In [23]: d
Out[23]: 
   x   y
0  0  -5
1  1  12
2  2  -5
",733291.0,,,,,2013-01-12 17:02:22,
14432914,2,14431646,2013-01-21 05:07:41,0,"As you mention  at the moment you save the index  but what we can do is reset_index  saving the old index as a column ('Date')

price2 = pricereset_index()

In [11]: price2
Out[11]: 
<class 'pandascoreframeDataFrame'>
Int64Index: 1006 entries  0 to 1005
Data columns:
Date    1006  non-null values
AAPL    1006  non-null values
GE      1006  non-null values
dtypes: datetime64[ns](1)  float64(2)


Following the docs (setting a SQLite connection in memory):

import sqlite3
from pandasio import sql
# Create your connection
cnx = sqlite3connect(':memory:')


We can save prices2 to cnx:

sqlwrite_frame(price2  name='price2'  con=cnx)


We can retrieve via read_frame:

p2 = sqlread_frame('select * from price2'  cnx)


However when stored (and retrieved) dates are unicode rather than Timestamp  to convert back to what we started with we could apply Timestamp to the column and set_index:

from pandaslib import Timestamp
p2Date = p2Dateapply(Timestamp)
p = p2set_index('Date')


We get back the same DataFrame as prices:

In [20]: p
Out[20]: 
<class 'pandascoreframeDataFrame'>
DatetimeIndex: 1006 entries  2009-01-02 00:00:00 to 2012-12-31 00:00:00
Data columns:
AAPL    1006  non-null values
GE      1006  non-null values
dtypes: float64(2)
",1240268.0,,1240268.0,,2013-01-21 23:35:54,2013-01-21 23:35:54,1.0
14657511,2,14657241,2013-02-02 01:01:09,3,"Method #1: print all rows where the ID is one of the IDs in duplicated:

>>> import pandas as pd
>>> df = pdread_csv(""dupcsv"")
>>> ids = df[""ID""]
>>> df[idsisin(ids[idsduplicated()])]sort(""ID"")
       ID ENROLLMENT_DATE        TRAINER_MANAGING        TRAINER_OPERATOR FIRST_VISIT_DATE
24  11795       27-Feb-12      0643D38-Hanover NH      0643D38-Hanover NH        19-Jun-12
6   11795        3-Jul-12  0649597-White River VT  0649597-White River VT        30-Mar-12
18   8096       19-Dec-11  0649597-White River VT  0649597-White River VT         9-Apr-12
2    8096        8-Aug-12      0643D38-Hanover NH      0643D38-Hanover NH        25-Jun-12
12   A036       30-Nov-11     063B208-Randolph VT     063B208-Randolph VT              NaN
3    A036        1-Apr-12      06CB8CF-Hanover NH      06CB8CF-Hanover NH         9-Aug-12
26   A036       11-Aug-12      06D3206-Hanover NH                     NaN        19-Jun-12


but I couldn't think of a nice way to prevent repeating ids so many times  I prefer method #2: groupby on the ID

>>> pdconcat(g for _  g in dfgroupby(""ID"") if len(g) > 1)
       ID ENROLLMENT_DATE        TRAINER_MANAGING        TRAINER_OPERATOR FIRST_VISIT_DATE
6   11795        3-Jul-12  0649597-White River VT  0649597-White River VT        30-Mar-12
24  11795       27-Feb-12      0643D38-Hanover NH      0643D38-Hanover NH        19-Jun-12
2    8096        8-Aug-12      0643D38-Hanover NH      0643D38-Hanover NH        25-Jun-12
18   8096       19-Dec-11  0649597-White River VT  0649597-White River VT         9-Apr-12
3    A036        1-Apr-12      06CB8CF-Hanover NH      06CB8CF-Hanover NH         9-Aug-12
12   A036       30-Nov-11     063B208-Randolph VT     063B208-Randolph VT              NaN
26   A036       11-Aug-12      06D3206-Hanover NH                     NaN        19-Jun-12
",487339.0,,,,,2013-02-02 01:01:09,1.0
9966145,2,9943848,2012-04-01 16:37:56,1,"is your issue how to get the predicted y values of your regression? Or is it how to use the regression coefficients to get predicted y values for a different set of samples for the exogenous variables? pandas y_predict and y_fitted should give you the same value and both should give you the same values as the predict method in scikitsstatsmodels

If you're looking for the regression coefficients  do ols_testbeta",1306530.0,,,,,2012-04-01 16:37:56,5.0
10706942,2,10706533,2012-05-22 17:22:44,1,I do not recommend trying to build pandas (or any Python library containing C extensions) on 64-bit Windows unless you are using the Enthought Python Distribution (which comes bundled with a viable build environment) or you are in the mood for a yak shaving expedition (you would need to install the VS2008 SDK and do some finagling of Python's distutils config) You can find one-click binary installers for pandas 64-bit Python 32 on the Python Package Index,776560.0,,,,,2012-05-22 17:22:44,3.0
11144585,2,11138859,2012-06-21 18:34:20,0,"I think you can use a Python lamba function  to achieve that  as evidenced in the following link:
http://pandassourceforgenet/dataframehtml#function-application",442595.0,,,,,2012-06-21 18:34:20,1.0
11147317,2,11138859,2012-06-21 21:52:33,0,Have you put the data in a Panel? If you do then dataapply(f  axis=time_ax) (where time_ax is the time axis) should do the trick Otherwise please post more context / data / examples of what's not working,776560.0,,,,,2012-06-21 21:52:33,1.0
11769834,2,11754334,2012-08-02 01:28:03,0,"Confirmed bug:

https://githubcom/pydata/pandas/issues/1720

Please use GitHub for bug reports",776560.0,,,,,2012-08-02 01:28:03,
12278712,2,12278347,2012-09-05 09:51:14,-2,You could use csv files as the common data format Both R and python pandas can easily work with that You might lose some precision  but if this is a problem depends on your specific problem ,1033808.0,,,,,2012-09-05 09:51:14,1.0
12287461,2,12278347,2012-09-05 18:18:16,1,"It would make sense to dropdown to pytables and store/get your data there 

Ultimately a DataFrame is a dict of Series which is what an HDF5 Table is There are limitations on the translation due to incompatible dtypes but for numerical data it should be straight forward 

The way pandas stores its HDF5 is viewed more like a binary blob It has to support all the nuances of a DataFrame which HDF5 does support cleanly 

https://githubcom/dalejung/trtools/blob/master/trtools/io/pytablespy

Has some that kind of pandas/hdf5 munging code ",1297165.0,,,,,2012-09-05 18:18:16,2.0
14407329,2,12278347,2013-01-18 20:45:51,2,"If you are still looking at this  take a look at this post on google groups It shows how to exchange data between pandas/R via HDF5 

https://groupsgooglecom/forum/?fromgroups#!topic/pydata/0LR72GN9p6w",644898.0,,,,,2013-01-18 20:45:51,
12451149,2,12450924,2012-09-16 22:07:47,2,"I would suggest using boolean indexing:

mask = frameindexmap(lambda x: not isinstance(x  str))
frame = frame[mask]
",776560.0,,,,,2012-09-16 22:07:47,2.0
13027878,2,13027147,2012-10-23 09:55:40,2,"Have you seen this example? It's for a broken y-axis plot in matplotlib

Hope this helps

Combining with pandas this gives:

import pandas as pd
import matplotlibpyplot as plt
from StringIO import StringIO

data = """"""\
        a       b       c       d       e
alpha   551    060    -012   2690   7628453
beta    339    094    -017   -020   -020
gamma   798    334    -141   774    2839493
delta   229    124    040    029    028
""""""

df = pdread_csv(StringIO(data)  sep='\s+')

f  axis = pltsubplots(2  1  sharex=True)
dfplot(kind='bar'  ax=axis[0])
dfplot(kind='bar'  ax=axis[1])
axis[0]set_ylim(20000  80000)
axis[1]set_ylim(-2  30)
axis[1]legend()set_visible(False)

axis[0]spines['bottom']set_visible(False)
axis[1]spines['top']set_visible(False)
axis[0]xaxistick_top()
axis[0]tick_params(labeltop='off')
axis[1]xaxistick_bottom()
d = 015
kwargs = dict(transform=axis[0]transAxes  color='k'  clip_on=False)
axis[0]plot((-d +d) (-d +d)  **kwargs)
axis[0]plot((1-d 1+d) (-d +d)  **kwargs)
kwargsupdate(transform=axis[1]transAxes)
axis[1]plot((-d +d) (1-d 1+d)  **kwargs)
axis[1]plot((1-d 1+d) (1-d 1+d)  **kwargs)
pltshow()


",1712956.0,,1548051.0,,2012-10-23 10:55:10,2012-10-23 10:55:10,1.0
13568602,2,11987270,2012-11-26 16:07:45,0,"I experienced the same MemoryError problem sorting large DataFrames when the module was run from IPython 

If you have a 64 bit processor  operating system and more than 2GB of RAM another solution is to run 64 bit Python  you can get a prepackaged 64 bit version of Python like Anaconda Community Edition or get the unofficial 64 binaries",1135883.0,,,,,2012-11-26 16:07:45,
12374642,2,12373840,2012-09-11 17:09:03,1,"Let me setup a toy example here:

In [38]: rng = pddate_range('2012-8-1'  freq='T'  periods=100)
In [39]: hashes = nprandomrandint(0  10  len(rng))
In [40]: obs = nparange(len(rng))
In [41]: df = DataFrame({'hash' : hashes  'timestamp' : rngasobject} 
   :                index=obs)


Now to get the time difference for each hash:

In [42]: grouped = dfset_index('hash'  append=True)groupby(level='hash')

In [44]: groupedtransform(lambda x: x-xmin())
Out[44]:
        timestamp
   hash
0  3      0:00:00
1  5      0:00:00
2  1      0:00:00
3  8      0:00:00
4  6      0:00:00
5  8      0:02:00
6  1      0:04:00
7  7      0:00:00
8  3      0:08:00
9  5      0:08:00
10 8      0:07:00
11 1      0:09:00
12 2      0:00:00



94 2      1:22:00
95 6      1:31:00
96 1      1:34:00
97 0      1:21:00
98 8      1:35:00
99 0      1:23:00
",1306530.0,,,,,2012-09-11 17:09:03,
12846154,2,12844529,2012-10-11 18:25:07,1,"How are you generating your data?

See how the output shows that your data is of 'object' type? the groupby operations specifically check whether each column is a numeric dtype first

In [31]: data
Out[31]: 
<class 'pandascoreframeDataFrame'>
DatetimeIndex: 2557 entries  2004-01-01 00:00:00 to 2010-12-31 00:00:00
Freq: <1 DateOffset>
Columns: 360 entries  -8975 to 8975
dtypes: object(360)


look 


Did you initialize an empty DataFrame first and then filled it? If so that's probably why it changed with the new version as before 09 empty DataFrames were initialized to float type but now they are of object type If so you can change the initialization to DataFrame(dtype=float)

You can also call frameastype(float)",1306530.0,,,,,2012-10-11 18:25:07,1.0
13240434,2,13239297,2012-11-05 21:10:18,0,"Are you sure you need to find a faster method? Your current method isn't that slow The following changes might simplify  but won't necessarily be any faster: 

Step 1: Take a random sample (with replacement) from the list of dataframes

rand_stocks = nprandomrandint(0  len(data)  size=batch_size) 


You can treat this array rand_stocks as a list of indices to be applied to your Series of dataframes The size is already batch size so that eliminates the need for the while loop and your comparison on line 156 

That is  you can iterate over rand_stocks and access the stock like so: 

for idx in rand_stocks: 
  stock = dataix[idx] 
  # Get a sample from this stock 


Step 2: Get a random datarange for each stock you have randomly selected 

start_idx = nprandomrandint(offset  len(stock)-timesteps)
d = data_t[start_idx:start_idx+timesteps]


I don't have your data  but here's how I put it together:

def random_sample(data=None  timesteps=100  batch_size=100  subset='train'):
    if subset=='train': offset = 0  #you can obviously change this back
    rand_stocks = nprandomrandint(0  len(data)  size=batch_size)
    ret_data = []
    for idx in rand_stocks:
        stock = data[idx]
        start_idx = nprandomrandint(offset  len(stock)-timesteps)
        d = stock[start_idx:start_idx+timesteps]
        ret_dataappend(d)
    return ret_data


Creating a dataset:  

In [22]: import numpy as np
In [23]: import pandas as pd

In [24]: rndrange = pdDateRange('1/1/2012'  periods=72  freq='H')
In [25]: rndseries = pdSeries(nprandomrandn(len(rndrange))  index=rndrange)
In [26]: rndserieshead()
Out[26]:
2012-01-02    2025795
2012-01-03    1731667
2012-01-04    0092725
2012-01-05   -0489804
2012-01-06   -0090041

In [27]: data = [rndseries rndseries rndseries rndseries rndseries rndseries]


Testing the function: 

In [42]: random_sample(data  timesteps=2  batch_size = 2)
Out[42]:
[2012-01-23    1464576
2012-01-24   -1052048 
 2012-01-23    1464576
2012-01-24   -1052048]
",484596.0,,484596.0,,2012-11-05 21:18:51,2012-11-05 21:18:51,
12323759,2,12322869,2012-09-07 18:49:06,2,"In [59]: df
Out[59]:
                             Symbol      Bid      Ask
Datetime
2012-06-01 00:00:00207000  EUR/USD  123618  123630
2012-06-01 00:00:00209000  EUR/USD  123618  123631
2012-06-01 00:00:00210000  EUR/USD  123618  123631
2012-06-01 00:00:00211000  EUR/USD  123623  123631
2012-06-01 00:00:00240000  EUR/USD  123623  123627
2012-06-01 00:00:00423000  EUR/USD  123622  123627
2012-06-01 00:00:00457000  EUR/USD  123620  123626
2012-06-01 00:00:01537000  EUR/USD  123620  123625
2012-06-01 00:00:03010000  EUR/USD  123620  123624
2012-06-01 00:00:03012000  EUR/USD  123620  123625

In [60]: grouped = dfgroupby('Symbol')

In [61]: ask =  grouped['Ask']resample('15Min'  how='ohlc')

In [62]: bid = grouped['Bid']resample('15Min'  how='ohlc')

In [63]: pandasconcat([ask  bid]  axis=1  keys=['Ask'  'Bid'])
Out[63]:
                                Ask                                 Bid
                               open     high      low    close     open     high      low   close
Symbol  Datetime
EUR/USD 2012-06-01 00:15:00  12363  123631  123624  123625  123618  123623  123618  12362
",1548051.0,,,,,2012-09-07 18:49:06,7.0
12681217,2,12680754,2012-10-01 21:15:03,1,"How about something like this:

In [55]: pdconcat([Series(row['var2']  row['var1']split(' '))              
                    for _  row in aiterrows()])reset_index()
Out[55]: 
  index  0
0     a  1
1     b  1
2     c  1
3     d  2
4     e  2
5     f  2


Then you just have to rename the columns",1306530.0,,,,,2012-10-01 21:15:03,3.0
13112913,2,13107598,2012-10-28 20:51:52,4,"One simple (and general) approach is to create a view of the dataframe with the subset you are interested in (or  stated for your case  a view with all columns except the ones you want to ignore)  and then use APPLY for that view 

In [116]: df
Out[116]: 
     a  b         c  d        f
0  one  3  0493808  a      bob
1  two  8  0150585  b    alice
2  one  6  0641816  c  michael
3  two  5  0935653  d      joe
4  one  1  0521159  e     kate


Use your favorite methods to create the view you need You could select a range of columns like so df_view = dfix[: 'b':'d']  but the following might be more useful for your scenario:

#I want all columns except two 
cols = dfcolumnstolist()    
mycols = [x for x in cols if not x in ['a' 'f']]
df_view = df[mycols]


Apply your function to that view (Note this doesn't yet change anything in df)

In [158]: df_viewapply(lambda x: x /2)
Out[158]: 
   b         c   d
0  1  0246904  20
1  4  0075293  25
2  3  0320908  28
3  2  0467827  28
4  0  0260579  24


Update the df using update()

In [156]: dfupdate(df_viewapply(lambda x: x/2))

In [157]: df
Out[157]: 
     a  b         c   d        f
0  one  1  0246904  20      bob
1  two  4  0075293  25    alice
2  one  3  0320908  28  michael
3  two  2  0467827  28      joe
4  one  0  0260579  24     kate
",484596.0,,484596.0,,2012-10-28 21:30:26,2012-10-28 21:30:26,1.0
13593673,2,13592618,2012-11-27 21:44:44,0,Probably not  see this  link on thread safe operations in Python,1443118.0,,,,,2012-11-27 21:44:44,
13593942,2,13592618,2012-11-27 22:01:48,3,The data in the underlying ndarrays can be accessed in a threadsafe manner  and modified at your own risk Deleting data would be difficult as changing the size of a DataFrame usually requires creating a new object I'd like to change this at some point in the future ,776560.0,,,,,2012-11-27 22:01:48,1.0
13825263,2,13824840,2012-12-11 17:21:01,2,"It does work  but you have to indicate the escape character for the embedded quotes: 

In [1]: data = '''SEARCH_TERM ACTUAL_URL
""bra tv bord"" ""http://wwwikeacom/se/sv/catalog/categories/departments/living_room/10475/?se%7cps%7cnonbranded%7cvardagsrum%7cgoogle%7ctv_bord""
""tv p\xc3\xa5 hjul"" ""http://wwwikeacom/se/sv/catalog/categories/departments/living_room/10475/?se%7cps%7cnonbranded%7cvardagsrum%7cgoogle%7ctv_bord""
""SLAGBORD  \\""Bergslagen\\""  IKEA:s 1700-tals serie"" ""http://wwwikeacom/se/sv/catalog/categories/departments/living_room/10475/?se%7cps%7cnonbranded%7cvardagsrum%7cgoogle%7ctv_bord""'''

In [2]: df = read_csv(StringIO(data)  escapechar='\\'  encoding='utf-8')

In [3]: df
Out[3]: 
                                      SEARCH_TERM                                         ACTUAL_URL
0                                     bra tv bord  http://wwwikeacom/se/sv/catalog/categories/d
1                                      tv p hjul  http://wwwikeacom/se/sv/catalog/categories/d
2  SLAGBORD  ""Bergslagen""  IKEA:s 1700-tals serie  http://wwwikeacom/se/sv/catalog/categories/d


see this gist",776560.0,,1240268.0,,2012-12-11 20:08:01,2012-12-11 20:08:01,
9794891,2,9787853,2012-03-20 21:02:32,3,"How about: df2combine_first(df1)? 

In [33]: df2
Out[33]: 
                   A         B         C         D
2000-01-03  0638998  1277361  0193649  0345063
2000-01-04 -0816756 -1711666 -1155077 -0678726
2000-01-05  0435507 -0025162 -1112890  0324111
2000-01-06 -0210756 -1027164  0036664  0884715
2000-01-07 -0821631 -0700394 -0706505  1193341
2000-01-10  1015447 -0909930  0027548  0258471
2000-01-11 -0497239 -0979071 -0461560  0447598

In [34]: df1
Out[34]: 
                   A         B         C
2000-01-03  2288863  0188175 -0040928
2000-01-04  0159107 -0666861 -0551628
2000-01-05 -0356838 -0231036 -1211446
2000-01-06 -0866475  1113018 -0001483
2000-01-07  0303269  0021034  0471715
2000-01-10  1149815  0686696 -1230991
2000-01-11 -1296118 -0172950 -0603887
2000-01-12 -1034574 -0523238  0626968
2000-01-13 -0193280  1857499 -0046383
2000-01-14 -1043492 -0820525  0868685

In [35]: df2comb
df2combine        df2combineAdd     df2combine_first  df2combineMult    

In [35]: df2combine_first(df1)
Out[35]: 
                   A         B         C         D
2000-01-03  0638998  1277361  0193649  0345063
2000-01-04 -0816756 -1711666 -1155077 -0678726
2000-01-05  0435507 -0025162 -1112890  0324111
2000-01-06 -0210756 -1027164  0036664  0884715
2000-01-07 -0821631 -0700394 -0706505  1193341
2000-01-10  1015447 -0909930  0027548  0258471
2000-01-11 -0497239 -0979071 -0461560  0447598
2000-01-12 -1034574 -0523238  0626968       NaN
2000-01-13 -0193280  1857499 -0046383       NaN
2000-01-14 -1043492 -0820525  0868685       NaN


Note that it takes the values from df1 for indices that do not overlap with df2 If this doesn't do exactly what you want I would be willing to improve this function / add options to it",776560.0,,,,,2012-03-20 21:02:32,2.0
10916118,2,10908295,2012-06-06 14:19:52,14,"Seriesvalue_counts gives you the histogram you're looking for:

In [9]: df['Qu1']value_counts()
Out[9]: 
4    2
3    2
1    1


So  apply this function to each of those 3 columns:

In [13]: table = df[['Qu1'  'Qu2'  'Qu3']]apply(lambda x: xvalue_counts())

In [14]: table
Out[14]: 
   Qu1  Qu2  Qu3
1    1    1    1
2  NaN    2    1
3    2    2  NaN
4    2  NaN    2
5  NaN  NaN    1

In [15]: table = tablefillna(0)

In [16]: table
Out[16]: 
   Qu1  Qu2  Qu3
1    1    1    1
2    0    2    1
3    2    2    0
4    2    0    2
5    0    0    1


Using tablereindex or tableix[some_array] you can rearrange the data

To transform to strings  use tablerename:

In [17]: tablerename(index=str)
Out[17]: 
   Qu1  Qu2  Qu3
1    1    1    1
2    0    2    1
3    2    2    0
4    2    0    2
5    0    0    1

In [18]: tablerename(index=str)index[0]
Out[18]: '1'
",776560.0,,,,,2012-06-06 14:19:52,1.0
11384801,2,11337437,2012-07-08 16:37:49,0,The dtypes listed are in alphabetical order The tsz column probably contains datetimedatetime Python objects (what is usually returned by database drivers for timestamp columns)  have you looked? ,776560.0,,,,,2012-07-08 16:37:49,
11685871,2,11685618,2012-07-27 10:19:34,0,"Currently pandas does not support hierarchical paths as you specified 

There is an open github issue about this: https://githubcom/pydata/pandas/issues/13

I'm not sure when we will get around to adding this feature  would more than welcome a pull request if you're interested in completing the skeleton code that's in the issue discussion",1306530.0,,,,,2012-07-27 10:19:34,
14086764,2,11685618,2012-12-29 23:15:50,1,"this is enabled as of version 0100

http://pandaspydataorg/pandas-docs/stable/iohtml#hierarchical-keys",644898.0,,,,,2012-12-29 23:15:50,
14666251,2,14655172,2013-02-02 20:34:35,0,"how about this

list_of_dfs = oslistdir(dir_with_data)
df = concat(list_of_dfs)
dfset_index('Date')
df = dfunstack()
",983191.0,,,,,2013-02-02 20:34:35,
10021367,2,10017938,2012-04-05 01:04:36,2,"It works for me with pandas 072:

print dfix[d1:d2]swaplevel(0 1)ix[['AAPL'  'MSFT']]
                 f1  f2  c1
sym  date                  
AAPL 2012-01-01   5   2   3
MSFT 2012-01-01   4   2   3
AAPL 2012-01-02   8   2   3
MSFT 2012-01-02   7   2   3

import pandas; pandas__version__
'072'
",688693.0,,,,,2012-04-05 01:04:36,2.0
10781127,2,10767163,2012-05-28 07:51:50,1,"What about something like:

Aix[Aindex - Bindex]


Aindex - Bindex is a set difference:

    In [30]: Aindex
    Out[30]: Int64Index([ 0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19]  dtype=int64)

    In [31]: Bindex
    Out[31]: Int64Index([  0    1    2    3  999]  dtype=int64)

    In [32]: Aindex - Bindex
    Out[32]: Int64Index([ 4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19]  dtype=int64)

    In [33]: Bindex - Aindex
    Out[33]: Int64Index([999]  dtype=int64)
",1063605.0,,,,,2012-05-28 07:51:50,4.0
11157607,2,11157450,2012-06-22 13:55:47,1,It seems that ipython notebook closes the figure after I execute the cell's code To format the axes I have to execute all the code in a single cell,821428.0,,995876.0,,2012-07-12 08:55:11,2012-07-12 08:55:11,1.0
11809183,2,11795992,2012-08-04 13:54:05,1,"With some suggestions from some people in #pydata on freenode  this is what I came up with:

data = []
for d in range(5):
    temp = dfix[:  ['event_id'  'obj_%d_id' % d  'obj_%d_foo' % d  'obj_%d_bar' % d]]
    tempcolumns = ['event_id'  'obj_id'  'obj_foo'  'obj_bar']
    # Giving columns unique names
    tempindex = temp['event_id']*10 + d
    # Creating a unique index
    dataappend(temp)

concat(data)


This works and is reasonably fast!",1574073.0,,776560.0,,2012-08-08 02:28:44,2012-08-08 02:28:44,
12307162,2,12307099,2012-09-06 19:37:18,6,"Try this:

dfix[dfA==0  'B'] = npnan
",1427416.0,,,,,2012-09-06 19:37:18,3.0
12607018,2,12307099,2012-09-26 17:14:39,3,"Because SO won't let me comment yet (grr)  answering Arthur's question about what's going on in BrenBarn's answer here

Here is from pandas docs on advanced indexing: http://pandaspydataorg/pandas-docs/stable/indexinghtml#indexing-advanced

The section after 'Assignment / setting values is possible when using ix:' will explain exactly what you need! Turns out dfix can be used for cool slicing/dicing of a dataframe Annnnd It can also be used to set things 


dfix[selection criteria  columns I want] = value


So Bren's answer is saying 'find me all the places where dfA == 0  select column B and set it to npnan'

New to this as well -- hope that helps",684543.0,,,,,2012-09-26 17:14:39,
12580527,2,12579150,2012-09-25 09:56:24,1,"Resample has an base argument which covers this case:

tsresample(rule='24H'  how='sum'  closed='left'  label='left'  base=17)


Output:

2012-01-01 17:00:00    24
2012-01-02 17:00:00    24
2012-01-03 17:00:00    12
Freq: 24H
",1240268.0,,,,,2012-09-25 09:56:24,1.0
13070107,2,13064400,2012-10-25 13:50:27,1,"AFAIK you cannot pass in a DatetimeIndex to resample As a workaround  just resample by the freq alias ('1Min') and then reindex to your generated index? 

I started a github issue to maybe think about adding in additional parameters to resample Feel free to leave more feedback there",1306530.0,,,,,2012-10-25 13:50:27,1.0
13166608,2,13020346,2012-10-31 20:02:44,0,"please consider adding the converter to pandas code:
https://githubcom/pydata/pandas/blob/master/pandas/tseries/converterpy

See also:
https://githubcom/pydata/pandas/issues/1180",1772165.0,,,,,2012-10-31 20:02:44,
13259044,2,13258974,2012-11-06 20:41:40,1,"Cutting out the items() middle-man:

what_i_want = {}
for elem in what_i_get:
    what_i_wantupdate(elem)
",1795505.0,,,,,2012-11-06 20:41:40,1.0
13259057,2,13258974,2012-11-06 20:42:26,4,"Instead of returning a dict from your function  just return the mapped value  then create one dict outside the mapping operation:

>>> d
   Stuff
0     a
1     b
2     c
3     d
>>> dict(zip(dStuff  dStuffmap(ord)))
{'a': 97  'b': 98  'c': 99  'd': 100}
",1427416.0,,,,,2012-11-06 20:42:26,1.0
13446733,2,13446480,2012-11-19 02:02:21,1,"df = pdDataFrame([(1  2)  (1  3)  (1  4)  (2  1) (2 2 )]  columns=['col1'  'col2'])

In [36]: df
Out[36]: 
   col1  col2
0     1     2
1     1     3
2     1     4
3     2     1
4     2     2

gp = dfgroupby('col1')aggregate(npcount_nonzero)

In [38]: gp
Out[38]: 
      col2
col1      
1        3
2        2


lets get where the count > 2

tf = gp[gpcol2 > 2]reset_index()
df[dfcol1 == tfcol1]

Out[41]: 
   col1  col2
0     1     2
1     1     3
2     1     4
",239007.0,,,,,2012-11-19 02:02:21,3.0
13446736,2,13446480,2012-11-19 02:02:36,-1,"You could create a mask on your aggregate:

bytag = datagroupby('tag')aggregate(npcount_nonzero)
mask = bytagpid >= 100
filtered_dataframe = bytag[mask]
",958118.0,,,,,2012-11-19 02:02:36,
13447176,2,13446480,2012-11-19 03:17:23,2,"Edit: Thanks to @WesMcKinney for showing this much more direct way:

data[datagroupby('tag')pidtransform(len) > 1]


import pandas
import numpy as np
data = pandasDataFrame(
    {'pid' : [1 1 1 2 2 3 3 3] 
     'tag' : [23 45 62 24 45 34 25 62] 
     })

bytag = datagroupby('tag')aggregate(npcount_nonzero)
tags = bytag[bytagpid >= 2]index
print(data[data['tag']isin(tags)])


yields

   pid  tag
1    1   45
2    1   62
4    2   45
7    3   62
",190597.0,,190597.0,,2012-11-25 12:31:33,2012-11-25 12:31:33,1.0
13653249,2,13653030,2012-11-30 21:13:35,0,"You could use pandasconcat:

import pandas as PD
from pandasutiltesting import rands

data = [PDSeries([rands(4) for j in range(6)] 
                  index = PDdate_range('1/1/2000'  periods = 6) 
                  name = 'col'+str(i)) for i in range(4)]

df = PDconcat(data  axis = 1  keys = [sname for s in data])
print(df)


yields

            col0  col1  col2  col3
2000-01-01  GqcN  Lwlj  Km7b  XfaA
2000-01-02  lhNC  nlSm  jCYu  XLVb
2000-01-03  sSRz  PFby  C1o5  0BJe
2000-01-04  khZb  Ny9p  crUY  LNmc
2000-01-05  hmLp  4rVp  xF2P  OmD9
2000-01-06  giah  psQb  T5RJ  oLSh
",190597.0,,,,,2012-11-30 21:13:35,1.0
13653308,2,13653030,2012-11-30 21:17:19,1,"a = pdSeries(data=[1 2 3])
b = pdSeries(data=[4 5 6])
aname = 'a'
bname= 'b'

pdDataFrame(zip(a b)  columns=[aname  bname])


or just concat dataframes

pdconcat([pdDataFrame(a) pdDataFrame(b)]  axis=1)

In [53]: %timeit pdDataFrame(zip(a b)  columns=[aname  bname])
1000 loops  best of 3: 362 us per loop

In [54]: %timeit pdconcat([pdDataFrame(a) pdDataFrame(b)]  axis=1)
1000 loops  best of 3: 808 us per loop
",239007.0,,239007.0,,2012-11-30 21:49:28,2012-11-30 21:49:28,2.0
13852311,2,13653030,2012-12-13 03:09:14,1,Check out DataFramefrom_items too,776560.0,,,,,2012-12-13 03:09:14,
13855833,2,13852008,2012-12-13 08:30:41,2,"I think the best way is to merge all dataframes together  then you could use all nice Panda functions to slice and mix-and-match anyway you want

Lets first create some sample data:

# node1
index = ['Avg IPC (w/ idle)'  'Avg CPI (w/ idle)'  'Avg IPC (w/o idle)'  'Avg CPI (w/o idle)'  'User IPC (w/o idle)']

core0 = [009  1117  048  210  070]
core1 = [012  803  078  128  102]
core2 = [006  1562  064  156  085]
core3 = [006  1697  063  159  084]
group = [008  1295  063  163  085]

data = {'core0': core0  'core1': core1  'core2': core2  'core3': core3  'group': group}
node01 = pdDataFrame(data  index=index)

# node2
index = ['Avg IPC (w/ idle)'  'Avg CPI (w/ idle)'  'Avg IPC (w/o idle)'  'Avg CPI (w/o idle)'  'User IPC (w/o idle)']

core0 = [033  1117  048  210  070]
core1 = [012  899  078  128  102]
core2 = [006  1562  064  156  999]
core3 = [006  1699  999  159  084]
group = [008  1295  063  999  085]

data = {'core0': core0  'core1': core1  'core2': core2  'core3': core3  'group': group}

node02 = pdDataFrame(data  index=index)

alldfs = {'node01': node01  'node02': node02}


The alldfs should be similar to your dict I would merge them like this:

# create 1 DataFrame
dfall = pdconcat(alldfs)

# name the levels for easy access
dfallindexnames = ['node' 'stat']
dfallcolumnsname = 'core'

# pivot the 'stat' layer to the columns so only the nodes are on the index
dfall = dfallunstack('stat')


This gives you a nice single DataFrame containing all data  a basic plotting function using Pandas build-in functionality can be as simple as:

def plotstat(df  stat):
    return dfxs(stat  axis=1  level=1)plot(kind='bar'  title=stat)

plotstat(dfall  'Avg IPC (w/ idle)')


Which gives:



You could of course use stack/unstack to structure your DataFrame a bit different depending on the amount of data and the way you will be using it most",1755432.0,,1755432.0,,2012-12-13 08:42:53,2012-12-13 08:42:53,1.0
10676196,2,10671227,2012-05-20 18:51:26,1,"I would add this as a comment  but I don't have the privilege to do that yet :)

It works for me in python and iPython 012; iPython 013 is still in development (see http://ipythonorg/ )  and  since the errors you're getting seem to involve formatting in the iPython 013 egg  I suspect that might be the cause Try with iPython 012 instead-- if it works  file a bug report with iPython and then probably stick with 012 until 013 is (more) stable",1284636.0,,,,,2012-05-20 18:51:26,7.0
12616394,2,11131647,2012-09-27 07:39:03,0,"It seems like some combination of resample and/or fillna is going to get you what you're looking for (realize this is coming a little late!)

Go grab your data just like you're doing You get back this things with a few gaps Check out this: 


import pandas as pd
import numpy as np

dates = pdDatetimeIndex(start='2012-01-01'  periods=10  freq='2D'
df = pdDataFrame(nprandomrandn(20)reshape(10 2) index=dates)




So now you have this data that has lots of gaps in it -- but you want to have this daily resolution data 

Just do: 


dfresample('1D')




This will fill in your dataframe with a bunch of NaNs where you have missing data And then when you do aggregations on them  just use functions (eg  npnansum  npmean) that ignore NaNs! 

Still a little unclear on the exact format of the data you've got Hope it helps",684543.0,,,,,2012-09-27 07:39:03,
12219964,2,12219771,2012-08-31 17:26:00,1,I'm getting errors running your code However  to convert a pandas Series to a numpy array  use the pandasSeriesvalues  method Wes's documentation is very well done Spend some time reviewing,687739.0,,,,,2012-08-31 17:26:00,
12221241,2,12219771,2012-08-31 19:07:29,2,"svalues will do the trick

From the documentation: DataFramevalues Convert the frame to its Numpy-array matrix representation""

I think you mean pandasDataFrame above (not Series) Seriesvalues exists as well",121704.0,,,,,2012-08-31 19:07:29,2.0
12411730,2,12411649,2012-09-13 17:32:12,2,"The following works for me It gives a series where column names are now the index  and the value for an index is True/False depending on whether all items in the column are 0

import pandas  numpy as np
# Create DataFrame ""df"" like yours

dfapply(lambda x: npall(x==0))
",567620.0,,,,,2012-09-13 17:32:12,5.0
13023268,2,13022392,2012-10-23 03:47:22,2,"It sounds like you need to have your original data lonlat represented as a pair of numpy arrays  then pass these arrays to a version of the function distance which accepts arrays 

For example  looking up the definition of haversine distance  you can fairly easily turn it into a vectorised formula as follows:

def haversine_pairwise(phi  lam):

    dphi = phi[1:]-phi[:-1]
    dlam = lam[1:]-lam[:-1]

    # r is assumed to be a known constant
    return r*(05*(1-cos(dphi)) + cos(phi[1:])*cos(phi[:-1])*05*(1-cos(dlam)))


I'm not familiar with these formulas myself  but hopefully this shows you how you can do it for whichever formula you want You would then use cumsum as you have already done The array slicing syntax which I have used is documented here in case it's not clear",482420.0,,,,,2012-10-23 03:47:22,3.0
13028519,2,13027147,2012-10-23 10:35:25,0,"The pandas docs offer several bar plots  for example:

df2 = pandasDataFrame(nprandomrand(10  4)  columns=['a'  'b'  'c'  'd'])
df2plot(kind='bar')


The picture you included in your question seems to be from precisely this example",1240268.0,,,,,2012-10-23 10:35:25,
13413845,2,13413590,2012-11-16 09:34:38,2,"Don't drop Just take rows where EPS is finite:

df = df[npisfinite(df['EPS'])]
",449449.0,,,,,2012-11-16 09:34:38,2.0
13434501,2,13413590,2012-11-17 20:27:33,3,"This question is already resolved  but 

also consider the solution suggested by Wouter in his original comment The ability to handle missing data  including dropna()  is built into pandas explicitly Aside from potentially improved performance over doing it manually  these functions also come with a variety of options which may be useful 

In [24]: df = pdDataFrame(nprandomrandn(10 3))

In [25]: dfix[::2 0] = npnan; dfix[::4 1] = npnan; dfix[::3 2] = npnan;

In [26]: df
Out[26]:
          0         1         2
0       NaN       NaN       NaN
1  2677677 -1466923 -0750366
2       NaN  0798002 -0906038
3  0672201  0964789       NaN
4       NaN       NaN  0050742
5 -1250970  0030561 -2678622
6       NaN  1036043       NaN
7  0049896 -0308003  0823295
8       NaN       NaN  0637482
9 -0310130  0078891       NaN

In [27]: dfdropna()     #drop all rows that have any NaN values
Out[27]:
          0         1         2
1  2677677 -1466923 -0750366
5 -1250970  0030561 -2678622
7  0049896 -0308003  0823295

In [28]: dfdropna(how='all')     #drop only if ALL columns are NaN
Out[28]:
          0         1         2
1  2677677 -1466923 -0750366
2       NaN  0798002 -0906038
3  0672201  0964789       NaN
4       NaN       NaN  0050742
5 -1250970  0030561 -2678622
6       NaN  1036043       NaN
7  0049896 -0308003  0823295
8       NaN       NaN  0637482
9 -0310130  0078891       NaN

In [29]: dfdropna(thresh = 2)   #Drop row if at least two values are NaN
Out[29]:
          0         1         2
1  2677677 -1466923 -0750366
2       NaN  0798002 -0906038
3  0672201  0964789       NaN
5 -1250970  0030561 -2678622
7  0049896 -0308003  0823295
9 -0310130  0078891       NaN


There are also other options (See docs at http://pandaspydataorg/pandas-docs/stable/generated/pandasDataFramedropnahtml)  including dropping columns instead of rows 

Pretty handy! ",484596.0,,,,,2012-11-17 20:27:33,
13570851,2,13570178,2012-11-26 18:23:36,1,"I've gotten pyodbc to work with my SQL Server instance  then  with some help from this thread  I got the sql return to load a dataframe  

I already setup a pyodbc connection  then made a call to it 

import pyodbc

import pandasiosql as psql

cnxn = pyodbcconnect(your_connection_info) 
cursor = cnxncursor()
sql = (""""""SELECT * FROM Source"""""")

df = psqlframe_query(sql  cnxn)
cnxnclose()


df should return your dataframe now  The hardest part for me was getting pyodbc up and running - I had to use freetds and it took a lot of trial and error to get it work  ",854739.0,,,,,2012-11-26 18:23:36,1.0
13852063,2,13691485,2012-12-13 02:37:59,1,"Working for me (v0100b1  though I am somewhat confident--but haven't checked-- this would also work in 091):

In [7]: x
Out[7]: 
   Chrom      Gene  Position
0     20    DZANK1  18446022
1     20      TGM6   2380332
2     20  C20orf96    271226

In [8]: y
Out[8]: 
   Chrom  Position Random
0     20  18446022    ABC
1     20   2380332    XYZ
2     20    271226    PQR

In [9]: pdmerge(x  y  how='left')
Out[9]: 
   Chrom      Gene  Position Random
0     20    DZANK1  18446022    ABC
1     20      TGM6   2380332    XYZ
2     20  C20orf96    271226    PQR


I'm very surprised that all the columns are object dtype There must be some kind of parsing problem with your data-- examine the values in each column (not what they look like  but what they actually are  strings  ints  what?)",776560.0,,,,,2012-12-13 02:37:59,
13616382,2,13611065,2012-11-28 23:38:41,0,"Pandas (and numpy) allow for boolean indexing  which will be much more efficient:

In [9]: df['col1'][df['col1']>=1]
Out[9]: 
1    1
2    2
Name: col1

In [10]: df[df['col1']>=1]
Out[10]: 
   col1  col2
1     1    11
2     2    12

In [11]: df[(df['col1']>=1)&(df['col1']<=1)]
Out[11]: 
   col1  col2
1     1    11


If you want to write helper functions for this  consider something along these lines:

In [11]: def b(x  col  op  n): 
    return op(x[col] n)

In [27]: def f(x  *b):
    return x[(nplogical_and(*b))]

In [30]: b1 = b(df  'col1'  ge  1)

In [31]: b2 = b(df  'col1'  le  1)

In [32]: f(df  b1  b2)
Out[32]: 
   col1  col2
1     1    11
",1240268.0,,1240268.0,,2012-11-29 09:44:10,2012-11-29 09:44:10,6.0
13842337,2,13832938,2012-12-12 14:54:01,1,"For data indexed with a regular frequency  pandas converts the underlying index to a PeriodIndex so that the resolution of the x-tick labels are updated automatically when zooming in and out So the ordinals you get are Period ordinals

In order to convert it back into datetime  you can do the following:

In [36]: pdPeriod(ordinal=int(t[0][0])  freq='H')
Out[36]: Period('2012-04-12 18:00'  'H')

In [37]: pdPeriod(ordinal=int(t[0][0])  freq='H')to_timestamp()
Out[37]: <Timestamp: 2012-04-12 18:00:00>


*Timestamp is a subclass of datetime that keeps nanoseconds

That being said  ideally we would hide the conversion from the user (or not have to do the conversion at all if possible!)  as soon as I have enough time to refactor all the plotting code",1306530.0,,,,,2012-12-12 14:54:01,1.0
14002174,2,14002158,2012-12-22 10:50:53,2,"You can read dates as UTC directly from read_csv by setting the date_parser function manually  for example:

from dateutiltz import tzutc
from dateutilparser import parse

def date_utc(s):
    return parse(s  tzinfos=tzutc)

df = read_csv('mycsv'  parse_dates=[0]  date_parser=date_utc)




If you are creating a timeseries  you can use the tz argument of date_range:

dd = pddate_range('2012-1-1 1:30'  periods=3  freq='min'  tz='UTC')

In [2]: dd
Out[2]: 
<class 'pandastseriesindexDatetimeIndex'>
[2012-01-01 01:30:00    2012-01-01 01:32:00]
Length: 3  Freq: T  Timezone: UTC




If your DataFrame/Series is already index by a timeseries  you can use the tz_localize method to set a timezone:

dftz_localize('UTC')


or if it already has a timezone  use tz_convert:

dftz_convert('UTC')
",1240268.0,,1240268.0,,2012-12-22 11:43:29,2012-12-22 11:43:29,3.0
14184415,2,14180615,2013-01-06 16:55:19,1,"You could groupby the index and take the first entry (see docs):

dfgroupby(level=0)first()


Example:

In [1]: df = pdDataFrame([[1]  [2]]  index=[1  1])

In [2]: df
Out[2]: 
   0
1  1
1  2

In [3]: dfgroupby(level=0)first()
Out[3]: 
   0
1  1
",1240268.0,,,,,2013-01-06 16:55:19,2.0
14335258,2,14331891,2013-01-15 10:05:01,2,"I dont really understand why you need to two columns with the same name  avoiding it would probably be the best

But to answer your question  this would return only 1 of the 'PE' columns:

dfTdrop_duplicates()TPE

STK_ID     RPT_Date
11_STK79   20130115    41932
21_STK58   20130115    14223
22_STK229  20130115    22436
23_STK34   20130115   -63252
Name: PE


or:

dfTix[0]T
",1755432.0,,1755432.0,,2013-01-15 13:00:31,2013-01-15 13:00:31,4.0
14043878,2,14043553,2012-12-26 17:21:58,1,"There's probably an easier way to do this using a panel  but I don't have any experience with time series yet  This is how I would accomplish what you want using DataFrames:

First make a dummy DataFrame:

In [231]: df2 = DataFrame(nprandomrand(100 3)*100  columns=['Day1' 'Day2' 'Day3'])

In [232]: df2head()
Out[232]:
        Day1       Day2       Day3
0  93347819  92866771  91381466
1   7819967  26415094  79477087
2  98792627  92940538  83774519
3  64182073  22563504  15631763
4  82460359  89743872  87511540


Now  make a new DataFrame by dropping the first column of df2

In [233]: df3 = df2ix[: 1:]

In [234]: df3head()
Out[234]:
        Day2       Day3
0  92866771  91381466
1  26415094  79477087
2  92940538  83774519
3  22563504  15631763
4  89743872  87511540


The ix notation allows you to slice columns  It can be confusing at first  but it reads in English as: ""Take all of the rows and only the columns from 1 to the end""

At this point both DataFrames have the same index  You don't need to create your own 'ID' unless you need it for something else  Pandas will automatically index each DataFrames for you  This aligns the DataFrames for all sorts of operations  It does the same thing with the columns  It will line up the DataFrames by the column names and perform whatever operations you want  Since you want to divide by the 'next' day  we have to change the columns in df3:

In [235]: df3columns = df2columns[:-1]

In [236]: df3head()
Out[236]:
        Day1       Day2
0  92866771  91381466
1  26415094  79477087
2  92940538  83774519
3  22563504  15631763
4  89743872  87511540


Now we have renamed the columns so they will align the way we want  Performing the division calculation is easy as Pandas will do all the alignment  No loops necessary!

In [244]: df4 = (df2/df3 < 95)

In [245]: df4head()
Out[245]:
    Day1   Day2   Day3
0  False  False  False
1   True   True  False
2  False  False  False
3  False  False  False
4   True  False  False
",919872.0,,,,,2012-12-26 17:21:58,
14079641,2,14043553,2012-12-29 07:21:22,0,"Since pandas in it's current form assumes time series data are arranged with time in the index  not the columns  transposing the DataFrame  at least temporarily  will enable the use of many built-in methods  such as  shift/diff/pct_change/etc

In [78]: df = DataFrame(nprandomrand(100  3) * 100 
                        columns=['Day1'  'Day2'  'Day3'])

In [79]: dfhead()
Out[79]: 
        Day1       Day2       Day3
0  27113276   0827977  37059887
1  48817798  19335033  12476411
2  27001015  18147742  33094676
3  38428321  95609824  72395564
4  63626472  36207677   1328216

In [80]: dft = dfT

In [82]: dftix[:  :5]
Out[82]: 
              0          1          2          3          4          5
Day1  27113276  48817798  27001015  38428321  63626472  25900132
Day2   0827977  19335033  18147742  95609824  36207677   0191767
Day3  37059887  12476411  33094676  72395564   1328216  37011027

In [89]: dftpct_change()ix[:  :5]
Out[89]: 
              0         1         2         3         4           5
Day1        NaN       NaN       NaN       NaN       NaN         NaN
Day2  -0969462 -0603935 -0327887  1488004 -0430934   -0992596
Day3  43759576 -0354725  0823625 -0242802 -0963317  191999688

In [94]: chg = (dftpct_change()dropna() < 95)Tastype(int)

In [95]: chghead()
Out[95]: 
   Day2  Day3
0     1     0
1     1     1
2     1     1
3     0     1
4     1     1
",243434.0,,,,,2012-12-29 07:21:22,
14248779,2,14248346,2013-01-10 00:14:22,0,"You should be able to use concat and unstack Here's an example:

df1 = pdSeries([1  2]  name='a')
df2 = pdSeries([3  4]  index=[1  2]  name='b')
d = {'A': s1  'B': s2} # a dict of Series

In [4]: pdconcat(d)
Out[4]: 
A  0    1
   1    2
B  1    3
   2    4

In [5]: pdconcat(d)unstack()T
Out[5]: 
    A   B
0   1 NaN
1   2   3
2 NaN   4
",1240268.0,,,,,2013-01-10 00:14:22,2.0
13994123,2,13993524,2012-12-21 16:53:53,3,"For the first part  you can use boolean indexing using get_level_values:

df[dfindexget_level_values('a')isin([5  7  10  13])]


For the second two  you can inspect the MultiIndex object by calling:

dfindex


(and this can be inspected/sliced)",1240268.0,,1240268.0,,2012-12-21 17:07:25,2012-12-21 17:07:25,7.0
13997780,2,13993524,2012-12-21 22:04:39,2,"Edit: This answer for pandas versions lower than 0100 only:

Okay @hayden had the right idea to start with:

An index has the method get_level_values() which returns  however  an array (in pandas versions < 0100) The isin() method doesn't exist for arrays but this works:

from pandas import lib
libismember(dfindexget_level_values('a')  set([5  7  10  13])


That only answers question 1 - but I'll give an update if I crack 2  3 (half done with @hayden's help)",288558.0,,680232.0,,2013-01-08 04:11:08,2013-01-08 04:11:08,
14163174,2,14162723,2013-01-04 18:57:52,1,"You can replace nan with None in your numpy array:

>>> x = nparray([1  npnan  3])
>>> y = npwhere(npisnan(x)  None  x)
>>> print y
[10 None 30]
>>> print type(y[1])
<type 'NoneType'>
",1361822.0,,,,,2013-01-04 18:57:52,1.0
14163209,2,14162723,2013-01-04 19:01:25,1,"@bogatron has it right  you can use where  it's worth noting that you can do this natively in pandas:

df1 = dfwhere((pdnotnull(df))  None)


Note: this changes the dtype of all columns to object

Example:

In [1]: df = pdDataFrame([1  npnan])

In [2]: df
Out[2]: 
    0
0   1
1 NaN

In [3]: df1 = dfwhere((pdnotnull(df))  None)

In [4]: df1
Out[4]: 
      0
0     1
1  None




I had previously incorrectly suggested that you could recast the DataFrames dtype to allow all datatypes types  using astype  and then the DataFrame fillna method:

df1 = dfastype(object)replace(npnan  'None')


Unfortunately neither this  nor using replace  works with None see this (closed) issue",1240268.0,,1240268.0,,2013-01-05 12:22:20,2013-01-05 12:22:20,5.0
14306921,2,14301913,2013-01-13 18:51:30,2,"Since you're not aggregating similarly indexed rows  try setting the index with a list of column names

In [2]: dfset_index(['Name'  'Destination'])
Out[2]: 
                   Length
Name  Destination        
Bob   Athens            3
      Rome              5
      Athens            2
Alice Rome              1
      Athens            3
      Rome              5
",243434.0,,,,,2013-01-13 18:51:30,1.0
6468875,2,6467832,2011-06-24 14:01:15,4,"You have a number of options using pandas  but you have to make a decision about how it makes sense to align the data given that they don't occur at the same instants

Use the values ""as of"" the times in one of the time series  here's an example:

    In [15]: ts
    Out[15]: 
    2000-01-03 00:00:00    -0722808451504
    2000-01-04 00:00:00    00125041039477
    2000-01-05 00:00:00    0777515530539
    2000-01-06 00:00:00    -035714026263
    2000-01-07 00:00:00    -155213541118
    2000-01-10 00:00:00    -0508166334892
    2000-01-11 00:00:00    058016097981
    2000-01-12 00:00:00    150766289013
    2000-01-13 00:00:00    -111114968643
    2000-01-14 00:00:00    0259320239297



    In [16]: ts2
    Out[16]: 
    2000-01-03 00:00:30    105595278907
    2000-01-04 00:00:30    -0568961755792
    2000-01-05 00:00:30    0660511172645
    2000-01-06 00:00:30    -00327384421979
    2000-01-07 00:00:30    0158094407533
    2000-01-10 00:00:30    -0321679671377
    2000-01-11 00:00:30    0977286027619
    2000-01-12 00:00:30    -0603541295894
    2000-01-13 00:00:30    115993249209
    2000-01-14 00:00:30    -0229379534767


you can see these are off by 30 seconds The reindex function enables you to align data while filling forward values (getting the ""as of"" value):

    In [17]: tsreindex(ts2index  method='pad')
    Out[17]: 
    2000-01-03 00:00:30    -0722808451504
    2000-01-04 00:00:30    00125041039477
    2000-01-05 00:00:30    0777515530539
    2000-01-06 00:00:30    -035714026263
    2000-01-07 00:00:30    -155213541118
    2000-01-10 00:00:30    -0508166334892
    2000-01-11 00:00:30    058016097981
    2000-01-12 00:00:30    150766289013
    2000-01-13 00:00:30    -111114968643
    2000-01-14 00:00:30    0259320239297

    In [18]: ts2corr(tsreindex(ts2index  method='pad'))
    Out[18]: -031004148593302283


note that 'pad' is also aliased by 'ffill' (but only in the very latest version of pandas on GitHub as of this time!)

Strip seconds out of all your datetimes The best way to do this is to use rename

    In [25]: ts2rename(lambda date: datereplace(second=0))
    Out[25]: 
    2000-01-03 00:00:00    105595278907
    2000-01-04 00:00:00    -0568961755792
    2000-01-05 00:00:00    0660511172645
    2000-01-06 00:00:00    -00327384421979
    2000-01-07 00:00:00    0158094407533
    2000-01-10 00:00:00    -0321679671377
    2000-01-11 00:00:00    0977286027619
    2000-01-12 00:00:00    -0603541295894
    2000-01-13 00:00:00    115993249209
    2000-01-14 00:00:00    -0229379534767


Note that if rename causes there to be duplicate dates an Exception will be thrown 

For something a little more advanced  suppose you wanted to correlate the mean value for each minute (where you have multiple observations per second):

    In [31]: ts_mean = tsgroupby(lambda date: datereplace(second=0))mean()

    In [32]: ts2_mean = ts2groupby(lambda date: datereplace(second=0))mean()

    In [33]: ts_meancorr(ts2_mean)
    Out[33]: -031004148593302283


These last code snippets may not work if you don't have the latest code from https://githubcom/wesm/pandas If mean() doesn't work on a GroupBy object per above try agg(npmean)

Hope this helps!",776560.0,,,,,2011-06-24 14:01:15,2.0
12068217,2,12066550,2012-08-22 07:27:59,1,Pandas 073 does not support index duplicates You need at least 080  between 080 and 081 several issues with duplicates in the index are fixed  so 081 (=most recent stable release) might be best However even 081 is not an answer to your problem  because this version has an issue with duplicate column names (you can not display a dataframe with duplicate column names),1548051.0,,,,,2012-08-22 07:27:59,1.0
12426686,2,12425602,2012-09-14 14:37:24,2,Scikit-learn should make you happy for the data processing (clustering) part,1665831.0,,,,,2012-09-14 14:37:24,5.0
12748091,2,12747417,2012-10-05 14:08:53,1,"I`m on pandas master  here default column names after reading a file are certainly not 0  1  2   Note that you can use dficol() to select a column by position This way there is no dependency if column names have been set  or default column names are used

In [93]: data = """"""\
1 2 3
4 5 6
""""""

In [94]: df = pdread_csv(StringIO(data)  header=None)

In [95]: df
Out[95]:
   X0  X1  X2
0   1   2   3
1   4   5   6

In [96]: dficol(0)
Out[96]:
0    1
1    4
Name: X0
",1548051.0,,,,,2012-10-05 14:08:53,5.0
12752599,2,12747417,2012-10-05 19:10:10,0,"dficol(N) didnt solve the problem

Traceback (most recent call last):
 File ""~/bin/snpdogpy""  line 220  in <module>
    main()
File ""~/bin/snpdogpy""  line 205  in main
sort_LD(rscode)
File ""/home/ferreirafm/bin/snpdogpy""  line 72  in sort_LD
cond = ((dficol(1) != code) & (dficol(2) != node))
File ""/usr/lib64/python32/site-packages/pandas/core/seriespy""  line 142  in wrapper
% type(other))
TypeError: Could not compare <class 'str'> type with Series


EDIT: Pandas-081",1289107.0,,,,,2012-10-05 19:10:10,
12769082,2,12747417,2012-10-07 13:04:14,0,"It seems that you compare a series of scalar values to a string:

In [73]: node = 'a'

In [74]: deco = 'b'

In [75]: data = [(10  'a'  1)  (11  'b'  2)  (12  'c'  3)]

In [76]: df = pdDataFrame(data)

In [77]: df
Out[77]: 
    0  1  2
0  10  a  1
1  11  b  2
2  12  c  3

In [78]: cond = ((df[1] != node) & (df[2] != deco))
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-78-0afad3702859> in <module>()
----> 1 cond = ((df[1] != node) & (df[2] != deco))

/home//python27/site-packages/pandas/core/seriespyc in wrapper(self  other)
    140             if npisscalar(res):
    141                 raise TypeError('Could not compare %s type with Series'
--> 142                                 % type(other))
    143             return Series(na_op(values  other) 
    144                           index=selfindex  name=selfname)

TypeError: Could not compare <type 'str'> type with Series


Note that pandas can handle strings and numbers in a series  but it not really makes sense to compare strings and numbers  so the error message is useful
However pandas should perhaps give a more detailed error message 

If your condition for the column 2 would be a number it would work: 

In [79]: deco = 3

In [80]: cond = ((df[1] != node) & (df[2] != deco))

In [81]: df[cond]
Out[81]: 
    0  1  2
1  11  b  2


Some comments:

Maybe some of your confusion is due to a design decision in pandas: 

If you read data from a file with read_csv the default column names of the resulting data frame are set to X1 to XN (and to X1 to XN for versions >= 09)  which are strings  

If you create a data frame from exiting arrays or lists or something the column names default to 0 to N and are integers

In [23]: df = pdread_csv(StringIO(data)  header=None)

In [24]: dfcolumns
Out[24]: Index([X1  X2  X3]  dtype=object)

In [25]: dfcolumns[0]
Out[25]: 'X1'

In [26]: type(dfcolumns[0])
Out[26]: str

In [27]: df = pdDataFrame(randn(2 3))

In [30]: dfcolumns
Out[30]: Int64Index([0  1  2])

In [31]: dfcolumns[0]
Out[31]: 0

In [32]: type(dfcolumns[0])
Out[32]: numpyint64


I opened a ticket to discuss this

So your 

In [60]: cond = ((df[1] != node) & (df[2] != deco))


should work for a dataframe created from an array or something  if the type of df[1] and df[2] is the same as the type of node and deco

If you have read a file with read_csv than  

In [60]: cond = ((df['X2'] != node) & (df['X3'] != deco))


should work with versions < 09  while it should be 

In [60]: cond = ((df['X2'] != node) & (df['X3'] != deco))


with versions >= 09",1301710.0,,1301710.0,,2012-10-07 20:12:51,2012-10-07 20:12:51,6.0
13482281,2,13064400,2012-11-20 21:15:59,0,"I posted a similar question the other day: Date ranges in Pandas

Wes replied saying he plans to extend resample like this eventually",649167.0,,,,,2012-11-20 21:15:59,
13579822,2,13574420,2012-11-27 08:10:53,0,"I have struggled with this as well The opposite  adding an extra level to a single (so it matches a MultiIndex)  also keeps me busy

I sometimes use this to keep the index intact:

print dfT[[('AA'  '[m]') == col for col in dfcolumns]]T

parameter         AA
unit             [m]
2000-01-01  0972434
2000-01-02 -0581852
2000-01-03 -0784172
2000-01-04 -0843441
2000-01-05 -1030200
2000-01-06 -0864225
2000-01-07 -0530056
2000-01-08 -0651367


But thats not the most flexible solution when your Index is more complex In this example it would work",1755432.0,,,,,2012-11-27 08:10:53,2.0
13841256,2,13736988,2012-12-12 13:49:44,0,"method:

def parse(datet):
dt=datetimestrptime(datet[0:10] '%Y-%m-%d')
delta=timedelta(hours=int(datet[11:13]) minutes=int(datet[14:17]))
return dt+delta

df=read_csv('c:/py/mimi' skiprows=7 parse_date={'datet':[0 1]} index_col='datet' date_parser=parse)
",1843099.0,,,,,2012-12-12 13:49:44,1.0
13906269,2,13906077,2012-12-16 22:52:27,2,"You nearly had it! The correct syntax for read_csv is (parse_dates):

In [1]: import pandas as pd

In [2]: pdread_csv(""testcsv""  parse_dates=[[0 1 2]])
Out[2]: 
        year_month_day
0  2011-01-10 00:00:00


Where testcsv is:

year month day
2011 1 10
",1240268.0,,1240268.0,,2012-12-16 22:57:52,2012-12-16 22:57:52,2.0
14102891,2,14102195,2012-12-31 14:41:17,4,"I think your method of of computing lags is just fine:

import pandas as pd
df = pdDataFrame(range(4)  columns = ['col'])

print(df['col'] - df['col']shift())
# 0   NaN
# 1     1
# 2     1
# 3     1
# Name: col

print(df['col'] + df['col']shift())
# 0   NaN
# 1     1
# 2     3
# 3     5
# Name: col


If you wish NaN plus (or minus) a number to be the number (not NaN)  use the add (or sub) method with fill_value = 0:

print(df['col']sub(df['col']shift()  fill_value = 0))
# 0    0
# 1    1
# 2    1
# 3    1
# Name: col

print(df['col']add(df['col']shift()  fill_value = 0))
# 0    0
# 1    1
# 2    3
# 3    5
# Name: col
",190597.0,,,,,2012-12-31 14:41:17,
14289730,2,14288864,2013-01-12 02:53:53,1,"Assuming your DataFrame is as follows (with index 'in')  you can use set_index:

In [1]: df = pdread_csv('nicsv'  sep='\s+'  index_col=0)

In [2]: df
Out[2]: 
    year  ni  d  m   x   y  q
in                           
1   2012   1  2  0 NaN NaN  3
6   2012   2  1  1   9   9  1
5   2012   3  1  1  17  17  1
3   2012   4  0  3  37  37  0
5   2012   5  1  0 NaN NaN  3
2   2012   6  3  1  15  15  3

In [3]: dfset_index('ni'  drop=False)
Out[3]: 
    year  ni  d  m   x   y  q
ni                           
1   2012   1  2  0 NaN NaN  3
2   2012   2  1  1   9   9  1
3   2012   3  1  1  17  17  1
4   2012   4  0  3  37  37  0
5   2012   5  1  0 NaN NaN  3
6   2012   6  3  1  15  15  3


Although this is probably ok in many cases  if you are concerned about speed and memory usage you can do this inplace (ie change df without creating a copy)

In [4]: dfset_index('ni'  drop=False  inplace=True)


inplace seems to be around 30% faster",1240268.0,,,,,2013-01-12 02:53:53,1.0
13326956,2,13326887,2012-11-10 23:07:08,2,"df[""group""][df[""pc""] < 066] = 2
df[""group""][df[""pc""] < 033] = 1
",653364.0,,,,,2012-11-10 23:07:08,
13471575,2,13464492,2012-11-20 10:48:15,1,"You can resample over an individual column (since each of these is a timeseries):

In [9]: df[0]resample('5Min'  how='ohlc')
Out[9]: 
                     open  high  low  close
2012-01-01 00:00:00   136   136  136    136
2012-01-01 00:05:00   462   499    0    451
2012-01-01 00:10:00   209   499    0    495
2012-01-01 00:15:00    25   499    0    344
2012-01-01 00:20:00   200   498    0    199


In [10]: type(df[0])
Out[10]: pandascoreseriesTimeSeries


It's not clear to me how this should output for a larger DataFrames (with multiple columns)  but perhaps you could make a Panel:

In [11]: newts = Panel(dict((col  df[col]resample('5Min'  how='ohlc'))
                                for col in dfcolumns))

In [12]: newts[0]
Out[12]: 
                     open  high  low  close
2012-01-01 00:00:00   136   136  136    136
2012-01-01 00:05:00   462   499    0    451
2012-01-01 00:10:00   209   499    0    495
2012-01-01 00:15:00    25   499    0    344
2012-01-01 00:20:00   200   498    0    199


Note: Perhaps there is a canonical output for resampling a DataFrame and it is yet to be implemented?",1240268.0,,,,,2012-11-20 10:48:15,1.0
13854901,2,13854476,2012-12-13 07:19:15,2,"transform is not that well documented  but it seems that the way it works is that what the transform function is passed is not the entire group as a dataframe  but a single column of a single group  I don't think it's really meant for what you're trying to do  and your solution with apply is fine

So suppose tipsgroupby('smoker')transform(func)  There will be two groups  call them group1 and group2  The transform does not call func(group1) and func(group2)  Instead  it calls func(group1['total_bill'])  then func(group1['tip'])  etc  and then func(group2['total_bill'])  func(group2['total_bill'])  Here's an example:

>>> print d
   A  B  C
0 -2  5  4
1  1 -1  2
2  0  2  1
3 -3  1  2
4  5  0  2
>>> def foo(df):
     print "">>>""
     print df
     print ""<<<""
     return df
>>> print dgroupby('C')transform(foo)
>>>
2    0
Name: A
<<<
>>>
2    2
Name: B
<<<
>>>
1    1
3   -3
4    5
Name: A
<<<
>>>
1   -1
3    1
4    0
Name: B
# etc


You can see that foo is first called with just the A column of the C=1 group of the original data frame  then the B column of that group  then the A column of the C=2 group  etc

This makes sense if you think about what transform is for  It's meant for applying transform functions on the groups  But in general  these functions won't make sense when applied to the entire group  only to a given column  For instance  the example in the pandas docs is about z-standardizing using transform  If you have a DataFrame with columns for age and weight  it wouldn't make sense to z-standardize with respect to the overall mean of both these variables  It doesn't even mean anything to take the overall mean of a bunch of numbers  some of which are ages and some of which are weights  You have to z-standardize the age with respect to the mean age and the weight with respect to the mean weight  which means you want to transform separately for each column

So basically  you don't need to use transform here  apply is the appropriate function here  because apply really does operate on each group as a single DataFrame  while transform operates on each column of each group",1427416.0,,,,,2012-12-13 07:19:15,1.0
14060374,2,14060111,2012-12-27 19:46:59,1,"To stop your column from being read as a number  use the converters parameter and specify str as the converter for the column containing your ""cusips""

For the header  as documented on the page you linked to  header is the number of the row which is to be considered the header; it is not a boolean saying ""do I have a header or not  Setting it to zero means to use row zero (ie  the first row) as the header  The documentation explicitly says:


  Specify None if there is no header row
",1427416.0,,,,,2012-12-27 19:46:59,1.0
14428450,2,14422976,2013-01-20 19:37:25,3,"The recommended way to install pandas is via pip (on Mac and Windows):

pip install pandas


On Linux it's python-pandas in whichever repository



It looks like you have tried to install from source  about which the docs mention:


  Installing from the git repository requires a recent installation of Cython as the cythonized C sources are no longer checked into source control Released source distributions will contain the built C files I recommend installing the latest Cython via easy_install -U Cython
  
  Note that you will not be able to import pandas if you open an interpreter in the source directory unless you build the C extensions in place:

python setuppy build_ext --inplace



Without compiling hashtablespyx (and a few other cython files)  pandas is unable to import them These are required for pandas (which explains your error message)",1240268.0,,,,,2013-01-20 19:37:25,3.0
14630250,2,14627380,2013-01-31 16:16:27,2,"You can use the DataFrame to_html method  which comes with formatters argument

An easier solution would be to surround by <span class=""significant""> and </span>  (rather than *)",1240268.0,,1240268.0,,2013-01-31 16:38:02,2013-01-31 16:38:02,1.0
9739828,2,9723000,2012-03-16 15:13:30,4,Pass dateutilparserparse (or another datetime conversion function) in the converters argument to read_csv,776560.0,,,,,2012-03-16 15:13:30,
10817896,2,10816994,2012-05-30 14:03:23,0,"I do not have any experience with pandas but from what I can make out from your code 

df = DataFrame(data  index = index)


and the error  it seems that the index is not supposed to be a mutable object like python lists Maybe this will work:

df = DataFrame(data  index = tuple(index))


Also it doesn't seem obvious that your index_row & data_row are lists themselves & you are appending them in index & data lists",860421.0,,860421.0,,2012-05-30 14:08:25,2012-05-30 14:08:25,1.0
10818587,2,10816994,2012-05-30 14:41:42,0,"You'd better leave all the date-time interpolation to pandas and just feed it with a clean input stream Then you can separate fields using read_fwf (for fixed-width formatted lines) For example:

import pandas
import StringIO

buf = StringIOStringIO()
bufwrite(''join(line
    for line in open('ftxt')
    if not linestartswith('TEST')))
bufseek(0)

df = pandasread_fwf(buf  [(0  24)  (24  27)  (27  30)] 
        index_col=0  names=['switch'  'value'])
print df


Output:

                        switch  value
2012-05-01 00:00:00203    OFF      0
2012-05-01 00:00:11203    OFF      0
2012-05-01 00:00:22203     ON      1
2012-05-01 00:00:33203     ON      1
2012-05-01 00:00:44203    OFF      0
2012-05-02 00:00:00203    OFF      0
2012-05-02 00:00:11203    OFF      0
2012-05-02 00:00:22203    OFF      0
2012-05-02 00:00:33203     ON      1
2012-05-02 00:00:44203     ON      1
2012-05-02 00:00:55203    OFF      0
",1063605.0,,,,,2012-05-30 14:41:42,9.0
11330247,2,11314693,2012-07-04 13:39:22,1,"Unfortunately  yes At the moment sqlite is the only ""flavor"" supported by write_frame See https://githubcom/pydata/pandas/blob/master/pandas/io/sqlpy#L155

def write_frame(frame  name=None  con=None  flavor='sqlite'):
    """"""
    Write records stored in a DataFrame to SQLite The index will currently be
    dropped
    """"""
    if flavor == 'sqlite':
        schema = get_sqlite_schema(frame  name)
    else:
        raise NotImplementedError


Writing a simple write_frame should be fairly easy  though For example  something like this might work (untested!):

import pymssql                                                        
conn = pymssqlconnect(host='SQL01'  user='user'  password='password'  database='mydatabase')
cur = conncursor()                                                   

# frame is your dataframe                                             
wildcards = ' 'join(['?'] * len(framecolumns))                      
data = [tuple(x) for x in framevalues]
table_name = 'Table' 

curexecutemany(""INSERT INTO %s VALUES(%s)"" % (table_name  wildcards)  data)
conncommit()
",1063605.0,,,,,2012-07-04 13:39:22,1.0
13891083,2,13890673,2012-12-15 09:36:15,2,"You can simply use the instance method mean of the DataFrame and than plot the results There is no need for transposition

In [14]: dfmean()
Out[14]: 
pol1    0578502
pol2    0393610
pol3    0634424
pol4    0607450

In [15]: dfmean()plot(kind='bar')
Out[15]: <matplotlibaxesAxesSubplot at 0x4a327d0>




Update

If you want to plot the bars of all columns and the mean you can append the mean:

In [95]: average = dfmean()

In [96]: averagename = 'average'

In [97]: df = dfappend(average)

In [98]: df
Out[98]: 
             pol1      pol2      pol3      pol4
art      0661592  0479202  0700451  0345085
mcf      0235517  0665981  0778774  0610344
mesa     0838396  0035648  0424047  0866920
average  0578502  0393610  0634424  0607450

In [99]: dfplot(kind='bar')
Out[99]: <matplotlibaxesAxesSubplot at 0x52f4390>




If your layout doesn't fit in to the subplot tight_layout will adjust the matplotlib parameters",1301710.0,,1301710.0,,2012-12-15 22:35:06,2012-12-15 22:35:06,1.0
14086167,2,14084234,2012-12-29 21:41:07,1,"You may find it faster to extract the index as a column and use apply and bfill
Something like this:

df['datetime'] = dfindex
df['stops'] = dfapply(lambda x: x['datetime']
                                 if x['pct_change'] < -0015
                                 else npnan 
                        axis=1)
df['stops'] = df['stops']bfill()
",1240268.0,,,,,2012-12-29 21:41:07,
14128336,2,14084234,2013-01-02 19:58:16,1,"How about this:

result = df[dfpct_change < -0015]reindex(filtered_dates  method='bfill')

The only problem with this is that if an interval does NOT contain a value below -0015  it will retrieve one from a future interval If you add a column containing the date you can see the time each row came from  then set rows to NA if the retrieved timestamp exceeds the next ""bin edge""",776560.0,,,,,2013-01-02 19:58:16,
14267566,2,14265539,2013-01-10 21:27:27,2,"can you move this to GitHub? I need to review the code but there are a number of edge cases where I didn't test really deeply-""leveled"" hierarchical indexes So this is probably a legitimate bug",776560.0,,,,,2013-01-10 21:27:27,
14412206,2,14412181,2013-01-19 07:37:57,0,"try 

dfto_csv('mydftxt'  sep='\t')
",1675954.0,,,,,2013-01-19 07:37:57,
14412246,2,14412181,2013-01-19 07:43:23,2,"dfprice1 returns a Series Luckily Series also has a to_csv method similar to the DataFrame's:

Definition: Seriesto_csv(self  path  index=True  sep=' '  na_rep='' 
float_format=None  header=False  index_label=None  mode='w'  nanRep=None 
encoding=None)


Example usage:

dfprice1to_csv('outfilecsv')
",1199589.0,,,,,2013-01-19 07:43:23,
14417983,2,14412181,2013-01-19 19:24:46,0,"FYI there is also:

DataFrameto_html()
DataFrameto_excel()
DataFrameto_latex()
and many others but not for file serialisation
Only needed to use to_excel: the usage is in the method documentation or on http://pandaspydataorg/pandas-docs/stable/",1094031.0,,,,,2013-01-19 19:24:46,
9648600,2,9647656,2012-03-10 17:32:24,1,"The Table format stores all of the data in record form  ie all of the values are stored in a single column There's an alternate table format that is possible to use (one column per DataFrame column)  but I haven't implemented that yet Basically the table format is designed to support queries

Mixed-type DataFrame can be stored if you do table=False  though Would welcome more work on these features  ",776560.0,,,,,2012-03-10 17:32:24,1.0
14531149,2,14530556,2013-01-25 22:11:16,1,"You can pass anchored offsets to resample  among other options they cover this case

For example the weekly frequency from Monday:

tsresample('W-MON')
",1240268.0,,,,,2013-01-25 22:11:16,
7779260,2,7776679,2011-10-15 16:58:28,11,The append function has an optional argument ignore_index which you should use here to join the records together  since the index isn't meaningful for your application,776560.0,,,,,2011-10-15 16:58:28,1.0
10909560,2,7776679,2012-06-06 07:01:50,0,"You could first identify the index-duplicated (not value) row using groupby method  and then do a sum/mean operation on all the rows with the duplicate index

data1 = data1groupby(data1index)sum()
data2 = data2groupby(data2index)sum()
",935940.0,,1743811.0,,2012-10-20 06:56:19,2012-10-20 06:56:19,
10163303,2,10158613,2012-04-15 15:19:08,2,"Are you by chance doing import pandas inside a directory containing the source code from PyPI or GitHub? Alternately  you may be missing a dependency that was not checked for in 071 If you are not importing from a source directory  could you please edit the pandas/initpy file (in C:\Python27\Lib\site-packages) to have a print statement after the import of pandas_tseries and show me what it says:

try:
    import pandas_tseries as lib
except Exception  e:  # pragma: no cover

    print e # <-- ADD THIS LINE

    if 'No module named' in str(e):
        raise ImportError('C extensions not built: if you installed already '
                          'verify that you are not importing from the source '
                          'directory')
    else:
        raise


If you could move this to the issue tracker that would be preferred Thanks!",776560.0,,,,,2012-04-15 15:19:08,
11630790,2,10158613,2012-07-24 12:23:11,1,Had the same issue Resolved by checking dependencies - make sure you have numpy > 161 and python-dateutil > 15 installed ,878571.0,,,,,2012-07-24 12:23:11,
12068757,2,10158613,2012-08-22 08:03:40,0,"I had the same error I did not build pandas myself so i thought i should not get this error as mentioned on the pandas site So i was confused on how to resolved this error
The pandas site says that matplotlib is an optional depenedency so i didn't install it initially But interestingly  after installing matplotlib the error disappeared I am not sure what effect it had 
it found something!",899811.0,,,,,2012-08-22 08:03:40,
13150130,2,11199437,2012-10-31 01:44:35,0,"Note: I'm unable to upload images due to some issue with imgur I'll try again later 

Take advantage of pandas matplotlib helper / wrappers by calling pdDataFrameboxplot() I believe this will take care of the NaN values for you It will also put both Series in the same plot so you can easily compare data 

Example
Create a dataframe with some NaN values and negative values

In [7]: df = pdDataFrame(nprandomrand(10  5))    
In [8]: dfix[2:4 3] = npnan
In [9]: dfix[2:3 4] = -045
In [10]: df
Out[10]: 
          0         1         2         3         4
0  0391882  0776331  0875009  0350585  0154517
1  0772635  0657556  0745614  0725191  0483967
2  0057269  0417439  0861274       NaN -0450000
3  0997749  0736229  0084077       NaN -0450000
4  0886303  0596473  0943397       NaN  0816650
5  0018724  0459743  0472822  0598056  0273341
6  0894243  0097513  0691781  0802758  0785258
7  0222901  0292646  0558909  0220400  0622068
8  0458428  0039280  0670378  0457238  0912308
9  0516554  0445004  0356060  0861035  0433503


Note that I can count the number of NaN values like so: 

In [14]: df[3]isnull()sum()   # Count NaNs in the 4th column
Out[14]: 3


A box plot is simply: 

In [16]: dfboxplot()


You could create a semi-log boxplot  for example  by: 

In [23]: nplog(df)boxplot()


Or  more generally  modify / transform to you heart's content  and then boxplot 

In [24]: df_mod = nplog(df)dropna()    
In [25]: df_modboxplot()
",484596.0,,,,,2012-10-31 01:44:35,
9846881,2,9794697,2012-03-23 21:45:21,1,"I think that using DataFramecombine_first could be the way to go  but depending on the scale of the data  it might be more useful to have a method like ""update"" that just modified particular rows in an existing DataFrame combine_first is more general and can cause the result to be of a different size than either of the inputs (because the indexes will get unioned together) 

https://githubcom/pydata/pandas/issues/961",776560.0,,,,,2012-03-23 21:45:21,1.0
11128080,2,11015974,2012-06-20 20:57:25,0,"with open('/tmp/badata') as dataF:
    oldk  oldsub = None  None
    for key  subi in groupby(map(strsplit dataF)  lambda x: (x[1] x[2])):
        if oldk == None:
            oldk  oldsub = key  list(subi)
        else:       
            newsub = list(subi)
            print ' 'join(oldk)  '->'  ' 'join(key)  float(oldsub[-1][3])-float(newsub[0][3])
            oldk  oldsub = None  None


gets this

07:00:00 B -> 07:00:00 A 00
07:00:00 B -> 07:00:00 A -20
07:00:01 B -> 07:00:01 A -20
07:00:02 A -> 07:00:03 B 20
07:00:03 A -> 07:00:03 B 10
07:00:04 A -> 07:00:04 B 10


if you change 

if oldk == None:


to

if oldk == None or oldk[0] != key[0]:


you`ll get

07:00:00 B -> 07:00:00 A 00
07:00:00 B -> 07:00:00 A -20
07:00:01 B -> 07:00:01 A -20
07:00:03 B -> 07:00:03 A -10
07:00:04 A -> 07:00:04 B 10
",1470442.0,,,,,2012-06-20 20:57:25,
11856636,2,11362943,2012-08-08 01:57:03,0,I would suggest you reach out to the pystatsmodels mailing list on this,776560.0,,,,,2012-08-08 01:57:03,
11742134,2,11740587,2012-07-31 14:12:16,1,"You should be able to simply do Asub(B) For example:

df = DataFrame(nprandomrandn(4  2)  columns=['one' 'two'])
A = dfix[1:  ['one'  'two']]
B = dfix[:2  ['one']]apply(lambda x: x *2)


If A is:

     one        two
1   -0999523   -2111082
2   -2197760   -0412689
3   -0534728    0037255


and B is:

    one
0   -1940326
1   -1999046
2   -4395521


Asub(B) will give you:

     one        two
0    NaN        NaN
1    0999523   NaN
2    2197760   NaN
3    NaN        NaN


Or have I misunderstood the question?",1452002.0,,1452002.0,,2012-07-31 15:15:06,2012-07-31 15:15:06,
12100941,2,12100396,2012-08-23 22:33:52,1,"In pandas 081  try this:

group_df = dfgroupby(group_name)agg([npmean  npstd  npcount_nonzero])
group_dfrename(None  lambda coltuple: '_'join(coltuple)  False  True)


See the DataFrame documentation for more details",329289.0,,329289.0,,2012-08-28 06:31:41,2012-08-28 06:31:41,2.0
12449440,2,12427056,2012-09-16 18:19:28,2,"On master this is implemented (i used random data for 'mean')  see image at the bottom
If you prefer not to upgrade you can also set it manually:

In [143]: ax = dfplot(kind='bar')

In [144]: axset_xticklabels(['|'join(t) for t in dfindex])




EDIT:

 In [167]: ax = dfplot(kind='bar')

 In [168]: axset_xticklabels(dfindexformat(names=False))


",1548051.0,,1548051.0,,2012-09-17 06:33:38,2012-09-17 06:33:38,3.0
12957363,2,12947521,2012-10-18 14:45:25,1,"You can use the converters field from pandasread_csv

def convert_bool(col):
    if str(col)title() ==  ""True"": #check for nan
        return ""YES""
    elif str(col)title() == ""False"":
        return ""NO""
    else:
        return col
pandasread_csv(file_in  converters={""C3"": lambda x:convert_bool(x)})
",1064197.0,,1064197.0,,2012-10-18 14:50:27,2012-10-18 14:50:27,5.0
10762516,2,10751127,2012-05-25 23:48:46,1,"Returning a Series  rather than tuple  should produce a new multi-column DataFrame For example 

return pandasSeries({'pvalue': pvalue  'mean_ratio': mean_ratio})
",243434.0,,,,,2012-05-25 23:48:46,1.0
11518797,2,11264307,2012-07-17 08:34:29,1,"This feature has been released (in the development version  and probably pandas 081) now
It is possible to

dfset_index(column_to_add  append=True (  inplace=True)


and

dfreset_index(level=column_to_remove_from_index)


This comes along with a substantial speedup versus resetting n columns and then adding n+1 to the index",942591.0,,,,,2012-07-17 08:34:29,
11512106,2,11511880,2012-07-16 20:34:51,2,"The reason your ADX call fails is because it expects an xts or matrix-like object with 3 columns: High  Low  Close  Your object contains 4 columns  Drop the date column before passing r_dataframe to ADX and everything should work  You can then add the datetime column back to the ADX output

Or  if you can set the rownames attribute of your R dataframe to the values of the Date column and then remove the Date column  you can convert your R dataframe to an xts object by calling asxts(rdataframe)  Then you can pass that to ADX and convert the result back to a pandas DataFrame",271616.0,,,,,2012-07-16 20:34:51,2.0
12335228,2,11511880,2012-09-08 23:14:40,0,dalejung on GitHub has done quite a bit of work recently in creating a tighter pandas-xts interface with rpy2  you might get in touch with him or join the PyData mailing list,776560.0,,,,,2012-09-08 23:14:40,
11886086,2,11885916,2012-08-09 14:50:15,2,"While monkeying around I discovered the answer to #2:

results[""b"" ""d""] gives me the value where group1=='b' & group2=='d'",37751.0,,,,,2012-08-09 14:50:15,
11886434,2,11885916,2012-08-09 15:09:37,5,"So for remaining #1

In [9]: df
Out[9]:
  group1 group2  value1  value2
0      a      c     11     71
1      a      c     20     80
2      a      d     30     90
3      b      d     40    100
4      b      d     50    110
5      b      e     60    120

In [10]: results
Out[10]:
group1  group2
a       c          2605
        d          9000
b       d         20500
        e         36000

In [11]: dfset_index(['group1'  'group2']  inplace=True)['results'] = results

In [12]: df
Out[12]:
               value1  value2  results
group1 group2
a      c          11     71    2605
       c          20     80    2605
       d          30     90    9000
b      d          40    100   20500
       d          50    110   20500
       e          60    120   36000

In [13]: dfreset_index()
Out[13]:
  group1 group2  value1  value2  results
0      a      c     11     71    2605
1      a      c     20     80    2605
2      a      d     30     90    9000
3      b      d     40    100   20500
4      b      d     50    110   20500
5      b      e     60    120   36000
",1548051.0,,1548051.0,,2012-08-09 15:16:32,2012-08-09 15:16:32,4.0
12353924,2,12353359,2012-09-10 14:35:51,0,"I wasn't abe to reproduce that behavior (I tried creating DataFrames from integers  floats and numpy arrays)  buy I think that is a better idea to assing NaN to the tax_rate column and then overwrite the values when ebt is non zero:

dt['tax_rate'] = numpynan
dt['tax_rate'][dtebt != 0] = dttax[dtebt != 0] / dtebt[dtebt != 0]
",536801.0,,,,,2012-09-10 14:35:51,
12361644,2,12353359,2012-09-11 01:43:19,1,"@bigbug  how are you getting the data out of the SQLite backend? If you look in pandasiosql  the read_frame method has a coerce_float parameter that should convert numerical data to float if possible

Your second example works because the DataFrame constructor tries to be clever about types If you set the dtype to object then it fails:

In [16]: dt = DataFrame({'tax':[0 0 0]  'ebt':[0 0 0]} index=index dtype=object)

In [17]: dttax/dtebt
---------------------------------------------------------------------------
ZeroDivisionError                         Traceback (most recent call last)


Check your data importing code again and let me know what you find?",1306530.0,,,,,2012-09-11 01:43:19,5.0
13100624,2,13084342,2012-10-27 13:33:17,2,"I think you want this?

In [29]: dfgroupby(level='STK_ID')apply(lambda x: pdexpanding_mean(x))
Out[29]: 
                     sales   net_pft
STK_ID RPT_Date                     
600141 20101231  46780000  1833000
       20110331  30252500  1108500
       20110630  31079333  1116333
       20110930  35906000  1318000
       20111231  41861800  1519400
       20120331  38399500  1375500
       20120630  39621286  1406286
600809 20101231  30166000  4945000
       20110331  24445000  5003000
       20110630  25946000  5530667
       20110930  28368750  5916750
       20111231  31671400  6294400
       20120331  30082833  6066167
       20120630  31236286  6323571
",1306530.0,,,,,2012-10-27 13:33:17,1.0
13371090,2,13370525,2012-11-14 00:10:23,3,"You can just use reindex on a time series using your date range Also it looks like you would be better off using a TimeSeries instead of a DataFrame (see documentation)  although reindexing is also the correct method for adding missing index values to DataFrames as well

For example  starting with:

date_index = pdDatetimeIndex([pddatetime(2003 6 24)  pddatetime(2003 8 13) 
        pddatetime(2003 8 19)  pddatetime(2003 8 22)  pddatetime(2003 8 24)])

ts = pdSeries([2 1 2 1 5]  index=date_index)


Gives you a time series like your example dataframe's head:

2003-06-24    2
2003-08-13    1
2003-08-19    2
2003-08-22    1
2003-08-24    5


Simply doing 

tsreindex(pddate_range(min(date_index)  max(date_index)))


then gives you a complete index  with NaNs for your missing values (you can use fillna if you want to fill the missing values with some other values - see here):

2003-06-24     2
2003-06-25   NaN
2003-06-26   NaN
2003-06-27   NaN
2003-06-28   NaN
2003-06-29   NaN
2003-06-30   NaN
2003-07-01   NaN
2003-07-02   NaN
2003-07-03   NaN
2003-07-04   NaN
2003-07-05   NaN
2003-07-06   NaN
2003-07-07   NaN
2003-07-08   NaN
2003-07-09   NaN
2003-07-10   NaN
2003-07-11   NaN
2003-07-12   NaN
2003-07-13   NaN
2003-07-14   NaN
2003-07-15   NaN
2003-07-16   NaN
2003-07-17   NaN
2003-07-18   NaN
2003-07-19   NaN
2003-07-20   NaN
2003-07-21   NaN
2003-07-22   NaN
2003-07-23   NaN
2003-07-24   NaN
2003-07-25   NaN
2003-07-26   NaN
2003-07-27   NaN
2003-07-28   NaN
2003-07-29   NaN
2003-07-30   NaN
2003-07-31   NaN
2003-08-01   NaN
2003-08-02   NaN
2003-08-03   NaN
2003-08-04   NaN
2003-08-05   NaN
2003-08-06   NaN
2003-08-07   NaN
2003-08-08   NaN
2003-08-09   NaN
2003-08-10   NaN
2003-08-11   NaN
2003-08-12   NaN
2003-08-13     1
2003-08-14   NaN
2003-08-15   NaN
2003-08-16   NaN
2003-08-17   NaN
2003-08-18   NaN
2003-08-19     2
2003-08-20   NaN
2003-08-21   NaN
2003-08-22     1
2003-08-23   NaN
2003-08-24     5
Freq: D  Length: 62
",1452002.0,,1452002.0,,2012-11-14 00:22:51,2012-11-14 00:22:51,1.0
13587250,2,13584149,2012-11-27 15:15:51,1,"If you know the index is the same between the two and you don't care about the column names 
just do:


DataFrame(df1values + df2values  df1index  df1columns)
",1306530.0,,,,,2012-11-27 15:15:51,3.0
13842286,2,13842088,2012-12-12 14:51:02,4,"dfxs('C') by default  returns a new dataframe with a copy of the data  so 

dfxs('C')['x']=10


modifies this new dataframe only

dfx returns a view of the df dataframe  so 

dfx['C']=10


modifies df itself

Alternatively 

dfxs('C'  copy = False)['x']=10


does modify df",190597.0,,190597.0,,2012-12-12 14:56:28,2012-12-12 14:56:28,
13434963,2,12947521,2012-11-17 21:30:06,0,"Note in upcoming pandas 092 (which includes a new file parser engine) you'll be able to do:

In [1]: paste
data = """"""A B C
Yes No Yes
No Yes Yes
Yes  Yes
No No No""""""

result = read_csv(StringIO(data)  dtype=object)
## -- End pasted text --

In [2]: result
Out[2]: 
     A    B    C
0  Yes   No  Yes
1   No  Yes  Yes
2  Yes  NaN  Yes
3   No   No   No

In [3]: result = read_csv(StringIO(data)  dtype=object  na_filter=False)

In [4]: result
Out[4]: 
     A    B    C
0  Yes   No  Yes
1   No  Yes  Yes
2  Yes       Yes
3   No   No   No


Or even (to get a NumPy array):

In [5]: result = read_csv(StringIO(data)  dtype='S3'  na_filter=False  as_recarray=True)

In [6]: result
Out[6]: 
array([('Yes'  'No'  'Yes')  ('No'  'Yes'  'Yes')  ('Yes'  ''  'Yes') 
       ('No'  'No'  'No')]  
      dtype=[('A'  '|S3')  ('B'  '|S3')  ('C'  '|S3')])
",776560.0,,,,,2012-11-17 21:30:06,
13148611,2,13148429,2012-10-30 22:38:49,4,"One easy way would be to reassign the dataframe with a list of the columns  rearranged as needed 

This is what you have now: 

In [6]: df
Out[6]:
          0         1         2         3         4      mean
0  0445598  0173835  0343415  0682252  0582616  0445543
1  0881592  0696942  0702232  0696724  0373551  0670208
2  0662527  0955193  0131016  0609548  0804694  0632596
3  0260919  0783467  0593433  0033426  0512019  0436653
4  0131842  0799367  0182828  0683330  0019485  0363371
5  0498784  0873495  0383811  0699289  0480447  0587165
6  0388771  0395757  0745237  0628406  0784473  0588529
7  0147986  0459451  0310961  0706435  0100914  0345149
8  0394947  0863494  0585030  0565944  0356561  0553195
9  0689260  0865243  0136481  0386582  0730399  0561593

In [7]: cols = dfcolumnstolist()

In [8]: cols
Out[8]: [0L  1L  2L  3L  4L  'mean']


Rearrange cols in any way you want This is how I moved the last element to the first position: 

In [12]: cols = cols[-1:] + cols[:-1]

In [13]: cols
Out[13]: ['mean'  0L  1L  2L  3L  4L]


Then reorder the dataframe like this: 

In [16]: df = df[cols]  #    OR    df = dfix[:  cols]

In [17]: df
Out[17]:
       mean         0         1         2         3         4
0  0445543  0445598  0173835  0343415  0682252  0582616
1  0670208  0881592  0696942  0702232  0696724  0373551
2  0632596  0662527  0955193  0131016  0609548  0804694
3  0436653  0260919  0783467  0593433  0033426  0512019
4  0363371  0131842  0799367  0182828  0683330  0019485
5  0587165  0498784  0873495  0383811  0699289  0480447
6  0588529  0388771  0395757  0745237  0628406  0784473
7  0345149  0147986  0459451  0310961  0706435  0100914
8  0553195  0394947  0863494  0585030  0565944  0356561
9  0561593  0689260  0865243  0136481  0386582  0730399
",484596.0,,484596.0,,2012-10-31 15:51:22,2012-10-31 15:51:22,2.0
13316001,2,13148429,2012-11-09 21:04:03,2,"How about:

dfinsert(0  'mean'  dfmean(1))


http://pandaspydataorg/pandas-docs/stable/dsintrohtml#column-selection-addition-deletion",776560.0,,,,,2012-11-09 21:04:03,
14151942,2,13148429,2013-01-04 06:04:46,0,"This question has been answered before:

dfreindex_axis(sorted(dfcolumns)  axis=1)
",1479269.0,,,,,2013-01-04 06:04:46,
13386668,2,13385663,2012-11-14 20:21:10,0,"The easiest way to do this is to use the DataFramepct_change() method

Here is a quick example

In[1]: aapl = get_data_yahoo('aapl'  start='11/1/2012'  end='11/13/2012')

In[2]: appl
Out[2]: 
          Open    High     Low   Close    Volume  Adj Close
Date                                                           
2012-11-01  59822  60300  59417  59654  12903500     59383
2012-11-02  59589  59695  57475  57680  21406200     57418
2012-11-05  58352  58777  57760  58462  18897700     58196
2012-11-06  59023  59074  58009  58285  13389900     58020
2012-11-07  57384  57454  55575  55800  28344600     55800
2012-11-08  56063  56223  53529  53775  37719500     53775
2012-11-09  54042  55488  53372  54706  33211200     54706
2012-11-12  55415  55450  53865  54283  18421500     54283
2012-11-13  53891  55048  53636  54290  19033900     54290

In[3]: aaplpct_change()
Out[3]:
                Open      High       Low     Close    Volume  Adj Close
Date                                                                   
2012-11-01       NaN       NaN       NaN       NaN       NaN        NaN
2012-11-02 -0003895 -0010033 -0032684 -0033091  0658945  -0033090
2012-11-05 -0020759 -0015378  0004959  0013558 -0117186   0013550
2012-11-06  0011499  0005053  0004311 -0003028 -0291453  -0003024
2012-11-07 -0027769 -0027423 -0041959 -0042635  1116864  -0038263
2012-11-08 -0023020 -0021426 -0036815 -0036290  0330747  -0036290
2012-11-09 -0036049 -0013073 -0002933  0017313 -0119522   0017313
2012-11-12  0025406 -0000685  0009237 -0007732 -0445323  -0007732
2012-11-13 -0027502 -0007250 -0004251  0000129  0033244   0000129
",1742701.0,,,,,2012-11-14 20:21:10,4.0
14024566,2,14024287,2012-12-24 18:24:44,3,"The groupby method returns a dataframe indexed by 'sequence'  When adding two dataframes  the rows are aligned by the indices  In this case  the indices for grpA and grpB are both 'sequence' so the resulting DataFrame C adds the appropriate rows together

A = DataFrame({'sequence': [1 2 3 1]  'shares': [100 200 50 200]})
B = DataFrame({'sequence': [1 2 2 3]  'shares': [100 200 50 50]})

grpA = Agroupby('sequence')sum()
grpB = Bgroupby('sequence')sum()

In [60]: grpA + grpB
Out[60]:
          shares
sequence
1            400
2            450
3            100
",919872.0,,919872.0,,2013-01-15 14:09:05,2013-01-15 14:09:05,2.0
14217343,2,14216572,2013-01-08 14:39:03,0,"I assume portfolio is a dictionary that maps stock symbols to the number of those stocks you own I also assume that ordersix[stime]['sym'] is the stock symbol of the current buy or sell order In which case  it doesn't make sense to use ordersix[stime]['sym'][0] as a key into portfolio; that will give you only the first character of the stock symbol Ex if the order is for stock ""ABC""  you are attempting to access portfolio[""A""] Just use ordersix[stime]['sym'] instead

Also  it may be useful to abstract away most of the map/list indexing  as it will be more human-readable Something like this:

portfolio=DataFrame({}  columns = symbols index=index)
portfolio=portfoliofillna(0)
for time in timestamps:
    valuesix[time]['total']=npsum(portfolioix['quantity']values*closeix[time]values)+cash
    for stime in ordersindex:
        order = ordersix[stime]

        if time != stime or order['type'] not in [""Buy""  ""Sell""]:
            continue

        symbol = order['sym']

        #update the quantity of stocks in my portfolio
        orderQuantity = order[""quan""]
        amountOwned = portfolio[symbol]
        if order[""type""] == ""Buy"":
            amountOwned += orderQuantity
        else:
            amountOwned -= orderQuantity
        portfolio[symbol] = amountOwned

        #net value of portfolio (?)
        summ = npsum(portfolioix['quantity']values * closeix[stime]values)

        #how much did this order cost/gain me?
        orderCost = orderQuantity*closeix[stime][symbol]
        if order[""type""] == ""Buy"":
            cash -= orderCost
        else:
            cash += orderCost

        #net value of my portfolio plus my available cash?
        valuesix[time]['total']=cash+summ
",953482.0,,953482.0,,2013-01-08 14:49:55,2013-01-08 14:49:55,1.0
14530027,2,14529838,2013-01-25 20:40:24,1,"For the first part you can pass a dict of column names for keys and a list of functions for the values:

In [28]: df
Out[28]:
          A         B         C         D         E  GRP
0  0395670  0219560  0600644  0613445  0242893    0
1  0323911  0464584  0107215  0204072  0927325    0
2  0321358  0076037  0166946  0439661  0914612    1
3  0133466  0447946  0014815  0130781  0268290    1

In [26]: f = {'A':['sum' 'mean']  'B':['prod']}

In [27]: dfgroupby('GRP')agg(f)
Out[27]:
            A                   B
          sum      mean      prod
GRP
0    0719580  0359790  0102004
1    0454824  0227412  0034060


UPDATE 1:

Because the aggregate function works on Series  references to the other column names are lost  To get around this  you can reference the full dataframe and index it using the group indices within the lambda function

Here's a hacky workaround:

In [67]: f = {'A':['sum' 'mean']  'B':['prod']  'D': lambda g: dfix[gindex]Esum()}

In [69]: dfgroupby('GRP')agg(f)
Out[69]:
            A                   B         D
          sum      mean      prod  <lambda>
GRP
0    0719580  0359790  0102004  1170219
1    0454824  0227412  0034060  1182901


Here  the resultant 'D' column is made up of the summed 'E' values

UPDATE 2:

Here's a method that I think will do everything you ask  First make a custom lambda function  Below  g references the group  When aggregating  g will be a Series  Passing gindex to dfix[] selects the current group from df  I then test if column C is less than 05  The returned boolean series is passed to g[] which selects only those rows meeting the criteria

In [95]: cust = lambda g: g[dfix[gindex]['C'] < 05]sum()

In [96]: f = {'A':['sum' 'mean']  'B':['prod']  'D': {'my name': cust}}

In [97]: dfgroupby('GRP')agg(f)
Out[97]:
            A                   B         D
          sum      mean      prod   my name
GRP
0    0719580  0359790  0102004  0204072
1    0454824  0227412  0034060  0570441
",919872.0,,919872.0,,2013-01-25 21:28:31,2013-01-25 21:28:31,4.0
9624150,2,9621362,2012-03-08 20:09:06,2,"No  there is no implementation of that exact algorithm Created a GitHub issue about it here:

https://githubcom/pydata/pandas/issues/886

I'd be happy to take a pull request for this-- implementation should be straightforward Cython coding and can be integrated into pandasstatsmoments",776560.0,,,,,2012-03-08 20:09:06,1.0
10658068,2,10594515,2012-05-18 18:42:10,2,"See: http://pandaspydataorg/pandas-docs/stable/dsintrohtml#console-display

It prints a summary because the data is too wide for your terminal This can be configured with pandasset_printoptions You almost certainly need to specify header=0 (this is the default  I believe)  so df = read_fwf('sampletxt'  colspecs=colspecs) should be sufficient",776560.0,,,,,2012-05-18 18:42:10,
11921678,2,11920721,2012-08-12 11:00:41,2,"from pandas import *
df = DataFrame([[0  1  1]  [1  1  0]  [1  0  1] ]  columns=['a' 'b' 'c'])

foo = []
for i in dfindex:
    fooappend( dfcolumns[dfix[i] == 1])
DataFrame(foo  index = dfindex)


Which returns:

   0  1
0  b  c
1  a  b
2  a  c
",1315131.0,,,,,2012-08-12 11:00:41,
11979845,2,11920721,2012-08-16 02:18:09,3,"You can also summon some deeper pandas-fu and do:

In [28]: dfapply(lambda x: xastype(object)replace(1  xname))
Out[28]: 
            a  b  c
2012-06-12  0  b  c
2012-06-13  a  b  0
2012-06-14  a  0  c
2012-06-15  a  0  c
2012-06-16  a  b  0
2012-06-17  a  0  c
",776560.0,,,,,2012-08-16 02:18:09,
12184631,2,12184558,2012-08-29 18:51:16,4,"The error is about the module urllibparse  not parse The documentation of the urlparse module says:


  The urlparse module is renamed to urllibparse in Python 3


which means your code expects to be run with Python 3 You are running Python 27 which means it won't work (because the module urllibparse is still called urlparse) Perhaps you need to look into installing a version of Portable Python that supports Python 3?",1267329.0,,,,,2012-08-29 18:51:16,1.0
12409459,2,12408826,2012-09-13 15:12:43,2,"If your subscription date is a datetimedatetime instance  then you could use (untested) something like (where df is your DataFrame):

dfgroupby(lambda L: (Lyear  Lmonth))


You'll need to adjust the groupby if the datetime isn't your DataFrame's index",1252759.0,,,,,2012-09-13 15:12:43,1.0
12409471,2,12408826,2012-09-13 15:13:17,1,"Does this help?

Let`s create a series that hold the member id and date of subscription

In [21]: s = pandasSeries(range(100)  pandasdate_range('2010-10-10'  periods=100))


Group by year and month  count number of subscriptions per (Year/Month) combination

In [22]: grouped = sgroupby([sindexyear  sindexmonth])

In [23]: nr_subscriptions = groupedcount()

In [24]: nr_subscriptionsindexnames = ['Year'  'Month']

In [25]: nr_subscriptionsnames = 'nr_subscriptions'

In [26]: nr_subscriptions
Out[26]: 
Year  Month
2010  10       22
      11       30
      12       31
2011  1        17
",1548051.0,,,,,2012-09-13 15:13:17,1.0
12707465,2,12704305,2012-10-03 11:23:18,4,"You can get the columns from the cursor description:

columns = [column[0] for column in cursordescription]",1452002.0,,,,,2012-10-03 11:23:18,
12718379,2,12704305,2012-10-03 23:38:18,1,"Improving on the previous answer  in the context of pandas  I found this does exactly what I expect:

DFcolumns = DataFrame(npmatrix(cursordescription))[0]
",1479269.0,,,,,2012-10-03 23:38:18,
14060382,2,14060111,2012-12-27 19:47:41,2,"If we have a data file which looks like

65248E10 11
55555E55 22


then we can read it in with something like

>>> pdread_table(""cusiptxt""  header=None  delimiter="" ""  converters={0: str})
          0   1
0  65248E10  11
1  55555E55  22


where we use header=None to tell it that there aren't any headers  we use delimiter="" "" to tell it there's a space delimiter (adjust to match your data format)  and converters={0: str} to tell it that after reading the first column in as a string  we want to turn it into a string (ie in this case do nothing to it) rather than process it further  Instead of converters={0: str}  dtype=(str  int) would have worked too  but this way we can still let pandas figure out what the other columns are

The problem with using header=0 is that 0 here doesn't mean ""no header""  it means use row number #0 (the first row) as the headers ",487339.0,,,,,2012-12-27 19:47:41,7.0
14257011,2,14256839,2013-01-10 11:33:59,0,"Try:

import pandas as pd
df = pdread_table(""14256839_inputtxt""  sep="" ""  na_values=""NULL"")
print df
print dfdtypes


This gives me

   Aaa  Bbb
0  Foo    0
1  Bar    1
2  Baz  NaN
Aaa     object
Bbb    float64
",1156006.0,,,,,2013-01-10 11:33:59,
14257036,2,14256839,2013-01-10 11:35:17,2,"I think this is issue #2599  ""read_csv treats zeroes as nan if column contains any nan""  which is now closed  I can't reproduce in my development version:

In [27]: with open(""testtxt"") as fp:
   :     for line in fp:
   :         print repr(line)
   :         
'Aaa\tBbb\n'
'Foo\t0\n'
'Bar\t1\n'
'Baz\tNULL\n'

In [28]: pdread_table(""testtxt"")
Out[28]: 
   Aaa  Bbb
0  Foo    0
1  Bar    1
2  Baz  NaN

In [29]: pd__version__
Out[29]: '0101dev-f7f7e13'
",487339.0,,,,,2013-01-10 11:35:17,6.0
14386296,2,14386117,2013-01-17 19:08:00,2,"This problem has its roots in NumPy

def entered_long(df):
  return buy_pattern(df) & (dfHigh > dfHighshift(1))


entered_long is returning an array-like object NumPy refuses to guess if an array is True or False:

In [48]: x = nparray([ True   True   True]  dtype=bool)

In [49]: bool(x)

ValueError: The truth value of an array with more than one element is ambiguous Use aany() or aall()


To fix this  use any or all to specify what you mean for an array to be True:

def calc_position(df):
  # sum of current positions + any new positions

  if entered_long(df)any():  # or all()


The any() method will return True if any of the items in entered_long(df) are True
The all() method will return True if all the items in entered_long(df) are True",190597.0,,,,,2013-01-17 19:08:00,1.0
8992714,2,8991709,2012-01-24 19:17:34,40,"pandas is faster because I came up with a better algorithm  which is implemented very carefully using a fast hash table implementation (https://githubcom/attractivechaos/klib) and in C/Cython to avoid the Python interpreter overhead for the non-vectorizable parts The algorithm is described in some detail in my presentation here: http://wesmckinneycom/blog/?p=437

The comparison with datatable is actually a bit interesting because the whole point of datatable is that it contains pre-computed indexes for various columns to accelerate operations like data selection and merges In this case (database joins) pandas DataFrame contains no pre-computed information that is being used for the merge  so it's a ""cold"" merge so to speak If I had stored the factorized versions of the join keys  the join would be significantly faster as that is the biggest bottleneck (factor-izing) in the algorithm

I should also add that the internal design of pandasDataFrame is much more amenable to these kinds of operations than R's dataframe (which is just a list of arrays internally)",776560.0,,776560.0,,2012-01-24 19:23:25,2012-01-24 19:23:25,2.0
14359211,2,14358567,2013-01-16 13:07:53,0,"You could use npdiff() to test where a segment starts/ends and iterate over those results Its a very simple solution  so probably not the most performent one

a = nparray([3 3 3 3 3 4 4 4 4 4 1 1 1 1 4 4 12 12 12])

prev = 0
splits = npappend(npwhere(npdiff(a) != 0)[0] len(a)+1)+1

for split in splits:
    print nparange(1 asize+1 1)[prev:split]
    prev = split


Results in:

[1 2 3 4 5]
[ 6  7  8  9 10]
[11 12 13 14]
[15 16]
[17 18 19]
",1755432.0,,1755432.0,,2013-01-16 13:37:18,2013-01-16 13:37:18,1.0
14360423,2,14358567,2013-01-16 14:16:30,1,"One-liner:

dfreset_index()groupby('A')['index']apply(lambda x: nparray(x))


Code for example:

In [1]: import numpy as np

In [2]: from pandas import *

In [3]: df = DataFrame([3]*4+[4]*4+[1]*4  columns=['A'])
In [4]: df
Out[4]:
    A
0   3
1   3
2   3
3   3
4   4
5   4
6   4
7   4
8   1
9   1
10  1
11  1

In [5]: dfreset_index()groupby('A')['index']apply(lambda x: nparray(x))
Out[5]:
A
1    [8  9  10  11]
3      [0  1  2  3]
4      [4  5  6  7]


You can also directly access the information from the groupby object:

In [1]: grp = dfgroupby('A')

In [2]: grpindices
Out[2]:
{1L: array([ 8   9  10  11]  dtype=int64) 
 3L: array([0  1  2  3]  dtype=int64) 
 4L: array([4  5  6  7]  dtype=int64)}

In [3]: grpindices[3]
Out[3]: array([0  1  2  3]  dtype=int64)


To address the situation that DSM mentioned you could do something like:

In [1]: df['block'] = (dfAshift(1) != dfA)astype(int)cumsum()

In [2]: df
Out[2]:
    A  block
0   3      1
1   3      1
2   3      1
3   3      1
4   4      2
5   4      2
6   4      2
7   4      2
8   1      3
9   1      3
10  1      3
11  1      3
12  3      4
13  3      4
14  3      4
15  3      4


Now groupby both columns and apply the lambda function:

In [77]: dfreset_index()groupby(['A' 'block'])['index']apply(lambda x: nparray(x))
Out[77]:
A  block
1  3          [8  9  10  11]
3  1            [0  1  2  3]
   4        [12  13  14  15]
4  2            [4  5  6  7]
",919872.0,,919872.0,,2013-01-16 14:40:01,2013-01-16 14:40:01,2.0
14558497,2,14558387,2013-01-28 08:56:02,1,"You can use range selection Eg to remove column3  you can use:

data = npzeros((401125 )  dtype = object)
for i  row in enumerate(csv_file_object):
    data[i] = row[:2] + row[3:]


This will work  assuming that csv_file_object yields lists If it is eg a simple file object created with csv_file_object = open(""filecvs"")  add split in your loop:

data = npzeros((401125 )  dtype = object)
for i  row in enumerate(csv_file_object):
    row = rowsplit()
    data[i] = row[:2] + row[3:]
",724873.0,,,,,2013-01-28 08:56:02,1.0
14558806,2,14558387,2013-01-28 09:17:28,4,"Use pandas Also it seems to me  that for various type of data as yours  the pandasDataFrame may be  better fit

from StringIO import StringIO
from pandas import *
import numpy as np

data = """"""column1  column2  column3  column4  column5
1         none     2       'gona'    53
2         34       2       'gina'    55
3         none     2       'gana'    51
4         43       2       'gena'    50
5         none     2       'guna'    57""""""

data = StringIO(data)
print read_csv(data  delim_whitespace=True)drop('column3' axis =1)


out:

   column1 column2 column4  column5
0        1    none  'gona'      53
1        2      34  'gina'      55
2        3    none  'gana'      51
3        4      43  'gena'      50
4        5    none  'guna'      57


If you need an array instead of DataFrame  use the to_records() method:

dfto_records(index = False)
#output:
recarray([(1L  'none'  ""'gona'""  53) 
           (2L  '34'  ""'gina'""  55) 
           (3L  'none'  ""'gana'""  51) 
           (4L  '43'  ""'gena'""  50) 
           (5L  'none'  ""'guna'""  57)]  
            dtype=[('column1'  '<i8')  ('column2'  '|O4') 
                   ('column4'  '|O4')  ('column5'  '<f8')])
",1199589.0,,1199589.0,,2013-01-28 09:31:45,2013-01-28 09:31:45,4.0
12906708,2,12906450,2012-10-16 02:28:57,1,"The CSV stores string representations of data  so it's not necessarily going to scale in an obvious way with the number of columns unless all columns have roughly the same size in string representation  It's quite plausible that your CSV could increase a lot in size if your original data had only a few decimal places  If you read in numbers like 01  02  3  17  whatever  and then z-scale them  you're likely to get results with many decimal places  As a simple example  I did this:

>>> df = pandasDataFrame([[2  3  5]]  columns=[""A""  ""B""  ""C""])
>>> df
   A  B  C
0  2  3  5
>>> dfto_csv('someCSVcsv')
>>> df**05
          A         B         C
0  1414214  1732051  2236068
>>> (df**05)to_csv('someCSV2csv')


I didn't add any rows or columns to the data at all  just took the square root  but the second CSV is 4 times the size of the first  because the second one has lots of decimal places that take more bytes to write out in string form  You're likely to get similarly long decimals when you divide by the standard deviation",1427416.0,,,,,2012-10-16 02:28:57,1.0
13036848,2,13035764,2012-10-23 18:27:46,3,"Oh my This is actually so simple!

grouped = df3groupby(level=0  by=['rownum']) # `by` not needed it seems
df4 = groupedlast()
df4
                      A   B  rownum
2001-01-01 00:00:00  20 -50       6
2001-01-01 01:00:00 -30  60       7
2001-01-01 02:00:00  40 -70       8
2001-01-01 03:00:00   3   3       3
2001-01-01 04:00:00   4   4       4
2001-01-01 05:00:00   5   5       5
",1552748.0,,,,,2012-10-23 18:27:46,
13370603,2,13352369,2012-11-13 23:20:53,1,Fixed underlying bug today on GitHub: https://githubcom/pydata/pandas/issues/2236,776560.0,,,,,2012-11-13 23:20:53,
14124963,2,14124710,2013-01-02 15:52:26,1,"To obtain the unique values in a column you can use the unique Series method  which will return a numpy array of the unique values (and it is fast!)

dflongunique()
# returns numpy array of unique values


You could then use numpyappend:

npappend(dflongunique()  dfshortunique())


Note: This just appends the two unique results together and so itself is not unique!



Here's a (trivial) example:

import pandas as pd
import numpy as np
df = pdDataFrame([[1  2]  [1  4]]  columns=['long' 'short'])

In [4]: df
Out[4]: 
   long  short
0     1      2
1     1      4

In [5]: dflongunique()
Out[5]: array([1])

In [6]: dfshortunique()
Out[6]: array([2  4])


And then appending the resulting two arrays:

In [7]: npappend(dflongunique()  dfshortunique())
Out[7]: array([1  2  4])


Using @Zalazny7's set is significantly faster (since it runs over the array only once) and somewhat upsettingly it's even faster than npunique (which sorts the resulting array!)",1240268.0,,1240268.0,,2013-01-02 16:22:29,2013-01-02 16:22:29,0.0
14125116,2,14124710,2013-01-02 16:01:16,1,"Adding to Hayden's answer  you could also use the set() method for the same result  The performance is slightly better if that's a consideration:

In [28]: %timeit set(npappend(df[0] df[1]))
100000 loops  best of 3: 196 us per loop

In [29]: %timeit npappend(df[0]unique()  df[1]unique())
10000 loops  best of 3: 55 us per loop
",919872.0,,,,,2013-01-02 16:01:16,3.0
14308712,2,14308512,2013-01-13 22:01:22,0,"This digs into the pandas internals in questionable ways  but

ax = pltgca()
axget_xaxis()get_major_formatter()scaled[365] = '%y'
pltdraw()


format spec",380231.0,,380231.0,,2013-01-14 06:14:26,2013-01-14 06:14:26,4.0
14498016,2,14497777,2013-01-24 09:34:03,1,"In some circles this operation is known as the ""asof"" join Here is an implementation:

def diffCols(df1  df2):
    """""" Find columns in df1 not present in df2
    Return df1columns  - df2columns maintaining the order which the resulting
    columns appears in df1

    Parameters:
    ----------
    df1 : pandas dataframe object
    df2 : pandas dataframe objct
    Pandas already offers df1columns - df2columns  but unfortunately
    the original order of the resulting columns is not maintained
    """"""
    return [i for i in df1columns if i not in df2columns]


def aj(df1  df2  overwriteColumns=True  inplace=False):
    """""" KDB+ like asof join
    Finds prevailing values of df2 asof df1's index The resulting dataframe
    will have same number of rows as df1

    Parameters
    ----------
    df1 : Pandas dataframe
    df2 : Pandas dataframe
    overwriteColumns : boolean  default True
         The columns of df2 will overwrite the columns of df1 if they have the same
         name unless overwriteColumns is set to False In that case  this function
         will only join columns of df2 which are not present in df1
    inplace : boolean  default False
        If True  adds columns of df2 to df1 Otherwise  create a new dataframe with
        columns of both df1 and df2

    *Assumes both df1 and df2 have datetime64 index """"""
    joiner = lambda x : xasof(df1index)
    if not overwriteColumns:
        # Get columns of df2 not present in df1
        cols = diffCols(df2  df1)
        if len(cols) > 0:
            df2 = df2ix[: cols]
    result = df2apply(joiner)
    if inplace:
        for i in resultcolumns:
            df1[i] = result[i]
        return df1
    else:
        return result


Internally  this uses pandasSeriesasof()",367273.0,,,,,2013-01-24 09:34:03,
14503270,2,14497777,2013-01-24 14:08:42,1,"What about using Seriessearchsorted() to return the index of y where you would insert x  You could then subtract one from that value and use it to index y

In [1]: x
Out[1]:
0    1301
1    1402

In [2]: y
Out[2]:
0    1300
1    1301
2    1330
3    1400

In [3]: y[ysearchsorted(x)-1]
Out[3]:
0    1300
3    1400


note: the above example uses int64 Series",919872.0,,,,,2013-01-24 14:08:42,1.0
9857283,2,9850954,2012-03-25 01:46:55,1,"The aggregate() method on groupby objects can be used to create a new DataFrame from a groupby object in a single step  (I'm not aware of a cleaner way to extract the first/last row of a DataFrame though)

In [12]: dfgroupby('obj_id')agg(lambda df: dfsort('data_date')[-1:]values[0])
Out[12]: 
         data_date  value
obj_id                   
1       2009-07-28  15860
2       2011-10-01  35200
4       2011-11-01  59500


You can also perform aggregation on individual columns  in which case the aggregate function works on a Series object

In [25]: dfgroupby('obj_id')['value']agg({'diff': lambda s: smax() - smin()})
Out[25]: 
          diff
obj_id        
1            0
2       165000
4        34640
",243434.0,,243434.0,,2012-12-20 02:58:51,2012-12-20 02:58:51,
9916744,2,9850954,2012-03-28 22:43:04,0,"I don't know of a better way off the top of my head I created an issue here to someday implement a specialized function exactly for this purpose:

https://githubcom/pydata/pandas/issues/978",776560.0,,,,,2012-03-28 22:43:04,
11384763,2,11314693,2012-07-08 16:33:00,2,For the record  I've kind of been hoping that some kind souls with access to various flavors of SQL databases would help add SQL-compatibility features to pandas ,776560.0,,,,,2012-07-08 16:33:00,
14660715,2,11314693,2013-02-02 09:59:16,0,"Just to save someone else who tried to use this some time It turns out the line:

wildcards = ' 'join(['?'] * len(framecolumns))


should be:

wildcards = ' 'join(['%s'] * len(framecolumns))


Hope that helps",1994810.0,,,,,2013-02-02 09:59:16,
11643893,2,11640243,2012-07-25 06:28:37,5,"I think this might work:

import matplotlibpyplot as plt
from pandas import DataFrame
df = DataFrame(nprandomrandn(5  3)  columns=['A'  'B'  'C'])

fig  ax = pltsubplots()
ax2  ax3 = axtwinx()  axtwinx()
rspine = ax3spines['right']
rspineset_position(('axes'  125))
ax3set_frame_on(True)
ax3patchset_visible(False)
figsubplots_adjust(right=075)

dfAplot(ax=ax  style='b-')
dfBplot(ax=ax2  style='r-'  secondary_y=True)
dfCplot(ax=ax3  style='g-')


Output: http://imgurcom/6aM2Y",1306530.0,,,,,2012-07-25 06:28:37,1.0
12369472,2,12368180,2012-09-11 12:13:43,3,"Maybe set_index is what you want? pivot is a reshape operation:

In [4]: frameset_index(['date'  'mat'  'strike'])
Out[4]: 
                            dataframe name tenor     capvol
date     mat strike                                        
20120903 1y  025    EUR CapFloor Volat_3m    3m  15220216
             050    EUR CapFloor Volat_3m    3m  15196937
             100    EUR CapFloor Volat_3m    3m  14926697
             150    EUR CapFloor Volat_3m    3m  15294075
             200    EUR CapFloor Volat_3m    3m  15722935
             225    EUR CapFloor Volat_3m    3m  15932589


In [7]: dfcapvolunstack('mat')
Out[7]: 
mat                     1y
date     strike           
20120903 025    15220216
         050    15196937
         100    14926697
         150    15294075
         200    15722935
         225    15932589
",776560.0,,,,,2012-09-11 12:13:43,1.0
13079936,2,13079852,2012-10-26 02:21:48,2,"Have a look at the join method of dataframes  use the lsuffix and rsuffix attributes to create new names for the joined columns  It works like this:

>>> x
          A         B         C
0  0838119 -1116730  0167998
1 -1143761  0051970  0216113
2 -0614441  0208978 -0630988
3  0114902 -0248791 -0503172
4  0836523 -0802074  1478333
>>> y
          A         B         C
0 -0455859 -0488645 -1618088
1 -2295255  0524681  1021320
2 -0484612  1101463 -0081476
3 -0475076  0915797 -0998777
4 -0847538  0057044  1053533
>>> xjoin(y  lsuffix=""_x""  rsuffix=""_y"")
        A_x       B_x       C_x       A_y       B_y       C_y
0  0838119 -1116730  0167998 -0455859 -0488645 -1618088
1 -1143761  0051970  0216113 -2295255  0524681  1021320
2 -0614441  0208978 -0630988 -0484612  1101463 -0081476
3  0114902 -0248791 -0503172 -0475076  0915797 -0998777
4  0836523 -0802074  1478333 -0847538  0057044  1053533
",1427416.0,,,,,2012-10-26 02:21:48,
13082062,2,13079852,2012-10-26 06:43:54,3,"pdconcat is also an option

In [17]: pdconcat([GOOG  AAPL]  keys=['GOOG'  'AAPL']  axis=1)
Out[17]:
             GOOG                  AAPL
             Open   High    Low    Open    High     Low
Date
2011-01-03  2101  2105  2078  59648  60559  59648
2011-01-04  2112  2120  2105  60562  60618  60012
2011-01-05  2119  2121  2090  60007  61033  60005
2011-01-06  2067  2082  2055  61068  61843  61005
2011-01-07  2071  2077  2027  61591  61825  61013
",1548051.0,,,,,2012-10-26 06:43:54,1.0
13337623,2,13333159,2012-11-12 02:25:17,1,"After converting the Series to a DataFrame  copy the index into it's own column  (DatetimeIndexformat() is useful here as it returns a string representation of the index  rather than Timestamp/datetime objects)

In [510]: df = pdDataFrame(data)

In [511]: df['OrigDate'] = dfindexformat()

In [513]: df
Out[513]: 
             Price    OrigDate
Date                          
2002-09-09  23325  2002-09-09
2002-09-11  23305  2002-09-11
2002-09-16  23025  2002-09-16
2002-09-18  23010  2002-09-18
2002-09-19  23005  2002-09-19


For resampling without aggregation  there is a helper method asfreq()

In [528]: dfasfreq(""D""  method='bfill')
Out[528]: 
             Price    OrigDate
2002-09-09  23325  2002-09-09
2002-09-10  23305  2002-09-11
2002-09-11  23305  2002-09-11
2002-09-12  23025  2002-09-16
2002-09-13  23025  2002-09-16
2002-09-14  23025  2002-09-16
2002-09-15  23025  2002-09-16
2002-09-16  23025  2002-09-16
2002-09-17  23010  2002-09-18
2002-09-18  23010  2002-09-18
2002-09-19  23005  2002-09-19


This is effectively short-hand for the following  where last() is invoked on the intermediate DataFrameGroupBy objects

In [529]: dfresample(""D""  how='last'  fill_method='bfill')
Out[529]: 
             Price    OrigDate
Date                          
2002-09-09  23325  2002-09-09
2002-09-10  23305  2002-09-11
2002-09-11  23305  2002-09-11
2002-09-12  23025  2002-09-16
2002-09-13  23025  2002-09-16
2002-09-14  23025  2002-09-16
2002-09-15  23025  2002-09-16
2002-09-16  23025  2002-09-16
2002-09-17  23010  2002-09-18
2002-09-18  23010  2002-09-18
2002-09-19  23005  2002-09-19
",243434.0,,,,,2012-11-12 02:25:17,1.0
12641777,2,12641606,2012-09-28 14:16:00,0,"This has no error checking or optimisation at all  but is this what you want:

def sort_on(lines  col_idx):
  return sorted(lines  key=lambda l: float(lsplit()[col_idx]))

lines = """"""\
15 rs1820451 32681212 0441 0493 05358 989 29 0 0441 T:A 
14 rs1820450 32680556 0441 0493 05358 989 29 0 0441 G:C 
38 rs1820447 32693541 0421 0332 00915 944 26 0 0211 G:A 
37 rs1820446 32693440 0483 0499 09633 1000 30 0 0475 G:T 
7 rs1808502 32660555 0517 046 0543 1000 30 0 0358 C:G 
24 rs17817908 32687035 0407 0362 06159 989 29 0 0237 C:T 
22 rs17817896 32686160 0407 0362 06159 989 29 0 0237 T:A 
66 rs17236946 32717247 0492 0453 07762 989 29 0 0347 T:C
""""""splitlines()

sorted_lines = sort_on(lines  3)
print ""\n""join(sorted_lines)
",1604257.0,,,,,2012-09-28 14:16:00,2.0
12641819,2,12641606,2012-09-28 14:18:27,1,If you want to sort on a column or multiple columns you need to use dfsort()  dfsort_index() sorts on the index only,1548051.0,,,,,2012-09-28 14:18:27,1.0
12642259,2,12641606,2012-09-28 14:46:04,0,"Unless I made a mistake  it still doesn't work

outsorted = outdatasort(columns='Name'  ascending=False  axis=0)

38 rs1820447 32693541 0421 0332 00915 944 26 0 0211 G:A 
37 rs1820446 32693440 0483 0499 09633 1000 30 0 0475 G:T 
7 rs1808502 32660555 0517 046 0543 1000 30 0 0358 C:G 
24 rs17817908 32687035 0407 0362 06159 989 29 0 0237 C:T 
22 rs17817896 32686160 0407 0362 06159 989 29 0 0237 T:A 
66 rs17236946 32717247 0492 0453 07762 989 29 0 0347 T:C 
39 rs17236939 32694770 0483 0499 09633 1000 30 0 0475 C:G
",1289107.0,,,,,2012-09-28 14:46:04,4.0
12648275,2,12641606,2012-09-28 22:41:05,0,"For futures references  here goes a possible solution 

    cond = ((df['L1'] != rscode) & (df['L2'] != rscode))
    outname = inf + '_test'
    df['L3'] = df['L1']map(lambda x: int(str(x)[2:]))        
    outdata = dfdrop(df[cond]indexvalues)sort(columns='L3'  ascending=False  axis=0)
    # export outdata using Datadrameto_csv with the original df cols


Improvements are welcome
Best ",1289107.0,,,,,2012-09-28 22:41:05,
13033776,2,13033270,2012-10-23 15:18:42,1,"For some reason  the Python zlib module has the ability to decompress gzip data  but it does not have the ability to directly compress to that format  At least as far as what is documented  This is despite the remarkably misleading documentation page header ""Compression compatible with gzip""

You can compress to the zlib format instead using zlibcompress or zlibcompressobj  and then strip the zlib header and trailer and add a gzip header and trailer  since both the zlib and gzip formats use the same compressed data format  This will give you data in the gzip format  The zlib header is fixed at two bytes and the trailer at four bytes  so those are easy to strip  Then you can prepend a basic gzip header of ten bytes: ""\x1f\x8b\x08\0\0\0\0\0\0\xff"" (C string format) and append a four-byte CRC in little-endian order  The CRC can be computed using zlibcrc32",1180620.0,,,,,2012-10-23 15:18:42,4.0
13102877,2,13033270,2012-10-27 18:19:16,1,We plan to add better serialization with compression eventually Stay tuned to pandas development,776560.0,,,,,2012-10-27 18:19:16,1.0
13301453,2,13293810,2012-11-09 02:54:20,0,"This probably isn't the most elegant way to do it  but it gets the job done

In[1]: import numpy as np

In[2]: import pandas as pd

In[3]: df = pdDataFrame(npgenfromtxt('/Users/spencerlyon2/Desktop/testcsv'  dtype=str)[1:]  columns=['ID'])

In[4]: df
Out[4]: 
                       ID
0  00013007854817840016671868
1  00013007854817840016749251
2  00013007854817840016754630
3  00013007854817840016781876
4  00013007854817840017028824
5  00013007854817840017963235
6  00013007854817840018860166


Just replace '/Users/spencerlyon2/Desktop/testcsv' with the path to your file",1742701.0,,1742701.0,,2012-11-10 01:06:15,2012-11-10 01:06:15,1.0
13557050,2,13555607,2012-11-26 00:14:59,2,"I am new to programming and python in general but I managed to throw together a dirty fix  the legends are now correct  the colors are not

def plot_normalized(agged  show_errorbars  filename):
  combined = {}
  for k in agged:
    combined[k] = agged[k]['CPS_norm_mean']
  combined = pandasDataFrame(combined)

  ax=combinedplot()

  if show_errorbars:
    for k in agged:
      plterrorbar(
        x=agged[k]index 
        y=agged[k]['CPS_norm_mean'] 
        yerr=agged[k]['CPS_norm_std'] 
        label = k #added
      )

  if show_errorbars: #try this  dirty fix
   labels  handles = axget_legend_handles_labels()
   N = len(handles)/2
   pltlegend(labels[:N]  handles[N:])

  #Why does the fix work?:
  #labels  handles = axget_legend_handles_labels()
  #print handles
  #out:
  #[u'Blank'  u'H9A'  u'Q180K'  u'Wildtype'  'Q180K'  'H9A'  'Wildtype'  'Blank']
  #Right half has correct order  these are the labels from label=k above in errorplot



  pltxlabel('Time')
  pltylabel('CPS/Absorbency')
  plttitle('CPS/Absorbency vs Time')
  pltsavefig(filename)


Produces: 
",948652.0,,,,,2012-11-26 00:14:59,1.0
13793474,2,13793321,2012-12-10 00:35:51,0,"You are looking for a merge:

df1merge(df2  on='Date_Time')


The keywords are the same as for join  but join uses only the index  see ""Database-style DataFrame joining/merging""

Here's a simple example:

import pandas as pd
df1 = pdDataFrame([[1  2  3]])
df2 = pdDataFrame([[1  7  8] [4  9  9]]  columns=[0  3  4])

In [4]: df1
Out[4]: 
   0  1  2
0  1  2  3

In [5]: df2
Out[5]: 
   0  3  4
0  1  7  8
1  4  9  9

In [6]: df1merge(df2  on=0)
Out[6]: 
   0  1  2  3  4
0  1  2  3  7  8

In [7]: df1merge(df2  on=0  how='outer')
Out[7]: 
   0   1   2  3  4
0  1   2   3  7  8
1  4 NaN NaN  9  9


If you try and join on a column you get an error:

In [8]: df1join(df2  on=0)
# error!
Exception: columns overlap: array([0]  dtype=int64)


See ""Joining key columns on an index""",1240268.0,,1240268.0,,2012-12-10 01:36:13,2012-12-10 01:36:13,8.0
13998331,2,13996302,2012-12-21 23:07:35,1,"I'm not sure of the mechanics  but this works  Note  the returned value is just an ndarray  I think you could apply any cumulative or ""rolling"" function in this manner and it should have the same result

I have tested it with cumprod  cummax and cummin and they all returned an ndarray  I think pandas is smart enough to know that these functions return a series and so the function is applied as a transformation rather than an aggregation

In [35]: dfgroupby('id')['x']cumsum()
Out[35]:
0     0
1     1
2     3
3     3
4     7
5    12


Edit: I found it curious that this syntax does return a Series:

In [54]: dfgroupby('id')['x']transform('cumsum')
Out[54]:
0     0
1     1
2     3
3     3
4     7
5    12
Name: x
",919872.0,,,,,2012-12-21 23:07:35,
13998600,2,13996302,2012-12-21 23:41:42,3,"In [16]: dfgroupby('id')['x']apply(pdrolling_mean  2  min_periods=1)
Out[16]: 
0    00
1    05
2    15
3    30
4    35
5    45

In [17]: dfgroupby('id')['x']cumsum()
Out[17]: 
0     0
1     1
2     3
3     3
4     7
5    12
",243434.0,,,,,2012-12-21 23:41:42,
14267450,2,14199168,2013-01-10 21:19:52,1,"from the sourcecode:

max_groups = 1L
for x in group_sizes:
    max_groups *= long(x)

if max_groups > 2**63:  # pragma: no cover
    raise Exception('Combinatorial explosion! (boom)')


And  in the same file

# max groups = largest possible number of distinct groups
left_key  right_key  max_groups = self_get_group_keys()


The line max_groups *= long(x) indicates that it is not additive  thus critical",1156006.0,,,,,2013-01-10 21:19:52,
13389827,2,13385663,2012-11-15 00:32:51,1,"Instead of slicing  use shift to move the index position of values in a DataFrame/Series For example:

returns = (vfiax_monthlyopen - vfiax_monthlyopenshift(1))/vfiax_monthlyopen


This is what pct_change is doing under the bonnet You can also use it for other functions eg:

(3*vfiax_monthlyopen + 2*vfiax_monthlyopenshift(1))/5


You might also want to looking into the rolling and window functions for other types of analysis of financial data",1452002.0,,1452002.0,,2012-11-15 00:45:03,2012-11-15 00:45:03,1.0
13606221,2,13603181,2012-11-28 13:30:27,4,"The reason your not seeing anything is because the default plot style is only a line But the line gets interupted at NaN's so only multiple consequtive values will be plotted And the latter doesnt happen in your case You need to change the style of plotting  which depends on what you want to see

For starters  try adding:

plot(marker='o')


That should make all data points appear as circles It easily gets cluttered so adjusting markersize  edgecolor etc might be usefull Im not fully adjusted to how Pandas is using matplotlib so i often switch to matplotlib myself if plots get more complicated  eg: 

pltplot(dfR2indexto_pydatetime()  dfR2  'o-')
",1755432.0,,,,,2012-11-28 13:30:27,1.0
13879543,2,13878959,2012-12-14 13:24:59,0,"In pandas it is called 'expanding' instead of cumulative I think:

http://pandaspydataorg/pandas-docs/dev/computationhtml#expanding-window-moment-functions 

Anyway  you can use the exanding_sum function to achieve what you want: http://pandaspydataorg/pandas-docs/dev/generated/pandasstatsmomentsexpanding_sumhtml",653364.0,,,,,2012-12-14 13:24:59,
13879549,2,13878959,2012-12-14 13:25:21,2,"As @JonClements mentions  you can do this using the cumsum DataFrame method:

from pandas import DataFrame
df = DataFrame({0: {'10/10/2012': 50  '10/11/2012': -10  '10/12/2012': 100}  1: {'10/10/2012': 0  '10/11/2012': 90  '10/12/2012': -5}})

In [3]: df
Out[3]: 
              0   1
10/10/2012   50   0
10/11/2012  -10  90
10/12/2012  100  -5

In [4]: dfcumsum()
Out[4]: 
              0   1
10/10/2012   50   0
10/11/2012   40  90
10/12/2012  140  85
",1240268.0,,,,,2012-12-14 13:25:21,
14035931,2,14035817,2012-12-26 04:14:32,1,"ix's main purpose is to allow numpy like indexing with support for row and column labels  So I'm not sure your use-case is the intended purpose  Here are a couple of ways I can think of  mostly trivial:

In [142]: dfix[:][-2:]
Out[142]:
          0         1         2         3
8  0386882 -0836112 -0108250 -0433797
9  0642468 -0399255 -0911456 -0497720

In [161]: dfix[dfindex[-2:] :]
Out[161]:
          0         1         2         3
8  0386882 -0836112 -0108250 -0433797
9  0642468 -0399255 -0911456 -0497720


I don't think ix supports negative indexing at all  It seems to just ignore it altogether:

In [181]: dfix[-100: :]
Out[181]:
          0         1         2         3
0 -1144137 -1042034 -2158838  0674055
1 -0424184  1237318 -1846130  0575357
2 -0844974 -0541060  2197364 -0031898
3  0846263  1244450 -1570566 -0477919
4 -0193445  0171045 -0235587 -1185583
5  1361539 -1107389 -1321081 -0776407
6  0505907 -1364414 -2093770  0144016
7 -0888465 -0329153  0491264 -0363472
8  0386882 -0836112 -0108250 -0433797
9  0642468 -0399255 -0911456 -0497720


Edit: From the pandas documentation we have:


  Label-based indexing with integer axis labels is a thorny topic It
  has been discussed heavily on mailing lists and among various members
  of the scientific Python community In pandas  our general viewpoint
  is that labels matter more than integer locations Therefore  with an
  integer axis index only label-based indexing is possible with the
  standard tools like ix The following code will generate exceptions:

s = Series(range(5))
s[-1]
df = DataFrame(nprandomrandn(5  4))
df
dfix[-2:]

  
  This deliberate decision was made to prevent ambiguities and subtle
  bugs (many users reported finding bugs when the API change was made to
  stop falling back on position-based indexing)
",919872.0,,919872.0,,2012-12-26 04:28:33,2012-12-26 04:28:33,
8997908,2,8991709,2012-01-25 04:42:42,20,"It looks like Wes may have discovered a known issue in datatable when the number of unique strings (levels) is large: 10 000

Does Rprof() reveal most of the time spent in the call sortedmatch(levels(i[[lc]])  levels(x[[rc]])?  This isn't really the join itself (the algorithm)  but a preliminary step

Recent efforts have gone into allowing character columns in keys  which should resolve that issue by integrating more closely with R's own global string hash table Some benchmark results are already reported by testdatatable() but that code isn't hooked up yet to replace the levels to levels match

Are pandas merges faster than datatable for regular integer columns?  That should be a way to isolate the algorithm itself vs factor issues

Also  datatable has time series merge in mind Two aspects to that: i) multi column ordered keys such as (id datetime) ii) fast prevailing join (roll=TRUE) aka last observation carried forward

I'll need some time to confirm as it's the first I've seen of the comparison to datatable as presented",403310.0,,403310.0,,2012-01-25 05:27:58,2012-01-25 05:27:58,3.0
11263357,2,11253390,2012-06-29 14:10:32,1,"You're probably being hit hard by this part of the currentGroup function:

attributionCalcDF['GROUP_LIST']ix[idx]


Try saving that to a temporary variable and using the temp variable inside startswith I'm planning to add vectorized string functions to pandas soon  so that will be a big help in these cases  too",776560.0,,,,,2012-06-29 14:10:32,1.0
11461644,2,11459106,2012-07-12 22:26:18,3,"import numpy as np
tsgroupby([by('year')  by('month')  by('day')])apply(lambda x: npmean(x))
",567620.0,,,,,2012-07-12 22:26:18,3.0
11824434,2,11824341,2012-08-06 07:55:13,1,"To find datetimepyx  I'd suggest starting with:

find /usr/local/lib/python27 -name 'datetimepyx'


To find the definition of the Timestamp class  I'd start with:

grep -r 'class Timestamp' /usr/local/lib/python27/dist-packages/pandas-0<tab>
",1547619.0,,,,,2012-08-06 07:55:13,2.0
11824464,2,11824341,2012-08-06 07:57:17,1,"The pyx files are Cython files  You might find them installed  but they might not be if you didn't build the library from source  However  all the pandas source is available on github  The source for datetimepyx is here

Cython looks similar to Python  so if you're just trying to browse the source to figure out what's causing the error  you might find it useful to see the Cython source  But you won't be able to debug it like regular Python  because it's not",1427416.0,,,,,2012-08-06 07:57:17,1.0
11826333,2,11824341,2012-08-06 10:16:10,0,"For you convenience i copied pandaslib_string_to_dts function code below
Based on your traceback  this is the one throwing the exception

cdef inline _string_to_dts(object val  pandas_datetimestruct* dts):
  cdef:
      npy_bool islocal  special
      PANDAS_DATETIMEUNIT out_bestunit
      int result

  if PyUnicode_Check(val):
      val = PyUnicode_AsASCIIString(val);

  result = parse_iso_8601_datetime(val  len(val)  PANDAS_FR_ns 
                                   NPY_UNSAFE_CASTING 
                                   dts  &islocal  &out_bestunit  &special)
  if result == -1:
      raise ValueError('Unable to parse %s' % str(val))


To me it looks like you provide input to pandasdate_range which can not be parsed to a date and your code in process_csv throws a ValueError exception like in the example below

>>> import pandas
>>> pandasdate_range('hello')
Traceback (most recent call last):
  File ""<stdin>""  line 1  in <module>
  File ""/lib/python27/site-packages/pandas-082dev_90842ba-py27-linux-x86_64egg/pandas/tseries/indexpy""  line 1317  in date_range
    freq=freq  tz=tz  normalize=normalize  name=name)
  File ""/lib/python27/site-packages/pandas-082dev_90842ba-py27-linux-x86_64egg/pandas/tseries/indexpy""  line 176  in __new__
    tz=tz  normalize=normalize)
  File ""/lib/python27/site-packages/pandas-082dev_90842ba-py27-linux-x86_64egg/pandas/tseries/indexpy""  line 254  in _generate
    start = Timestamp(start)
  File ""datetimepyx""  line 87  in pandaslibTimestamp__new__ (pandas/src/tseriesc:26892)
  File ""datetimepyx""  line 511  in pandaslibconvert_to_tsobject (pandas/src/tseriesc:32019)
  File ""datetimepyx""  line 640  in pandaslib_string_to_dts (pandas/src/tseriesc:33452)
ValueError: Unable to parse hello
",1548051.0,,,,,2012-08-06 10:16:10,
14558827,2,14558387,2013-01-28 09:18:39,3,"Assuming you're reading the CSV rows and sticking them into a numpy array  the easiest and best solution is almost definitely preprocessing the data before it gets to the array  as Maciek D's answer shows (If you want to do something more complicated than ""remove column 3"" you might want something like [value for i  value in enumerate(row) if i not in (1  3  5)]  but the idea is still the same)

However  if you've already imported the array and you want to filter it after the fact  you probably want take or delete:

>>> d=nparray([[1 None 2 'gona' 53] [2 34 2 'gina' 55] [3 None 2 'gana' 51] [4 43 2 'gena' 50] [5 None 2 'guna' 57]])
>>> npdelete(d  2  1)
array([[1  None  gona  53] 
       [2  34  gina  55] 
       [3  None  gana  51] 
       [4  43  gena  50] 
       [5  None  guna  57]]  dtype=object)
>>> nptake(d  [0  1  3  4]  1)
array([[1  None  gona  53] 
       [2  34  gina  55] 
       [3  None  gana  51] 
       [4  43  gena  50] 
       [5  None  guna  57]]  dtype=object)


For the simple case of ""remove column 3""  delete makes more sense; for a more complicated case  take probably makes more sense

If you haven't yet worked out how to import the data in the first place  you could either use the built-in csv module and something like Maciek D's code and process as you go  or use something like pandasread_csv and post-process the result  as root's answer shows

But it might be better to use a native numpy data format in the first place instead of CSV",908494.0,,908494.0,,2013-01-28 09:25:08,2013-01-28 09:25:08,
8273193,2,8273092,2011-11-25 18:51:44,1,"Best place to ask this question would be on the mailing list (luckily Google Alerts let me know about this right away!) It looks like your install failed and it's still using pandas 030 Were there any errors when you ran python setuppy install? You may have to upgrade your Cython installation if you're building from a git snapshot (sudo easy_install -U Cython) If that doesn't work  try nuking your existing pandas install:

sudo rm -rf /Library/Frameworks/EPD64framework/Versions/71/lib/python27/site-packages/pandas

and reinstalling The tarball on PyPI includes pre-built Cython sources If you're building from source anyway I recommend building from the head revision on git anyway I'm days away from releasing pandas 060",776560.0,,,,,2011-11-25 18:51:44,1.0
8281757,2,8273092,2011-11-26 21:28:53,1,"The problem was resolved by:

1) installing the pandas 060 release which Wes made available November 25/2011

2) removing the existing scikits package  which is bundled with EPD 71 (version 02) and installing the latest release using 'easy_install -U scikitsstatsmodels'

The above were suggestions made by Wes and folks on the pystatsmodel mailing list (thank you to all)",668624.0,,668624.0,,2011-11-26 22:18:07,2011-11-26 22:18:07,
10429674,2,10422438,2012-05-03 10:37:38,2,"You can use

storekeys()


to get a list of the stored objects",1301710.0,,,,,2012-05-03 10:37:38,
11246050,2,11235604,2012-06-28 13:44:35,0,Can you please report issues like these on GitHub (http://githubcom/pydata/pandas/issues) The warning is not a concern  I may disable the unit test that is causing it ,776560.0,,,,,2012-06-28 13:44:35,1.0
11435635,2,11423690,2012-07-11 15:04:44,1,"EDIT: corrected code

How about this:

adjyear = npwhere(dfseason == 'SPRING'  dfyear + 1  dfyear)
adjyearname = 'year'

grouped = dfgroupby(['method'  'replicate'  'site'  adjyear])
grouped = grouped['sp1'  'sp2'  'sp3']    

groupedsum()[groupedsize() > 1]


This gives me:

In [20]: groupedsum()[groupedsize() > 1]
Out[20]: 
                            sp1  sp2  sp3
method replicate site year               
EDGE   1         AAA  2006    2    4    6
                 BBB  2006    2    4    6
       2         AAA  2006    2    4    6
RIFFLE 1         AAA  2006    2    4    6
                 BBB  2006    2    4    6
       2         AAA  2006    2    4    6
",776560.0,,776560.0,,2012-07-11 20:24:13,2012-07-11 20:24:13,
11788556,2,11787883,2012-08-03 02:47:01,2,"You can try creating a cumulative adjustment factor series in one shot then you don't need to loop:

(p['Dividends']fillna(1) + 1)cumprod()
",1306530.0,,,,,2012-08-03 02:47:01,1.0
12169357,2,12169170,2012-08-29 00:27:33,4,"You can get the maximum like this:

>>> df = DataFrame({""A"": [1 2 3]  ""B"": [-2  8  1]})
>>> df
   A  B
0  1 -2
1  2  8
2  3  1
>>> df[[""A""  ""B""]]
   A  B
0  1 -2
1  2  8
2  3  1
>>> df[[""A""  ""B""]]max(axis=1)
0    1
1    8
2    3


and so:

>>> df[""C""] = df[[""A""  ""B""]]max(axis=1)
>>> df
   A  B  C
0  1 -2  1
1  2  8  8
2  3  1  3


If you know that ""A"" and ""B"" are the only columns  you could even get away with

>>> df[""C""] = dfmax(axis=1)


And you could use apply(max  axis=1) too  I guess",487339.0,,,,,2012-08-29 00:27:33,
13831248,2,12520727,2012-12-12 01:16:38,0,"In [1]: df = pdDataFrame({'duty': {('11/12/2012'  '10:00'): 0  ('12/12/2012'  '10:00'): 0  ('12/12/2012'  '11:00'): 1}  'prices': {('11/12/2012'  '10:00'): 1  ('12/12/2012'  '10:00'): 2  ('12/12/2012'  '11:00'): 3}})

In [2]: dfindex = pdMultiIndexfrom_tuples(df1index)

In [3]: df
Out[3]: 
                  duty  prices
11/12/2012 10:00     0       1
12/12/2012 10:00     0       2
           11:00     1       3

In [4]: g = dfgroupby(level=0)


You can do some standard groupby operations  or write your own using transformation or aggregation:

In [5]: gsum()
Out[5]: 
            duty  prices
11/12/2012     0       1
12/12/2012     1       5

In [6]: gtransform(lambda x: x - xmean())
Out[6]: 
                  duty  prices
11/12/2012 10:00   00     00
12/12/2012 10:00  -05    -05
           11:00   05     05

In [7]: gagg({'prices': npmean  'duty': len})
Out[7]: 
            duty  prices
11/12/2012     1     10
12/12/2012     2     25


There's more examples in the docs",1240268.0,,,,,2012-12-12 01:16:38,
12996115,2,12996021,2012-10-21 08:15:00,3,Check the pandas package out http://pandaspydataorg/ Maybe this is what you are looking for,1417317.0,,,,,2012-10-21 08:15:00,
12996290,2,12996021,2012-10-21 08:45:06,4,"It's an interesting question

I found there are 2 packages in Python:

Pandas and you can use for forecasting:
Regression
Some EMA
See more: http://pandaspydataorg/

Statmodels you can use for forecasting:
AR
ARMA 
VAR
See more: http://statsmodelssourceforgenet/


However I don't know if all of forecasting package from R you can find them in Python",1719510.0,,,,,2012-10-21 08:45:06,
13197558,2,13180499,2012-11-02 14:49:20,1,"I would suggest you reshape the data and do a single shift versus the groupby approach:

result = dfunstack(0)shift(1)stack()


This switches the order of the levels so you'd want to swap and reorder:

result = resultswaplevel(0  1)sortlevel(0)


You can verify it's been lagged by one period (you want shift(1) instead of shift(-1)):

In [17]: resultix[1]
Out[17]: 
              a         b         c         d         e
month                                                  
1      0752511  0600825  0328796  0852869  0306379
2      0251120  0871167  0977606  0509303  0809407
3      0198327  0587066  0778885  0565666  0172045
4      0298184  0853896  0164485  0169562  0923817
5      0703668  0852304  0030534  0415467  0663602
6      0851866  0629567  0918303  0205008  0970033
7      0758121  0066677  0433014  0005454  0338596
8      0561382  0968078  0586736  0817569  0842106
9      0246986  0829720  0522371  0854840  0887886
10     0709550  0591733  0919168  0568988  0849380
11     0997787  0084709  0664845  0808106  0872628
12     0008661  0449826  0841896  0307360  0092581
13     0727409  0791167  0518371  0691875  0095718
14     0928342  0247725  0754204  0468484  0663773
15     0934902  0692837  0367644  0061359  0381885
16     0828492  0026166  0050765  0524551  0296122
17     0589907  0775721  0061765  0033213  0793401
18     0532189  0678184  0747391  0199283  0349949

In [18]: dfix[1]
Out[18]: 
              a         b         c         d         e
month                                                  
0      0752511  0600825  0328796  0852869  0306379
1      0251120  0871167  0977606  0509303  0809407
2      0198327  0587066  0778885  0565666  0172045
3      0298184  0853896  0164485  0169562  0923817
4      0703668  0852304  0030534  0415467  0663602
5      0851866  0629567  0918303  0205008  0970033
6      0758121  0066677  0433014  0005454  0338596
7      0561382  0968078  0586736  0817569  0842106
8      0246986  0829720  0522371  0854840  0887886
9      0709550  0591733  0919168  0568988  0849380
10     0997787  0084709  0664845  0808106  0872628
11     0008661  0449826  0841896  0307360  0092581
12     0727409  0791167  0518371  0691875  0095718
13     0928342  0247725  0754204  0468484  0663773
14     0934902  0692837  0367644  0061359  0381885
15     0828492  0026166  0050765  0524551  0296122
16     0589907  0775721  0061765  0033213  0793401
17     0532189  0678184  0747391  0199283  0349949


Perf isn't too bad with this method (it might be a touch slower in 090):

In [19]: %time result = dfunstack(0)shift(1)stack()
CPU times: user 146 s  sys: 024 s  total: 170 s
Wall time: 171 s
",776560.0,,776560.0,,2012-11-02 15:48:02,2012-11-02 15:48:02,4.0
11826408,2,11824341,2012-08-06 10:21:14,0,Turns out I was just parsing the wrong field  based on a command switch that defaulted to None Not thinking clearly :/,1325447.0,,,,,2012-08-06 10:21:14,
11852089,2,11824341,2012-08-07 18:40:35,0,Please post potential bug reports on GitHub (or the mailing list) Thanks-- although this did get more answers than GitHub would have,776560.0,,,,,2012-08-07 18:40:35,
12228299,2,12224778,2012-09-01 13:47:53,3,"To use the ""str*"" methods on a column  you could reset the index  filter rows with a column ""str*"" method call  and re-create the index

In [72]: x = dfreset_index(); x[xRPT_Datestrendswith(""0630"")]set_index(['STK_ID'  'RPT_Date'])
Out[72]: 
                      sales        cogs    net_pft
STK_ID RPT_Date                                   
000876 20060630   857483000   729541000   67157200
       20070630  1146245000  1050808000  113468500
       20080630  1932470000  1777010000  133756300
002254 20070630   501221000   289167000  118012200


However  this approach is not particularly fast

In [73]: timeit x = dfreset_index(); x[xRPT_Datestrendswith(""0630"")]set_index(['STK_ID'  'RPT_Date'])
1000 loops  best of 3: 178 ms per loop


Another approach builds on the fact that a MultiIndex object behaves much like
a list of tuples

In [75]: dfindex
Out[75]: 
MultiIndex
[('000876'  '20060331') ('000876'  '20060630') ('000876'  '20060930')
 ('000876'  '20061231') ('000876'  '20070331') ('000876'  '20070630')
 ('000876'  '20070930') ('000876'  '20071231') ('000876'  '20080331')
 ('000876'  '20080630') ('000876'  '20080930') ('002254'  '20061231')
 ('002254'  '20070331') ('002254'  '20070630') ('002254'  '20070930')]


Building on that  you can create a boolean array from a MultiIndex with dfindexmap() and use the result to filter the frame

In [76]: df[dfindexmap(lambda x: x[1]endswith(""0630""))]
Out[76]: 
                      sales        cogs    net_pft
STK_ID RPT_Date                                   
000876 20060630   857483000   729541000   67157200
       20070630  1146245000  1050808000  113468500
       20080630  1932470000  1777010000  133756300
002254 20070630   501221000   289167000  118012200


This is also quite a bit faster

In [77]: timeit df[dfindexmap(lambda x: x[1]endswith(""0630""))]
1000 loops  best of 3: 240 us per loop
",243434.0,,243434.0,,2012-09-03 19:10:06,2012-09-03 19:10:06,3.0
12598379,2,12597926,2012-09-26 09:16:28,3,"Assuming there is a typo in the Text column  text <-> text1?
I`ll go from your first DataFrame

In [11]: df
Out[11]: 
   Form nr Element    Type   Test Options
0     1      Name1  select  text1     op1
1     1      Name1  select   text     op2
2     1      Name1  select   text     op3
3     1      Name2   input  text2     NaN
4     2      Name1   input  text2     NaN

In [12]: dfset_index(['Form'  'nr Element'  'Type'  'Test'])
Out[12]: 
                             Options
Form nr Element Type   Test         
1    Name1      select text1     op1
                       text      op2
                       text      op3
     Name2      input  text2     NaN
2    Name1      input  text2     NaN
",1548051.0,,,,,2012-09-26 09:16:28,1.0
13020027,2,13019719,2012-10-22 21:16:33,2,"Use BDay() to get the business days in range 

from pandastseriesoffsets import *

In [185]: s
Out[185]: 
2011-01-01   -0011629
2011-01-02   -0089666
2011-01-03   -1314430
2011-01-04   -1867307
2011-01-05    0779609
2011-01-06    0588950
2011-01-07   -2505803
2011-01-08    0800262
2011-01-09    0376406
2011-01-10   -0469988
Freq: D

In [186]: sasfreq(BDay())
Out[186]: 
2011-01-03   -1314430
2011-01-04   -1867307
2011-01-05    0779609
2011-01-06    0588950
2011-01-07   -2505803
2011-01-10   -0469988
Freq: B


With slicing:

In [187]: x=datetime(2011  1  5)

In [188]: y=datetime(2011  1  9)

In [189]: six[x:y]
Out[189]: 
2011-01-05    0779609
2011-01-06    0588950
2011-01-07   -2505803
2011-01-08    0800262
2011-01-09    0376406
Freq: D

In [190]: six[x:y]asfreq(BDay())
Out[190]: 
2011-01-05    0779609
2011-01-06    0588950
2011-01-07   -2505803
Freq: B


and count()

In [191]: six[x:y]asfreq(BDay())count()
Out[191]: 3
",1199589.0,,1199589.0,,2012-10-22 21:25:24,2012-10-22 21:25:24,1.0
13384494,2,13293810,2012-11-14 17:58:57,5,"Just want to reiterate this will work in pandas >= 091:

In [2]: read_csv('samplecsv'  dtype={'ID': object})
Out[2]: 
                           ID
0  00013007854817840016671868
1  00013007854817840016749251
2  00013007854817840016754630
3  00013007854817840016781876
4  00013007854817840017028824
5  00013007854817840017963235
6  00013007854817840018860166


I'm creating an issue about detecting integer overflows also

EDIT: See resolution here: https://githubcom/pydata/pandas/issues/2247",776560.0,,776560.0,,2012-12-10 15:59:06,2012-12-10 15:59:06,1.0
13457533,2,13457335,2012-11-19 16:01:45,3,"When you write df[dfapply(lambda x: (x[1]==3) & (x[2]==2) & (x[4]==5)  axis=1)]  you're calling your lambda for each of the 100000 rows in the dataframe  This has substantial overhead as a Python method call must be executed for each row

When you write df[(df[1]==3) & (df[2]==2) & (df[4]==5)]  there's no overhead; instead  the operation is applied to each column in a single operation  and the loop is executed in native code with the potential for vectorization (eg SSE)

This isn't exclusive to Pandas; in general  any numpy operation will be much faster if you treat arrays and matrices in aggregate instead of calling Python functions or inner loops on individual items",567292.0,,,,,2012-11-19 16:01:45,1.0
13763836,2,13762121,2012-12-07 13:14:45,1,"import pandas as PD

a = PDDataFrame({'25%': {'10m': 2429999828338623} 
                  '50%': {'10m': 37100000381469727} 
                  '75%': {'10m': 55} 
                  'count': {'10m': 6046560} 
                  'max': {'10m': 25920000076293945} 
                  'mean': {'10m': 41893915969076261} 
                  'min': {'10m': 00} 
                  'std': {'10m': 24321994530033586}})

index = PDMultiIndexfrom_arrays([['10m'] ['all']]  names = ['dist'  'angle'])
aindex = index

b = PDDataFrame({'25%': {'00_900': 20799999237060547 
                          '1800_2700': 29900000095367432 
                          '2700_3600': 2119999885559082 
                          '900_1800': 23681280016899109} 
                  '50%': {'00_900': 30579087734222412 
                          '1800_2700': 46399998664855957 
                          '2700_3600': 34000000953674316 
                          '900_1800': 34006340503692627} 
                  '75%': {'00_900': 4369999885559082 
                          '1800_2700': 68199996948242188 
                          '2700_3600': 5130000114440918 
                          '900_1800': 46960808038711548} 
                  'count': {'00_900': 1198420 
                            '1800_2700': 2340740 
                            '2700_3600': 1263760 
                            '900_1800': 1243640} 
                  'max': {'00_900': 14369999885559082 
                          '1800_2700': 25920000076293945 
                          '2700_3600': 19549999237060547 
                          '900_1800': 14930000305175781} 
                  'mean': {'00_900': 33417930869221379 
                           '1800_2700': 51125810579269269 
                           '2700_3600': 37938859684522601 
                           '900_1800': 3670476718299061} 
                  'min': {'00_900': 00 
                          '1800_2700': 00 
                          '2700_3600': 00 
                          '900_1800': 00} 
                  'std': {'00_900': 17153268584149644 
                          '1800_2700': 28194581011555386 
                          '2700_3600': 21909571297061241 
                          '900_1800': 18334834361369423}})

index = PDMultiIndexfrom_arrays(
    [['']*4 
     ['00_900' '1800_2700' '2700_3600' '900_1800']] 
    names = ['dist'  'angle'])
bindex = index

PDset_printoptions(precision = 3)
new_df = PDconcat([a b])
print(new_df[['count' 'mean' 'std' 'min' 'max' '25%' '50%' '75%']])


yields

                   count  mean   std  min    max   25%   50%   75%
dist angle                                                        
10m  all          604656  419  243    0  2592  243  371  550
     00_900     119842  334  172    0  1437  208  306  437
     1800_2700  234074  511  282    0  2592  299  464  682
     2700_3600  126376  379  219    0  1955  212  340  513
     900_1800   124364  367  183    0  1493  237  340  470


Note that above the multi-index for b used an empty string for the first index: eg ('' '00_900') You could change that to ('10m'  '00_900') by defining

index = PDMultiIndexfrom_arrays(
    [['10m']*4 
     ['00_900' '1800_2700' '2700_3600' '900_1800']] 
    names = ['dist'  'angle'])
bindex = index


It prints the same way by default  or if you call 

PDset_printoptions(multi_sparse = True)
",190597.0,,190597.0,,2012-12-07 13:40:06,2012-12-07 13:40:06,
13029258,2,9850954,2012-10-23 11:16:32,3,"I like crewbum's answer  probably this is faster (sorry  didn't tested this yet  but i avoid sorting everything):

dfgroupby('obj_id')agg(lambda df: dfvalues[df['data_date']valuesargmax()])


it uses numpys ""argmax"" function to find the rowindex in which the maximum appears",1768121.0,,,,,2012-10-23 11:16:32,1.0
10546350,2,10545957,2012-05-11 06:17:58,1,I might try to concatenate the files before feeding them to pandas If you're in Linux or Mac you could use cat  otherwise a very simple Python function could do the job for you,1242647.0,,,,,2012-05-11 06:17:58,1.0
10555558,2,10545957,2012-05-11 16:55:31,0,"Are these files in a csv format You could use the read_csv
http://pandassourceforgenet/iohtml

Once you have read the files and save it in two dataframes  you could merge the two dataframes or add additional columns to one of the two dataframes( assuming common index) Pandas should be able to fill in missing rows",1257953.0,,,,,2012-05-11 16:55:31,
10563786,2,10545957,2012-05-12 12:35:10,1,"Potentially horribly inefficient but

Why not use read_csv  to build two (or more) dataframes  then use join to put them together?

That said  it would be easier to answer your question if you provide some data or some of the code you've used thus far

Cheers and good luck ",1292631.0,,,,,2012-05-12 12:35:10,
14490980,2,10545957,2013-01-23 22:58:43,0,"The pandas concat command is your friend here Lets say you have all you files in a directory  targetdir You can:

make a list of the files  
load them as pandas dataframes   
and concatenate them together
`

import os
import pandas as pd

#list the files
filelist = oslistdir(targetdir) 
#read them into pandas
df_list = [read_table(file) for file in filelist]
#concatenate them together
big_df = pdconcat(df_list)
",983191.0,,,,,2013-01-23 22:58:43,
11306265,2,11306167,2012-07-03 07:23:21,3,"From the documentation:

Slicing with labels is semantically slightly different because the slice start and stop are inclusive in the label-based case",449449.0,,,,,2012-07-03 07:23:21,
11722472,2,11720334,2012-07-30 13:20:59,2,"You can use the xs function to select on levels

Starting with:

              A         B
one 1 -2712137 -0131805
    2 -0390227 -1333230
    3  0047128  0438284
two 1  0055254 -1434262
    2  2392265 -1474072
    3 -1058256 -0572943


You can then create a new dataframe using:

DataFrame({'one':dfxs('one' level=0)[1:3]apply(npmean)  'two':dfxs('two' level=0)[1:3]apply(npmean)})transpose()


which gives the result:

            A         B
one -0171549 -0447473
two  0667005 -1023508


To do the same without specifying the items in the level  you can use groupby:

grouped = dfgroupby(level=0)
d = {}

for g in grouped:
    d[g[0]] = g[1][1:3]apply(npmean)

DataFrame(d)transpose()


I'm not sure about panels - it's not as well documented  but something similar should be possible",1452002.0,,1452002.0,,2012-07-30 13:50:36,2012-07-30 13:50:36,2.0
13276773,2,11720334,2012-11-07 19:27:40,0,"Do the following:

# Specify the indices you want to work with
idxs = [(""one""  elem) for elem in [2 3]] + [(""two""  elem) for elem in [2 3]]

# Compute grouped mean over only those indices
dfix[idxs]mean(level=0)
",567620.0,,567620.0,,2012-11-07 19:34:40,2012-11-07 19:34:40,
12059534,2,12041519,2012-08-21 16:49:59,0,"You should upgrade to 081 and take advantage of all the new timeseries functionality
Please checkout http://pandaspydataorg for documentation

In the newest versions  checkout functions like between_time to filter within certain time ranges",1306530.0,,,,,2012-08-21 16:49:59,2.0
13244747,2,13244095,2012-11-06 05:08:52,4,"Try using pandasread_fwf and specify a list of column widths (including whitespace):

In [35]: url = 'http://archiveicsuciedu/ml/machine-learning-databases/auto-mpg/auto-mpgdata'

In [36]: widths = [7  4  10  10  11  7  4  4  30]

In [37]: df = pdread_fwf(url  widths=widths  header=None  na_values=['?'])

In [38]: dfirow(0)
Out[38]: 
X0                              18
X1                               8
X2                             307
X3                             130
X4                            3504
X5                              12
X6                              70
X7                               1
X8    ""chevrolet chevelle malibu""

Name: 0
",1306530.0,,1240268.0,,2012-11-06 10:55:00,2012-11-06 10:55:00,3.0
13741439,2,13740672,2012-12-06 10:19:32,3,"your call to the function ""weekday"" does not work as it operates on the index of datamy_dt  which is an int64 array (this is where the error message comes from)

you could create a new column in data containing the weekdays using something like:

data['weekday'] = data['my_dt']apply(lambda x: xweekday())


then you can filter for weekdays with:

weekdays_only = data[data['weekday'] < 5 ]


I hope this helps",1768121.0,,,,,2012-12-06 10:19:32,2.0
13981054,2,13980516,2012-12-20 21:47:35,0,This seems to work with latest matplotlib library installed I forgot to restart the ipython notebook process or reload the matplotlib after I upgraded it,814907.0,,,,,2012-12-20 21:47:35,2.0
14130395,2,14129979,2013-01-02 22:39:16,3,"This is kind of fun  I make no guarantees that this is the real pandas-fu; I'm still at the ""numpy + better indexing"" stage of learning pandas myself  That said  something like this should get the job done

First  we make a toy correlation matrix to play with:

>>> import pandas as pd
>>> import numpy as np
>>> frame = pdDataFrame(nprandomrandn(1000  5)  columns=['a'  'b'  'c'  'd'  'e'])
>>> corr = framecorr()
>>> corr
          a         b         c         d         e
a  1000000  0022246  0018614  0022592  0008520
b  0022246  1000000  0033029  0049714 -0008243
c  0018614  0033029  1000000 -0016244  0049010
d  0022592  0049714 -0016244  1000000 -0015428
e  0008520 -0008243  0049010 -0015428  1000000


Then we make a copy  and use tril_indices_from to get at the lower indices to mask them:

>>> c2 = corrcopy()
>>> c2values[nptril_indices_from(c2)] = npnan
>>> c2
    a        b         c         d         e
a NaN  006952 -0021632 -0028412 -0029729
b NaN      NaN -0022343 -0063658  0055247
c NaN      NaN       NaN -0013272  0029102
d NaN      NaN       NaN       NaN -0046877
e NaN      NaN       NaN       NaN       NaN


and now we can do stats on the flattened array:

>>> c2unstack()mean()
-00072054178481488901
>>> c2unstack()std()
0043839624201635466
",487339.0,,,,,2013-01-02 22:39:16,1.0
14131840,2,14129979,2013-01-03 01:23:02,1,"Another potential one line answer:

In [1]: corr
Out[1]:
          a         b         c         d         e
a  1000000  0022246  0018614  0022592  0008520
b  0022246  1000000  0033029  0049714 -0008243
c  0018614  0033029  1000000 -0016244  0049010
d  0022592  0049714 -0016244  1000000 -0015428
e  0008520 -0008243  0049010 -0015428  1000000

In [2]: corrvalues[nptriu_indices_from(corrvalues 1)]mean()
Out[2]: 0016381


Edit: added performance metrics

Performance of my solution:

In [3]: %timeit corrvalues[nptriu_indices_from(corrvalues 1)]mean()
10000 loops  best of 3: 481 us per loop


Performance of Theodros Zelleke's one-line solution:

In [4]: %timeit corrunstack()ix[zip(*nptriu_indices_from(corr  1))]mean()
1000 loops  best of 3: 823 us per loop


Performance of DSM's solution:

In [5]: def method1(df):
   :     df2 = dfcopy()
   :     df2values[nptril_indices_from(df2)] = npnan
   :     return df2unstack()mean()
   :

In [5]: %timeit method1(corr)
1000 loops  best of 3: 242 us per loop
",919872.0,,919872.0,,2013-01-03 01:52:10,2013-01-03 01:52:10,
13426879,2,13416344,2012-11-17 01:52:15,1,"You can use groupby and then apply to achieve what you want:

diffs = datagroupby(lambda idx: idx[0])apply(lambda row: row - rowshift(1))


For a full example  suppose you create a test data set for 14 Nov to 16 Nov:

import pandas as pd
from numpyrandom import randn
from datetime import time

# Create date range with 10 minute intervals  and filter out irrelevant times
times = pdbdate_range(start=pddatetime(2012 11 14 0 0 0) end=pddatetime(2012 11 17 0 0 0)  freq='10T')
filtered_times = [x for x in times if xtime() >= time(9 30) and xtime() <= time(16 20)]
prices = randn(len(filtered_times))

# Create MultiIndex and data frame matching the format of your CSV
arrays = [[xdate() for x in filtered_times]
          [xtime() for x in filtered_times]]
tuples = zip(*arrays)

m_index = pdMultiIndexfrom_tuples(tuples  names=['date'  'time'])
data = pdDataFrame({'prices': prices}  index=m_index)


You should get a DataFrame a bit like this:

                       prices
date       time              
2012-11-14 09:30:00  0696054
           09:40:00 -1263852
           09:50:00  0196662
           10:00:00 -0942375
           10:10:00  1915207


As mentioned above  you can then get the differences by grouping by the first index and then subtracting the previous row for each row:

diffs = datagroupby(lambda idx: idx[0])apply(lambda row: row - rowshift(1))


Which gives you something like:

                       prices
date       time              
2012-11-14 09:30:00       NaN
           09:40:00 -1959906
           09:50:00  1460514
           10:00:00 -1139036
           10:10:00  2857582


Since you are grouping by the date  the function is not applied for 16:20 - 09:30

You might want to consider using a TimeSeries instead of a DataFrame  because it will give you far greater flexibility with this kind of data Supposing you have already loaded your DataFrame from the CSV file  you can easily convert it into a TimeSeries and perform a similar function to get the differences:

dt_index = pdDatetimeIndex([datetimecombine(i[0] i[1]) for i in dataindex])
# or dt_index = pdDatetimeIndex([datetimecombine(idate itime) for i in dataindex]) 
# if you don't have an multi-level index on data yet
ts = pdSeries(datapricesvalues  dt_index)
diffs = tsgroupby(lambda idx: idxdate())apply(lambda row: row - rowshift(1))


However  you would now have access to the built-in time series functions such as resampling See here for more about time series in pandas",1452002.0,,1452002.0,,2012-11-18 18:26:06,2012-11-18 18:26:06,
13810867,2,13416344,2012-12-10 23:07:33,0,"@MattiJohn's construction gives a filtered list of length 86 772--when run over 1/3/2007-8/30/2012 for 42 times (10 minute intervals) Observe the data cleaning issues

Here the data of prices coming from the csv is length: 62 034 
Hence  simply importing from the csv  as follows  is problematic:

filtered_times = [x for x in times if xtime() >= time(9 30) and xtime() <= time(16 20)]
DF=pdread_csv('MR10mincsv')
prices = DFprice
 # IE rather than the generic: prices = randn(len(filtered_times))  above


The fact that the real data falls short of the length it ""should be"" means there are data cleaning issues Often we do not have the full times as bdate_time will generate (half days in the market  etc  holidays) 

Your solution is elegant But I am not sure how to overcome the mismatch between the actual data and the a priori  prescribed dataframe

Your second TimesSeries suggestion seems to still require construction of a datetime index similar to the first one For example  if I were use the following two lines to get the actual data of interest:

DF=pdread_csv('MR10mincsv')
data=pdDFset_index(['date' 'time'])


dt_index = pdDatetimeIndex([datetimecombine(i[0] i[1]) for i in dataindex])


It will generate a:

TypeError: combine() argument 1 must be datetimedate  not str


How does one make a bdate_time array completely informed by the actual data available?

Thank you to (@MattiJohn) and to anyone with interest in continuing this discussion",1374969.0,,,,,2012-12-10 23:07:33,
11434755,2,11423790,2012-07-11 14:21:44,2,"Try nlsn['Adj Close'][::-1]asfreq('M'  method='ffill')

And if you can get your function to return ascending order DatetimeIndex that would allow you to skip the extra sorting here",1306530.0,,398968.0,,2012-11-26 10:52:12,2012-11-26 10:52:12,1.0
11801803,2,11794935,2012-08-03 19:05:11,2,"Try 'iterrows' DataFrame class method for efficiently iterating through the rows of a DataFrameSee chapter 672 of the pandas 081 guide

from pandas import *
import numpy as np

df = DataFrame({'A' : [5 6 3]  'B' : [0 0 0]  'C':[0 0 0]  'D' : [3 4 5]})

for idx  row in dfiterrows():
    if row['B'] == 0:
        row['B'] = min(row['A']  row['D'])
    if row['C'] == 0:
        row['C'] = min(row['A']  row['D'])
",1521724.0,,,,,2012-08-03 19:05:11,
11819672,2,11794935,2012-08-05 20:07:32,2,"A combination of boolean indexing and apply can do the trick
Below an example on replacing zero element for column C

In [22]: df
Out[22]:
   A  B  C  D
0  8  3  5  8
1  9  4  0  4
2  5  4  3  8
3  4  8  5  1

In [23]: bi = dfC==0

In [24]: dfix[bi  'C'] = df[bi][['A'  'D']]apply(max  axis=1)

In [25]: df
Out[25]:
   A  B  C  D
0  8  3  5  8
1  9  4  9  4
2  5  4  3  8
3  4  8  5  1
",1548051.0,,,,,2012-08-05 20:07:32,2.0
12183460,2,12183432,2012-08-29 17:32:26,1,"How about

df['C'] = df['A'] * 10 / df['B']
",1119345.0,,1119345.0,,2012-08-29 19:43:58,2012-08-29 19:43:58,5.0
12523035,2,12522963,2012-09-21 01:12:35,2,"That converter is a bit hacky; might I recommend something more robust like this?

def convert_dmds(s):
    deg  min = s[:-1]split(';')
    sign = 1 if s[-1] in 'NE' else -1
    return sign * (float(deg) + float(min) / 600)

def convert_gps(s):
    lat  lon = ssplit(' ')
    return (convert_dmds(lat)  convert_dmds(lon))


Also  the error indicates that you are trying to convert something that is clearly not a GPS string -- a header row  perhaps?",1204143.0,,,,,2012-09-21 01:12:35,
12525747,2,12522963,2012-09-21 07:06:06,0,"Your converter is not ok

In [67]: convertFunc = lambda x : float((x[0:5]+x[6:12])replace(';' ''))

In [68]: convertFunc('4;077693770E')
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)

ValueError: invalid literal for float(): 407693770


On top of a dodgy converter  i think you apply the converter to the wrong column (look at the exception you get)",1548051.0,,,,,2012-09-21 07:06:06,
13004098,2,13003769,2012-10-22 01:54:35,1,"I would try pandasmerge using the suffixes= option

import pandas as pd
import datetime as dt

df_1 = pdDataFrame({'x' : [dtdatetime(2012 10 21) + dttimedelta(n) for n in range(10)]  'y' : range(10)})
df_2 = pdDataFrame({'x' : [dtdatetime(2012 10 21) + dttimedelta(n) for n in range(10)]  'y' : range(10)})
df = pdmerge(df_1  df_2  on='x'  suffixes=['_1'  '_2'])


I am interested to see if the experts have a more algorithmic approach to merge a list of data frames",334755.0,,334755.0,,2012-10-22 02:21:21,2012-10-22 02:21:21,4.0
13008876,2,13003769,2012-10-22 09:54:30,3,"In [65]: pdconcat(data  axis=1)
Out[65]:
                     AvgStatisticData  AvgStatisticData  AvgStatisticData  AvgStatisticData
2012-10-14 14:00:00         39335996         47854712         54171233         65813114
2012-10-14 15:00:00         40210110         55041512         48718387         71397868
2012-10-14 16:00:00         48282816         55488026         59978616         76213973
2012-10-14 17:00:00         40593039         51688483         50984514         72729002
2012-10-14 18:00:00         40952014         57916672         54924745         73196415
",1548051.0,,,,,2012-10-22 09:54:30,
13184642,2,13184553,2012-11-01 20:03:42,1,"The problem is here:

df2['a'] =df2['a']map(f)
                   ^^^^^^


Where f = lambda x : ' 'join(x)

There's no point joining it again  you want to split it to a list:

df2['a'] = df2['a']map(lambda L: Lsplit(' '))
",1252759.0,,,,,2012-11-01 20:03:42,1.0
13659944,2,13659881,2012-12-01 13:34:16,2,"If you use groupby  you will get what you want 

dgroupby(['ip'  'useragent'])count()['ip']


produces:

ip          useragent               
19216801 a           2
            b           1
19216802 b           1
",1452002.0,,,,,2012-12-01 13:34:16,1.0
13961138,2,13958129,2012-12-19 21:02:24,6,"You nearly had it! First create the groupby object:

means = dfgroupby('State')mean()

In [5]: means
Out[5]: 
       Revenue
State         
FL         75
GA         75
NY         25


Then apply this to each state in the DataFrame:

df['mean'] = df['State']apply(lambda x: meansix[x]['Revenue'])

In [7]: df
Out[7]: 
            Revenue State  mean
2012-01-01        1    NY   25
2012-02-01        2    NY   25
2012-03-01        3    NY   25
2012-04-01        4    NY   25
2012-05-01        5    FL   75
2012-06-01        6    FL   75
2012-07-01        7    GA   75
2012-08-01        8    GA   75
2012-09-01        9    FL   75
2012-10-01       10    FL   75
",1240268.0,,,,,2012-12-19 21:02:24,2.0
14035284,2,13958129,2012-12-26 01:57:31,3,"Using join or merge works too:

In [68]: revs = dfgroupby('State')Revenuemean()

In [69]: revsname = 'Mean Revenue'

In [70]: dfjoin(revs  on='State')
Out[70]: 
            Revenue State  Mean Revenue
2012-01-01        1    NY           25
2012-02-01        2    NY           25
2012-03-01        3    NY           25
2012-04-01        4    NY           25
2012-05-01        5    FL           75
2012-06-01        6    FL           75
2012-07-01        7    GA           75
2012-08-01        8    GA           75
2012-09-01        9    FL           75
2012-10-01       10    FL           75
",776560.0,,,,,2012-12-26 01:57:31,1.0
14307460,2,14300768,2013-01-13 19:49:12,1,"I think this does what you want:

In [1]: df
Out[1]:
   RollBasis  ToRoll
0          1       1
1          1       4
2          1      -5
3          2       2
4          3      -4
5          5      -2
6          8       0
7         10     -13
8         12      -2
9         13      -5

In [2]: def f(x):
   :     ser = dfToRoll[(dfRollBasis >= x) & (dfRollBasis < x+5)]
   :     return sersum()


The above function takes a value  in this case RollBasis  and then indexes the data frame column ToRoll based on that value  The returned series consists of ToRoll values that meet the RollBasis + 5 criterion  Finally  that series is summed and returned

In [3]: df['Rolled'] = dfRollBasisapply(f)

In [4]: df
Out[4]:
   RollBasis  ToRoll  Rolled
0          1       1      -4
1          1       4      -4
2          1      -5      -4
3          2       2      -4
4          3      -4      -6
5          5      -2      -2
6          8       0     -15
7         10     -13     -20
8         12      -2      -7
9         13      -5      -5


Code for the toy example DataFrame in case someone else wants to try:

In [1]: from pandas import *

In [2]: import io

In [3]: text = """"""\
   :    RollBasis  ToRoll
   : 0          1       1
   : 1          1       4
   : 2          1      -5
   : 3          2       2
   : 4          3      -4
   : 5          5      -2
   : 6          8       0
   : 7         10     -13
   : 8         12      -2
   : 9         13      -5
   : """"""

In [4]: df = read_csv(ioBytesIO(text)  header=0  index_col=0  sep='\s+')
",919872.0,,,,,2013-01-13 19:49:12,1.0
14307961,2,14300768,2013-01-13 20:46:14,2,"Based on Zelazny7's answer  I created this more general solution:

def rollBy(what  basis  window  func):
    def applyToWindow(val):
        chunk = what[(val<=basis) & (basis<val+window)]
        return func(chunk)
    return basisapply(applyToWindow)

>>> rollBy(dToRoll  dRollBasis  5  sum)
0    -4
1    -4
2    -4
3    -4
4    -6
5    -2
6   -15
7   -20
8    -7
9    -5
Name: RollBasis


It's still not ideal as it is very slow compared to rolling_apply  but perhaps this is inevitable",1427416.0,,1427416.0,,2013-01-13 21:57:16,2013-01-13 21:57:16,
14283678,2,14199168,2013-01-11 17:40:00,5,"The output of your method:

In [29]: merged
Out[29]: 
         bob_id1  gender john_id1 bob_id2 mary_id1 mary_id2
employee                                                   
bob            a    male      NaN       b      NaN      NaN
john         NaN    male        x     NaN      NaN      NaN
mary         NaN  female      NaN     NaN        c        d


A solution with pandas built-in dfcombine_first:

In [28]: reduce(lambda x y: xcombine_first(y)  [df1  df2  df3])
Out[28]: 
         bob_id1 bob_id2  gender john_id1 mary_id1 mary_id2
employee                                                   
bob            a       b    male      NaN      NaN      NaN
john         NaN     NaN    male        x      NaN      NaN
mary         NaN     NaN  female      NaN        c        d


To add a suffix to the columns of each frame  I'd suggest renaming the columns before calling combine_first

On the other hand  you may want to look into an operation like pdconcat([df1  df2  df3]  keys=['d1'  'd2'  'd3']  axis=1)  which produces a dataframe with MultiIndex columns  In this case  may want to consider making gender part of the index or live with it's duplication",243434.0,,243434.0,,2013-01-11 18:31:40,2013-01-11 18:31:40,2.0
14513503,2,14513339,2013-01-25 00:41:33,1,"The set_index method returns a new DataFrame by default  rather than applying this inplace (in fact  most pandas functions are similar) It has an inplace argument:

sset_index('Date'  inplace=True)
splot()


which works as you intended!

Note: to convert the Index to a DatetimeIndex you can use to_datetime:

sindex = sindexto_datetime()




Which is to say  s remained unchanged by you set_index('Date'):

In [63]: s = pdread_csv('spycsv'  na_values=["" ""])

In [64]: sset_index('Date')
Out[64]: 
<class 'pandascoreframeDataFrame'>
Index: 5033 entries  1993-01-29 00:00:00 to 2013-01-23 00:00:00
Data columns:
Open         5033  non-null values
High         5033  non-null values
Low          5033  non-null values
Close        5033  non-null values
Volume       5033  non-null values
Adj Close    5033  non-null values
dtypes: float64(5)  int64(1)

In [65]: s
Out[65]: 
<class 'pandascoreframeDataFrame'>
Int64Index: 5033 entries  0 to 5032
Data columns:
Date         5033  non-null values
Open         5033  non-null values
High         5033  non-null values
Low          5033  non-null values
Close        5033  non-null values
Volume       5033  non-null values
Adj Close    5033  non-null values
dtypes: float64(5)  int64(1)  object(1)
",1240268.0,,1240268.0,,2013-01-25 22:00:26,2013-01-25 22:00:26,8.0
9421580,2,9421412,2012-02-23 21:25:24,4,"The result of ewma(df  span=21) is already a DataFrame  so when you pass it to the DataFrame constructor along with a list of columns  it ""selects"" out the columns that you passed It's difficult in this particular case to break the link between label and data If you had done instead:

In [23]: smoothed = DataFrame(ewma(df  span = 21)values  index=dfindex  columns = ['smooth1' 'smooth2'])
In [24]: smoothedhead()
Out[24]: 
    smooth1   smooth2
0  0218350  0877693
1  0400214  0813499
2  0308564  0739426
3  0433341  0641891
4  0525260  0620541


that is no problem of course

smoothed = ewma(df  span=21)
smoothedcolumns = ['smooth1'  'smooth2']


is perfectly fine too",776560.0,,,,,2012-02-23 21:25:24,1.0
10451201,2,10449663,2012-05-04 14:53:06,1,"Do you have multiple versions of Python? Something must be borked with your Python path Have a look at syspath:

import sys
for x in syspath: print x


pandas should be installed in C:\Python27\Lib\site-packages\pandas or in an egg directory there You can find out exactly where by firing up the regular python interpreter and doing


import pandas
print pandas


Not sure what could be wrong in IPython ",776560.0,,,,,2012-05-04 14:53:06,7.0
14047867,2,14035817,2012-12-27 00:12:26,2,"This is a bug:

In [1]: df = pdDataFrame(nprandomrandn(10  4))

In [2]: df
Out[2]: 
          0         1         2         3
0 -3100926 -0580586 -1216032  0425951
1 -0264271 -1091915 -0602675  0099971
2 -0846290  1363663 -0382874  0065783
3 -0099879 -0679027 -0708940  0138728
4 -0302597  0753350 -0112674 -1253316
5 -0213237 -0467802  0037350  0369167
6  0754915 -0569134 -0297824 -0600527
7  0644742  0038862  0216869  0294149
8  0101684  0784329  0218221  0965897
9 -1482837 -1325625  1008795 -0150439

In [3]: dfix[-2:]
Out[3]: 
          0         1         2         3
0 -3100926 -0580586 -1216032  0425951
1 -0264271 -1091915 -0602675  0099971
2 -0846290  1363663 -0382874  0065783
3 -0099879 -0679027 -0708940  0138728
4 -0302597  0753350 -0112674 -1253316
5 -0213237 -0467802  0037350  0369167
6  0754915 -0569134 -0297824 -0600527
7  0644742  0038862  0216869  0294149
8  0101684  0784329  0218221  0965897
9 -1482837 -1325625  1008795 -0150439


https://githubcom/pydata/pandas/issues/2600

Note that df[-2:] will work:

In [4]: df[-2:]
Out[4]: 
          0         1         2         3
8  0101684  0784329  0218221  0965897
9 -1482837 -1325625  1008795 -0150439
",776560.0,,,,,2012-12-27 00:12:26,1.0
14247708,2,14247586,2013-01-09 22:33:07,4,"You could use the function isnull instead of the method:

In [56]: df = pdDataFrame([range(3)  [0  npNaN  0]  [0  0  npNaN]  range(3)  range(3)])

In [57]: df
Out[57]: 
   0   1   2
0  0   1   2
1  0 NaN   0
2  0   0 NaN
3  0   1   2
4  0   1   2

In [58]: pdisnull(df)
Out[58]: 
       0      1      2
0  False  False  False
1  False   True  False
2  False  False   True
3  False  False  False
4  False  False  False

In [59]: pdisnull(df)any(axis=1)
Out[59]: 
0    False
1     True
2     True
3    False
4    False


leading to the rather compact:

In [60]: df[pdisnull(df)any(axis=1)]
Out[60]: 
   0   1   2
1  0 NaN   0
2  0   0 NaN
",487339.0,,,,,2013-01-09 22:33:07,
14590035,2,14570400,2013-01-29 18:48:17,1,"create a new DataFrame:

tso1=tsogroupby(['Day' 'Year'])mean()unstack()
#################creat the datetime index#################################
date=[]
for i in range(len(tso1index)-1):
    dateappend(parserparse(num2date(tso1index[i])replace(year=2000)isoformat("" "")))
dateappend(parserparse(num2date(tso1index[len(tso1index)-2])replace(year=2000)isoformat("" "")))
######################################################################################################
ax = pandasDataFrame(tso1values index=date)plot()
",1843099.0,,,,,2013-01-29 18:48:17,
11344012,2,9641916,2012-07-05 12:07:21,0,"Might be a 32-bit vs 64-bit compatibility issue  See:
how to install numpy and scipy on OS X?

Despite the title  similar problems can occur with other operating systems if you mix 32-bit and 64-fit versions",1472080.0,,,,,2012-07-05 12:07:21,
11955915,2,9641916,2012-08-14 15:38:20,0,"@user248237:

I second Keith's suggestion that its probably a 32/64 bit compatibility issue I ran into the same problem just this week while trying to install a different module  Check the versions of each of your modules and make everything matches  In general  I would stick to the 32 bit versions -- not all modules have official 64 bit support  I uninstalled my 64 bit version of python and replaced it with a 32 bit one  reinstalled the modules  and haven't had any problems since  ",1460123.0,,,,,2012-08-14 15:38:20,
12003130,2,9641916,2012-08-17 09:22:33,0,"Just to make sure:

Did you install pandas from the sources ? Make sure it's using the version of NumPy you want
Did you upgrade NumPy after installing pandas?  Make sure to recompile pandas  as there can be some changes in the ABI (but w/ that version of NumPy  I doubt it's the case)
Are you calling pandas and/or Numpy from their source directory ? Bad idea  NumPy tends to choke on that
",1491200.0,,,,,2012-08-17 09:22:33,
13764242,2,13762121,2012-12-07 13:39:55,1,"@unutbu is correct  here is without constructing the index by hand

df = aappend(b)
dfindex = MultiIndexfrom_arrays([aindextolist()*(len(b) + 1)  
                                   [""all""] + bindextolist()    ] )
df

                 25%    50%    75%    count      max    mean   min  std  
10m all          243   371   55    6047e+05  2592  4189  0    2432
    00_900     208   3058  437   1198e+05  1437  3342  0    1715
    1800_2700  299   464   682   2341e+05  2592  5113  0    2819
    2700_3600  212   34    513   1264e+05  1955  3794  0    2191
    900_1800   2368  3401  4696  1244e+05  1493  367   0    1833
",1798300.0,,,,,2012-12-07 13:39:55,1.0
13864569,2,13762121,2012-12-13 16:50:20,1,"other fun options:

In [19]: result = pdconcat([a  b])rename({'10m': 'all'})

In [20]: resultset_index(nparray(['10m'] * 5)  append=True)swaplevel(0 1)
Out[20]: 
                  count  mean   std  min    max   25%   50%   75%
10m all          604656  419  243    0  2592  243  371  550
    00_900     119842  334  172    0  1437  208  306  437
    1800_2700  234074  511  282    0  2592  299  464  682
    2700_3600  126376  379  219    0  1955  212  340  513
    900_1800   124364  367  183    0  1493  237  340  470
",776560.0,,,,,2012-12-13 16:50:20,
14137316,2,14132216,2013-01-03 10:46:35,2,"Don't use the zip  you can keep the data in pandas native datastructures
Here prices should have read correctly as floats in the DataFrame

You can do something like sub then groupby 'date':

df['dif'] = a1sub(h  fill_value=0)
g = dfgroubpy('date')['dif']sum()




Note you can use read_csv keyword parse_dates as datetime objects:

df = pdread_csv(""filecsv"" 
                 names=""date time price mag signal""split()
                 parse_dates=[['date' 'time']])
",1240268.0,,,,,2013-01-03 10:46:35,5.0
14438691,2,14438509,2013-01-21 12:32:58,3,"
  I know that having the entire array in memory is unwise


You might be overthinking it A 100K x 6 array of float64 takes just ~5MB of RAM On my computer  sorting such an array takes about 27ms:

In [37]: a = nprandomrand(100000  6)

In [38]: %timeit a[a[: 1]argsort()]
10 loops  best of 3: 272 ms per loop
",367273.0,,,,,2013-01-21 12:32:58,
14438767,2,14438509,2013-01-21 12:36:54,2,"Unless you have a very old computer  you should put the entire array in memory Assuming they are floats  it will only take 100000*6*4/2**20 = 229 Mb Twice as much for doubles You can use numpy's sort or argsort for sorting For example  you can get the sorting indices from your second column:

import numpy as np
a = nprandomnormal(0  1  size=(100000 6))
idx = a[:  1]argsort()


And then use these to index the columns you want  or the whole array:

b = a[idx]


You can even use different types of sort and check their speed:

In [33]: %timeit idx = a[:  1]argsort(kind='quicksort')
100 loops  best of 3: 126 ms per loop

In [34]: %timeit idx = a[:  1]argsort(kind='mergesort')
100 loops  best of 3: 144 ms per loop

In [35]: %timeit idx = a[:  1]argsort(kind='heapsort')
10 loops  best of 3: 214 ms per loop


So you see that for an array of this size it doesn't really matter",1580351.0,,,,,2013-01-21 12:36:54,1.0
9198129,2,9189425,2012-02-08 17:10:15,3,"This is pretty much just an oversight / inconsistency I created a GitHub issue here for it:

https://githubcom/wesm/pandas/issues/765

EDIT: Implemented this today so you can pass a StringIO to to_csv  in git master now and will be part of forthcoming 070 release",776560.0,,776560.0,,2012-02-08 23:15:46,2012-02-08 23:15:46,
12062263,2,12041519,2012-08-21 20:08:28,1,"pandas 081
For hourly sampled data:

In [57]: import pandas

In [58]: import numpy

In [59]: index = pandasdate_range(start='2011-01-09'  periods=240  freq='H')

In [60]: s = pandasSeries(nprandomrandn(len(index))  index)

In [61]: s_night = s[(sindexhour >= 21) | (sindexhour <= 6)]

In [62]: def day_or_night(dates):
   :     r = []
   :     for date in dates:
   :         if (datehour >= 21) | (datehour <= 6):
   :             d = datetimedatetime(dateyear  datemonth  dateday)
   :             if (datehour <= 6):
   :                 d = d - pandasoffsetsDay()
   :             rappend(d)
   :         else:
   :             rappend('day')
   :     return r
   :

In [63]: s_nightgroupby(day_or_night(s_nightindex))mean()
Out[63]:
2011-01-08    0652095
2011-01-09    0004129
2011-01-10    0457892
2011-01-11   -0078547
2011-01-12    0008087
2011-01-13    0043568
2011-01-14    0505970
2011-01-15    0150971
2011-01-16    0107265
2011-01-17    0117811
2011-01-18   -0191193
",1548051.0,,,,,2012-08-21 20:08:28,
12076139,2,12041519,2012-08-22 15:08:07,0,"I finally found a solution that works:

hr = drmap(lambda x: xhour)
meantime = lambda x: xreplace(hour=0)

datra = pdDateRange('2011/1/1'  '2011/12/31'  offset=pddatetoolsday)
rise = pdTimeSeries(npcos(((datramap(lambda x: (x-datetime(xyear 1 1))total_seconds() / 86400) + 10) / 183 * nppi)) * -2 + 17  index=datra)
set = pdTimeSeries(npcos(((datramap(lambda x: (x-datetime(xyear 1 1))total_seconds() / 86400) + 10) / 183 * nppi)) * 25 + 5  index=datra)

i=0
def bias_night(liste  group):
while (i<546):
    if (i<364):
        z = group[dr[hr>unter11[i]]]combine_first(group[dr[hr<auf11[i+1]]])groupby(meantime)mean()
        listeappend(z[i])
    else:
        z = group[dr[hr>unter11[i-365]]]combine_first(group[dr[hr<auf11[i-365+1]]])groupby(meantime)mean()
        listeappend(z[i])
    i = i+1
t = group[dr[hr>unter11[364]]]combine_first(group[dr[hr<auf11[0]]])groupby(meantime)mean()
listeinsert(364  t[364])


liste is an empty list and group is one of my TimeSeries In the end  i just have to subtract the resulting lists to get what I want

2011-01-09   -1179578
2011-01-10   -0978171
2011-01-11   -0335977
2011-01-12    0080671
2011-01-13   -0324661
2011-01-14    0012359
2011-01-15   -0549079
",1612198.0,,,,,2012-08-22 15:08:07,
12246801,2,12223689,2012-09-03 11:19:12,0,"In [238]: map = {'Alabama': 'region_1'  'Arizona': 'region_1'  'Arkansas': 'region_2'}

In [239]: weigths = pandasSeries([25  75  33]  index=['Alabama'  'Arizona'  'Arkansas'])

In [240]: df_states = pandasDataFrame({'map': pandasSeries(map)  'weigths': weigths})

In [241]: df_states
Out[241]:
               map  weigths
Alabama   region_1     025
Arizona   region_1     075
Arkansas  region_2     033

In [242]: df_regional = df_statesgroupby('map')sum()

In [243]: df_regional
Out[243]:
          weigths
map
region_1     100
region_2     033
",1548051.0,,,,,2012-09-03 11:19:12,
12254295,2,12223689,2012-09-03 21:39:36,0,"Assuming two dataframes  df_states and df_regional  with the following 
structure:

In [36]: df_states
Out[36]: 
          Weight    Region
Alabama     025  region_1
Arizona     075  region_1
Arkansas    033  region_2

In [37]: df_regional
Out[37]: 
          Value
region_1    100
region_2     80


Does pandasmerge arrange the data in a way that seems useful?

In [39]: df = pandasmerge(df_states  df_regional  left_on='Region'  right_index=True)

In [40]: df
Out[40]: 
          Weight    Region  Value
Alabama     025  region_1    100
Arizona     075  region_1    100
Arkansas    033  region_2     80

In [41]: dfWeight * dfValue
Out[41]: 
Alabama     250
Arizona     750
Arkansas    264
",243434.0,,,,,2012-09-03 21:39:36,1.0
12007981,2,9641916,2012-08-17 14:46:24,0,Try to update to numpy version 161 Helped for me!,579730.0,,,,,2012-08-17 14:46:24,
10829714,2,10824906,2012-05-31 08:11:17,1,"You can use aggregate to define your aggregate function  which will just keep the first element of a column and drop the others

    In [60]: grp = dfgroupby(['A'  'B'])

    In [61]: grpaggregate({'C': lambda c: cix[cfirst_valid_index()]})
    Out[61]:
                 C
    A   B  
    foo ape   last
        bar  happy
",1063605.0,,,,,2012-05-31 08:11:17,
11691757,2,11689668,2012-07-27 16:18:18,1,"This looks like a bug in pandas Looking at the source code  in pandastoolsplotting  lines 554:556:

empty = df[col]count() == 0                                       
# is this right?                                                   
values = df[col]values if not empty else npzeros(len(df))        


If the column contains only NaNs  then empty is True and values is set to npzeros()

Note: I did not add the ""is this right?"" comment: it's in the source code! (pandas v081)

I've raised a bug about it: https://githubcom/pydata/pandas/issues/1696",1063605.0,,1063605.0,,2012-07-27 20:16:25,2012-07-27 20:16:25,
11971139,2,11970820,2012-08-15 14:26:08,3,"[EDIT] The issue which you're seeing is because of this code:

selfresults = selfresultsappend()


this isn't atomic So in some cases  the thread will be interrupted after reading selfresults (or while appending) but before it can assign the new frame to selfresults -> this instance will be lost

The correct solution is to wait use the results objects to get the results and then append all of them in the main thread",34088.0,,34088.0,,2012-08-16 09:02:26,2012-08-16 09:02:26,9.0
12187770,2,12186994,2012-08-29 23:14:01,0,"statsmodels doesn't have a Johansen cointegration test And  I have never seen it in any other python package either

statsmodels has VAR and structural VAR  but no VECM (vector error correction models) yet

update: 

As Wes mentioned  there is now a pull request for Johansen's cointegration test for statsmodels I have translated the matlab version in LeSage's spatial econometrics toolbox and wrote a set of tests to verify that we get the same results 
It should be available in the next release of statsmodels",333700.0,,333700.0,,2012-09-11 01:12:19,2012-09-11 01:12:19,
12335041,2,12186994,2012-09-08 22:39:46,1,See http://githubcom/statsmodels/statsmodels/pull/453,776560.0,,,,,2012-09-08 22:39:46,1.0
12417924,2,12416932,2012-09-14 03:59:01,3,"You can pass multiple keys to groupby as a list:

from pandas import *
from numpyrandom import randn
rng = date_range('1/1/2011'  periods=7200  freq='H')
ts = Series(randn(len(rng))  index=rng)
for key  data in tsgroupby([rngyear  rngmonth]):
    print key  datasum()
",772649.0,,,,,2012-09-14 03:59:01,
12714937,2,12711551,2012-10-03 18:52:50,2,"dfappend(dfin  ignore_index=True) returns a new DataFrame  it does not change df in place
Use df = dfappend(dfin  ignore_index=True) But even with this change i think this will not give what you need Append extends a frame on axis=1 (columns)  but i believe you want to combine the data on axis=0 (rows)

In this scenario (reading multiple files and use all data to create a single DataFrame)  i would use pandasconcat() The code below will give you a frame with columns named by colnames  and the rows are formed by the data in the csv files

def files2df(colnames  ext):
    files = sorted(globglob(ext))
    frames = [read_csv(inf  sep='\t'  skiprows=1  names=colnames) for inf in files]
    return concat(frames  ignore_index=True)


I did not try this code  just wrote it here  maybe you need tweak it to get it running  but the idea is clear (i hope)",1548051.0,,,,,2012-10-03 18:52:50,1.0
13184793,2,13184553,2012-11-01 20:15:04,1,"No need to covert the list into string in the first place  list's will be converted to string automatically Just write the dataframe containing a list  and use astliteral_eval on df2

                                             a  b  c
0   ['john quincy'  'tom jones'  'jerry rice']  9  7
1  ['bob smith'  'sally ride'  'little wayne']  2  3
2                   ['seven'  'eight'  'nine']  4  0
3                  ['ten'  'eleven'  'twelve']  5  9

df1to_csv('tempcsv')
df2 = read_csv('tempcsv')


Use astliteral_eval to get the string back to list:

import ast
fd2['a']=df2['a']apply(lambda x: astliteral_eval(x))
type(df2['a'][1])


Output:

list
",1199589.0,,1199589.0,,2012-11-01 20:33:17,2012-11-01 20:33:17,3.0
13708892,2,13675749,2012-12-04 17:50:04,1,"loffset just changes the labels  without changing how your data is grouped into the new frequency So using your example:

max_date = max(dates)
offset = timedelta(seconds=(max_datesecond % 5)-5
                  microseconds=max_datemicrosecond-1)
inhomogeneous_secondish_seriesresample('5s'  loffset=offset)


would give you:

2012-02-05 17:00:38239999   -0200153
2012-02-05 17:00:43239999   -0009347
2012-02-05 17:00:48239999   -0432871
2012-02-05 17:00:53239999    0748638
Freq: 5S


From what I understand  this is not what you want - the last value should be the mean of the two last values in the dataset  instead of just the last value 

To change how the frequencies are anchored  you can use base However  because this needs to be an integer  so you should use an appropriate microsecond frequency like:

freq_base = (max_datesecond % 5)*1000000 + max_datemicrosecond
inhomogeneous_secondish_seriesresample('5000000U'  base=freq_base)
",1452002.0,,,,,2012-12-04 17:50:04,0.0
13979130,2,13978682,2012-12-20 19:28:22,1,"In [6]: fxd = {fxlevels[i]: i for i in range(len(fxlevels))}

In [7]: fylabels = [fxd[v] for v in fy]

In [8]: fylevels = fxlevels

In [9]: fy
Out[9]: 
Categorical: 
array([a  c  e]  dtype=object)
Levels (5): Index([a  b  c  d  e]  dtype=object)
",243434.0,,243434.0,,2012-12-21 04:58:38,2012-12-21 04:58:38,2.0
14106218,2,14105774,2012-12-31 21:33:10,0,"Apparently  it's normal for every field of NaT to be -1 Taken from pandas/tslibpyx of pandas version 0100:

class NaTType(_NaT):

    def __new__(cls):
        cdef _NaT base

        base = _NaT__new__(cls  1  1  1)
        mangle_nat(base)
        basevalue = NPY_NAT

        return base

    def __repr__(self):
        return 'NaT'

    def weekday(self):
        return -1

    def toordinal(self):
        return -1

fields = ['year'  'quarter'  'month'  'day'  'hour' 
          'minute'  'second'  'microsecond'  'nanosecond' 
          'week'  'dayofyear']
for field in fields:
    prop = property(fget=lambda self: -1)
    setattr(NaTType  field  prop)


The unfortunate reality of sentinel values like NaT is that you have to change your behaviour when you have such a value

You can't simply expect NaTyear to have the value 2012 Instead  you should do something with the NaTs in your container What that something is (removing them from the container  or choosing a default value  or raising an error) really depends on how you want to want to do with the container",824425.0,,,,,2012-12-31 21:33:10,
14106334,2,14105774,2012-12-31 21:54:04,0,"What makes you think this is a bug  rather than defined behavior?

First:

In [16]: pandasNaTyear
Out[16]: -1


So  there's nothing odd about it being in a DatetimeIndex; that's how NaT always works

And it's entirely internally consistent  as well as consistent with lots of other stuff in numpy and elsewhere that uses -1 as a special value for (hopefully unsigned) integral types

Yes  -1 doesn't really work as a NaN  since you can do arithmetic with it and get non-NaN (and incorrect) results  and it does odd things in some other cases (try pandasNaTisoformat())  but what other option is there? As long as year is defined to be some kind of numpy integral type  it has to return an integral value So  what are the options?

Return either an int or None Then calling year returns an array(dtype=object)
Return a float  so NaTyear can be NaN
Raise an exception for NaTyear itself  or when trying to do it within an array
Return some special integer value If not -1  what value would you use?
They all suck in different ways  but the last seems to suck least  and be the most consistent with everything else in the universe The ideal solution might be to have integer-with-NaN types in numpy  but that's a much larger issue that designing a wrapper around numpy datetimes

By the way  it's worth noting that numpy 16 doesn't have a NaT value for datetime64  so a pandasNaT actually maps to datetime64(-1)  for exactly the same reasons Now that numpy 17 has npdatetime64('NaT')  that could change But that still doesn't change the fact that integers don't have a NaN",908494.0,,908494.0,,2012-12-31 22:04:06,2012-12-31 22:04:06,5.0
14349645,2,14349055,2013-01-16 00:44:17,5,"I would check out Bokeh which aims to ""provide a compelling Python equivalent of ggplot in R"" Example here",1574687.0,,,,,2013-01-16 00:44:17,1.0
14349766,2,14349055,2013-01-16 00:57:14,18,"This blog post is the best I've seen so far
http://messymindnet/2012/07/making-matplotlib-look-like-ggplot/

It doesn't focus on your standard R plots like you see in most of the ""getting started""-type examples Instead it tries to emulate the style of ggplot2  which seems to be nearly universally heralded as stylish and well-designed  

To get the axis spines like you see the in bar plot  try to follow one of the first few examples here: http://wwwloriafr/~rougier/coding/gallery/

Lastly  to get the axis tick marks pointing outward  you can edit your matplotlibrc files to say xtickdirection : out and ytickdirection : out

Combining these concepts together we get something like this:

import numpy as np
import matplotlib
import matplotlibpyplot as plt
# Data to be represented
X = nprandomrandn(256)

# Actual plotting
fig = pltfigure(figsize=(8 6)  dpi=72  facecolor=""white"")
axes = pltsubplot(111)
heights  positions  patches = axeshist(X  color='white')

axesspines['right']set_color('none')
axesspines['top']set_color('none')
axesxaxisset_ticks_position('bottom')

# was: axesspines['bottom']set_position(('data' 11*Xmin()))
axesspines['bottom']set_position(('axes'  -005))
axesyaxisset_ticks_position('left')
axesspines['left']set_position(('axes'  -005))

axesset_xlim([npfloor(positionsmin())  npceil(positionsmax())])
axesset_ylim([0 70])
axesxaxisgrid(False)
axesyaxisgrid(False)
figtight_layout()
pltshow()


The position of the spines can be specified a number of ways If you run the code above in IPython  you can then do axesspines['bottom']set_position? to see all of your options



So yeah It's not exactly trivial  but you can get close",1552748.0,,1552748.0,,2013-01-16 23:31:05,2013-01-16 23:31:05,7.0
14351567,2,14349055,2013-01-16 04:49:32,16,"To my knowledge  there is no built-in solution in matplotlib that will directly give to your figures a similar look than the ones made with R 

Some packages  like mpltools  adds support for stylesheets using Matplotlibs rc-parameters  and can help you to obtain a ggplot look (see the ggplot style for an example)

However  since everything can be tweaked in matplotlib  it might be easier for you to directly develop your own functions to achieve exactly what you want  As an example  below is a snippet that will allow you to easily customize the axes of any matplotlib plot

def customaxis(ax  c_left='k'  c_bottom='k'  c_right='none'  c_top='none' 
               lw=3  size=20  pad=8):

    for c_spine  spine in zip([c_left  c_bottom  c_right  c_top] 
                              ['left'  'bottom'  'right'  'top']):
        if c_spine != 'none':
            axspines[spine]set_color(c_spine)
            axspines[spine]set_linewidth(lw)
        else:
            axspines[spine]set_color('none')
    if (c_bottom == 'none') & (c_top == 'none'): # no bottom and no top
        axxaxisset_ticks_position('none')
    elif (c_bottom != 'none') & (c_top != 'none'): # bottom and top
        axtick_params(axis='x'  direction='out'  width=lw  length=7 
                      color=c_bottom  labelsize=size  pad=pad)
    elif (c_bottom != 'none') & (c_top == 'none'): # bottom but not top
        axxaxisset_ticks_position('bottom')
        axtick_params(axis='x'  direction='out'  width=lw  length=7 
                       color=c_bottom  labelsize=size  pad=pad)
    elif (c_bottom == 'none') & (c_top != 'none'): # no bottom but top
        axxaxisset_ticks_position('top')
        axtick_params(axis='x'  direction='out'  width=lw  length=7 
                       color=c_top  labelsize=size  pad=pad)
    if (c_left == 'none') & (c_right == 'none'): # no left and no right
        axyaxisset_ticks_position('none')
    elif (c_left != 'none') & (c_right != 'none'): # left and right
        axtick_params(axis='y'  direction='out'  width=lw  length=7 
                       color=c_left  labelsize=size  pad=pad)
    elif (c_left != 'none') & (c_right == 'none'): # left but not right
        axyaxisset_ticks_position('left')
        axtick_params(axis='y'  direction='out'  width=lw  length=7 
                       color=c_left  labelsize=size  pad=pad)
    elif (c_left == 'none') & (c_right != 'none'): # no left but right
        axyaxisset_ticks_position('right')
        axtick_params(axis='y'  direction='out'  width=lw  length=7 
                       color=c_right  labelsize=size  pad=pad)


EDIT: for non touching spines  see the function below which induces a 10 pts displacement of the spines (taken from this example on the matplotlib website)

def adjust_spines(ax spines):
    for loc  spine in axspinesitems():
        if loc in spines:
            spineset_position(('outward' 10)) # outward by 10 points
            spineset_smart_bounds(True)
        else:
            spineset_color('none') # don't draw spine


For example  the code and the two plots below show you the default output from matplotib (on the left)  and the output when the functions are called (on the right):

import numpy as np
import matplotlibpyplot as plt

fig (ax1 ax2) = pltsubplots(figsize=(8 5)  ncols=2)
ax1plot(nprandomrand(20)  nprandomrand(20)  'ok')
ax2plot(nprandomrand(20)  nprandomrand(20)  'ok')

customaxis(ax2) # remove top and right spines  ticks out
adjust_spines(ax2  ['left'  'bottom']) # non touching spines

pltshow()




Of course  it will take time for you to figure out which parameters have to be tweaked in matplotlib to make your plots look exactly like the R ones  but I am not sure there are other options right now ",975979.0,,975979.0,,2013-01-16 15:24:27,2013-01-16 15:24:27,3.0
14652632,2,14642435,2013-02-01 18:21:50,0,"You could replace:

d = {vnames[0] :vartemp[0]    vnames[3]: vartemp[3]}
hs = pdDataFrame(d  index=times)


with 

hs = pdDataFrame(vartemp[0:4]  columns=vnames[0:4]  index=times)




Saying that  pandas can read HDF5 directly  so perhaps the same is true for netCDF (which is based on HDF5)",1240268.0,,,,,2013-02-01 18:21:50,1.0
9968147,2,9944436,2012-04-01 20:42:23,4,"Your approach is sound  but fails because each function in the dict-of-functions applied to agg()
receives a Series object reflecting the column matched by the key value  Therefore  it's not necessary to
filter on column label again  With this  and assuming groupby preserves order 
you can slice the Series to extract the first/last element of the Open/Close
columns (note: groupby documentation does not claim to preserve order of original data
series  but seems to in practice)

In [50]: dfgroupby(dr5minuteasof)agg({'Low': lambda s: smin()  
                                         'High': lambda s: smax() 
                                         'Open': lambda s: s[0] 
                                         'Close': lambda s: s[-1] 
                                         'Volume': lambda s: ssum()})
Out[50]: 
                      Close    High     Low    Open  Volume
key_0                                                      
1999-01-04 10:20:00  11806  11819  11801  11801      34
1999-01-04 10:25:00  11789  11815  11776  11807      91
1999-01-04 10:30:00  11791  11792  11776  11780      16


For reference  here is a table to summarize the expected 
input and output types of an aggregation function based on the groupby object type and how the aggregation function(s) is/are passed to agg()

                  agg() method     agg func    agg func          agg()
                  input type       accepts     returns           result
GroupBy Object
SeriesGroupBy     function         Series      value             Series
                  dict-of-funcs    Series      value             DataFrame  columns match dict keys
                  list-of-funcs    Series      value             DataFrame  columns match func names
DataFrameGroupBy  function         DataFrame   Series/dict/ary   DataFrame  columns match original DataFrame
                  dict-of-funcs    Series      value             DataFrame  columns match dict keys  where dict keys must be columns in original DataFrame
                  list-of-funcs    Series      value             DataFrame  MultiIndex columns (original cols x func names)


From the above table  if aggregation requires access to more than one
column  the only option is to pass a single function to a
DataFrameGroupBy object  Therefore  an alternate way to accomplish the original task is to define
a function like the following:

def ohlcsum(df):
    df = dfsort()
    return {
       'Open': df['Open'][0] 
       'High': df['High']max() 
       'Low': df['Low']min() 
       'Close': df['Close'][-1] 
       'Volume': df['Volume']sum()
      }


and apply agg() with it:

In [30]: dfgroupby(dr5minuteasof)agg(ohlcsum)
Out[30]: 
                       Open    High     Low   Close  Volume
key_0                                                      
1999-01-04 10:20:00  11801  11819  11801  11806      34
1999-01-04 10:25:00  11807  11815  11776  11789      91
1999-01-04 10:30:00  11780  11792  11776  11791      16


Though pandas may offer some cleaner built-in magic in the future  hopefully this explains how to work with today's agg() capabilities",243434.0,,,,,2012-04-01 20:42:23,4.0
7580456,2,7577546,2011-09-28 08:35:50,4,"I tested with apply  it seems that when there are many sub groups  it's very slow the groups attribute of grouped is a dict  you can choice index directly from it:

subsampled = dfix[(choice(x) for x in groupedgroupsitervalues())]
",772649.0,,,,,2011-09-28 08:35:50,2.0
10057962,2,10015284,2012-04-07 20:11:27,5,"It's very hard for me to understand what you're asking; an illustration of exactly what you're looking for  with example data  would help make things more clear

I think what you should do:

rng = DateRange(start  end  offset=datetoolsSecond(15)

to create the date range To put data in a DataFrame indexed by that  you should add the columns and reindex them to the date range above using method='ffill':

df = DataFrame(index=rng)
df[colname] = seriesreindex(dfindex  method='ffill')


Per datetimetimedelta  datetimedatetime is part of the pandas namespace  so if you did from pandas import * then any import datetime you had done before that would be masked by the datetimedatetime reference inside the pandas namespace",776560.0,,,,,2012-04-07 20:11:27,
12335205,2,11233502,2012-09-08 23:09:44,1,I think it has to do with the lack of intercept Can you please report an issue on GitHub?,776560.0,,,,,2012-09-08 23:09:44,
11856897,2,11794584,2012-08-08 02:35:09,0,I don't know a good way just by looking at it I suspect a short Cython function (http://cythonorg) might be the best way to speed this up,776560.0,,,,,2012-08-08 02:35:09,
12140833,2,12140417,2012-08-27 11:07:30,3,"In [30]: df = pandasread_csv('foo1csv'  sep='[\s]{2 }')

In [30]: df
Out[30]:
   ItemId  ClientId  PriceQuoted      ItemDescription
0       1         1           10      scroll of Sneak
1       1         2           12      scroll of Sneak
2       1         3           13      scroll of Sneak
3       2         2         2500  scroll of Invisible
4       2         4         2200  scroll of Invisible

In [31]: df1 = df[['ItemId'  'ItemDescription']]drop_duplicates()set_index('ItemId')

In [32]: df1
Out[32]:
            ItemDescription
ItemId
1           scroll of Sneak
2       scroll of Invisible

In [33]: df2 = df[['ItemId'  'ClientId'  'PriceQuoted']]

In [34]: df2
Out[34]:
   ItemId  ClientId  PriceQuoted
0       1         1           10
1       1         2           12
2       1         3           13
3       2         2         2500
4       2         4         2200
",1548051.0,,,,,2012-08-27 11:07:30,
12373535,2,12372899,2012-09-11 15:50:04,1,This is actually by design The limit keyword is designed to go with the method keyword because you have to specify the ordering (ie  forward-fill or back-fill) and you don't have that with value,1306530.0,,,,,2012-09-11 15:50:04,2.0
12593896,2,12588031,2012-09-26 02:48:52,1,"Off-hand I can't recall a function that does this in pandas as a one-liner  but you can do something like this:

In [35]: st = dfrmix[:  ['id1'  'id2']]stack()

In [36]: all_ids = Series(stindexget_level_values(1)  
                          stindexget_level_values(0) 
                          name='all_ids')[stvalues]

In [37]: dfrmjoin(all_ids  how='left')
Out[37]: 
   A    id1    id2 all_ids
0  1   True  False     id1
1  2   True   True     id1
1  2   True   True     id2
2  3  False  False     NaN
",1306530.0,,567620.0,,2012-09-26 13:43:21,2012-09-26 13:43:21,1.0
12882439,2,12877189,2012-10-14 12:58:04,5,"As mentioned in the comments  it is a general floating point problem

However you can use the float_format key word of to_csv to hide it:

dfto_csv('pandasfilecsv'  float_format='%3f')


will give you:

Bob 0085
Alice 0005


in your output file",1301710.0,,,,,2012-10-14 12:58:04,
10114652,2,10114399,2012-04-11 22:06:26,1,"Try using merge (http://pandaspydataorg/pandas-docs/stable/merginghtml#database-style-dataframe-joining-merging):

In [14]: right
Out[14]: 
    ST_NAME  value2
0    Oregon   6218
1  Nebraska   0001

In [15]: merge(left  right)
Out[15]: 
    ST_NAME  value  value2
0  Nebraska  2491   0001
1    Oregon  4685   6218

In [18]: merge(left  right  on='ST_NAME'  sort=False)
Out[18]: 
    ST_NAME  value  value2
0    Oregon  4685   6218
1  Nebraska  2491   0001


DataFramejoin is a bit of legacy method and apparently doesn't do column-on-column joins (originally it did index on column using the on parameter  hence the ""legacy"" designation)",776560.0,,,,,2012-04-11 22:06:26,1.0
11246012,2,11237557,2012-06-28 13:42:19,0,"I've reported the bug on GitHub (best place for bug reports): 

https://githubcom/pydata/pandas/issues/1544

Should have a resolution today or tomorrow",776560.0,,,,,2012-06-28 13:42:19,1.0
11246539,2,11237557,2012-06-28 14:11:36,0,"I fixed the parser bug shown in the stack trace that you pasted However  I'm wondering whether your date column is named ""TRUE"" or did you mean to just pass a boolean? I haven't dug through pandas history but I know that in 08 we are supporting much more complex date parsing behavior as part of the time series API so here we're interpreting the string value as a column name",1306530.0,,,,,2012-06-28 14:11:36,1.0
12157914,2,12147392,2012-08-28 11:04:06,1,"In case of fixed width file  no need to do anything special to strip white space  or handle missing fields Below a small example of a fixed width file  three columns each of width 5 There is trailing and leading white space + missing data

In [57]: data = """"""\
A    B     C     
 0    foo       
3    bar     20
  1        30
""""""

In [58]: df = pandasread_fwf(StringIO(data)  widths=[5  5  5])

In [59]: df
Out[59]: 
   A    B   C
0  0  foo NaN
1  3  bar   2
2  1  NaN   3

In [60]: dfdtypes
Out[60]: 
A      int64
B     object
C    float64
",1548051.0,,,,,2012-08-28 11:04:06,
12383860,2,12382197,2012-09-12 08:15:50,1,"A first possibility:

>>> choices = npdiag([1]*5)
>>> choices[[zindex(i) for i in x]]


As noted elsewhere  you can change the list comprehension [zindex(i) for i in x] by npsearchsorted(z  x)

>>> choices[npsearchsorted(z  x)]


Note that as suggested in a comment by @seberg  you should use npeye(len(x)) instead of npdiag([1]*len(x)) The npeye function directly gives you a 2D array with 1 on the diagonal and 0 elsewhere",1491200.0,,1491200.0,,2012-09-15 12:31:38,2012-09-15 12:31:38,2.0
12384370,2,12382197,2012-09-12 08:47:14,1,"This is numpy method for the case of z being sorted You did not specifiy that If pandas needs something differently  I don't know:

# Assuming z is sorted
indices = npsearchsorted(z  x)


Now I really don't know why you want a boolean mask  these indices can be applied to z to give back x already and are more compact

z[indices] == x # if z included all x
",455221.0,,,,,2012-09-12 08:47:14,3.0
13053411,2,12382197,2012-10-24 16:23:56,1,"Surprised no one mentioned theouter method of numpyequal:

In [51]: npequalouter(s  z)
Out[51]: 
array([[ True  False  False  False  False] 
       [ True  False  False  False  False] 
       [False  False   True  False  False] 
       [ True  False  False  False  False] 
       [False   True  False  False  False]]  dtype=bool)

In [52]: npequalouter(s  z)astype(int)
Out[52]: 
array([[1  0  0  0  0] 
       [1  0  0  0  0] 
       [0  0  1  0  0] 
       [1  0  0  0  0] 
       [0  1  0  0  0]])
",776560.0,,,,,2012-10-24 16:23:56,
14641252,2,14639825,2013-02-01 06:57:08,1,"The function get_dummies gets you most of the way:

In [11]: s
Out[11]: 
0    T
1    T
2    T
3    T
4    K
Name: 1

In [12]: pdget_dummies(s  prefix=sname  prefix_sep='')
Out[12]: 
   1K  1T
0   0   1
1   0   1
2   0   1
3   0   1
4   1   0


And those columns which have differing values:

In [21]: (dfix[0] != df)any()
Out[21]: 
0    False
1     True
2     True
3    False
4     True
5    False


Putting these together:

In [31]: I = dfcolumns[(dfix[0] != df)any()]

In [32]: J = [pdget_dummies(df[i]  prefix=df[i]name  prefix_sep='') for i in I]

In [33]: df[[]]join(J)
Out[33]: 
   1K  1T  2A  2S  4D  4E  4H
0   0   1   1   0   1   0   0
1   0   1   1   0   1   0   0
2   0   1   0   1   0   1   0
3   0   1   1   0   1   0   0
4   1   0   1   0   0   0   1


Note: I created the initial DataFrame as follows  however this may be done more efficiently depending on your situation: 

df = pdDataFrame(map(list  'MTAQDD MTAQDD MTSQED MTAQDD MKAQHD'split(' ')))
",1240268.0,,1240268.0,,2013-02-01 07:03:02,2013-02-01 07:03:02,
9941490,2,9941288,2012-03-30 10:46:02,2,"first_series[second_series == 'Minus'] *= -1
first_series[second_seriesisnull()] = npnan


gives you:

0    18346606
1    -7646455
2          NaN
",449449.0,,,,,2012-03-30 10:46:02,1.0
11139823,2,11139757,2012-06-21 13:58:07,1,"The documentation does mention head/tail

And for your code  use pivot_table:

a = pdsDataFrame({'A' : ['CU' 'CU' 'CU' 'CU' 'CU' 'AU' 'AU' 'AU' 'AU' 'AU'] 
                   'B':[1 2 3 4 5 1 2 3 4 5]})sort(['A'])

apivot_table(rows='A'  values='B'  aggfunc=lambda x: xtail(2)mean())


returns

AU    45
CU    45
",449449.0,,449449.0,,2012-06-21 14:03:44,2012-06-21 14:03:44,2.0
11852536,2,11728836,2012-08-07 19:13:01,0,I have never done this myself-- if you find some way  post to the PyData mailing list or GitHub Otherwise  you could use Cython's OpenMP integration to get multicore processing on the data in a DataFrame,776560.0,,,,,2012-08-07 19:13:01,
11973007,2,11728836,2012-08-15 16:15:15,0,I am doing something similar using a custom Manager You should be able to gather more information from this question,370696.0,,,,,2012-08-15 16:15:15,
12054983,2,12047193,2012-08-21 12:36:18,1,"resoverall is a sqlalchemy ResultProxy object You can read more about it in the sqlalchemy docs  the latter explains basic usage of working with Engines and Connections Important here is that resoverall is dict like

Pandas likes dict like objects to create its data structures  see the online docs

Good luck with sqlalchemy and pandas",1548051.0,,,,,2012-08-21 12:36:18,1.0
12056933,2,12047193,2012-08-21 14:20:22,2,"I can't help you with SQLAlchemy -- I always use pyodbc  MySQLdb  or psychopg2 as needed But when doing so  a function as simple as the one below tends to suit my needs:

import pydobc
import numpy as np
import pandas

cnn  cur = myConnectToDBfunction()
cmd = ""SELECT * FROM myTable""
curexecute(cmd)
dataframe = __processCursor(cur  dataframe=True)

def __processCursor(cur  dataframe=False  index=None):
    '''
    Processes a database cursor with data on it into either
    a structured numpy array or a pandas dataframe

    input:
    cur - a pyodbc cursor that has just received data
    dataframe - bool if false  a numpy record array is returned
                if true  return a pandas dataframe
    index - list of column(s) to use as index in a pandas dataframe
    '''
    datatypes = []
    colinfo = curdescription
    for col in colinfo:
        if col[1] == unicode:
            datatypesappend((col[0]  'U%d' % col[3]))
        elif col[1] == str:
            datatypesappend((col[0]  'S%d' % col[3]))
        elif col[1] in [float  decimalDecimal]:
            datatypesappend((col[0]  'f4'))
        elif col[1] == datetimedatetime:
            datatypesappend((col[0]  'O4'))
        elif col[1] == int:
            datatypesappend((col[0]  'i4'))

    data = []
    for row in cur:
        dataappend(tuple(row))

    array = nparray(data  dtype=datatypes)
    if dataframe:
        output = pandasDataFramefrom_records(array)

        if index is not None:
            output = outputset_index(index)

    else:
        output = array

    return output
",1552748.0,,,,,2012-08-21 14:20:22,1.0
12593202,2,12590131,2012-09-26 01:03:36,2,"I think xs might do what you want

To get all shank 1's (ie where the first level of the MultiIndex is equal to 1) 

dfxs(1  axis=1  level=0)


This is pretty flexible if you need to cross-section by a different level of the MultiIndex as well",1639821.0,,,,,2012-09-26 01:03:36,1.0
12890620,2,12886240,2012-10-15 06:54:09,1,"pdDataFramefrom_records([i]  index=0)
",1548051.0,,1548051.0,,2012-10-15 10:45:13,2012-10-15 10:45:13,0.0
13021797,2,13021654,2012-10-23 00:06:36,3,"Sure  you can use get_loc():

In [45]: df = DataFrame({""pear"": [1 2 3]  ""apple"": [2 3 4]  ""orange"": [3 4 5]})

In [46]: dfcolumns
Out[46]: Index([apple  orange  pear]  dtype=object)

In [47]: dfcolumnsget_loc(""pear"")
Out[47]: 2


although to be honest I don't often need this myself  Usually access by name does what I want it to (df[""pear""]  df[[""apple""  ""orange""]]  or maybe dfcolumnsisin([""orange""  ""pear""]))  although I can definitely see cases where you'd want the index number ",487339.0,,,,,2012-10-23 00:06:36,
13036844,2,13021654,2012-10-23 18:27:34,1,DSM's solution works  but if you wanted a direct equivalent to which you could do (dfcolumns == name)nonzero(),776560.0,,,,,2012-10-23 18:27:34,1.0
13231460,2,13229750,2012-11-05 11:45:37,2,"You want to write this as follows:

class Foo(pdDataFrame):
  def __init__(self):
    super(Foo  self)__init__()
    selfbar = None


See the Python's __init__ syntax question",1240268.0,,,,,2012-11-05 11:45:37,
13231508,2,13229750,2012-11-05 11:48:41,1,"In [12]: class Foo(pdDataFrame):
   :     def __init__(self  bar=None):
   :         super(Foo  self)__init__()
   :         selfbar = bar      


which results in:-

In [30]: my_special_dataframe = Foo(bar=1)

In [31]: my_special_dataframebar
Out[31]: 1

In [32]: my_special_dataframe2 = Foo() 

In [33]: my_special_dataframe2bar   
",482506.0,,482506.0,,2012-11-05 11:56:35,2012-11-05 11:56:35,
13423940,2,13421947,2012-11-16 20:28:22,1,"# First convert to pandas Period
period = pandastseriesperiodPeriod(ordinal=int(d1)  freq=axfreq)
# Then convert to pandas timestamp
ts = periodto_timestamp()
# Then convert to date object
dt = tsto_datetime()
",639792.0,,,,,2012-11-16 20:28:22,
13753560,2,13753213,2012-12-06 22:11:57,2,"sum(1) means sum over axis = 1 The terminology comes from numpy 

For a 2+ dimensional object  the 0-axis refers to the rows Summing over the 0-axis means summing over the rows  which amounts to summing ""vertically"" (when looking at the table) 

The 1-axis refers to the columns Summing over the 1-axis means summing over the columns  which amounts to summing ""horizontally""

numpyargsort returns an array of indices which tell you how to sort an array For example:

In [72]: import numpy as np

In [73]: x = nparray([521  3  1  2  1  1  5])

In [74]: npargsort(x)
Out[74]: array([2  4  5  3  1  6  0])


The 2 in the array returned by npargsort means the smallest value in x is x[2]  which equals 1 The next smallest is x[4] which is also 1 And so on

If we define

totals = dfsum(1)
print(totals)
# tz                     521
# Africa/Cairo             3
# Africa/Casablanca        1
# Africa/Ceuta             2
# Africa/Johannesburg      1
# Africa/Lusaka            1
# America/Anchorage        5


then totalsargsort() is argsorting the values [521  3  1  2  1  1  5] We've seen the result; it is the same as numpyargsort:

[2  4  5  3  1  6  0]


These values are simply made into a Series  with the same index as totals:

print(totalsargsort())
# tz                     2
# Africa/Cairo           4
# Africa/Casablanca      5
# Africa/Ceuta           3
# Africa/Johannesburg    1
# Africa/Lusaka          6
# America/Anchorage      0


Associating the totalsindex with this argsort indices does not appear have intrinsic meaning  but if you compute totals[totalsargsort()] you see the rows of totals in sorted order:

print(totals[totalsargsort()])
# Africa/Casablanca        1
# Africa/Johannesburg      1
# Africa/Lusaka            1
# Africa/Ceuta             2
# Africa/Cairo             3
# America/Anchorage        5
# tz                     521
",190597.0,,190597.0,,2012-12-06 22:46:46,2012-12-06 22:46:46,1.0
11492771,2,11492215,2012-07-15 14:36:30,2,"It's not so great either  but for this case  if you pass a function returning True at least it wouldn't require changing df:

>>> from pandas import *
>>> df = DataFrame( nprandomrandn(5 3)  index = list( ""ABCDE"")  columns = list(""abc"") )
>>> dfgroupby(lambda x: True)agg({'a' : npsum  'b' : npmean } )
             a         b
True  1836649 -0692655
>>> 
>>> df['total'] = 'total'
>>> dfgroupby(['total'])agg({'a' : npsum  'b' : npmean } ) 
              a         b
total                    
total  1836649 -0692655


You could use various builtins instead of lambda x: True but they're less explicit and only work accidentally",487339.0,,,,,2012-07-15 14:36:30,
11493108,2,11492215,2012-07-15 15:20:22,1,"Having an analogous DataFrameaggregate method is a good idea Creating an issue here:

https://githubcom/pydata/pandas/issues/1623",776560.0,,,,,2012-07-15 15:20:22,3.0
11883859,2,11870058,2012-08-09 12:51:28,1,"What you are trying to do seem to work fine  except for the warnings in the repr (for which i created an issue: https://githubcom/pydata/pandas/issues/1749)

In [57]: df
Out[57]:
          a         b         c
0 -0119342  0286710  0266750
1 -1514301  0556106 -2743888
2 -0156469 -0352915 -1963398
3  1165479  1364303  0648178
4  1541738  0714239 -1468896

In [58]: s = dfapply(f  axis=1)

In [59]: type(s)
Out[59]: pandascoreseriesSeries

In [60]: type(s[0])
Out[60]: pandascoreframeDataFrame

In [61]: s[0]
Out[61]:
         ab       bc        ac
0  0167368  055346  0147408

In [62]: s
Exception ValueError: ValueError('Cannot call bool() on DataFrame' ) in 'util_checknull' ignored
Exception ValueError: ValueError('Cannot call bool() on DataFrame' ) in 'util_checknull' ignored
Exception ValueError: ValueError('Cannot call bool() on DataFrame' ) in 'util_checknull' ignored
Exception ValueError: ValueError('Cannot call bool() on DataFrame' ) in 'util_checknull' ignored
Exception ValueError: ValueError('Cannot call bool() on DataFrame' ) in 'util_checknull' ignored
Exception ValueError: ValueError('Cannot call bool() on DataFrame' ) in 'util_checknull' ignored
Exception ValueError: ValueError('Cannot call bool() on DataFrame' ) in 'util_checknull' ignored
Exception ValueError: ValueError('Cannot call bool() on DataFrame' ) in 'util_checknull' ignored
Exception ValueError: ValueError('Cannot call bool() on DataFrame' ) in 'util_checknull' ignored
Exception ValueError: ValueError('Cannot call bool() on DataFrame' ) in 'util_checknull' ignored
Out[62]:
0             ab       bc        ac
0  0167368  0553
1             ab        bc        ac
0 -0958195 -218
2             ab        bc        ac
0 -0509384 -231
3             ab        bc        ac
0  2529782  201
4             ab        bc        ac
0  2255977 -075
",1548051.0,,,,,2012-08-09 12:51:28,
12183507,2,12182744,2012-08-29 17:36:10,4,"The documentation explains this clearly The apply method accept a python function which should have a single parameter If you want to pass more parameters you should use functoolspartial as suggested by Joel Cornett in his comment

An example:

>>> import functools
>>> import operator
>>> add_3 = functoolspartial(operatoradd 3)
>>> add_3(2)
5
>>> add_3(7)
10


You can also pass keywords arguments using partial

An other way would be to create a lambda:

my_seriesapply((lambda x: your_func(a b c d  x)))


But I think using partial is better",510937.0,,,,,2012-08-29 17:36:10,2.0
12407691,2,12406162,2012-09-13 13:43:36,2,"instead of call plot(test[""x""][5:10])  you can call the plot method of Series object:

test[""x""][5:10]plot()


The reason: test[""x""][5:10] is a Series object with integer index from 5 to 10 plot() try to get index 0 of it  that will cause error",772649.0,,,,,2012-09-13 13:43:36,2.0
12423603,2,12420598,2012-09-14 11:27:40,1,"Constructing from a reST table is not possible  but would be interesting You can use read_csv to read in a table See also read_clipboard and read_fwf (fixed width)

In [22]: table = """"""\
   : id1         id2         net       nnet       desc
   : 1001        1002             100       00  Closed part of queue
   : 1002        NaN               00       30  Opened part of queue
   : """"""

In [23]: df = pandasread_csv(StringIO(table)  sep='[\s]{2 }')

In [24]: df
Out[24]: 
    id1   id2  net  nnet                  desc
0  1001  1002   10     0  Closed part of queue
1  1002   NaN    0     3  Opened part of queue
",1548051.0,,,,,2012-09-14 11:27:40,2.0
12555491,2,12555323,2012-09-23 19:22:27,1,"One way to do it would be to use map:

df1['e'] = df1['a']map(lambda x: nprandomrandom())
",1240268.0,,,,,2012-09-23 19:22:27,2.0
12555510,2,12555323,2012-09-23 19:24:45,4,"Use the original df1 indexes to create the series:

df1['e'] = Series(nprandomrandn(sLength)  index=df1index)
",308903.0,,,,,2012-09-23 19:24:45,3.0
13843741,2,12555323,2012-12-12 16:04:31,0,This is the simple way of adding a new column: df['e'] = e,1645853.0,,165988.0,,2012-12-12 16:26:41,2012-12-12 16:26:41,
12846006,2,12844900,2012-10-11 18:15:47,0,"I made a github issue regarding your question Need to add the relevant feature to pandas

Case 3 is achievable directly via fill_method:

In [25]: s
Out[25]: 
2012-01-31    0
2012-02-29    1
2012-03-31    2
Freq: M

In [26]: sresample('W'  fill_method='ffill')
Out[26]: 
2012-02-05    0
2012-02-12    0
2012-02-19    0
2012-02-26    0
2012-03-04    1
2012-03-11    1
2012-03-18    1
2012-03-25    1
2012-04-01    2
Freq: W-SUN


But for others you'll have to do some contorting right now that will hopefully be remedied by the github issue before the next release

Also it looks like you want the upcoming 'span' resampling convention as well that will upsample from the start of the first period to the end of the last period I'm not sure there is an easy way to anchor the start/end points for a DatetimeIndex but it should at least be there for PeriodIndex",1306530.0,,,,,2012-10-11 18:15:47,2.0
13120202,2,13119515,2012-10-29 10:51:48,2,"Not using MultiIndexes (if you have them  you can get rid of them through dfreset_index()):

chans = [1 1 1 2 2 2 1 1 1 2 2 2]
df = pdDataFrame(dict(ch=chans  det=[1 2 3 1 2 3 1 2 3 1 2 3]  time=6*[0]+6*[1]  counts=nprandomrandint(0 500 12)))


Use groupby and mean as an aggregation function:

>>> dfgroupby(['time'  'ch'])['counts']mean()
time  ch
0     1     315000000
      2     296666667
1     1     178333333
      2     221666667
Name: counts


Other aggregation functions can be passed via agg:

>>> dfgroupby(['time'  'ch'])['counts']agg(npptp)
",54567.0,,,,,2012-10-29 10:51:48,2.0
13123189,2,13119515,2012-10-29 14:05:50,1,"Same as @meteore but with a MultiIndex

In [55]: df
Out[55]:
             counts
ch det time
1  1   0        123
   2   0        121
   3   0        125
2  1   0        212
   2   0        210
   3   0        210
1  1   1        124
   2   1        125
   3   1        123
2  1   1        210
   2   1        209
   3   1        213

In [56]: dfindex
Out[56]:
MultiIndex
[(1L  1L  0L) (1L  2L  0L) (1L  3L  0L) (2L  1L  0L) (2L  2L  0L)
 (2L  3L  0L) (1L  1L  1L) (1L  2L  1L) (1L  3L  1L) (2L  1L  1L)
 (2L  2L  1L) (2L  3L  1L)]

In [57]: dfindexnames
Out[57]: ['ch'  'det'  'time']

In [58]: dfgroupby(level=['ch'  'time'])mean()
Out[58]:
             counts
ch time
1  0     123000000
   1     124000000
2  0     210666667
   1     210666667


Be carefull with floats & groupby (this is independent of a MultiIndex or not)  groups can differ due to numerical representation/accuracy-limitations related to floats",1548051.0,,,,,2012-10-29 14:05:50,
11174387,2,11174367,2012-06-24 01:40:39,1,"Have a look at the pandasconcat() function  When you read in your files  you can use concat to join the resulting DataFrames into one  then just use normal pandas averaging techniques to average them

To use it  just pass it a list of the DataFrames you want joined together:

>>> x
          A         B         C
0 -0264438 -1026059 -0619500
1  0927272  0302904 -0032399
2 -0264273 -0386314 -0217601
3 -0871858 -0348382  1100491
>>> y
          A         B         C
0  1923135  0135355 -0285491
1 -0208940  0642432 -0764902
2  1477419 -1659804 -0431375
3 -1191664  0152576  0935773
>>> pandasconcat([x  y])
          A         B         C
0 -0264438 -1026059 -0619500
1  0927272  0302904 -0032399
2 -0264273 -0386314 -0217601
3 -0871858 -0348382  1100491
0  1923135  0135355 -0285491
1 -0208940  0642432 -0764902
2  1477419 -1659804 -0431375
3 -1191664  0152576  0935773
",1427416.0,,1427416.0,,2012-06-24 01:53:26,2012-06-24 01:53:26,10.0
11174716,2,11174367,2012-06-24 03:21:48,0,"I figured out one way to do it

pandas DataFrames can be added together with the DataFrameadd() function: http://pandassourceforgenet/generated/pandasDataFrameaddhtml

So I can add the DataFrames together then divide by the number of DataFrames  eg:

avgDataFrame = DataFrameList[0]

for i in range(1  len(DataFrameList)):
    avgDataFrame = avgDataFrameadd(DataFrameList[i])

avgDataFrame = avgDataFrame / len(DataFrameList)
",1383444.0,,,,,2012-06-24 03:21:48,
11194573,2,11174367,2012-06-25 17:49:49,5,"Check it out:

In [14]: glued = pdconcat([x  y]  axis=1  keys=['x'  'y'])

In [15]: glued
Out[15]: 
          x                             y                    
          A         B         C         A         B         C
0 -0264438 -1026059 -0619500  1923135  0135355 -0285491
1  0927272  0302904 -0032399 -0208940  0642432 -0764902
2 -0264273 -0386314 -0217601  1477419 -1659804 -0431375
3 -0871858 -0348382  1100491 -1191664  0152576  0935773

In [16]: gluedswaplevel(0  1  axis=1)sortlevel(axis=1)
Out[16]: 
          A                   B                   C          
          x         y         x         y         x         y
0 -0264438  1923135 -1026059  0135355 -0619500 -0285491
1  0927272 -0208940  0302904  0642432 -0032399 -0764902
2 -0264273  1477419 -0386314 -1659804 -0217601 -0431375
3 -0871858 -1191664 -0348382  0152576  1100491  0935773

In [17]: glued = gluedswaplevel(0  1  axis=1)sortlevel(axis=1)

In [18]: glued
Out[18]: 
          A                   B                   C          
          x         y         x         y         x         y
0 -0264438  1923135 -1026059  0135355 -0619500 -0285491
1  0927272 -0208940  0302904  0642432 -0032399 -0764902
2 -0264273  1477419 -0386314 -1659804 -0217601 -0431375
3 -0871858 -1191664 -0348382  0152576  1100491  0935773


For the record  swapping the level and reordering was not necessary  just for visual purposes

Then you can do stuff like:

In [19]: gluedgroupby(level=0  axis=1)mean()
Out[19]: 
          A         B         C
0  0829349 -0445352 -0452496
1  0359166  0472668 -0398650
2  0606573 -1023059 -0324488
3 -1031761 -0097903  1018132
",776560.0,,,,,2012-06-25 17:49:49,1.0
11774271,2,11773862,2012-08-02 09:05:01,2,"You could get the result you want by using the loffset and closed options 

>>> tsresample('D'  how='mean'  closed=""left""  loffset=datetimetimedelta(days=-1))
2012-01-01    0200299
",386327.0,,,,,2012-08-02 09:05:01,3.0
12137609,2,12127315,2012-08-27 07:09:19,1,"Plotting a dataframe (df here below) with the option suplots=True creates for each column of your dataframe an axis It is needed to iterate on all axes to set the ticklabel_format

In [59]: axes = dfplot(subplots=True)

In [60]: for axis in axes:
   :     axisticklabel_format(axis='y'  useOffset=False)
",1548051.0,,,,,2012-08-27 07:09:19,1.0
12885682,2,12877189,2012-10-14 19:23:31,0,"I created an issue to examine in a bit more detail here:

http://githubcom/pydata/pandas/issues/2069

EDIT: If you can  please put a standalone reproduction of the problem on the GitHub issue I'm not able to reproduce it",776560.0,,776560.0,,2012-11-03 17:48:23,2012-11-03 17:48:23,
12984678,2,12984205,2012-10-20 02:19:33,1,"What you have is fine but you must write the file in binary mode if you are working on a windows machine:

fh=open('checkpng' 'wb')


Also  close the file after writing:

fhclose()


Your code above combined with these minor tweaks saves a read-able png on my machine",1658227.0,,,,,2012-10-20 02:19:33,
13203497,2,13202326,2012-11-02 21:48:04,0,"I suspect the problem is in the printing of numpy datetime64[ns] objects If you take those funny date values and convert them back into pandas Timestamp objects  they look normal

pandasTimestamp(data_frameix[0]['datetime'])


should give a normal-looking result",250839.0,,,,,2012-11-02 21:48:04,1.0
13423323,2,13421929,2012-11-16 19:42:03,0,"dxs(1)[0:3]


          0         1
0 -0716206  0119265
1 -0782315  0097844
2  2042751 -1116453
",1548051.0,,,,,2012-11-16 19:42:03,2.0
13701036,2,13701035,2012-12-04 10:39:28,0,"I see two ways of getting this  both of which look like a detour  which makes me think there must be a better way which I'm overlooking

Converting the MultiIndex into columns: df[dfreset_index()[""B""] == 2]
Swapping the name I want to use to the start of the MultiIndex and then use lookup by index: dfswaplevel(0  ""B"")ix[2]
",1274613.0,,,,,2012-12-04 10:39:28,
13702133,2,13701035,2012-12-04 11:44:00,0,"I think you are looking to grouping by index levels (see GroupBy with MultiIndex)
Here's a short  and not very exciting  example:

In [126]: df = DataFrame([[1 2 3 4] [2 2 npnan 6]] columns=[""A""  ""B""  ""C""  ""D""])

In [127]: df1 = dfset_index(['A' 'B'])

In [128]: df1
Out[128]: 
      C  D
A B       
1 2   3  4
2 2 NaN  6

In [129]: df1groupby(level='B'  axis=0)mean()
Out[129]: 
   C  D
B      
2  3  5
",1240268.0,,1240268.0,,2012-12-04 11:55:09,2012-12-04 11:55:09,
13755051,2,13701035,2012-12-07 00:23:33,1,"I would suggest either:

dfxs(2  level='B')

or

df[dfindexget_level_values('B') == val]

I'd like to make the syntax for the latter operation a little nicer",776560.0,,,,,2012-12-07 00:23:33,
13993480,2,13988111,2012-12-21 16:12:26,1,"When you import pandas it registers a bunch of unit converters with matplotlib  This is from more updated versions of both libraries  but I assume that the overall behavior is the same  

In [4]: import matplotlibunits as muints

In [5]: muintsregistry
Out[5]: 
  {datetimedate: <matplotlibdatesDateConverter instance at 0x2ab8908> 
   datetimedatetime: <matplotlibdatesDateConverter instance at 0x2ab8ab8>}


In [6]: import pandas

In [7]: muintsregistry
Out[7]: 
{pandastseriesperiodPeriod: <pandastseriesconverterPeriodConverter instance at 0x2627e60> 
 pandastslibTimestamp: <pandastseriesconverterDatetimeConverter instance at 0x264ea28> 
 datetimedate: <pandastseriesconverterDatetimeConverter instance at 0x2532fc8> 
 datetimedatetime: <pandastseriesconverterDatetimeConverter instance at 0x2627ab8> 
 datetimetime: <pandastseriesconverterTimeConverter instance at 0x2532f38>}


This registry is used by axis (with a few layers of re-direction) to determine how to format information that is not numbers and it matches on the class of the thing it is trying to label with (hence  the entries in the dictionary keyed to datetime*)  

I suspect you can fix this by replacing the offending entries in dict",380231.0,,380231.0,,2012-12-21 16:19:59,2012-12-21 16:19:59,1.0
12273693,2,12269158,2012-09-05 02:21:45,1,"How about something like this:

In [3]: pandasDataFrame(list(tsvalues)  index=tsindex)
Out[3]: 
   0  1  2  3   4
a  1  2  3  4   5
b  6  7  8  9  10
",1306530.0,,,,,2012-09-05 02:21:45,1.0
12523037,2,12521909,2012-09-21 01:12:41,0,"Assuming the index is already aligned  you probably just want to align the columns in both DataFrame in the right order and divide the values of both DataFrames

Supposed mapping = {'a' : 'e'  'b' : 'd'  'c' : 'f'}:

v1 = df1reindex(columns=['a'  'b'  'c'])values
v2 = df2reindex(columns=['e'  'd'  'f'])values
rs = DataFrame(v1 / v2  index=v1index  columns=['a'  'b'  'c'])
",1306530.0,,,,,2012-09-21 01:12:41,1.0
12841042,2,12840847,2012-10-11 13:45:40,0,"How about this?

sreindex(DatetimeIndex(start=sindex[0]replace(day=1)  end=sindex[-1]  freq='D'))
",449449.0,,,,,2012-10-11 13:45:40,6.0
12979842,2,12979062,2012-10-19 17:58:07,0,"That's not possible right now unfortunately Aliasing is static 
It would be a good feature to add though Please check back on github (https://githubcom/pydata/pandas/issues/2085) Additional feedback or a PR would be appreciated",1306530.0,,,,,2012-10-19 17:58:07,1.0
13155030,2,13151514,2012-10-31 09:40:42,3,"I'm not convinced this is a pandas issue at all

Does 

import matplotlibpyplot as plt
pltplot(range(10))
pltshow()


bring up a plot?

If not:

How did you install matplotlib? Was it from source or did you install it from a package manager/pre-built binary?

I suspect that if you run:

import matplotlib            
print matplotlibrcParams['backend']


The result will be a non-GUI backend (almost certainly ""Agg"") This suggests you don't have a suitable GUI toolkit available (I personally use Tkinter which means my backend is reported as ""TkAgg"")

The solution to this depends on your operating system  but if you can install a GUI library (one of Tkinter  GTK  QT4  PySide  Wx) then pyplotshow() should hopefully pop up a window for you

HTH ",741316.0,,,,,2012-10-31 09:40:42,1.0
13420016,2,13419822,2012-11-16 15:55:06,4,"All functions in Python are ""pass by reference""  there is no ""pass by value"" If you want to make an explicit copy of a pandas object  try new_frame = framecopy()",776560.0,,,,,2012-11-16 15:55:06,
13692791,2,13692418,2012-12-03 22:19:23,2,"Yes  ix caches results bix returns a _NDFrameIndexer Its __getitem__ method calls the DataFrame's get_value method  which calls the _get_item_cache method  which caches results

The caching may also explain why accessing the first DataFrame 50 times was faster than accessing from 50 DataFrames",190597.0,,190597.0,,2012-12-04 01:51:28,2012-12-04 01:51:28,
13693385,2,13692418,2012-12-03 23:06:00,1,"Note: there is a hash table population step the first time you look up a location in an axis index That's probably what you're seeing here and would be obscured by using timeit (because the hash table is computed once  stored  and reused) Also explains the increased memory usage 

In a future version of pandas I plan to improve the performance of this type of code on simple data with simple sequential axis indexes I'll record your use case on the GitHub issue tracker

https://githubcom/pydata/pandas/issues/2420",776560.0,,,,,2012-12-03 23:06:00,
13786327,2,13784192,2012-12-09 09:40:46,1,"Here's a couple of suggestions:

Use date_range for the index:

import datatime
import pandas as pd
import numpy as np

todays_date = datetimedatetimenow()date()
index = pddate_range(todays_date-datetimetimedelta(10)  periods=10  freq='D')

columns = ['A' 'B'  'C']


Note: we could create an empty DataFrame (with NaNs) simply by writing:

df_ = pdDataFrame(index=index  columns=columns)
df_ = df_fillna(0) # with 0s rather than NaNs


To do these type of calculations for the data  use a numpy array:

data = nparray([nparange(10)]*3)T


Hence we can create the DataFrame:

In [10]: df = pdDataFrame(data  index=index  columns=columns)

In [11]: df
Out[11]: 
            A  B  C
2012-11-29  0  0  0
2012-11-30  1  1  1
2012-12-01  2  2  2
2012-12-02  3  3  3
2012-12-03  4  4  4
2012-12-04  5  5  5
2012-12-05  6  6  6
2012-12-06  7  7  7
2012-12-07  8  8  8
2012-12-08  9  9  9
",1240268.0,,,,,2012-12-09 09:40:46,3.0
13923312,2,13918355,2012-12-17 22:29:51,2,"Thanks for providing sample data  I've updated this answer with general
suggestions given anticipated array sizes in the 100's of million

Line profile

Line profiling the guts of your lambda function shows that most time is spent
in Bix[] (which has been refactored here to only be called once)

In [91]: lprun -f stackfoo1 AAapply(stackfoo1  B=B  axis=1)
Timer unit: 1e-06 s

File: stackpy
Function: foo1 at line 4
Total time: 0006651 s

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
     4                                           def foo1(row  B):
     5         6         6158   10263     926      subset = Bix[row[0]]ts
     6         6          418     697      63      idx = npsearchsorted(subset  row[2])
     7         6           56      93      08      val = subsetirow(idx)
     8         6           19      32      03      return val

Consider built-in data types and raw numpy arrays over higher-level constructs

Since B behaves like a dict here and the same key is accessed many times  let's compare dfix to a normal Python
dictionary (precomputed elsewhere)  A dictionary with 1M keys (unique A values) should only require ~34MB (33% capacity: 3 * 1e6 * 12 bytes)

In [102]: timeit Bix['a']
10000 loops  best of 3: 122 us per loop

In [103]: timeit dct['a']
10000000 loops  best of 3: 532 ns per loop

Replace function calls with loops

The last major improvement I can think of would be to replace dfapply() with a for loop to avoid calling any function 200M times (or however large A is)
Hopefully these ideas help

Original  expressive solution  though not memory efficient:

In [5]: CC = AAmerge(B  left_on='a'  right_index=True)

In [6]: CC[CCts_x <= CCts_y]groupby(['a'  'b'])first()
Out[6]: 
     ts_x  ts_y
a b            
a x     4     4
  y     6     7
  z     5     5
b x     4     7
  z     5     7
c y     6     8
",243434.0,,243434.0,,2012-12-18 07:37:43,2012-12-18 07:37:43,9.0
13923414,2,13918355,2012-12-17 22:38:11,2,"Another option using numpy's boolean array notation  which seems an order of magnitude faster than the original (in this tiny example  and I suspect it'll be even better on larger datasets):I suspect this is largely because picking the minimum is much faster task than sorting

In [11]: AAapply(lambda row: (Btsvalues[(Btsvalues >= row['ts']) &
                                           (Bindex == row['a'])]min()) 
                          axis=1)
Out[11]: 
0    4
1    7
2    5
3    7
4    7
5    8

In [12]: %timeit AAapply(lambda row: (Btsvalues[(Btsvalues >= row['ts']) &(Bindex == row['a'])]min())  axis=1)
1000 loops  best of 3: 146 ms per loop


This seems like the fastest method if you were to simply adding this as a column to AA

If you were creating a new dataframe as in you example - trying to test this ""fairly"" - it is slower (but still twice as fast as the original):

In [13]: %timeit C = AAapply(lambda row: (row[0]  row[1]  Bix[row[0]]irow(npsearchsorted(Bts[row[0]]  row[2])))  axis=1)set_index(['a'  'b'])
100 loops  best of 3: 103 ms per loop

In [14]: %timeit C = AAapply(lambda row: (row[0]  x[1]  Btsvalues[(Btsvalues >= row['ts']) & (Bindex == row['a'])]min())  axis=1)
100 loops  best of 3: 432 ms per loop
",1240268.0,,1240268.0,,2012-12-17 23:07:28,2012-12-17 23:07:28,2.0
12715567,2,12711551,2012-10-03 19:34:40,0,"Also  I found another solution  but don't know which one is faster

def files2df(colnames  ext):
    dflist = [ ]
    for inf in sorted(globglob(ext)):
        dflistappend(read_csv(inf  names = colnames  sep='\t'  skiprows=1))
        #print(dflist)                                                                                                                            
    df = concat(dflist  axis = 0  ignore_index=True)
    #print(dfto_string())                                                                                                                        
    return df
",1289107.0,,,,,2012-10-03 19:34:40,4.0
12948922,2,12946578,2012-10-18 07:06:36,1,"Plotting is by default your data versus the index In case of a numerical index  look at the index after sorting on a column you might want to use plot(use_index=False) See example below

In [2]: df = pdDataFrame(nprandomrandn(10  2)  columns=['A'  'B'])

In [3]: df
Out[3]:
          A         B
0 -0938196  2220319
1 -0022503  0564602
2  0033094 -0717969
3  2466486  1229651
4 -0641596 -1016921
5  0094125  1531526
6  0579631  1398635
7 -0854799 -0930904
8 -1177894 -1501657
9  0341655 -0917243

In [4]: dfsort(columns='A'  inplace=True)
Out[4]:
          A         B
8 -1177894 -1501657
0 -0938196  2220319
7 -0854799 -0930904
4 -0641596 -1016921
1 -0022503  0564602
2  0033094 -0717969
5  0094125  1531526
9  0341655 -0917243
6  0579631  1398635
3  2466486  1229651

In [5]: df['A']plot(use_index=False)
Out[5]: <matplotlibaxesAxesSubplot at 0xb56ac6c>


",1548051.0,,,,,2012-10-18 07:06:36,1.0
13053381,2,13052844,2012-10-24 16:22:22,2,"You can set the ticks to where you want just like you set the xticks 

import numpy as np
ax0yaxisset_ticks(nparange(70000 80000 2500))


This will create four ticks evenly spaced for your ax0 subplot You can do something similar for your other subplots ",484596.0,,,,,2012-10-24 16:22:22,
13053967,2,13052844,2012-10-24 16:55:56,0,"An improvement over the approach suggestion by Aman is the following:

import matplotlibpyplot as plt

fig = pltfigure()
ax = figadd_subplot(1  1  1)

#  plot some things 

# Find at most 101 ticks on the y-axis at 'nice' locations
max_yticks = 100
yloc = pltMaxNLocator(100)
axyaxisset_major_locator(yloc)

pltshow()


Hope this helps",1712956.0,,,,,2012-10-24 16:55:56,
13389331,2,13388330,2012-11-14 23:41:26,0,"I don't think that you can plot a panel

To extract the Series (and plot) from the panel you can use ix with the following syntax:

scoresix['gain'  'ESP'  2:21]plot()
",1240268.0,,1240268.0,,2012-11-15 13:15:20,2012-11-15 13:15:20,
13548893,2,13548721,2012-11-25 06:36:53,0,"You may not be getting what you think you are when you do testOpen[1:]/testClose  Pandas matches up the rows based on their index  so you're still getting each element of one column divided by its corresponding element in the other column (not the element one row back)  Here's an example:

>>> print d
   A  B   C
0  1  3   7
1 -2  1   6
2  8  6   9
3  1 -5  11
4 -4 -2   0
>>> dA / dB
0    0333333
1   -2000000
2    1333333
3   -0200000
4    2000000
>>> dA[1:] / dB
0         NaN
1   -2000000
2    1333333
3   -0200000
4    2000000


Notice that the values returned are the same for both operations  The second one just has nan for the first one  since there was no corresponding value in the first operand

If you really want to operate on offset rows  you'll need to dig down to the numpy arrays that underpin the pandas DataFrame  to bypass pandas's index-aligning features  You can get at these innards with the values attribute of a column

>>> dAvalues[1:] / dBvalues[:-1]
array([-066666667   8           016666667   08       ])


Now you really are getting each value divided by the one before it in the other column  Note that here you have to explicitly slice the second operand to leave off the last element  to make them equal in length

So you can do the same to divide a column by an offset version of itself:

>>> dAvalues[1:] / dAvalues[:-1]
45: array([-2     -4      0125  -4   ])
",1427416.0,,,,,2012-11-25 06:36:53,
14294757,2,14281644,2013-01-12 15:24:18,2,This will convert to a map: speedsix[3]to_dict(),919872.0,,,,,2013-01-12 15:24:18,1.0
14406096,2,14405975,2013-01-18 19:21:58,2,"You can use the setsymmetric_difference function:

In [1]: df1 = DataFrame(list('ABCDE')  columns=['x'])

In [2]: df1
Out[2]:
   x
0  A
1  B
2  C
3  D
4  E

In [3]: df2 = DataFrame(list('CDEF')  columns=['y'])

In [4]: df2
Out[4]:
   y
0  C
1  D
2  E
3  F

In [5]: set(df1x)symmetric_difference(df2y)
Out[5]: set(['A'  'B'  'F'])
",919872.0,,,,,2013-01-18 19:21:58,1.0
14617925,2,14614512,2013-01-31 03:24:35,1,"this is a little pseudo codish  but I think should be quite fast
straightfoward disk based merge  with all tables on disk The
key is that you are not doing selection per se  just indexing
into the table via start/stop  which is quite fast

Selecting the rows that meet a criteria in B (using A's ids) won't
be very fast because I think it might be bringing the data into python space
rather than an in-kernel search (I am not sure  but you might want
to investiage on pytablesorg more in the in-kernel optmization section 
there is a way to tell if its going to be in-kernel or not)

Also if you are up to it  this is a very parallel problem (just don't write
the results to the same file from multiple processes  pytables is not write-safe
for that)

For your merge_a_b operation I think you can use a standard pandas join
which is quite efficient (when in-memory)

One other option (depending on how 'big' A) is  might be to separate A into 2 pieces
(that are indexed the same)  using a smaller (maybe use single column) in the first
table; instead of storing the merge results per se  store the row index; later
you can pull out the data you need (kind of like using an indexer and take)
see http://pandaspydataorg/pandas-docs/stable/iohtml#multiple-table-queries

A = HDFStore('Ah5')
B = HDFStore('Bh5')

nrows_a = Aget_storer('df')nrows
nrows_b = Bget_storer('df')nrows
a_chunk_size = 1000000
b_chunk_size = 1000000

for a in xrange(int(nrows_a / a_chunk_size) + 1):

    a_start_i = a * a_chunk_size
    a_stop_i  = min((a + 1) * a_chunk_size  nrows_a)

    a = Aselect('df'  start = a_start_i  stop = a_stop_i)

    for b in xrange(int(nrows_b / b_chunk_size) + 1):

        b_start_i = b * chunk_size
        b_stop_i = min((b + 1) * b_chunk_size  nrows_b)

        b = Bselect('df'  start = b_start_i  stop = b_stop_i)

        # this is your result store
        storeappend('df_result'  merge_a_b())
",644898.0,,644898.0,,2013-01-31 03:35:09,2013-01-31 03:35:09,4.0
10147050,2,10145224,2012-04-13 19:22:11,4,"From docs:


  on: Columns (names) to join on Must be found in both the left and
  right DataFrame objects If not passed and left_index and right_index
  are False  the intersection of the columns in the DataFrames will be
  inferred to be the join keys


I don't know why this is not in the docstring  but it explains your problem 

You can either give left_index and right_index:

In : pndmerge(freq  hist  right_index=True  left_index=True)
Out:
        freq  count
series
0       001      1
1       004      4
2       014     14
3       012     12
4       021     21
5       014     14
6       017     17
7       007      7
8       005      5
9       001      1
10      001      1
11      003      3


Or you can make your index a column and use on:

In : freq2 = freqreset_index()

In : hist2 = histreset_index()

In : pndmerge(freq2  hist2  on='series')
Out:
    series  freq  count
0        0  001      1
1        1  004      4
2        2  014     14
3        3  012     12
4        4  021     21
5        5  014     14
6        6  017     17
7        7  007      7
8        8  005      5
9        9  001      1
10      10  001      1
11      11  003      3


Alternatively and more simply  DataFrame has join method which does exactly what you want:

In : freqjoin(hist)
Out:
        freq  count
series
0       001      1
1       004      4
2       014     14
3       012     12
4       021     21
5       014     14
6       017     17
7       007      7
8       005      5
9       001      1
10      001      1
11      003      3
",843822.0,,,,,2012-04-13 19:22:11,2.0
14020738,2,14020365,2012-12-24 11:26:09,3,"You could use the DataFrameadd method if the values in the nested dicts were numbers rather than strings For example:

import pandas as pd

dict1 = {'M1': {'H': 1  'J' : 2}  'M2': {'H': 1  'J' : 2} 
         'M3': {'H': 1  'J' : 2}}
dict2 = {'M1': {'H': 4  'J' : 6}  'M2': {'H': 2  'J' : 5} 
         'M4': {'H': 9  'J' : 8}}

df1 = pdDataFrame(dict1)T
df2 = pdDataFrame(dict2)T

print(df1)

#     H  J
# M1  1  2
# M2  1  2
# M3  1  2

print(df2)
#     H  J
# M1  4  6
# M2  2  5
# M4  9  8

print(df1add(df2  fill_value = 0))

#     H  J
# M1  5  8
# M2  3  7
# M3  1  2
# M4  9  8


If you show the data in the csv files  perhaps we can suggest how to read it in so that the values are numbers rather than strings

Alternatively  you could convert the strings to numbers after the csv has been parsed:

In [1]: dict1 = {'M1': {'H': '1'  'J' : '2'}  'M2': {'H': '1'  'J' : '2'}  'M3': {'H': '1'  'J' : '2'}}

In [2]: dict1 = {key:{k:int(v) for k v in dctitems()} for key dct in dict1items()}

In [3]: dict1
Out[3]: {'M1': {'H': 1  'J': 2}  'M2': {'H': 1  'J': 2}  'M3': {'H': 1  'J': 2}}


but I think it would be preferable to parse it correctly from the beginning  rather than patch it up this way later

If the dicts contain both numerical and string values  then you could combine them using a join  followed by a groupy and aggregation For example 

import pandas as pd
import numpy as np

def combine(values):
    if any(isinstance(v  basestring) for v in values):
        result = valuesdropna()tolist()
    else:
        result = valuessum()
    return result

dict1 = { 'M1': {'H': 1  'J' : 2  'D' : 'ABC/DEF1txt'} 
          'M2': {'H': 1  'J' : 2  'D' : 'ABC/DEF2txt'} 
          'M3': {'H': 1  'J' : 2  'D' : 'ABC/DEF3txt'} }
dict2 = { 'M1': {'H': 4  'J' : 6  'D' : 'ABC/DEF1txt'} 
          'M2': {'H': 2  'J' : 5  'D' : 'ABC/DEF2txt'} 
          'M4': {'H': 9  'J' : 8  'D' : 'ABC/DEF3txt'}}

df1 = pdDataFrame(dict1)T
df2 = pdDataFrame(dict2)T
df = df1join(df2  rsuffix = '_'  how = 'outer')T
grouped = dfgroupby(lambda label: labelrstrip('_'))
print(groupedaggregate(combine)T)


yields

                               D  H  J
M1  [ABC/DEF1txt  ABC/DEF1txt]  5  8
M2  [ABC/DEF2txt  ABC/DEF2txt]  3  7
M3                [ABC/DEF3txt]  1  2
M4                [ABC/DEF3txt]  9  8
",190597.0,,190597.0,,2012-12-26 11:36:52,2012-12-26 11:36:52,1.0
14306902,2,14300137,2013-01-13 18:48:58,4,"Try passing columns of the DataFrame directly to matplotlib instead of extracting them as numpy arrays  as in the examples below

df = pdDataFrame(nprandomrandn(10 2)  columns=['col1' 'col2'])
df['col3'] = nparange(len(df))**2 * 100 + 100

In [5]: df
Out[5]: 
       col1      col2  col3
0 -1000075 -0759910   100
1  0510382  0972615   200
2  1872067 -0731010   500
3  0131612  1075142  1000
4  1497820  0237024  1700


Vary scatter point size based on another column

pltscatter(dfcol1  dfcol2  s=dfcol3)




Vary scatter point color based on another column

colors = npwhere(dfcol3 > 300  'r'  'k')
pltscatter(dfcol1  dfcol2  s=120  c=colors)




Scatter plot with legend

However  the easiest way I've found to create a scatter plot with legend is to call pltscatter once for each point type

cond = dfcol3 > 300
subset_a = df[cond]dropna()
subset_b = df[~cond]dropna()
pltscatter(subset_acol1  subset_acol2  s=120  c='b'  label='col3 > 300')
pltscatter(subset_bcol1  subset_bcol2  s=60  c='r'  label='col3 <= 300') 
pltlegend()




Update

From what I can tell  matplotlib simply skips points with NA x or y coordinates or NA style settings (eg  color/size)  To find points skipped due to NA  try the isnull method: df[dfcol3isnull()]

To split a list of points into many types  take a look at numpy select  which is a vectorized if-then-else implementation and accepts an optional default value  For example:

df['subset'] = npselect([dfcol3 < 150  dfcol3 < 400  dfcol3 < 600] 
                         [0  1  2]  -1)
for color  label in zip('bgrm'  [0  1  2  -1]):
    subset = df[dfsubset == label]
    pltscatter(subsetcol1  subsetcol2  s=120  c=color  label=str(label))
pltlegend()


",243434.0,,243434.0,,2013-01-16 07:45:17,2013-01-16 07:45:17,4.0
12060886,2,12047193,2012-08-21 18:28:30,2,"Here's the shortest code that will do the job:

from pandas import DataFrame
df = DataFrame(resoverallfetchall())
dfcolumns = resoverallkeys()


You can go fancier and parse the types as in Paul's answer",243238.0,,,,,2012-08-21 18:28:30,1.0
14487936,2,12047193,2013-01-23 19:38:02,0,"Via mikebmassey from a similar question

import pyodbc
import pandasiosql as psql

cnxn = pyodbcconnect(connection_info) 
cursor = cnxncursor()
sql = ""SELECT * FROM TABLE""

df = psqlframe_query(sql  cnxn)
cnxnclose()
",386279.0,,,,,2013-01-23 19:38:02,
12252786,2,12250475,2012-09-03 18:52:56,3,"How about something like this:

In [103]: print data
EUR MS 3M;20111025;7d;11510
EUR MS 3M;20111024;7d;11530
EUR MS 3M;20111025;1m;11580
EUR MS 3M;20111024;1m;11590

In [104]: frame = pdread_csv(StringIO(data)  sep=';'  
                              names=['frame_name'  'index'  'column'  'value'])
In [105]: name = frameix[0  'frame_name']

In [106]: rs = frameix[:  1:]pivot('index'  'column'  'value')

In [107]: rsname = name

In [108]: rs
Out[108]:
column       1m     7d
index
20111024  1159  1153
20111025  1158  1151
",1306530.0,,,,,2012-09-03 18:52:56,1.0
12514711,2,12514590,2012-09-20 14:22:29,2,"The parameter na_values must be ""list like"" (see this answer)

A string is ""list like"" so:

na_values='abc' # would transform the letters 'a'  'b' and 'c' each into `nan`
# is equivalent to
na_values=['a' 'b' 'c']`


Similarly:

na_values=''
# is equivalent to
na_values=[] # and this is not what you want!


This means that you need to use na_values=['']",1240268.0,,1240268.0,,2012-09-20 14:54:46,2012-09-20 14:54:46,4.0
12516156,2,12514590,2012-09-20 15:41:55,1,"What version of pandas are you on? Interpreting empty string as NaN is the default behavior for pandas and seem to parse the empty strings fine in your data snippet both in v073 and current master without using the na_values parameter at all

In [10]: data = """"""\
10/08/2012 12:10:10 name1 081 402 50;185701400N 4;077693770E 792 1050 00106 430 00301
10/08/2012 12:10:11 name2     1087 140 00099 970 00686
""""""

In [11]: read_csv(StringIO(data)  header=None)T
Out[11]: 
                   0           1
X1       10/08/2012  10/08/2012
X2         12:10:10    12:10:11
X3            name1       name2
X4             081         NaN
X5             402         NaN
X6   50;185701400N         NaN
X7    4;077693770E         NaN
X8             792       1087
X9             105         14
X10          00106      00099
X11             43         97
X12          00301      00686
",1306530.0,,,,,2012-09-20 15:41:55,
12847586,2,12829428,2012-10-11 20:03:34,3,"Try (requires pandas >= 081):

splits = x['name']split()
df['first'] = splitsstr[0]
df['last'] = splitsstr[1]
",776560.0,,,,,2012-10-11 20:03:34,1.0
12976590,2,12974404,2012-10-19 14:39:59,2,"As @meteore points out  it's a problem with the string repr of the npdatetime64 type in NumPy 16x
The underlying data  should still be correct To workaround this problem  you can do something like:

In [15]: df
Out[15]: 
   L                  TS         V
0  A 2000-01-01 00:00:00  0752035
1  A 2000-01-01 04:00:00 -1047444
2  A 2000-01-01 08:00:00  1177557
3  B 2000-01-01 12:00:00  0394590
4  B 2000-01-01 16:00:00  1835067
5  B 2000-01-01 20:00:00 -0768274
6  C 2000-01-02 00:00:00 -0564037
7  C 2000-01-02 04:00:00 -2644367
8  C 2000-01-02 08:00:00 -0571187
9  C 2000-01-02 12:00:00  1618557

In [16]: dfTSastype(object)min()
Out[16]: datetimedatetime(2000  1  1  0  0)

In [17]: dfTSastype(object)max()
Out[17]: datetimedatetime(2000  1  2  12  0)
",1306530.0,,,,,2012-10-19 14:39:59,4.0
13983540,2,13983498,2012-12-21 02:34:32,2,"Still pretty hacky  but it works:

import pandas as pd
import matplotlibpyplot as plt
import numpy as np

# Create original dataframe
df = pdDataFrame(nprandomrand(5 4)  index=['art' 'mcf' 'mesa' 'perl' 'gcc'] 
                        columns=['pol1' 'pol2' 'pol3' 'pol4'])
# Estimate average
average = dfmean()
averagename = 'average'

# Append dummy row with zeros and then average
row = pdDataFrame([dict({p:00 for p in dfcolumns})  ])

df = dfappend(row)
df = dfreindex(npwhere(dfindex  dfindex  ''))
df = dfappend(average)
print df

dfplot(kind='bar')
pltshow()


",190597.0,,,,,2012-12-21 02:34:32,
14268804,2,14262433,2013-01-10 22:57:22,3,"I routinely use tens of gigabytes of data in just this fashion
eg I have tables on disk that I read via queries  create data and append back
see the docs at http://pandaspydataorg/pandas-docs/dev/iohtml#hdf5-pytables

here is a thread of doing (partially at least what I think u want to do)
and some suggestions in how to store your data
https://groupsgooglecom/forum/m/?fromgroups#!topic/pydata/cmw1F3OFJSc (see the later parts of the post)

can u give an example of what you want to accomplish?

if you can provide some more details  like:

size of data (approximately)  # of rows  columns  types of columns; are you appending
rows at all  or just columns? 
what do your typical operations look like:
eg do a query on these columns to select a bunch of rows and specific columns 
    then do an operation (in-memory) like so  create new columns  save these
describe these (can you give a toy example)
after that processing  then what do you do? is step 2 ad hoc  or repeatable?
your input flat files: how many  rough total size in Gb  are these organized by say records and each one contains different fields  or do they have some records per file with all of the fields in each file?
do you ever select subsets of rows (records) based on criteria (eg select the rows with field A > 5)? and then do something  or do you just select fields A B C with all of the records (and then do something)?
do you 'work on' all of your columns (in groups)  or are there say a good proportion that you may only use for say reports (eg you want to keep the data around  but you don't need to pull in that column explicity  until say final results time)?
pls give as much detail as you can; I can help you develop a structure 

solution

make sure you have 0101-dev installed (the following is pseudo code)

read up on these: 

http://pandaspydataorg/pandas-docs/dev/iohtml#iterating-through-files-chunk-by-chunk

http://pandaspydataorg/pandas-docs/dev/iohtml#multiple-table-queries

Since pytables is optimized to really operate on rows (and in fact that's what you query on)  we will create a table for each group of fields This way its easy to select a small group of fields (this will of course work with a big table  but it is more efficient to do it this way - I think I may be able to fix this limitation in the future - but this is more intuitive anyhow)

import numpy as np
import pandas as pd

# create a store
store = pdHDFStore('mystoreh5')

# this is the key to your storage:
#    this maps your fields to a specific group  and defines what you want to have as 
#    data_columns
#    you might want to create a nice class wrapping this
#    (as you will want to have this map and its inversion)  
group_map = dict(
    A = dict(fields = ['field_1' 'field_2' ]  dc = ['field_1''field_5']) 
    B = dict(fields = ['field_10'         ]  dc = ['field_10']) 
    
    REPORTING_ONLY = dict(fields = ['field_1000' 'field_1001']  dc = []) 

)

group_map_inverted = dict()
for g  v in group_mapitems():
    group_map_invertedupdate(dict([ (f g) for f in v['fields'] ]))


Reading in the files and creating the storage (we are essentially going to do what append_to_multiple does:

for f in files:

   # read in the file  addtl options hmay be necessary here
   # the chunksize is not strictly necessary  you may be able to slurp each file 
   # into memory in which case just eliminate this part of the loop 
   # (you can also change chunksize if necessary)
   for chunk in pdread_table(f  chunksize=50000):

       # we are going to append to each table by group
       # we are not going to create indexes at this time
       # but ARE going to create (some) data_columns

       # figure out the field groupings
       for g  v in group_mapitems():

             # create the frame for this group
             frame = chunkreindex(columns = v['fields'  copy = False)    

             # append it
             storeappend(g  frame  index=False  data_columns = v['dc'])


Ok  now you have all of the tables in the file (actually you could store them in separate files if you wish  you would prob have to add the filename to the group_mapbut prob won't be ncessary

This is how you get columns and create new ones:

frame = storeselect(group_that_I_want)

# you can optionally specify: columns = a list of the columns IN THAT GROUP (if you wanted to select only say 3 out of the 20 columns in this sub-table)
# and a where clause if you want a subset of the rows

# do calculations on this frame
new_frame = cool_function_on_frame(frame)

# to 'add colums'  create a new group (you prob want to limint the columns in this new_group
# to be only NEW ones (eg so you don't overlap from the other tables) - add this info to the group_map
storeappend(new_group  new_framereindex(columns = new_columns_created  copy = False)  data_columns = new_columns_created)


When you are ready for post_processing

# This may be a bit tricky; let me know you are actually doing; I may need to modify this function to be a bit more general
report_data = storeselect_as_multiple([ groups_1  groups_2etc ]  where =['field_1>0'  'field_1000=foo']  selector = group_1)


Note about data_columns You don't actually need to define ANY data_columns; they allow you to sub-select rows based on the column eg something like:

storeselect(group  where = [ 'field_1000=foo'  'field_1001>0' ])


So they may be most interesting to you in the final report generation stage (essentially a data column is segregated from other columns  you might impact efficiency somewhat if you define a lot)

you also might want to create a function which takes a list of fields  lookups up the groups in the groups_map  then selects these and concetates the results so you get the resulting frame (this is essentially what select_as_multiple does) - that way the structure would be pretty transparent to you

you might want to create indexes on certain data columns (makes row-subsetting much faster)
you may want to enable compression

let me know when you have questions!",644898.0,,644898.0,,2013-01-11 22:07:03,2013-01-11 22:07:03,13.0
14287518,2,14262433,2013-01-11 22:11:52,2,"This is the case for pymongo  I have also prototyped using sql server  sqlite  HDF  ORM (SQLAlchemy) in python  First and foremost pymongo is a document based DB so each person would be a document (dict of attributes)  Many people form a collection and you can have many collections (people  stock market  income)

pddateframe -> pymongo Note: I use the chunksize in read_csv to keep it to 5 to 10k records(pymongo drops the socket if larger)

aCollectioninsert((a[1]to_dict() for a in dfiterrows()))


querying: gt = greater than

pdDataFrame(list(mongoCollectionfind({'anAttribute':{'$gt':2887000  '$lt':2889000}})))


find returns an iterator so I commonly use ichunked to chop into smaller iterators  

How about a join since I normally get 10 data sources to paste together:

aJoinDF = pandasDataFrame(list(mongoCollectionfind({'anAttribute':{'$in':Att_Keys}})))


then (in my case sometimes i have to agg on aJoinDF first before its ""mergeable"")

df = pandasmerge(df  aJoinDF  on=aKey  how='left')


And you can then write the new info to your main collection via the update method below (logical collection vs physical datasources)

collectionupdate({primarykey:foo} {key:change})


On smaller lookups just denormalize  eg you have code in the document and you just add the field code txt and do a dict lookup as you create documents

Now you have a nice dataset around a person you can unleash your logic on each case and make more attributes Finally you can read into pandas your 3 to memory max key indicators and do pivots/agg/data exploration  This works for me for 3 million records with numbers/big text/categories/codes/floats/

You can also use the two methods built into MongoDB (map reduce and agg framework)  See mongodb_agg_framework as it seems to be easier than map reduce and looks handy for quick aggregate work  Notice I didn't need to define my fields or relations  I can add items to a document  So at the current state of the rapidly changing numpy  pandas  python toolset  MongoDB helps me just get to work :)",1649635.0,,,,,2013-01-11 22:11:52,2.0
14586194,2,14583576,2013-01-29 15:19:02,2,"I'm not sure I understand your question correctly; maybe this one below works?

data['Cat1'][data['Counter']rank(ascending=0) - 1]


--EDIT--

As in the comment  my solution would be 

data['ranking'] = datagroupby('Cat1')['Counter']rank(ascending=0)


I can't think of anything else  sorry Maybe others will have a different perspective",567989.0,,567989.0,,2013-01-29 18:51:05,2013-01-29 18:51:05,6.0
10132552,2,10130734,2012-04-12 22:04:48,0,"Check out the set_value method (which returns a reference to a new object if the size if mutated) But don't expect it to be fast (compared with a nested dict):

In [7]: prices
Out[7]: 
Empty DataFrame
Columns: array([AAPL  GOOG]  dtype=object)
Index: array([]  dtype=object)

In [8]: prices = pricesset_value(t1  'AAPL'  5)

In [9]: prices
Out[9]: 
                            AAPL  GOOG
2012-04-12 18:02:28178331     5   NaN


It would be nice to add a method at some point for more efficiently resizing a DataFrame by gluing on data at the end (NumPy does have a facility for this) ",776560.0,,,,,2012-04-12 22:04:48,1.0
12449785,2,12406162,2012-09-16 19:03:26,2,"HYRY explained why you get the KeyError
To plot with slices using matplotlib you can do:

In [157]: plot(test['x'][5:10]values)
Out[157]: [<matplotliblinesLine2D at 0xc38348c>]

In [158]: plot(test['x'][5:10]reset_index(drop=True))
Out[158]: [<matplotliblinesLine2D at 0xc37e3cc>]


x  y plotting in one go with 073

In [161]: test[5:10]set_index('x')['y']plot()
Out[161]: <matplotlibaxesAxesSubplot at 0xc48b1cc>
",1548051.0,,,,,2012-09-16 19:03:26,5.0
12627465,2,12625650,2012-09-27 18:03:33,5,"Have a look at df['column_label]str
Below example will drop all rows where column A holds 'a' character and 'B' equals 20

In [46]: df
Out[46]:
     A   B
0  foo  10
1  bar  20
2  baz  30

In [47]: cond = df['A']strcontains('a') & (df['B'] == 20)

In [48]: dfdrop(df[cond]indexvalues)
Out[48]:
     A   B
0  foo  10
2  baz  30
",1548051.0,,,,,2012-09-27 18:03:33,1.0
13790058,2,12898266,2012-12-09 17:57:59,0,"This gives every gives fourth row in test starting at 'i':

testix[i::4] 


Using the same basic loop as above  just append the set of every forth row starting at 0 to 3 after you run your code above

data = []    
for i in range(0 3:):     
    temp = testix[i::4] 
    dataappend(temp)
test2 = pdconcat(data ignore_index=True)


Update:
I realize now that what's you'd want isn't every fourth row but every mth row  so this would just be the loop suggestions above Sorry 

Update 2:
Maybe not We can take advantage of the fact that even though concatenate doesn't return the order you want what it does return has a fixed mapping to what you do want d is the number of rows per timestamp and m is the number of timestamps 

You seem to want the rows from test as follows:
[0 m 2m 3m 1 m+1 2m+1 3m+1 2 m+2 2m+2 3m+2  m-1 2m-1 3m-1 4m-1]

I'm sure there are much nicer ways to generate that list of indices  but this worked for me

d = 4
m = 10
small = (nparange(0 m)reshape(m 1)repeat(d 1)Treshape(-1 1))
shifter = (nparange(0 d)repeat(m)reshape(-1 1)T * m) 
NewIndex = (shifterreshape(d -1) + smallreshape(d -1))Treshape(-1 1)
NewIndex = NewIndexreshape(-1)
test = testix[NewIndex]
",1563557.0,,1563557.0,,2012-12-09 23:58:38,2012-12-09 23:58:38,2.0
13028965,2,13028796,2012-10-23 10:58:53,1,"You can just delete nan item from the Counter instance which is dictionary-like object:

from numpy import nan
del count[nan]


BTW You can use Countermost_common method:

f = countmost_common(5)
",1739375.0,,1739375.0,,2012-10-23 11:29:11,2012-10-23 11:29:11,3.0
13029042,2,13028796,2012-10-23 11:04:15,0,"use value_counts() to count the non-Nan values:

        one       two     three
a  0196508 -0465768 -0710062
b       NaN       NaN       NaN
c  0532602  1835499  0465047
d       NaN       NaN       NaN
e  0175336 -0471934 -1517812
f -2392756 -0021330 -0239647
g       NaN       NaN       NaN
h -0612554  0238613 -1060184

df2['one']value_counts()

 0532602    1
 0196508    1
 0175336    1
-0612554    1
-2392756    1
",1199589.0,,,,,2012-10-23 11:04:15,
13332422,2,13331698,2012-11-11 15:13:10,0,"As indicated you can apply directly the function Given:

import pandas as p
import numpy as np
df = pDataFrame(nparange(12)reshape(3 4))


where df:

     0   1   2   3
0    0   1   2   3
1    4   5   6   7
2    8   9   10  11


and a function:

def sumit(x  y):
    return x+y


you can do:

df[4] = sumit(df[0]  df[1])


that gives df:

     0   1   2   3   4
0    0   1   2   3   1
1    4   5   6   7   9
2    8   9   10  11  17
",308903.0,,,,,2012-11-11 15:13:10,
13552256,2,13548721,2012-11-25 15:07:04,1,"If you're looking to do operations between the column and lagged values  you should be doing something like testOpen / testOpenshift()
shift realigns the data and takes an optional number of periods",1306530.0,,,,,2012-11-25 15:07:04,1.0
13788201,2,13785932,2012-12-09 14:17:10,1,"There is little point in changing the index itself - since you can just generate using date_range with the desired frequency parameter as in your question 

I assume what you are trying to do is change the frequency of a Time Series that contains data  in which case you can use resample (documentation) For example if you have the following time series:

dt_index = pddate_range('2012-1-1 00:00001' periods=3  freq='1ms')
ts = pdSeries(randn(3)  index=dt_index)


2012-01-01 00:00:00           0594618
2012-01-01 00:00:00001000    0874552
2012-01-01 00:00:00002000   -0700076
Freq: L


Then you can change the frequency to seconds using resample  specifying how you want to aggregate the values (mean  sum etc):

tsresample('S'  how='sum')

2012-01-01 00:00:00    0594618
2012-01-01 00:00:01    0174475
Freq: S
",1452002.0,,,,,2012-12-09 14:17:10,2.0
13788301,2,13785932,2012-12-09 14:28:02,2,"Whilst @Matti's answer is clearly the correct way to deal with your situation  I thought I would add an answer how you might round a Timestamp to the nearest second:

from pandaslib import Timestamp

t1 = Timestamp('2012-1-1 00:00:00')
t2 = Timestamp('2012-1-1 00:00:00000333')

In [4]: t1
Out[4]: <Timestamp: 2012-01-01 00:00:00>

In [5]: t2
Out[5]: <Timestamp: 2012-01-01 00:00:00000333>

In [6]: t2microsecond
Out[6]: 333

In [7]: t1value
Out[7]: 1325376000000000000L

In [8]: t2value
Out[8]: 1325376000000333000L

# Alternatively: t2value - t2value % 1000000000
In [9]: long(round(t2value  -9)) # round milli-  micro- and nano-seconds
Out[9]: 1325376000000000000L

In [10]: Timestamp(long(round(t2value  -9)))
Out[10]: <Timestamp: 2012-01-01 00:00:00>


Hence you can apply this to the entire index:

def to_the_second(ts):
    return Timestamp(long(round(tsvalue  -9)))

dtindexmap(to_the_second)
",1240268.0,,1240268.0,,2012-12-09 15:05:37,2012-12-09 15:05:37,3.0
14149319,2,14149156,2013-01-04 00:10:22,3,"You could use get_level_values:

firsts = df1indexget_level_values('first')
df1['value2'] = df2ix[firsts]values


Note: you are almost doing a join here (except the df1 is MultiIndex) so there may be a neater way to describe this



In an example (similar to what you have):

df1 = pdDataFrame([['a'  'x'  0123]  ['a' 'x'  0234] 
                    ['a'  'y'  0451]  ['b'  'x'  0453]] 
                   columns=['first'  'second'  'value1']
                   )set_index(['first'  'second'])
df2 = pdDataFrame([['a'  10] ['b'  20]] 
                   columns=['first'  'value'])set_index(['first'])

firsts = df1indexget_level_values('first')
df1['value2'] = df2ix[firsts]values

In [5]: df1
Out[5]: 
              value1  value2
first second                
a     x        0123      10
      x        0234      10
      y        0451      10
b     x        0453      20
",1240268.0,,1240268.0,,2013-01-04 00:17:39,2013-01-04 00:17:39,
14208134,2,14149156,2013-01-08 04:17:51,1,"As the ix syntax is a powerful shortcut to reindexing  but in this case you are actually not doing any combined rows/column reindexing  this can be done a bit more elegantly (for my humble taste buds) with just using reindexing:

Preparation from hayden:

df1 = pdDataFrame([['a'  'x'  0123]  ['a' 'x'  0234] 
                    ['a'  'y'  0451]  ['b'  'x'  0453]] 
                   columns=['first'  'second'  'value1']
                   )set_index(['first'  'second'])
df2 = pdDataFrame([['a'  10] ['b'  20]] 
                   columns=['first'  'value'])set_index(['first'])


Then this looks like this in iPython:

In [4]: df1
Out[4]: 
              value1
first second        
a     x        0123
      x        0234
      y        0451
b     x        0453

In [5]: df2
Out[5]: 
       value
first       
a         10
b         20

In [7]: df2reindex(df1index  level=0)
Out[7]: 
              value
first second       
a     x          10
      x          10
      y          10
b     x          20

In [8]: df1['value2'] = df2reindex(df1index  level=0)

In [9]: df1
Out[9]: 
              value1  value2
first second                
a     x        0123      10
      x        0234      10
      y        0451      10
b     x        0453      20


The mnemotechnic for what level you have to use in the reindex method:
It states for the level that you already covered in the bigger index
So  in this case df2 already had level 0 covered of the df1index",680232.0,,,,,2013-01-08 04:17:51,
14037726,2,14035148,2012-12-26 08:05:10,1,You can use a library like PyNIO to read your file into pe numpy arrays and feed them to pandasPyNIO allows reading several file formats including classic netCDF3 and netCDF4netcdf4-python can also read these netCDF formats and is py33 compatible,308903.0,,,,,2012-12-26 08:05:10,
14492358,2,14035148,2013-01-24 01:18:17,0,"If your NetCDF file (or OPeNDAP dataset) follows CF Metadata conventions you can take advantage of them by using the NetCDF4-Python package  which makes accessing them in Pandas really easy  (I'm using the Enthought Python Distribution which includes both Pandas and NetCDF4-Python) 

In the example below  the NetCDF file is being served via OPeNDAP  and the NetCDF4-Python library lets you open and work with a remote OPeNDAP dataset just as if it was a local NetCDF file  which is pretty slick  If you want to see the attributes of the NetCDF4 file  point your browser at: http://geoport-devwhoiedu/thredds/dodsC/HUDSON_SVALLEY/5951adc-a1hnchtml

You should be able to run this without changes:

from matplotlib import pyplot as plt
import pandas as pd
import netCDF4

url='http://geoport-devwhoiedu/thredds/dodsC/HUDSON_SVALLEY/5951adc-a1hnc'
vname = 'Tx_1211'
station = 0

nc = netCDF4Dataset(url)
h = ncvariables[vname]
times = ncvariables['time']
jd = netCDF4num2date(times[:] timesunits)
hs = pdSeries(h[: station] index=jd)

fig = pltfigure(figsize=(12 4))
ax = figadd_subplot(111)
hsplot(ax=ax title='%s at %s' % (hlong_name ncid))
axset_ylabel(hunits)


The result may be seen here in the Ipython Notebook: 
http://nbvieweripythonorg/4615153/",2005869.0,,,,,2013-01-24 01:18:17,
14447383,2,14334898,2013-01-21 21:18:36,0,Ok  this seemed to be a bug/feature of pandas 090 and/or statsmodels 042 Upgraded to pandas 0100 and statsmodels 050 and everything works fine now,1073420.0,,,,,2013-01-21 21:18:36,
14543051,2,14542145,2013-01-27 00:15:40,2,"It doesn't look like it's a well publicized feature yet  but you can use expanding_apply to achieve the returns calculation:

In [1]: s
Out[1]:
0    012
1   -013
2    023
3    017
4    029
5   -011

In [2]: pdexpanding_apply(s  lambda s: reduce(lambda x  y: x * (1+y)  s  1))

Out[2]:
0    1120000
1    0974400
2    1198512
3    1402259
4    1808914
5    1609934


I'm not 100% certain  but I believe expanding_apply works on the applied series starting from the first index through the current index  I use the built-in reduce function that works exactly like your Clojure function

Docstring for expanding_apply:

Generic expanding function application

Parameters
----------
arg : Series  DataFrame
func : function
    Must produce a single value from an ndarray input
min_periods : int
    Minimum number of observations in window required to have a value
freq : None or string alias / date offset object  default=None
    Frequency to conform to before computing statistic
center : boolean  default False
    Whether the label should correspond with center of window

Returns
-------
y : type of input argument
",919872.0,,,,,2013-01-27 00:15:40,2.0
14544216,2,14542145,2013-01-27 03:47:44,2,"It's worth noting that it's often faster (as well as easier to understand) to write more verbosely in pandas  rather than write as a reduce

In your specific example I would just add and then cumprod:

In [2]: cadd(1)cumprod()
Out[2]: 
0    1120000
1    0974400
2    1198512
3    1402259
4    1808914
5    1609934


or perhaps init * cadd(1)cumprod()

Note: In some cases however  for example where memory is an issue  you may have to rewrite these in a more low-level/clever way  but it's usually worth trying the simplest method first (and testing against it eg using %timeit or profiling memory)",1240268.0,,1240268.0,,2013-01-27 06:11:13,2013-01-27 06:11:13,5.0
11042986,2,11041411,2012-06-15 00:00:13,4,"Not sure which version of pandas you are using but with 073 you can export your DataFrame to a TSV file and retain the indices by doing this:

dfto_csv('mydftsv'  sep='\t')


The reason you need to export to TSV versus CSV is since the column headers have   characters in them This should solve the first part of your question 

The second part gets a bit more tricky since from as far as I can tell  you need to beforehand have an idea of what you want your DataFrame to contain In particular  you need to know:

Which columns on your TSV represent the row MultiIndex
and that the rest of the columns should also be converted to a MultiIndex
To illustrate this  lets read back the TSV file we saved above into a new DataFrame:

In [1]: t_df = read_table('mydftsv'  index_col=[0 1 2])
In [2]: all(t_dfindex == dfindex)
Out[2]: True


So we managed to read mydftsv into a DataFrame that has the same row index as the original df But:

In [3]: all(t_dfcolumns == dfcolumns)
Out[3]: False


And the reason here is because pandas (as far as I can tell) has no way of parsing the header row correctly into a MultiIndex As I mentioned above  if you know beorehand that your TSV file header represents a MultiIndex then you can do the following to fix this:

In [4]: from ast import literal_eval
In [5]: t_dfcolumns = MultiIndexfrom_tuples(t_dfcolumnsmap(literal_eval)tolist()  
                                              names=['one' 'two' 'three'])
In [6]: all(t_dfcolumns == dfcolumns)
Out[6]: True
",696023.0,,,,,2012-06-15 00:00:13,1.0
14344104,2,11041411,2013-01-15 18:11:51,1,"You can change the print options using set_option:


  displaymulti_sparse:: boolean
       Default True  ""sparsify"" MultiIndex display
    (don't display repeated
          elements in outer levels within groups)


Now the DataFrame will be printed as desired: 

In [11]: pdset_option('multi_sparse'  False)

In [12]: df
Out[12]: 
one             A   A   A   A   A   A   A   A   A  A2  A2  A2  A2  A2  A2  A2  A2  A2
two             B   B   B  B2  B2  B2  B3  B3  B3   B   B   B  B2  B2  B2  B3  B3  B3
three           C  C2  C3   C  C2  C3   C  C2  C3   C  C2  C3   C  C2  C3   C  C2  C3
n location sex                                                                       
0 North    M    2   1   6   4   6   4   7   1   1   0   4   3   9   2   0   0   6   4
1 East     F    3   5   5   6   4   8   0   3   2   3   9   8   1   6   7   4   7   2
2 West     M    7   9   3   5   0   1   2   8   1   6   0   7   9   9   3   2   2   4
3 South    M    1   0   0   3   5   7   7   0   9   3   0   3   3   6   8   3   6   1
4 South    F    8   0   0   7   3   8   0   8   0   5   5   6   0   0   0   1   8   7
5 West     F    6   5   9   4   7   2   5   6   1   2   9   4   7   5   5   4   3   6
6 North    M    3   3   0   1   1   3   6   3   8   6   4   1   0   5   5   5   4   9
7 North    M    0   4   9   8   5   7   7   0   5   8   4   1   5   7   6   3   6   8
8 East     F    5   6   2   7   0   6   2   7   1   2   0   5   6   1   4   8   0   3
9 South    M    1   2   0   6   9   7   5   3   3   8   7   6   0   5   4   3   5   9


Note: in older pandas versions this was pdset_printoptions(multi_sparse=False)",1240268.0,,1240268.0,,2013-01-15 18:38:55,2013-01-15 18:38:55,
11548224,2,11548005,2012-07-18 18:43:27,7,"NaN can't be stored in an integer array This is a known limitation of pandas at the moment; I have been waiting for progress to be made with NA values in NumPy (similar to NAs in R)  but it will be at least 6 months to a year before NumPy gets these features  it seems:

http://pandaspydataorg/pandas-docs/stable/gotchashtml#support-for-integer-na",776560.0,,,,,2012-07-18 18:43:27,
13316015,2,13143050,2012-11-09 21:05:15,1,"It'd be nice to have a feature to help to pairwise column ops:

https://githubcom/pydata/pandas/issues/2212",776560.0,,,,,2012-11-09 21:05:15,
13407863,2,13405611,2012-11-15 23:09:07,3,"In [7]: pandasrolling_apply(x2  3  foo  min_periods=2)
Out[7]: 
0   NaN
1     1
2     3
3     6
4     9
5    12
6     9
7    12
8    15
9    24
",1306530.0,,,,,2012-11-15 23:09:07,
13652205,2,13636848,2012-11-30 19:56:02,1,"http://pandaspydataorg/pandas-docs/dev/merginghtml does not have a hook function to do this on the fly Would be nice though

I would just do a separate step and use difflib getclosest_matches to create a new column in  one of the 2 dataframes and the merge/join on the fuzzy matched column ",239007.0,,,,,2012-11-30 19:56:02,1.0
13680953,2,13636848,2012-12-03 10:06:04,0,"Similar to @locojay suggestion  you can apply difflib's get_closest_matches to df2's index and then apply a join:

In [23]: import difflib 

In [24]: difflibget_close_matches
Out[24]: <function difflibget_close_matches>

In [25]: df2index = df2indexmap(lambda x: difflibget_close_matches(x  df1index)[0])

In [26]: df2
Out[26]: 
      letter
one        a
two        b
three      c
four       d
five       e

In [31]: df1join(df2)
Out[31]: 
       number letter
one         1      a
two         2      b
three       3      c
four        4      d
five        5      e




If these were columns  in the same vein you could apply to the column then merge:

df1 = DataFrame([[1 'one'] [2 'two'] [3 'three'] [4 'four'] [5 'five']]  columns=['number'  'name'])
df2 = DataFrame([['a' 'one'] ['b' 'too'] ['c' 'three'] ['d' 'fours'] ['e' 'five']]  columns=['letter'  'name'])

df2['name'] = df2['name']apply(lambda x: difflibget_close_matches(x  df1['name'])[0])
df1merge(df2)
",1240268.0,,1240268.0,,2012-12-03 10:23:45,2012-12-03 10:23:45,1.0
13931877,2,13930367,2012-12-18 11:23:12,3,"Most numpy/scipy function require the arguments only to be ""array_like""  iterp1d is no exception Fortunately both Series and DataFrame are ""array_like"" so we don't need to leave pandas:

import pandas as pd
import numpy as np
from scipyinterpolate import interp1d

df = pdDataFrame([nparange(1  6)  [1  8  27  npnan  125]])T

In [5]: df
Out[5]: 
   0    1
0  1    1
1  2    8
2  3   27
3  4  NaN
4  5  125

df2 = dfdropna() # interpolate on the non nan
f = interp1d(df2[0]  df2[1]  kind='cubic')
#f(4) == array(639999999999992)

df[1] = df[0]apply(f)

In [10]: df
Out[10]: 
   0    1
0  1    1
1  2    8
2  3   27
3  4   64
4  5  125


Note: I couldn't think of an example off the top of my head to pass in a DataFrame into the second argument (y) but this ought to work too",1240268.0,,1240268.0,,2012-12-18 11:46:25,2012-12-18 11:46:25,6.0
14282638,2,14260251,2013-01-11 16:39:05,0,You could use converters={'KEY': eval} to make things a bit simpler It would be possible to add some more optimized version of this internally  too ,776560.0,,,,,2013-01-11 16:39:05,1.0
14380492,2,14380371,2013-01-17 13:47:25,2,"Just write to a textfile It's no magic:

import pandas as pd
df = pdDataFrame({""a"":range(10)  ""b"":range(10 20)})
with open(""my_tabletex""  ""w"") as f:
    fwrite(""\\begin{tabular}{"" + "" | ""join([""c""] * len(dfcolumns)) + ""}\n"")
    for i  row in dfiterrows():
        fwrite("" & ""join([str(x) for x in rowvalues]) + "" \\\\\n"")
    fwrite(""\\end{tabular}"")
",1156006.0,,1156006.0,,2013-01-17 13:58:43,2013-01-17 13:58:43,
8957495,2,8957175,2012-01-21 23:05:12,1,"I created a GitHub issue here 

https://githubcom/wesm/pandas/issues/663

I'm pretty sure I identified a circular reference between NumPy ndarray views causing a memory leak Just committed a fix:

https://githubcom/wesm/pandas/commit/4c3916310a86c3e4dab6d30858a984a6f4a64103

Can you install from source and let me know if that fixes your problem? 

BTW you might try using SparsePanel instead of Panel because Panel will convert all of the sub-DataFrames to dense form

Lastly  you might consider using groupby as an alternative to the O(N * M) chopping-up of the SparseDataFrame It's even shorter:


pan = dict(pdgroupby('Member'))
",776560.0,,,,,2012-01-21 23:05:12,2.0
10393475,2,10393447,2012-05-01 04:15:00,2,"Modify the lambda to pull the values into the new scope

method = property(lambda self=self  type=type: self[self['type'] == type])
",20862.0,,,,,2012-05-01 04:15:00,1.0
10404493,2,10393447,2012-05-01 21:17:26,0,"I would suggest not making a subclass of DataFrame  honestly  if you can avoid it The old Java adage ""favor composition over inheritance"" tends to be preferable in my experience ",776560.0,,,,,2012-05-01 21:17:26,1.0
11073962,2,11073609,2012-06-17 18:56:19,1,"You can group on any array/Series of the same length as your DataFrame --- even a computed factor that's not actually a column of the DataFrame  So to group by minute you can do:

dfgroupby(dfindexmap(lambda t: tminute))


If you want to group by minute and something else  just mix the above with the column you want to use:

dfgroupby([dfindexmap(lambda t: tminute)  'Source'])


Personally I find it useful to just add columns to the DataFrame to store some of these computed things (eg  a ""Minute"" column) if I want to group by them often  since it makes the grouping code less verbose",1427416.0,,,,,2012-06-17 18:56:19,1.0
11763887,2,11586989,2012-08-01 16:47:05,0,"I believe the issue has to do with specifying the tick labels for existing ticks  By default  there are fewer ticks than labels so only the first few labels are used  The following should work by first setting the number of ticks

ax1 = figadd_subplot(131)
ax1set_title(""A"")
ax1tick_params(axis='both'  direction='out')
ax1set_xticks(range(len(datacolumns)))
ax1set_xticklabels(datacolumns)
ax1set_yticks(range(len(dataindex)))
ax1set_yticklabels(dataindex)
im1 = ax1imshow(data  interpolation='nearest'  aspect='auto'  cmap=cmap)


This produces a tick for every year on the y-axis  so you might want to use a subset of the index values",1298998.0,,,,,2012-08-01 16:47:05,
12022003,2,12021754,2012-08-18 20:27:58,2,"http://pandaspydataorg/pandas-docs/stable/generated/pandasDataFrameheadhtml?highlight=head#pandasDataFramehead

df2 = dfhead(10)


should do the trick",521586.0,,,,,2012-08-18 20:27:58,
12025395,2,12021754,2012-08-19 09:02:03,1,dfix[10 :] gives you all the columns from the 10th row In your case you want everything up to the 10th row which is dfix[:9 :] Note that the right end of the slice range is inclusive: http://pandassourceforgenet/gotchashtml#endpoints-are-inclusive,243238.0,,,,,2012-08-19 09:02:03,
12342180,2,12021754,2012-09-09 19:21:23,1,"You can also do as a convenience:

df[:10]",776560.0,,,,,2012-09-09 19:21:23,
13337376,2,13331698,2012-11-12 01:39:09,3,"Here's an example using apply on the dataframe  which I am calling with axis = 1 

Note the difference is that instead of trying to pass two values to the function f  rewrite the function to accept a pandas Series object  and then index the Series to get the values needed 

In [49]: df
Out[49]: 
          0         1
0  1000000  0000000
1 -0494375  0570994
2  1000000  0000000
3  1876360 -0229738
4  1000000  0000000

In [50]: def f(x):    
   :  return x[0] + x[1]  
   :  

In [51]: dfapply(f  axis=1) #passes a Series object  row-wise
Out[51]: 
0    1000000
1    0076619
2    1000000
3    1646622
4    1000000


Depending on your use case  it is sometimes helpful to create a pandas group object  and then use apply on the group ",484596.0,,484596.0,,2012-11-12 14:57:18,2012-11-12 14:57:18,5.0
13761574,2,13757239,2012-12-07 10:48:58,3,"You are looking for apply (merge is like a database join):

In [1]: from pandas import DataFrame

In [2]: df = DataFrame([[1 11 2012] [1 10 2012]]  columns=['day' 'month' 'year'])

In [3]: df
Out[3]: 
   day month  year
0    1    11  2012
1    1    10  2012

In [4]: dfapply(lambda row: str(row['day'])+'/'+str(row['month'])+'/'+str(row['year'])  axis=1)
Out[4]: 
0    1/11/2012
1    1/10/2012


The axis=1 part means you are selecting columns rather than row

If you wanted to give a specific date you could use datetime:

In [5]: import datetime

In [6]: dfapply(lambda row: datetimedatetime(row['year'] row['month'] row['day'])  axis=1)
Out[6]: 
0    2012-11-01 00:00:00
1    2012-10-01 00:00:00


You can add these as columns in you dataframe as follows:

In [7]: df['new_date'] = dfapply(lambda row: str(row['day'])+'/'+str(row['month'])+'/'+str(row['year'])  axis=1)

In [8]: df
Out[8]: 
   day month  year   new_date
0    1    11  2012  1/11/2012
1    1    10  2012  1/10/2012




It's worth noting that pandas has an easy way to parse_dates when reading as a csv",1240268.0,,1240268.0,,2012-12-07 10:55:44,2012-12-07 10:55:44,
14092840,2,14092798,2012-12-30 17:25:44,4,"I suspect that if you do a View->Source in your browser you will see the expected <type > What's happening is that the browser thought this was an HTML tag  didn't recognize it  and simply threw it out 

Note: if I had not typed &amp;lt;type > in this answer  the same thing would have happened to my answer",17017.0,,,,,2012-12-30 17:25:44,3.0
14098124,2,14092798,2012-12-31 06:09:54,1,"A workaround is to print the dataframe  which makes it play better with the ipython notebook:

In [144]: print pdDataFrame(test_dict)

                 col1          col2
string1  <type 'str'>  <type 'str'>
string2  <type 'str'>  <type 'str'>


I find the lines in the HTML tables distracting  so this workaround is my default",1935494.0,,,,,2012-12-31 06:09:54,
14304621,2,14304506,2013-01-13 14:53:02,0,"I think this was a bug introduced in 010  namely issue #2605  
""AssertionError when using apply after GroupBy""  It's since been fixed

You can either wait for the 0101 release  which shouldn't be too long from now  or you can upgrade to the development version (either via git or simply by downloading the zip of master)",487339.0,,,,,2013-01-13 14:53:02,
10902452,2,10867028,2012-06-05 18:02:36,0,"Use the fillna method  but use it twice 'nan' = 'nan'  'NaN' = """" This would keep comma's lined up 
If the NAN werent there then the columns would not line up Remember: nan does not equal NaN ",428862.0,,,,,2012-06-05 18:02:36,2.0
11005208,2,10867028,2012-06-12 21:33:20,2,"I added a ticket to add an option of some sort here:

https://githubcom/pydata/pandas/issues/1450

In the meantime  resultfillna('') should do what you want

EDIT: in the development version (to be 080 final) if you specify an empty list of na_values  empty strings will stay empty strings in the result",776560.0,,776560.0,,2012-06-25 22:35:25,2012-06-25 22:35:25,
11419721,2,11416692,2012-07-10 18:37:44,2,"I made a github issue regarding your problem: https://githubcom/pydata/pandas/issues/1599 Please check back next week for a bug-fix release of pandas

Also  the offset alias for millisecond frequency in pandas is 'L' 'U' is the microsecond frequency alias",1306530.0,,,,,2012-07-10 18:37:44,
11950214,2,11940420,2012-08-14 10:19:09,2,"As far as i know the seperation you are looking for does not exist  but 
assuming the index is sorted (which is also required with your hack) a little change to the groupby function gives what you need

In [194]: cache = {}

In [195]: dfgroupby(lambda d: cachesetdefault(disocalendar()[:2]  d))agg(agg)
Out[195]:
             close    high     low    open     volume
2012-02-13  03546  03592  03271  03476  648333934
2012-02-20  03575  03682  03524  03590  264673454
",1548051.0,,,,,2012-08-14 10:19:09,
12133235,2,12133075,2012-08-26 19:31:37,1,Try using order  ie means = meansorder(),776560.0,,,,,2012-08-26 19:31:37,
12356541,2,12356501,2012-09-10 17:20:49,4,"I'd just use zip:

In [1]: from pandas import *

In [2]: def calculate(x):
   :     return x*2  x*3
   : 

In [3]: df = DataFrame({'a': [1 2 3]  'b': [2 3 4]})

In [4]: df
Out[4]: 
   a  b
0  1  2
1  2  3
2  3  4

In [5]: df[""A1""]  df[""A2""] = zip(*df[""a""]map(calculate))

In [6]: df
Out[6]: 
   a  b  A1  A2
0  1  2   2   3
1  2  3   4   6
2  3  4   6   9
",487339.0,,,,,2012-09-10 17:20:49,1.0
12505031,2,12504951,2012-09-20 01:34:06,7,"In [23]: %logstart /tmp/sessionlog
Activating auto-logging Current session state plus future input saved
Filename       : /tmp/sessionlog
Mode           : backup
Output logging : False
Raw input log  : False
Timestamping   : False
State          : active

In [24]: x = 1

In [25]: %logstop

In [26]: quit()
Do you really want to exit ([y]/n)? y


Then we can restore the session with:

% ipython -log /tmp/sessionlog 
Activating auto-logging Current session state plus future input saved
Filename       : ipython_logpy


In [1]: x
Out[1]: 1


For more on ""Session logging and restoring"" see the docs",190597.0,,,,,2012-09-20 01:34:06,1.0
12664893,2,12664590,2012-09-30 20:33:52,5,"Use the intercept keyword argument:

model_profit_tr = pdols(y=df_closed['Profit cum (%)']  
                         x=df_closed['Order']  
                         intercept=False)


From docs:

In [65]: help(pandasols) 
Help on function ols in module pandasstatsinterface:

ols(**kwargs)

    [snip]

    Parameters
    ----------
    y: Series or DataFrame
        See above for types
    x: Series  DataFrame  dict of Series  dict of DataFrame  Panel
    weights : Series or ndarray
        The weights are presumed to be (proportional to) the inverse of the
        variance of the observations  That is  if the variables are to be
        transformed by 1/sqrt(W) you must supply weights = 1/W
    intercept: bool
        True if you want an intercept  Defaults to True
    nw_lags: None or int
        Number of Newey-West lags  Defaults to None

    [snip]
",843822.0,,,,,2012-09-30 20:33:52,8.0
5486249,2,5486226,2011-03-30 12:29:18,1,"Yes there is:

http://wwwscipyorg/",60711.0,,,,,2011-03-30 12:29:18,
5486256,2,5486226,2011-03-30 12:29:49,9,"In pure Python  having your data in a Python list a  you could do

median = sum(sorted(a[-30:])[14:16]) / 20


(This assumes a has at least 30 items)

Using the NumPy package  you could use

median = numpymedian(a[-30:])
",279627.0,,279627.0,,2011-03-30 12:53:24,2011-03-30 12:53:24,
5486261,2,5486226,2011-03-30 12:30:22,2,"isn't the median just the middle value in a sorted range?

so  assuming your list is stock_data:

last_thirty = stock_data[-30:]
median = sorted(last_thirty)[15]


Now you just need to get the off-by-one errors found and fixed and also handle the case of stock_data being less than 30 elements

let us try that here a bit:

def rolling_median(data  window):
    if len(data) < window:
       subject = data[:]
    else:
       subject = data[-30:]
    return sorted(subject)[len(subject)/2]
",2260.0,,,,,2011-03-30 12:30:22,1.0
5486285,2,5486226,2011-03-30 12:32:22,4,"Have you considered pandas?  It is based on numpy and can automatically associate timestamps with your data  and discards any unknown dates as long as you fill it with numpynan It also offers some rather powerful graphing via matplotlib

Basically it was designed for financial analysis in python",667301.0,,667301.0,,2011-12-09 08:59:40,2011-12-09 08:59:40,1.0
10021417,2,10020591,2012-04-05 01:11:14,1,"You need to use groupby as such:

grouped = framegroupby(lambda x: xhour)
groupedagg({'radiation': npsum  'tamb': npmean})
# Same as: groupedagg({'radiation': 'sum'  'tamb': 'mean'})


with the output being:

        radiation      tamb
key_0                      
8      298581107  4883806
9      311176148  4983705
10     315531527  5343057
11     288013876  6022002
12       5527616  8507670


So in essence I am splitting on the hour value and then calculating the mean of tamb and the sum of radiation and returning back the DataFrame (similar approach to R's ddply) For more info I would check the documentation page for groupby as well as this blog post

Edit: To make this scale a bit better you could group on both the day and time as such:

grouped = framegroupby(lambda x: (xday  xhour))
groupedagg({'radiation': 'sum'  'tamb': 'mean'})
          radiation      tamb
key_0                        
(5  8)   298581107  4883806
(5  9)   311176148  4983705
(5  10)  315531527  5343057
(5  11)  288013876  6022002
(5  12)    5527616  8507670
",696023.0,,696023.0,,2012-04-05 01:24:14,2012-04-05 01:24:14,0.0
10021618,2,10020591,2012-04-05 01:45:17,2,"You can also downsample using the asof method of pandasDateRange objects

In [21]: hourly = pdDateRange(datetimedatetime(2012  4  5  8  0) 
                          datetimedatetime(2012  4  5  12  0) 
                          offset=pddatetoolsHour())

In [22]: framegroupby(hourlyasof)size()
Out[22]: 
key_0
2012-04-05 08:00:00    60
2012-04-05 09:00:00    60
2012-04-05 10:00:00    60
2012-04-05 11:00:00    60
2012-04-05 12:00:00    1
In [23]: framegroupby(hourlyasof)agg({'radiation': npsum  'tamb': npmean})
Out[23]: 
                     radiation  tamb 
key_0                                
2012-04-05 08:00:00  27154     4491
2012-04-05 09:00:00  26618     5253
2012-04-05 10:00:00  29235     4959
2012-04-05 11:00:00  28300     5489
2012-04-05 12:00:00  05414     9532
",243434.0,,,,,2012-04-05 01:45:17,1.0
14327775,2,14327664,2013-01-14 22:17:57,0,"If you print sma200  you will probably find lots of null or missing values This is because the threshold for number of non-nulls is high by default for rolling_mean

Try using

sma200 = pdrolling_mean(px  200  min_periods=2)


From the pandas docs:


  min_periods: threshold of non-null data points to require (otherwise result is NA)


You could also try changing the size of the window if your dataset is missing many points",206192.0,,,,,2013-01-14 22:17:57,2.0
14327941,2,14327664,2013-01-14 22:29:12,2,"pxffill() returns a new DataFrame To modify px itself  use inplace = True

pxffill(inplace = True)
sma200 = pdrolling_mean(px  200)
print(sma200)


yields

Data columns:
BELGBR    2085  non-null values
MSFT       2635  non-null values
dtypes: float64(2)
",190597.0,,,,,2013-01-14 22:29:12,
14550887,2,14550441,2013-01-27 18:49:51,2,"Okay  Starting from a badly formatted CSV we can't read:

>>> !cat unquotedcsv
1950's xyznl/user_003 bad  123
17th red flower xyznl/user_001 good 203
"""" xyznl/user_239 not very 345
>>> pdread_csv(""unquotedcsv""  header=None)
Traceback (most recent call last):
  File ""<ipython-input-40-7d9aadb2fad5>""  line 1  in <module>
    pdread_csv(""unquotedcsv""  header=None)
[]
  File ""parserpyx""  line 1572  in pandas_parserraise_parser_error (pandas/src/parserc:17041)
CParserError: Error tokenizing data C error: Expected 4 fields in line 2  saw 6


We can make a nicer version  taking advantage of the fact the last three columns are well-behaved:

import csv

with open(""unquotedcsv""  ""rb"") as infile  open(""quotedcsv""  ""wb"") as outfile:
    reader = csvreader(infile)
    writer = csvwriter(outfile)
    for line in reader:
        newline = [' 'join(line[:-3])] + line[-3:]
        writerwriterow(newline)


which produces

>>> !cat quotedcsv
1950's xyznl/user_003 bad  123
""17th red flower"" xyznl/user_001 good 203
 xyznl/user_239 not very 345


and then we can read it:

>>> pdread_csv(""quotedcsv""  header=None)
                 0                1         2    3
0           1950's  xyznl/user_003       bad  123
1  17th red flower  xyznl/user_001      good  203
2              NaN  xyznl/user_239  not very  345


I'd look into fixing this problem at source and getting data in a tolerable format  though  Tricks like this shouldn't be necessary  and it would have been very easy for it to be impossible to repair",487339.0,,,,,2013-01-27 18:49:51,8.0
9855938,2,9851018,2012-03-24 21:51:42,0,The extra storage is due to the row indexes being stored as 64-bit integers There is an open issue to address this memory usage issue for your use case: https://githubcom/pydata/pandas/issues/939,776560.0,,,,,2012-03-24 21:51:42,
10606901,2,10595327,2012-05-15 18:43:27,1,"Here is a solution that may do what you want:

key1 = tableindexlabels[0]
key2 = tablerank(ascending=False)

# sort by key1  then key2
sorter = nplexsort((key2  key1))

sorted_table = tabletake(sorter)


The result would look like this:

In [22]: table
Out[22]: 
A    B    
bar  one      0698202
     three    0801326
     two     -0205257
foo  one     -0963747
     three    0120621
     two      0189623
Name: C

In [23]: tabletake(sorter)
Out[23]: 
A    B    
bar  three    0801326
     one      0698202
     two     -0205257
foo  two      0189623
     three    0120621
     one     -0963747
Name: C


This would be good to build into pandas as an API method Not sure what it should look like though",776560.0,,,,,2012-05-15 18:43:27,1.0
14383654,2,14380371,2013-01-17 16:30:24,6,"DataFrames have a to_latex method:

In [42]: df = pdDataFrame(nprandomrandom((5  5)))

In [43]: df
Out[43]: 
          0         1         2         3         4
0  0886864  0518538  0359964  0167291  0940414
1  0834130  0022920  0265131  0059002  0530584
2  0648019  0953043  0263551  0595798  0153969
3  0207003  0015721  0931170  0045044  0432870
4  0039886  0898780  0728195  0112069  0468485

In [44]: print dfto_latex()
\begin{tabular}{|l|c|c|c|c|c|c|}
\hline
{} &         0 &         1 &         2 &         3 &         4 \\
\hline
0 &  0886864 &  0518538 &  0359964 &  0167291 &  0940414 \\
1 &  0834130 &  0022920 &  0265131 &  0059002 &  0530584 \\
2 &  0648019 &  0953043 &  0263551 &  0595798 &  0153969 \\
3 &  0207003 &  0015721 &  0931170 &  0045044 &  0432870 \\
4 &  0039886 &  0898780 &  0728195 &  0112069 &  0468485 \\
\hline
\end{tabular}


You can simply write this to a tex file

By default latex will render this as:



Note: the to_latex method offers several configuration options",1301710.0,,1240268.0,,2013-01-17 17:54:50,2013-01-17 17:54:50,2.0
14568392,2,14568070,2013-01-28 18:12:24,0,I've received the same error (ImportError: cannot import name hashtable) when trying to import pandas from the source code directory  Try starting the python interpreter from a different directory and import pandas again,243434.0,,,,,2013-01-28 18:12:24,1.0
10003697,2,10003171,2012-04-04 01:15:43,0,"How about

from pandas import *
df = DataFrame({ 
    'first': ['bar'  'bar'  'baz'  'baz'  'foo'  'foo'  'qux'  'qux'] 
    'second': ['one'  'two'  'one'  'two'  'one'  'two'  'one'  'two'] 
    'data': [-0424972  0567020  0276232  -1087401  -0673690  0113648  -1478427  0524988] 
})

df2 = dfjoin(dfgroupby(""first"")sum()rename(columns={""data"":""sum_data""})  
              on=""first"")
",772649.0,,,,,2012-04-04 01:15:43,3.0
10058045,2,10003171,2012-04-07 20:24:34,1,"How about:

df['datasum'] = dfgroupby('first')['data']transform(npsum)


You can also pass as_index=False to groupby to get more R-like behavior when aggregating (or call reset_index on the result)",776560.0,,,,,2012-04-07 20:24:34,1.0
10761312,2,10760364,2012-05-25 21:01:21,6,"You can run arbitrary javascript (with jQuery) either in markdown cells inside <script> tags  or via IPython's IPythoncoredisplayJavascript class  With these  you can manipulate (or ruin) the document to your heart's content  including adding stylesheets

For instance  the following will stripe appropriately classed tables:

<script type=""text/javascript"">
    $('head')append(
        ""<style type='text/css'>trodd{background-color: #def}</style>""
    );
</script>


If you just stick that in one of your markdown cells  then it will apply to everything on the page

Or  you might run the same code (minus <script> tags) from Python in a code cell:

from IPythoncoredisplay import Javascript  display
display(Javascript(""""""
    $('head')append(
        ""<style type='text/css'>trodd{background-color: #def}</style>""
    );
""""""))


But doing this will only affect your tables that are appropriately classed  which is up to the code that is writing the HTML (eg in Pandas)",938949.0,,,,,2012-05-25 21:01:21,1.0
14050441,2,10760364,2012-12-27 06:32:31,2,I just released a project called ipy_table to provide an easy mechanism for creating richly formatted data tables in IPython notebooks (colors  borders  alignment  float formatting  zebra shading  etc) The project is at https://githubcom/epmoyer/ipy_table  and you can get a good idea of it's capabilities from https://githubcom/epmoyer/ipy_table/blob/master/ipy_table-Referencepdf,1931388.0,,,,,2012-12-27 06:32:31,
12201723,2,12200693,2012-08-30 16:47:17,2,"In [97]: df = pandasDataFrame({'month': nprandomrandint(0 11  100)  'A': nprandomrandn(100)  'B': nprandomrandn(100)})

In [98]: dfjoin(dfgroupby('month')['A']sum()  on='month'  rsuffix='_r')
Out[98]:
           A         B  month       A_r
0  -0040710  0182269      0 -0331816
1  -0004867  0642243      1  2448232
2  -0162191  0442338      4  2045909
3  -0979875  1367018      5 -2736399
4  -1126198  0338946      5 -2736399
5  -0992209 -1343258      1  2448232
6  -1450310  0021290      0 -0331816
7  -0675345 -1359915      9  2722156
",1548051.0,,1548051.0,,2012-09-08 20:26:16,2012-09-08 20:26:16,6.0
12211043,2,12200693,2012-08-31 07:51:17,0,"Does this work?

capWeighting = lambda x: (x[""PriceReturn""]*x[""MarketCap""]/x[""MarketCap""]sum())sum()

bdata[""MarketReturn""] = bdatagroupby(""yearmonth"")transform(capWeighting)


I use reindex_like for this:

summedbdata = bdatagroupby(""yearmonth"")apply(lambda x: (x[""PriceReturn""]*x[""MarketCap""]/x[""MarketCap""]sum())sum())
summedbdataset_index('yearmonth')reindex_like(bdataset_index('yearmonth')sort_index()  method='ffill')
",1571826.0,,1571826.0,,2012-08-31 15:04:46,2012-08-31 15:04:46,6.0
12332974,2,12200693,2012-09-08 17:33:00,0,"While I'm still exploring all of the incredibly smart ways that apply concatenates the pieces it's given  here's another way to add a new column in the parent after a groupby operation

In [236]: df
Out[236]: 
  yearmonth    return
0    201202  0922132
1    201202  0220270
2    201202  0228856
3    201203  0277170
4    201203  0747347

In [237]: def add_mkt_return(grp):
   :     grp['mkt_return'] = grp['return']sum()
   :     return grp
   : 

In [238]: dfgroupby('yearmonth')apply(add_mkt_return)
Out[238]: 
  yearmonth    return  mkt_return
0    201202  0922132    1371258
1    201202  0220270    1371258
2    201202  0228856    1371258
3    201203  0277170    1024516
4    201203  0747347    1024516
",243434.0,,,,,2012-09-08 17:33:00,
12335016,2,12200693,2012-09-08 22:35:52,0,May I suggest the transform method (instead of aggregate)? If you use it in your original example it should do what you want (the broadcasting),776560.0,,,,,2012-09-08 22:35:52,2.0
12377080,2,12376863,2012-09-11 20:04:39,1,"You could have is_hammer in terms of row[""Open""] etc as follows

def is_hammer(rOpen rLow rClose rHigh):
    return lower_wick_at_least_twice_real_body(rOpen rLow rClose) \
       and closed_in_top_half_of_range(rHigh rLow rClose)


Then you can use map:

df[""isHammer""] = map(is_hammer  df[""Open""]  df[""Low""]  df[""Close""]  df[""High""])
",1240268.0,,,,,2012-09-11 20:04:39,1.0
12377083,2,12376863,2012-09-11 20:04:56,3,"The exact code will vary for each of the columns you want to do  but it's likely you'll want to use the map and apply functions  In some cases you can just compute using the existing columns directly  since the columns are Pandas Series objects  which also work as Numpy arrays  which automatically work element-wise for usual mathematical operations

>>> d
    A   B  C
0  11  13  5
1   6   7  4
2   8   3  6
3   4   8  7
4   0   1  7
>>> (dA + dB) / dC
0    4800000
1    3250000
2    1833333
3    1714286
4    0142857
>>> dA > dC
0     True
1     True
2     True
3    False
4    False


If you need to use operations like max and min within a row  you can use apply with axis=1 to apply any function you like to each row  Here's an example that computes min(A  B)-C  which seems to be like your ""lower wick"":

>>> dapply(lambda row: min([row['A']  row['B']])-row['C']  axis=1)
0    6
1    2
2   -3
3   -3
4   -7


Hopefully that gives you some idea of how to proceed

Edit: to compare rows against neighboring rows  the simplest approach is to slice the columns you want to compare  leaving off the beginning/end  and then compare the resulting slices  For instance  this will tell you for which rows the element in column A is less than the next row's element in column C:

d['A'][:-1] < d['C'][1:]


and this does it the other way  telling you which rows have A less than the preceding row's C:

d['A'][1:] < d['C'][:-1]


Doing ['A""][:-1] slices off the last element of column A  and doing ['C'][1:] slices off the first element of column C  so when you line these two up and compare them  you're comparing each element in A with the C from the following row",1427416.0,,1427416.0,,2012-09-11 20:56:40,2012-09-11 20:56:40,3.0
11982602,2,11979194,2012-08-16 07:34:54,6,"The problem is that Series uses new which is ensuring that a Series object is instantiated 

You can modify your class like so:

class Support(pdSeries):
    def __new__(cls  *args  **kwargs):
        arr = Series__new__(cls  *args  **kwargs)
        return arrview(Support)

    def supportMethod1(self):
        print 'I am support method 1'       
    def supportMethod2(self):
        print 'I am support method 2'


However  it's probably best to do a has-a instead of a is-a Or monkey patch the Series object The reason is that you will often lose your subclass while using pandas due to the nature of it's data storage Something as simple as 

six[:5] 
scumsum()


Will return a Series object instead of your subclass Internally  the data is stored in contiguous arrays and optimized for speed The data is only boxed with a class when needed and those classes are hardcoded Plus  it's not immediately obvious if something like six[:5] should return the same subclass That would depend on the semantics of your subclass and what metadata is attached to it 

http://nbviewermaxdrawdowncom/3366583 has some notes ",1297165.0,,,,,2012-08-16 07:34:54,1.0
11982635,2,11979194,2012-08-16 07:37:02,2,"Support() returns a Series object

On subclassing of Series and DataFrame see also: https://githubcom/pydata/pandas/issues/60

In [16]: class MyDict(dict):
   :     pass
   :

In [17]: md = MyDict()

In [18]: type(md)
Out[18]: __main__MyDict

In [21]: class MySeries(Series):
   :     pass
   :

In [22]: ms = MySeries()

In [23]: type(ms)
Out[23]: pandascoreseriesSeries
",1548051.0,,,,,2012-08-16 07:37:02,
12254375,2,12167634,2012-09-03 21:46:57,0,"#2 seems like it's best tackled with a combination of dfgroupby() and apply() on the resulting Groupby object  Perhaps an example is the best way to explain

Given a dataframe:

In [53]: df
Out[53]: 
            Value
2012-08-01     61
2012-08-02     52
2012-08-03     89
2012-08-06     44
2012-08-07     35
2012-08-08     98
2012-08-09     64
2012-08-10     48
2012-08-13    100
2012-08-14     95
2012-08-15     14
2012-08-16     55
2012-08-17     58
2012-08-20     11
2012-08-21     28
2012-08-22     95
2012-08-23     18
2012-08-24     81
2012-08-27     27
2012-08-28     81
2012-08-29     28
2012-08-30     16
2012-08-31     50

In [54]: def rankdays(df):
  :    if len(df) != 5:
  :        return pandasSeries()
  :    return pandasSeries(dfValuerank()  index=dfindexweekday)
  : 

In [52]: dfgroupby(lambda x: xweek)apply(rankdays)unstack()
Out[52]: 
    0  1  2  3  4
32  2  1  5  4  3
33  5  4  1  2  3
34  1  3  5  2  4
35  2  5  3  1  4
",243434.0,,243434.0,,2012-09-05 02:19:23,2012-09-05 02:19:23,
12370566,2,12370349,2012-09-11 13:14:27,1,"Given data and minimal length  you could check  whether the array

npdiff(npcumsum(npsign(data - npmean(data)))  length)


contains zero",449449.0,,,,,2012-09-11 13:14:27,
12370826,2,12370349,2012-09-11 13:27:44,2,"import numpy as np
x = nprandomrand(100)
f = npsign(x - xmean())
c = npcumsum(f)
d = c[9:] - c[:-9]
print npmax(d)  npmin(d)


if npmax(d) == 9 or npmin(d) == -9 then there are nine (or more) points in a row are on the same side of the mean

Or you can use following code to calculate the length of every row:

npdiff(npwhere(npdiff(npr_[-2 f -2]))[0])
",772649.0,,772649.0,,2012-09-11 13:42:39,2012-09-11 13:42:39,
12371018,2,12370349,2012-09-11 13:37:46,1,"another possibility: use correlate or convolve

>>> a = nprandomrandn(50)
>>> b = (a - amean()) > 0
>>> bastype(int)
array([0  1  1  0  0  1  0  0  0  0  1  1  1  0  1  0  0  0  0  1  1  1  1 
       1  1  1  0  1  1  0  1  0  0  1  0  1  0  1  1  0  0  0  0  1  1  0 
       1  1  1  1])

>>> c = npcorrelate(b  npones(3)  mode='valid')
>>> c
array([ 2   2   1   1   1   1   0   0   1   2   3   2   2 
        1   1   0   0   1   2   3   3   3   3   3   2   2 
        2   2   2   1   1   1   1   2   1   2   2   2   1 
        0   0   1   2   2   2   2   3   3])

>>> cmax() == 3
True
>>> cmin() == 0
True


It will be slower than HYRY cumsum version

aside: there is a runstest in statsmodels for testing similar runs",333700.0,,,,,2012-09-11 13:37:46,
11335177,2,11334098,2012-07-04 20:35:02,4,"Short answer: you can't and it's not clear why this could ever ""cause problems"" The 'Date' name is naming the Index of the DataFrame  which is different from any of the columns It gets printed with this offset specifically so you will not confuse it with a column of the frame You would not slice into the date with DataFrame['Date'] as per below:

>>> import numpy as np; import pandas; import datetime

>>> dfrm = pandasDataFrame(nprandomrand(10 3)  
 columns=['A' 'B' 'C']  
 index = pandasIndex(
 [datetimedate(2012 6 elem) for elem in range(1 11)] 
 name=""Date""))

>>> dfrm
                   A         B         C
Date                                    
2012-06-01  0283724  0863012  0798891
2012-06-02  0097231  0277564  0872306
2012-06-03  0821461  0499485  0126441
2012-06-04  0887782  0389486  0374118
2012-06-05  0248065  0032287  0850939
2012-06-06  0101917  0121171  0577643
2012-06-07  0225278  0161301  0708996
2012-06-08  0906042  0828814  0247564
2012-06-09  0733363  0924076  0393353
2012-06-10  0273837  0318013  0754807

>>> dfrm['Date']
Traceback (most recent call last):
  File ""<stdin>""  line 1  in <module>
  File ""/usr/local/lib/python27/dist-packages/pandas/core/framepy""  line 1458  in __getitem__
    return self_get_item_cache(key)
  File ""/usr/local/lib/python27/dist-packages/pandas/core/genericpy""  line 294  in _get_item_cache
    values = self_dataget(item)
  File ""/usr/local/lib/python27/dist-packages/pandas/core/internalspy""  line 625  in get
    _  block = self_find_block(item)
  File ""/usr/local/lib/python27/dist-packages/pandas/core/internalspy""  line 715  in _find_block
    self_check_have(item)
  File ""/usr/local/lib/python27/dist-packages/pandas/core/internalspy""  line 722  in _check_have
    raise KeyError('no item named %s' % str(item))
KeyError: 'no item named Date'


Longer answer:

You can change your DataFrame by adding the index into its own column if you'd like it to print that way For example:

>>> dfrm['Date'] = dfrmindex

>>> dfrm
                   A         B         C        Date
Date                                                
2012-06-01  0283724  0863012  0798891  2012-06-01
2012-06-02  0097231  0277564  0872306  2012-06-02
2012-06-03  0821461  0499485  0126441  2012-06-03
2012-06-04  0887782  0389486  0374118  2012-06-04
2012-06-05  0248065  0032287  0850939  2012-06-05
2012-06-06  0101917  0121171  0577643  2012-06-06
2012-06-07  0225278  0161301  0708996  2012-06-07
2012-06-08  0906042  0828814  0247564  2012-06-08
2012-06-09  0733363  0924076  0393353  2012-06-09
2012-06-10  0273837  0318013  0754807  2012-06-10


After this  you could simply change the name of the index so that nothing prints:

>>> dfrmreindex(pandasSeries(dfrmindexvalues  name=''))
                   A         B         C        Date

2012-06-01  0283724  0863012  0798891  2012-06-01
2012-06-02  0097231  0277564  0872306  2012-06-02
2012-06-03  0821461  0499485  0126441  2012-06-03
2012-06-04  0887782  0389486  0374118  2012-06-04
2012-06-05  0248065  0032287  0850939  2012-06-05
2012-06-06  0101917  0121171  0577643  2012-06-06
2012-06-07  0225278  0161301  0708996  2012-06-07
2012-06-08  0906042  0828814  0247564  2012-06-08
2012-06-09  0733363  0924076  0393353  2012-06-09
2012-06-10  0273837  0318013  0754807  2012-06-10


This seems a bit overkill Another option is to just change the index to integers or something after adding the Date as a column:

>>> dfrmreset_index()


or if you already moved the index into a column manually  then just

>>> dfrmindex = range(len(dfrm))

>>> dfrm
          A         B         C        Date
0  0283724  0863012  0798891  2012-06-01
1  0097231  0277564  0872306  2012-06-02
2  0821461  0499485  0126441  2012-06-03
3  0887782  0389486  0374118  2012-06-04
4  0248065  0032287  0850939  2012-06-05
5  0101917  0121171  0577643  2012-06-06
6  0225278  0161301  0708996  2012-06-07
7  0906042  0828814  0247564  2012-06-08
8  0733363  0924076  0393353  2012-06-09
9  0273837  0318013  0754807  2012-06-10


Or the following if you care about the order the columns appear:

>>> dfrmix[: [-1]+range(len(dfrmcolumns)-1)]
         Date         A         B         C
0  2012-06-01  0283724  0863012  0798891
1  2012-06-02  0097231  0277564  0872306
2  2012-06-03  0821461  0499485  0126441
3  2012-06-04  0887782  0389486  0374118
4  2012-06-05  0248065  0032287  0850939
5  2012-06-06  0101917  0121171  0577643
6  2012-06-07  0225278  0161301  0708996
7  2012-06-08  0906042  0828814  0247564
8  2012-06-09  0733363  0924076  0393353
9  2012-06-10  0273837  0318013  0754807


Added

Here are a few helpful functions to include in an iPython configuration script (so that they are loaded upon startup)  or to put in a module you can easily load when working in Python

###########
# Imports #
###########
import pandas
import datetime
import numpy as np
from dateutil import relativedelta
from pandasio import data as pdata


############################################
# Functions to retrieve Yahoo finance data #
############################################

# Utility to get generic stock symbol data from Yahoo finance
# Starts two days prior to present (or most recent business day)
# and goes back a specified number of days
def getStockSymbolData(sym_list  end_date=datetimedatetoday()+relativedeltarelativedelta(days=-1)  num_dates = 30):

    dReader = pdataDataReader
    start_date = end_date + relativedeltarelativedelta(days=-num_dates)
    return dict( (sym  dReader(sym  ""yahoo""  start=start_date  end=end_date)) for sym in sym_list )                     
###

# Utility function to get some AAPL data when needed
# for testing
def getAAPL(end_date=datetimedatetoday()+relativedeltarelativedelta(days=-1)  num_dates = 30):

    dReader = pdataDataReader
    return getStockSymbolData(['AAPL']  end_date=end_date  num_dates=num_dates)
###


I also made a class below to hold some data for common stocks:

#####
# Define a 'Stock' class that can hold simple info
# about a security  like SEDOL and CUSIP info This
# is mainly for debugging things and quickly getting
# info for a single security
class MyStock():

    def __init__(self  ticker='None'  sedol='None'  country='None'):
        selfticker = ticker
        selfsedol=sedol
        selfcountry = country
    ###


    def getData(self  end_date=datetimedatetoday()+relativedeltarelativedelta(days=-1)  num_dates = 30):
        return pandasDataFrame(getStockSymbolData([selfticker]  end_date=end_date  num_dates=num_dates)[selfticker])
    ###


#####
# Make some default stock objects for common stocks
AAPL = MyStock(ticker='AAPL'  sedol='03783310'  country='US')
SAP  = MyStock(ticker='SAP'   sedol='484628'    country='DE')
",567620.0,,567620.0,,2012-09-23 02:16:06,2012-09-23 02:16:06,7.0
11384818,2,11334098,2012-07-08 16:39:58,6,Try using the reset_index method which moves the DataFrame's index into a column (which is what you want  I think),776560.0,,,,,2012-07-08 16:39:58,
11767878,2,11762815,2012-08-01 21:31:04,2,"I found the answer by myself It is possible to provide a function to the 'how' argument of resample:

f = lambda x: Decimal(npmean(x))
tsresample('D'  how = f)
",1521724.0,,1521724.0,,2012-08-01 21:43:55,2012-08-01 21:43:55,
12187475,2,12053003,2012-08-29 22:38:41,2,"I've cross-linked the issue on GitHub:

http://githubcom/pydata/pandas/issues/1824

I was not personally aware of this issue  and frankly it's a bit disappointing that this is a problem for PyTables or HDF5 (whoever is the culprit) ",776560.0,,,,,2012-08-29 22:38:41,
12235906,2,12227974,2012-09-02 12:22:38,0,"What platform are you building the C extensions on? On the cluster machine  verify that the extension shared objects (so files on Linux/OSX and pyd files on Windows) are in the folder you copied: 

08:21 ~/code/pandas/pandas  $ ll
total 12568
-rw-r--r--   1 wesm  staff     1269 Aug 25 04:49 __init__py
-rw-r--r--   1 wesm  staff     1645 Aug 25 04:52 __init__pyc
-rwxr-xr-x   1 wesm  staff   799464 Aug 25 04:50 _algosso
-rwxr-xr-x   1 wesm  staff    65712 Aug 25 04:48 _enginesso
-rwxr-xr-x   1 wesm  staff   197056 Sep  1 22:13 _periodso
-rwxr-xr-x   1 wesm  staff   341032 Aug 25 04:53 _sparseso
drwxr-xr-x   6 wesm  staff      204 Aug 25 04:52 compat
drwxr-xr-x  44 wesm  staff     1496 Sep  1 17:34 core
drwxr-xr-x   3 wesm  staff      102 Aug 25 04:49 finance
-rw-r--r--   1 wesm  staff      394 Aug 25 04:49 infopy
-rw-r--r--   1 wesm  staff      535 Aug 25 04:52 infopyc
drwxr-xr-x  15 wesm  staff      510 Sep  1 23:14 io
-rwxr-xr-x   1 wesm  staff  2490600 Sep  1 22:13 libso
drwxr-xr-x   9 wesm  staff      306 Sep  1 17:34 rpy
drwxr-xr-x   6 wesm  staff      204 May 20 19:13 sandbox
-rw-r--r--   1 wesm  staff      844 Feb 22  2012 setuppy
drwxr-xr-x  17 wesm  staff      578 Aug 25 04:52 sparse
drwxr-xr-x  55 wesm  staff     1870 Sep  1 22:57 src
drwxr-xr-x  23 wesm  staff      782 Aug 25 04:52 stats
drwxr-xr-x  40 wesm  staff     1360 Sep  1 17:34 tests
drwxr-xr-x  16 wesm  staff      544 Aug 25 04:52 tools
drwxr-xr-x  28 wesm  staff      952 Sep  1 17:34 tseries
drwxr-xr-x  18 wesm  staff      612 Aug 25 04:52 util
-rw-r--r--   1 wesm  staff       54 Sep  1 22:57 versionpy
-rw-r--r--   1 wesm  staff      204 Sep  1 22:58 versionpyc
",776560.0,,,,,2012-09-02 12:22:38,3.0
12478593,2,12432803,2012-09-18 14:02:13,0,Why don't you try inserting a hard break point (import pdb; pdbset_trace()) after the assignment to vols if vols[i j] is NaN and debug that way?,776560.0,,,,,2012-09-18 14:02:13,1.0
12661071,2,12432803,2012-09-30 11:57:04,0,"I noticed that if I use self comprehension instead of loops this behaviour ceases
Did anyone have the same sort of behaviour?",1529852.0,,,,,2012-09-30 11:57:04,
12576229,2,12576151,2012-09-25 04:29:28,1,"As it says in the TypeError  min (and related functions) will only work on instances of npndarray; thus  the new subclass must inherit from the class you are trying to wrap

Then  since you extend the base class  you have to replace the methods with a suitable descriptor:

class RestrictedMethod(object):
    def __get__(self  obj  objtype):
        raise AttributeError(""Access denied"")

class RestrictingMetaWrapper(type):
    def __new__(cls  name  bases  dic):
        block = dicget('_block'  [])

        for attr in block:
            dic[attr] = RestrictedMethod()

        return type__new__(cls  name  bases  dic) # note we inject the base class here

class NArray(npndarray):
    __metaclass__ = RestrictingMetaWrapper
    _block = ['max']


Note: enterprising applications can still access ""restricted"" functionality through the base class methods (eg npndarraymax(na))

EDIT: Simplified the wrapper and made it transparently subclassable

Note that this can all be done in a simpler way using a class decorator:

class RestrictedMethod(object):
    def __get__(self  obj  objtype):
        raise AttributeError(""Access denied"")

def restrict_methods(*args):
    def wrap(cls):
        for attr in args:
            setattr(cls  attr  RestrictedMethod())
        return cls
    return wrap

@restrict_methods('max'  'abs')
class NArray(npndarray):
    pass
",1204143.0,,1204143.0,,2012-09-25 05:49:43,2012-09-25 05:49:43,3.0
14493078,2,14492898,2013-01-24 02:45:30,5,"Referencing previous rows in the manner you suggest is best accomplished with the Seriesshift() function:

In [1]: df = DataFrame(randn(10 3) columns=['O' 'L' 'H'])

In [2]: df
Out[2]:
          O         L         H
0  0605412  0739866 -0280222
1 -0707852  0785651  0855183
2 -0087119  0518924  0932167
3 -0913352  0369825  1277771
4  0434593 -2942903  0802413
5  0075669 -0135914  1374454
6  1112062  0314946  0882468
7 -0706078 -0202243  0838088
8 -1668152  0414585  0809932
9  1452937 -0048245  0635499

In [3]: df['OIR'] = ((dfLshift() <= dfO) & (dfO <= dfHshift()))

In [4]: df
Out[4]:
          O         L         H    OIR
0  0605412  0739866 -0280222  False
1 -0707852  0785651  0855183  False
2 -0087119  0518924  0932167  False
3 -0913352  0369825  1277771  False
4  0434593 -2942903  0802413   True
5  0075669 -0135914  1374454   True
6  1112062  0314946  0882468   True
7 -0706078 -0202243  0838088  False
8 -1668152  0414585  0809932  False
9  1452937 -0048245  0635499  False
",919872.0,,,,,2013-01-24 02:45:30,1.0
9283497,2,9283166,2012-02-14 20:10:20,1,"See here for my answer to a similar question and here for further information regarding mathplotlib's finance candlestick graph

To get just the adj close from your  sp500  you would use something like sp500[""Adj Close""] and then pass that to the relevant matplotlib plot command pltplot(datelist  sp500[""Adj Close""] ) where datelist is your list of dates on the x axis 

I believe you can get datelist by referencing sp500index  see here for more information 

As for your issue with passing it to the plot command  something like

datelist = [date2num(x) for x in sp500index] where the function date2num is from matplotlibdates package

After setting up the relevant subplot  and then call the appropriate fill command to fill_between_alpha the area under the line like the Yahoo graph you linked to 

See here under  the Fill Between and Alpha heading for another snippet that shows a filled line graph  with correct date printing

The initial link has a sample matplotlib snippet which also covers the date format and formatting in more detail",1184641.0,,1184641.0,,2012-02-15 01:10:28,2012-02-15 01:10:28,4.0
10464490,2,10458493,2012-05-05 17:55:50,0,"let me try to answer this basically i will pad or reindex with complete weekdays and sample every 5 days while drop missing data due to holiday or suspension

>>> coke = DataReader('KO'  'yahoo'  start=datetime(2012 1 1))

>>> startd=cokeindex[0]-timedelta(cokeindex[0]isoweekday()-1)

>>> rng = array(DateRange(str(startd)  periods=90))

>>> chunk=[]

>>> for i in range(18):

   chunkappend(coke[i*5:(i+1)*5]dropna())



then you can loop chunk to plot each week data",1377107.0,,,,,2012-05-05 17:55:50,1.0
10466763,2,10458493,2012-05-05 23:19:22,2,"Assuming a pandasTimeSeries object as the starting point  you can group
elements by ISO week number and ISO weekday with
datetimedateisocalendar()  The following statement  which ignores ISO year  aggregates the last sample of each day

In [95]: daily = tsgroupby(lambda x: xisocalendar()[1:])agg(lambda s: s[-1])

In [96]: daily
Out[96]: 
key_0
(1  1)     63
(1  2)     91
(1  3)     73

(20  5)    82
(20  6)    53
(20  7)    63
Length: 140


There may be cleaner way to perform the next step  but the goal is to change the index from an array of tuples to a MultiIndex object

In [97]: dailyindex = pandasMultiIndexfrom_tuples(dailyindex  names=['W'  'D'])

In [98]: daily
Out[98]: 
W   D
1   1    63
    2    91
    3    73
    4    88
    5    84
    6    95
    7    72

20  1    81
    2    53
    3    78
    4    64
    5    82
    6    53
    7    63
Length: 140


The final step is to ""unstack"" weekday from the
MultiIndex  creating columns for each weekday  and replace the weekday numbers with an abbreviation  to improve readability

In [102]: dofw = ""Mon Tue Wed Thu Fri Sat Sun""split()

In [103]: grid = dailyunstack('D')rename(columns=lambda x: dofw[x-1])

In [104]: grid
Out[104]: 
    Mon  Tue  Wed  Thu  Fri  Sat  Sun
W                                    
1    63   91   73   88   84   95   72
2    66   77   96   72   56   80   66

19   56   69   89   69   96   73   80
20   81   53   78   64   82   53   63


To create a line plot for each week  transpose the dataframe  so the columns are week numbers and rows are weekdays (note this step can be avoided by unstacking week number  in place of weekday  in the previous step)  and call plot

gridTplot()
",243434.0,,243434.0,,2012-05-05 23:55:57,2012-05-05 23:55:57,4.0
11414827,2,11414596,2012-07-10 13:51:12,2,"dfix[:-1]


returns the original DataFrame with the last row removed",449449.0,,,,,2012-07-10 13:51:12,1.0
11872393,2,11869910,2012-08-08 20:10:43,4,"I`m not entirely sure what you want  and your last line of code does not help either  but anyway:

""Chained"" filtering is done by ""chaining"" the criteria in the boolean index

In [96]: df
Out[96]:
   A  B  C  D
a  1  4  9  1
b  4  5  0  2
c  5  5  1  0
d  1  3  9  6

In [99]: df[(dfA == 1) & (dfD == 6)]
Out[99]:
   A  B  C  D
d  1  3  9  6


If you want to chain methods  you can add your own mask method and use that one

In [90]: def mask(df  key  value):
   :     return df[df[key] == value]
   :

In [92]: pandasDataFramemask = mask

In [93]: df = pandasDataFrame(nprandomrandint(0  10  (4 4))  index=list('abcd')  columns=list('ABCD'))

In [95]: dfix['d' 'A'] = dfix['a'  'A']

In [96]: df
Out[96]:
   A  B  C  D
a  1  4  9  1
b  4  5  0  2
c  5  5  1  0
d  1  3  9  6

In [97]: dfmask('A'  1)
Out[97]:
   A  B  C  D
a  1  4  9  1
d  1  3  9  6

In [98]: dfmask('A'  1)mask('D'  6)
Out[98]:
   A  B  C  D
d  1  3  9  6
",1548051.0,,,,,2012-08-08 20:10:43,
11893375,2,11869910,2012-08-09 23:20:59,2,"The answer from @lodagro is great I would extend it by generalizing the mask function as:

def mask(df  f):
  return df[f(df)]


Then you can do stuff like:

dfmask(lambda x: x[0] < 0)mask(lambda x: x[1] > 0)
",243238.0,,,,,2012-08-09 23:20:59,1.0
12184060,2,12115421,2012-08-29 18:14:38,2,Yes Not knowing the exact structure of your data  hard to give you exact code  but once you learn the library it should not be very difficult Check out my upcoming book (/shameless self promotion) or the pandas documentation for good learning resources,776560.0,,,,,2012-08-29 18:14:38,
12325334,2,12324162,2012-09-07 21:00:12,0," for i row in enumerate(dfvalues):
     ID = i
     sym timestamp open_price close_price = row
     print ID sym open_price close_price


but I dont think I quite understand what you are trying to do",541038.0,,,,,2012-09-07 21:00:12,1.0
12491917,2,12490762,2012-09-19 09:26:44,0,"When creating the DataFrame  you could set the MultiIndex as follows:

dfset_index(['first'  'second']  drop=False)


This way  the index column is not dropped and still accessible for your apply",1571826.0,,1571826.0,,2012-09-19 09:47:43,2012-09-19 09:47:43,1.0
12605055,2,12604909,2012-09-26 15:20:01,1,"You can use map to do this:

df['Date']=df['Date']map(lambda x: int(str(x)[-4:]))
                          #converts the last 4 characters of the string to an integer


The lambda function  is taking the input from the Date and converting it to a year You could write this more verbosely as:

def convert_to_year(date_in_some_format);
    date_as_string = str(date_in_some_format)
    year_as_string = date_in_some_format[-4:] # last four characters
    return int(year_as_string)

df['Date']=df['Date']map(convert_to_year)
",1240268.0,,1240268.0,,2012-09-26 15:27:24,2012-09-26 15:27:24,4.0
12927871,2,12926660,2012-10-17 05:48:07,1,"reindex realigns the existing index to the given index rather than changing the index
you can just do ffindex = range if you've made sure the lengths and the alignment matches

Parsing each original index value is much safer The easy approach is to do this by converting to a string:

In [132]: ints
Out[132]: Int64Index([201201  201201  201201    203905  203905  203905])

In [133]: conv = lambda x: datetimestrptime(str(x)  '%Y%m')

In [134]: dates = [conv(x) for x in ints]

In [135]: %timeit [conv(x) for x in ints]
1 loops  best of 3: 222 ms per loop


This is kind of slow  so if you have a lot observations you might want to use an optimize cython function in pandas:

In [144]: years = (ints // 100)astype(object)

In [145]: months = (ints % 100)astype(object)

In [146]: days = npones(len(years)  dtype=object)

In [147]: import pandaslib as lib

In [148]: %timeit Index(libtry_parse_year_month_day(years  months  days))
100 loops  best of 3: 547 ms per loop


Here ints has 10000 entries",1306530.0,,1306530.0,,2012-10-17 17:50:51,2012-10-17 17:50:51,3.0
12671598,2,12664590,2012-10-01 10:33:12,0,"Here is a sample showing solution:

#!/usr/bin/env python

import pandas as pd
import matplotlibpylab as plt
import numpy as np

data = [
(02  13) 
(13  39) 
(21  48) 
(29 55) 
(33 69)
]

df = pdDataFrame(data  columns=['X'  'Y'])

print(df)

# 2 degrees of freedom : slope / intercept
model_with_intercept = pdols(y=df['Y']  x=df['X']  intercept=True)
df['Y_fit_with_intercept'] = model_with_intercepty_fitted

# 1 degree of freedom : slope ; intersept=0
model_no_intercept = pdols(y=df['Y']  x=df['X']  intercept=False)
df['Y_fit_no_intercept'] = model_no_intercepty_fitted

# 1 degree of freedom : slope ; intersept=offset
offset = -1
df['Yoffset'] = df['Y'] - offset
model_with_offset = pdols(y=df['Yoffset']  x=df['X']  intercept=False)
df['Y_fit_offset'] = model_with_offsety_fitted + offset

print(model_with_intercept)
print(model_no_intercept)
print(model_with_offset)

dfplot(x='X'  y=['Y'  'Y_fit_with_intercept'  'Y_fit_no_intercept'  'Y_fit_offset'])
pltshow()
",1555275.0,,,,,2012-10-01 10:33:12,
12961158,2,12960574,2012-10-18 18:19:30,1,"Juniper may be on to something but after looking at the data  there is a comma at the end of each line And this quote: 


  index_col: column number  column name  or list of column numbers/names  to use as the index (row labels) of the resulting DataFrame By default  it will number the rows without using any column  unless there is one more data column than there are headers  in which case the first column is taken as the index


from the documentation shows that pandas believes you have n headers and n+1 data columns and is treating the first column as the index",1341437.0,,,,,2012-10-18 18:19:30,3.0
13327097,2,13299769,2012-11-10 23:27:20,0,"Perhaps the base parameter of dfresample() would help:

base : int  default 0
    For frequencies that evenly subdivide 1 day  the ""origin"" of the
    aggregated intervals For example  for '5min' frequency  base could
    range from 0 through 4 Defaults to 0


Here's an example:

In [44]: df = pdDataFrame(nprandomrand(28) 
:           index=pdDatetimeIndex(start='2012/9/1'  periods=28  freq='H'))

In [45]: df
Out[45]: 
                            0
2012-09-01 00:00:00  0970273
2012-09-01 01:00:00  0730171
2012-09-01 02:00:00  0508588
2012-09-01 03:00:00  0535351
2012-09-01 04:00:00  0940255
2012-09-01 05:00:00  0143483
2012-09-01 06:00:00  0792659
2012-09-01 07:00:00  0231413
2012-09-01 08:00:00  0071676
2012-09-01 09:00:00  0995202
2012-09-01 10:00:00  0236551
2012-09-01 11:00:00  0904853
2012-09-01 12:00:00  0652873
2012-09-01 13:00:00  0488400
2012-09-01 14:00:00  0396647
2012-09-01 15:00:00  0967261
2012-09-01 16:00:00  0554188
2012-09-01 17:00:00  0884086
2012-09-01 18:00:00  0418577
2012-09-01 19:00:00  0189584
2012-09-01 20:00:00  0577041
2012-09-01 21:00:00  0100332
2012-09-01 22:00:00  0294672
2012-09-01 23:00:00  0925425
2012-09-02 00:00:00  0630807
2012-09-02 01:00:00  0400261
2012-09-02 02:00:00  0156469
2012-09-02 03:00:00  0658608




In [46]: dfresample(""24H""  how=sum  label='left'  closed='left'  base=5)
Out[46]: 
                             0
2012-08-31 05:00:00   3684638
2012-09-01 05:00:00  11671068

In [47]: dfix[:5]sum()
Out[47]: 0    3684638

In [48]: dfix[5:]sum()
Out[48]: 0    11671068
",243434.0,,243434.0,,2012-11-17 19:14:36,2012-11-17 19:14:36,4.0
13475925,2,13475812,2012-11-20 14:59:23,1,"You can do it directly in Pandas:

s = pdread_csv('test'  header=None  index_col=0  parse_dates=True)
d = sgroupby(lambda x: xdate())aggregate(lambda x: sum(x) if len(x) >= 40 else npnan)

             X2
2012-01-01  1128
",449449.0,,449449.0,,2012-11-20 15:05:14,2012-11-20 15:05:14,1.0
13836910,2,13834690,2012-12-12 09:43:57,1,See http://jarantoblogspotin/2012/08/os-x-unable-to-execute-clang-no-suchhtml and http://wwwshellpersonnet/easy_install-clang-error/ You either don't have xcode or you don't have commandline tools installed,786691.0,,,,,2012-12-12 09:43:57,
10058145,2,10020591,2012-04-07 20:34:43,2,"To tantalize you  in pandas 080 (under heavy development in the timeseries branch on GitHub)  you'll be able to do:

In [5]: frameconvert('1h'  how='mean')
Out[5]: 
                     radiation      tamb
2012-04-05 08:00:00   7840989  8446109
2012-04-05 09:00:00   4898935  5459221
2012-04-05 10:00:00   5227741  4660849
2012-04-05 11:00:00   4689270  5321398
2012-04-05 12:00:00   4956994  5093980


The above mentioned methods are the right strategy with the current production version of pandas",776560.0,,,,,2012-04-07 20:34:43,4.0
11603242,2,10020591,2012-07-22 19:13:55,2,"I am answering my question to reflect the time series related changes in pandas >= 08 (all other answers are outdated)

Using pandas >= 08 the answer is:

In [75]: frameresample('1H'  how={'radiation': npsum  'tamb': npmean})
Out[14]: 
                         tamb   radiation
2012-04-05 08:00:00  4459108  275820847
2012-04-05 09:00:00  5343115  282242943
2012-04-05 10:00:00  4988707  280426667
2012-04-05 11:00:00  4946707  276593614
2012-04-05 12:00:00  4571588    2115954
",1301710.0,,1301710.0,,2013-01-10 09:50:34,2013-01-10 09:50:34,
10737915,2,10705715,2012-05-24 12:54:34,0,"Solved:

def weekdays(date):
    if 0 < int(datetimestrftime(date  ""%w"")) < 6:
        return True
    return False

def weekends(date):
    if int(datetimestrftime(date  ""%w"")) == (0 or 6):
        return True
    return False

telford = telfordselect(weekdays)
",1410646.0,,,,,2012-05-24 12:54:34,
11170956,2,11170653,2012-06-23 16:00:18,5,"If you use the apply method with a function what happens is that every item in the Series will be mapped with such a function Eg

>>> sapply(enumerate)
a    <enumerate object at 0x13cf910>
b    <enumerate object at 0x13cf870>
c    <enumerate object at 0x13cf820>
d    <enumerate object at 0x13cf7d0>
e    <enumerate object at 0x13ecdc0>


What you want to do is simply to enumerate the series itself

>>> list(enumerate(s))
[(0  'six')  (1  'seven')  (2  'six')  (3  'seven')  (4  'six')]


What if for example you wanted to sum the string of all the entities?

>>> "" ""join(s)
'six seven six seven six'


A more complex usage of apply would be this one:

>>> from functools import partial
>>> sapply(partial(map  lambda x: x*2 ))
a                ['ss'  'ii'  'xx']
b    ['ss'  'ee'  'vv'  'ee'  'nn']
c                ['ss'  'ii'  'xx']
d    ['ss'  'ee'  'vv'  'ee'  'nn']
e                ['ss'  'ii'  'xx']


[Edit]

Following the OP's question for clarifications: Don't confuse Series (1D) with DataFrames (2D) http://pandaspydataorg/pandas-docs/stable/dsintrohtml#dataframe - as I don't really see how you can talk about rows However you can include indices in your function by creating a new series (apply wont give you any information about the current index):

>>> Series([s[x]+"" my index is:  ""+x for x in skeys()]  index=skeys())
a      six index  a
b    seven index  b
c      six index  c
d    seven index  d
e      six index  e


Anyhow I would suggest that you switch to other data types to avoid huge memory leaks",999346.0,,999346.0,,2012-06-25 07:43:48,2012-06-25 07:43:48,3.0
13828263,2,11170653,2012-12-11 20:47:51,0,"Here's a neat way  using itertools's count and zip:

import pandas as pd
from itertools import count

s = pdSeries(['six'  'seven'  'six'  'seven'  'six'] 
                  index=['a'  'b'  'c'  'd'  'e'])

In [4]: zip(count()  s)
Out[4]: [(0  'six')  (1  'seven')  (2  'six')  (3  'seven')  (4  'six')]


Unfortunately  only as efficient than enumerate(list(s))!",1240268.0,,,,,2012-12-11 20:47:51,
11563542,2,11553370,2012-07-19 14:55:45,1,Seriesapply and Seriesmap use a specialized Cython method (pandaslibmap_infer) I wrote that is roughly 2x faster than using numpyvectorize ,776560.0,,,,,2012-07-19 14:55:45,2.0
11927922,2,11927715,2012-08-13 03:39:34,6,"You can specify the color option as a list directly to the plot function

from matplotlib import pyplot as plt
from itertools import cycle  islice
import pandas  numpy as np  # I find nprandomrandint to be better

# Make the data
x = [{i:nprandomrandint(1 5)} for i in range(10)]
df = pandasDataFrame(x)

# Make a list by cycling through the colors you care about
# to match the length of your data
my_colors = list(islice(cycle(['b'  'r'  'g'  'y'  'k'])  None  len(df)))

# Specify this list of colors as the `color` option to `plot`
dfplot(kind='bar'  stacked=True  color=my_colors)


To define your own custom list  you can do a few of the following  or just look up the Matplotlib techniques for defining a color item by its RGB values  etc You can get as complicated as you want with this

my_colors = ['g'  'b']*5 # <-- this concatenates the list to itself 5 times
my_colors = [(05 04 05)  (075  075  025)]*5 # <-- make two custom RGBs and repeat/alternate them over all the bar elements
my_colors = [(x/100  x/200  075) for x in range(len(df))] # <-- Quick gradient example along the Red/Green dimensions


The last example yields the follow simple gradient of colors for me:



I didn't play with it long enough to figure out how to force the legend to pick up the defined colors  but I'm sure you can do it

In general  though  a big piece of advice is to just use the functions from Matplotlib directly Calling them from Pandas is OK  but I find you get better options and performance calling them straight from Matplotlib",567620.0,,567620.0,,2012-12-20 13:17:50,2012-12-20 13:17:50,3.0
12192362,2,12187928,2012-08-30 07:57:13,0,"Pandas bootstrap_plot expects a Series as first argument

from pandas import Series
bootstrap_plot(Series(C)  size=17517  samples=1000  color='grey')
",1548051.0,,,,,2012-08-30 07:57:13,1.0
12358601,2,12358360,2012-09-10 20:00:09,6,"tl;dr:

In [177]: new_columns = dfcolumns[dfix[dflast_valid_index()]argsort()]

In [178]: dfreindex_axis(new_columns  axis=1)
Out[178]: 
        fff       aaa       ddd       ppp
0 -1352820  0293043  0331940 -0905920
1  0376450 -1147648  0050776 -0778044
2 -0842640 -2195892 -0147840 -0613346
3 -0028946  0112304  0143822 -1283472
4 -0144383  1211386  0688421 -1462860
5 -1975694 -0333355 -0308187  1027202


Explanation follows  First  build the DataFrame:

In [81]: from pandas import *

In [82]: df = DataFrame(nprandomrandn(6  4)  columns=['ddd'  'fff'  'aaa'  'ppp'])

In [83]: df
Out[83]: 
        ddd       fff       aaa       ppp
0  0331940 -1352820  0293043 -0905920
1  0050776  0376450 -1147648 -0778044
2 -0147840 -0842640 -2195892 -0613346
3  0143822 -0028946  0112304 -1283472
4  0688421 -0144383  1211386 -1462860
5 -0308187 -1975694 -0333355  1027202


Get the last row:

In [84]: last_row = dfix[dflast_valid_index()]

In [85]: last_row
Out[85]: 
ddd   -0308187
fff   -1975694
aaa   -0333355
ppp    1027202
Name: 5


Get the indices that would sort it:

In [86]: last_rowargsort()
Out[86]: 
ddd    1
fff    2
aaa    0
ppp    3
Name: 5


Get the columns in that order:

In [87]: dfcolumns[last_rowargsort()]
Out[87]: Index([fff  aaa  ddd  ppp]  dtype=object)


Use reindex_axis:

In [88]: dfreindex_axis(dfcolumns[last_rowargsort()]  axis=1)
Out[88]: 
        fff       aaa       ddd       ppp
0 -1352820  0293043  0331940 -0905920
1  0376450 -1147648  0050776 -0778044
2 -0842640 -2195892 -0147840 -0613346
3 -0028946  0112304  0143822 -1283472
4 -0144383  1211386  0688421 -1462860
5 -1975694 -0333355 -0308187  1027202


Profit!",487339.0,,487339.0,,2012-09-10 20:23:12,2012-09-10 20:23:12,
12531112,2,12520537,2012-09-21 13:00:17,0,"As suggested by @Wouter Overmiere  the following worked for me

X = dfTvalues #Transpose values 
Y = pdist(X)
Z = linkage(Y)
dendrogram(Z  labels = dfcolumns)
",1649335.0,,,,,2012-09-21 13:00:17,
12720578,2,12720159,2012-10-04 05:07:57,0,"string1 = ""Likes %s""
string2 = ""Likes %s and %s""
string3 = ""Likes %s  %s  and %s""

def findveg(row):
    veggies = []
    if DF['cucumber_lover'][row] == 1:
        veggiesappend('cucumber')
    if DF['carrot_lover'][row] == 1:
        veggiesappend('carrot')
    if DF['spinach_lover'][row] == 1:
        veggiesappend('spinach')
    return tuple(veggies)

flist['msg'] = ''
for row in DFindex:
        veggies = findfac(row)
        if len(veggies) == 1:
            findveg['msg'][row] = string1 % veggies
        if len(veggies) == 2:
            findveg['msg'][row] = string2 % veggies
        if len(veggies) == 3:
            findveg['msg'][row] = string3 % veggies
",1479269.0,,,,,2012-10-04 05:07:57,
12974987,2,12974774,2012-10-19 13:10:24,0,"Use

datagroupby(data['dates']map(lambda x: xday))
",1199589.0,,,user647772,2012-10-19 13:49:19,2012-10-19 13:49:19,
12975201,2,12974774,2012-10-19 13:21:40,4,"From pandas documentation: http://pandaspydataorg/pandas-docs/stable/pandaspdf

 # 72 hours starting with midnight Jan 1st  2011
 In [1073]: rng = date_range(1/1/2011  periods=72  freq=H)
",1719510.0,,,,,2012-10-19 13:21:40,3.0
12976914,2,12974774,2012-10-19 14:55:17,4,"Yeah  I would definitely use Pandas for this The trickiest part is just figuring out the datetime parser for pandas to use to load in the data After that  its just a resampling of the subsequent DataFrame 

In [62]: parse = lambda x: datetimedatetimestrptime(x  '%Y-%m-%d %I:%M:%S%p')
In [63]: dframe = pandasread_table(""datatxt""  delimiter="" ""  index_col=0  parse_dates=True  date_parser=parse)
In [64]: print dframe
                                 Price1                                   Price 2
Dates                                                                            
2002-10-15 23:17:03                                06                        50
2002-10-15 23:20:04                                14                        24
2002-10-15 23:22:12                                41                        91
2002-10-16 12:21:03                                16                        14
2002-10-16 12:22:03                                77                        37
In [78]: means = dframeresample(""D""  how='mean'  label='left')
In [79]: print means
                                 Price1                                   Price 2
Dates                                                                            
2002-10-15                                    2033333                       550
2002-10-16                                    4650000                       255


where datatxt:

Dates                           Price1                       Price 2
2002-10-15  11:17:03pm           06                           50
2002-10-15  11:20:04pm           14                           24
2002-10-15  11:22:12pm           41                           91
2002-10-16  12:21:03pm           16                           14
2002-10-16  12:22:03pm           77                           37
",1443118.0,,1240268.0,,2012-12-30 18:25:14,2012-12-30 18:25:14,1.0
13849796,2,13423689,2012-12-12 22:28:09,0,The issue is that PiCloud's default environment runs an older version of pandas (030) which causes incompatibilities  The easiest solution is to use PiCloud's Ubuntu Precise Environment  which runs pandas 090,182284.0,,,,,2012-12-12 22:28:09,
13682381,2,13682044,2012-12-03 11:33:51,1,"data['result'] = data['result']map(lambda x: xlstrip('+-')rstrip('aAbBcC'))
",449449.0,,,,,2012-12-03 11:33:51,2.0
12379527,2,12370349,2012-09-11 23:58:19,1,"As I mentioned in a comment  you may want to try using some stride tricks

First  let's make an array of the size of your anomalies: we can put it as npint8 to save some space

anomalies = x - xmean()
signs = npsign(anomalies)astype(npint8)

Now for the strides If you want to consider N consecutive points  you'll use

from nplibstride_tricks import as_strided
strided = as_strided(signs  
                     strides=(signsitemsize signsitemsize)  
                     shape=(signsshape N))


That gives us a (xsize  N) rollin array: the first row is x[0:N]  the second x[1:N+1] Of course  the last N-1 rows will be meaningless  so from now on we'll use

strided = strided[:-N+1]

Let's sum along the rows

consecutives = stridedsum(axis=-1)


That gives us an array of size (xsize-N+1) of values between -N and +N: we just have to find where the absolute values are N: 

(indices ) = npnonzero(consecutives == N)


indices is the array of the indices i of your array x for which the values x[i:i+N] are on the same side of the mean
Example with x=nprandomrand(10) and N=3

>>> x = array([ 057016436   079360943   089535982   083632245   031046202 
            091398363   062358298   072148491   099311681   094852957])
>>> signs = npsign(x-xmean())astype(npint8)
array([-1   1   1   1  -1   1  -1  -1   1   1]  dtype=int8)
>>> strided = as_strided(signs strides=(1 1) shape=(signssize 3))
array([[  -1     1     1] 
       [   1     1     1] 
       [   1     1    -1] 
       [   1    -1     1] 
       [  -1     1    -1] 
       [   1    -1    -1] 
       [  -1    -1     1] 
       [  -1     1     1] 
       [   1     1  -106] 
       [   1  -106   -44]]  dtype=int8)
>>> consecutive=strided[:-N+1]sum(axis=-1)
array([ 1   3   1   1  -1  -1  -1   1])
>>> npnonzero(npabs(consecutive)==N)
(array([1]) )
",1491200.0,,1491200.0,,2012-09-12 00:06:26,2012-09-12 00:06:26,
12505089,2,12504976,2012-09-20 01:43:43,1,"You could use the tolist method as an intermediary:

In [99]: import pandas as pd

In [100]: d1 = pdDataFrame({'ticker' : ['spx 5/25/2001 p500'  'spx 5/25/2001 p600'  'spx 5/25/2001 p700']})

In [101]: d1tickerstrsplit()tolist()
Out[101]: 
[['spx'  '5/25/2001'  'p500'] 
 ['spx'  '5/25/2001'  'p600'] 
 ['spx'  '5/25/2001'  'p700']]


From which you could make a new DataFrame:

In [102]: d2 = pdDataFrame(d1tickerstrsplit()tolist()  
   :                   columns=""symbol date price""split())

In [103]: d2
Out[103]: 
  symbol       date price
0    spx  5/25/2001  p500
1    spx  5/25/2001  p600
2    spx  5/25/2001  p700


For good measure  you could fix the price:

In [104]: d2[""price""] = d2[""price""]strreplace(""p"" """")astype(float)

In [105]: d2
Out[105]: 
  symbol       date  price
0    spx  5/25/2001    500
1    spx  5/25/2001    600
2    spx  5/25/2001    700


PS: but if you really just want the last column  apply would suffice:

In [113]: temp2apply(lambda x: x[2])
Out[113]: 
0    p500
1    p600
2    p700
Name: ticker
",487339.0,,,,,2012-09-20 01:43:43,1.0
13053267,2,12504976,2012-10-24 16:13:48,1,"Do this:

In [43]: temp2str[-1]
Out[43]: 
0    p500
1    p600
2    p700
Name: ticker
",776560.0,,,,,2012-10-24 16:13:48,
12711422,2,12711211,2012-10-03 15:09:03,1,"In [604]: d = {'dates' : mydates  'prices': prices  'mylist': mylist}

In [605]: df= DataFrame(d)

In [606]: print df
   dates    mylist  prices
0      0  6907895    304
1      6 -2461538    325
2     15 -1577287    317
3     21  4807692    312
4     30  4281346    327
5     37  4985337    341
6     45  5586592    358
7     53 -3968254    378


EDIT:

As  Wouter Overmeire pointed out  you can use DataFrame({'prices': prices  'mylist': mylist}  index=columnanme) to index by column",1199589.0,,1199589.0,,2012-10-03 15:29:50,2012-10-03 15:29:50,3.0
13688105,2,13682044,2012-12-03 17:00:37,2,"There's a bug here: currently cannot pass arguments to strlstrip and strrstrip:

http://githubcom/pydata/pandas/issues/2411

EDIT: 2012-12-07 this works now on the dev branch:

In [8]: df['result']strlstrip('+-')strrstrip('aAbBcC')
Out[8]: 
1     52
2     62
3     44
4     30
5    110
Name: result
",776560.0,,776560.0,,2012-12-07 16:31:06,2012-12-07 16:31:06,1.0
13873014,2,13872533,2012-12-14 05:09:24,2,"Try:

ax = df1plot()
df2plot(ax=ax)
",1306530.0,,,,,2012-12-14 05:09:24,
13876784,2,13872533,2012-12-14 10:24:59,3,"Although Chang's answer explains how to plot multiple times on the same figure  in this case you might be better off in this case using a groupby and unstacking:

(Assuming you have this in dataframe  with datetime index already)

In [1]: df
Out[1]:
            value  
datetime                         
2010-01-01      1  
2010-02-01      1  
2009-01-01      1  

# create additional month and year columns for convenience
df['Month'] = map(lambda x: xmonth  dfindex)
df['Year'] = map(lambda x: xyear  dfindex)    

In [5]: dfgroupby(['Month' 'Year'])mean()unstack()
Out[5]:
       value      
Year    2009  2010
Month             
1          1     1
2        NaN     1


Now it's easy to plot (each year as a separate line):

dfgroupby(['Month' 'Year'])mean()unstack()plot()
",1240268.0,,,,,2012-12-14 10:24:59,1.0
14016590,2,14016247,2012-12-24 03:02:12,3,"For DataFrame df:

import numpy as np
index = df['b']index[df['b']apply(npisnan)]


will give you back the MultiIndex that you can use to index back into df  eg:

df['a']ix[index[0]]
>>> 1452354


For the integer index:

df_index = dfindexvaluestolist()
[df_indexindex(i) for i in index]
>>> [3  6]
",696023.0,,,,,2012-12-24 03:02:12,
14033137,2,14016247,2012-12-25 18:41:23,1,"Here is a simpler solution:

inds = pdisnull(df)any(1)nonzero()[0]

In [9]: df
Out[9]: 
          0         1
0  0450319  0062595
1 -0673058  0156073
2 -0871179 -0118575
3  0594188       NaN
4 -1017903 -0484744
5  0860375  0239265
6 -0640070       NaN
7 -0535802  1632932
8  0876523 -0153634
9 -0686914  0131185

In [10]: pdisnull(df)any(1)nonzero()[0]
Out[10]: array([3  6])
",776560.0,,,,,2012-12-25 18:41:23,1.0
14270628,2,14270163,2013-01-11 02:16:12,2,"HDF5-pytables is a pretty good storage back-end for pandas

Pytables can expand to be managed like a file system BUT care must be taken if you need any kind of multi-access (if you really need that you should use a traditional database that provides these features)",644898.0,,1240268.0,,2013-01-26 19:24:43,2013-01-26 19:24:43,
14271696,2,14270163,2013-01-11 04:30:05,0,Redis with redis-py is one solution Redis is really fast and there are nice Python bindings Pytables  as mentioned above  is a good choice as well PyTables is HDF5  and is really really fast ,1443118.0,,,,,2013-01-11 04:30:05,
14661768,2,14661701,2013-02-02 12:11:11,2,"Use DataFramedrop and pass it a Series of index labels:

In [65]: df
Out[65]: 
       one  two
one      1    4
two      2    3
three    3    2
four     4    1


In [66]: dfdrop(dfindex[[1 3]])
Out[66]: 
       one  two
one      1    4
three    3    2
",733291.0,,733291.0,,2013-02-02 12:34:41,2013-02-02 12:34:41,1.0
12962979,2,12962765,2012-10-18 20:13:50,1,Do you have repeating columns in the two DataFrames? If yes  try to resolve that to have unique column names in the two frames  and execute the reindex again,1405399.0,,,,,2012-10-18 20:13:50,5.0
13358304,2,13354866,2012-11-13 09:32:59,2,"boolean indexing all the way

In [65]: df[pdSeries(dfindexget_level_values('A'))isin(vals[vals['values']]index)]
Out[65]: 
      C
A B    
1 1  10
  2  15


Note that you can use xs on a multiindex

In [66]: dfxs(1)
Out[66]: 
    C
B    
1  10
2  15
",1548051.0,,,,,2012-11-13 09:32:59,
13583024,2,13582449,2012-11-27 11:16:51,0,"The result of klaggregate(npsum) is a normal DataFrame  you just have to assign it to a variable to further use it With some random data:

>>> df = DataFrame({'A' : ['foo'  'bar'  'foo'  'bar' 
>>>                         'foo'  'bar'  'foo'  'foo'] 
                  'B' : ['one'  'one'  'two'  'three' 
                         'two'  'two'  'one'  'three'] 
                  'C' : randn(8)  'D' : randn(8)})
>>> grouped = dfgroupby('A')
>>> grouped
<pandascoregroupbyDataFrameGroupBy object at 0x04E2F630>
>>> test = groupedaggregate(npsum)
>>> test
            C         D
A                      
bar -1852376  2204224
foo -3398196 -0045082
",653364.0,,,,,2012-11-27 11:16:51,1.0
13898325,2,13854140,2012-12-16 03:21:12,0,"There may be a better way  but perhaps dfmerge() would work here  dfmerge() works on two DataFrames  so the values computed for each (atom  ion) pair  which are in a Series after apply()  need to be placed in a DataFrame first  at which time the final column name can also be specified

In [9]: grouped_vals = grouped_dataapply(calc_group_func)

In [10]: grouped_vals
Out[10]: 
atomic_number  ion_number
0              0             0517541
               1             0046833
1              0             0253188
               1             0440194

In [11]: linesmerge(pdDataFrame({'group_val': grouped_vals}) 
   :             left_on=['atomic_number'  'ion_number'] 
   :             right_index=True)
Out[11]: 
    atomic_number  ion_number  group_val
id                                      
a               0           0   0517541
b               0           0   0517541
c               0           1   0046833
d               0           1   0046833
e               1           0   0253188
f               1           0   0253188
g               1           1   0440194
h               1           1   0440194
",243434.0,,,,,2012-12-16 03:21:12,
13978709,2,13977719,2012-12-20 18:58:48,0,"I could get a workaround by the following code:

    up = fpivot_table('user_time'  rows='date_time')
    sp = fpivot_table('sys_time'  rows='date_time')
    wp = fpivot_table('wait_io_time'  rows='date_time')
    u=pandasDataFrame(up)
    u['sys_time']=sp
    u['wait_io_time']=wp
    my_colors = [""#FF6666""  ""#00CC33""  ""#44EEEE""] 
    print u


Out:

                           user_time    sys_time  wait_io_time
    date_time                                          
    2012-11-01 08:59:27          3         1          0
    2012-11-01 08:59:32          0         0          0
    2012-11-01 08:59:37         20         2          1
    2012-11-01 08:59:42          0         0          0
    2012-11-01 08:59:47          0         0          0


There should be more straightforward ways to achieve this  but I am newB in pandas

Moreover  the uplot() functions fails in plotting a time series graph ""AttributeError: 'numpyint64' object has no attribute 'ordinal'"" So waiting to hear from others for a better solution",814907.0,,,,,2012-12-20 18:58:48,
13979142,2,13977719,2012-12-20 19:28:52,1,"You want to use set_index:

df1 = dfset_index('date_time')


Which selects the column 'date_time' as an index for the new DataFrame



Note: The behaviour you are coming across in the DataFrame constructor is demonstrated as follows:

df = pdDataFrame([[1 2] [3 4]])
df1 = pdDataFrame(df  index=[1 2])

In [3]: df1
Out[3]: 
    0   1
1   3   4
2 NaN NaN
",1240268.0,,1240268.0,,2012-12-20 19:35:32,2012-12-20 19:35:32,3.0
13214246,2,13213039,2012-11-03 22:44:49,2,"I think the easiest way to do this is to join on index I've tweaked the original variables to DataFrames to enable this (Note: they ought to be DataFrames rather than Series anyway):

r = pdDataFrame({'r': [1 2 3 4]})
s = pdDataFrame({'s': [2 4 1]})
u = pdDataFrame({'v': [8 6]})
v = pdDataFrame({'u': [4 3 1]})

rjoin([s  u  v]  how='outer')
#    r   s   v   u
# 0  1   2   8   4
# 1  2   4   6   3
# 2  3   1 NaN   1
# 3  4 NaN NaN NaN
",1240268.0,,,,,2012-11-03 22:44:49,
13462244,2,13460889,2012-11-19 21:07:07,3,"Caveats:

DataFrames have a lot of attributes If a DataFrame attribute is a number  you probably just want to return that number But if the DataFrame attribute is DataFrame you probably want to return a Container What should we do if the DataFrame attribute is a Series or a descriptor? To implement Container__getattr__ properly  you really
have to write unit tests for each and every attribute
Unit testing is also needed for __getitem__
You'll also have to define and unit test __setattr__ and __setitem__  __iter__  __len__  etc
Pickling is a form of serialization  so if DataFrames are picklable  I'm not sure how Containers really help with serialization
Some comments:

__getattr__ is only called if the attribute is not in self__dict__ So you do not need if item in self__dict__ in your __getattr__
selfcontained__getattr__(item) calls selfcontained's
__getattr__ method directly That is usually not what you want to
do  because it circumvents the whole Python attribute lookup
mechanism For example  it ignores the possibility that the attribute
could be in selfcontained__dict__  or in the __dict__ of one of
the bases of selfcontained__class__ or if item refers to a
descriptor Instead use getattr(selfcontained  item)
import pandas
import numpy as np

def tocontainer(func):
    def wrapper(*args  **kwargs):
        result = func(*args  **kwargs)
        return Container(result)
    return wrapper

class Container(object):
   def __init__(self  df):
       selfcontained = df
   def __getitem__(self  item):
       result = selfcontained[item]
       if isinstance(result  type(selfcontained)):
           result = Container(result)
       return result
   def __getattr__(self  item):
       result = getattr(selfcontained  item)
       if callable(result):
           result = tocontainer(result)
       return result
   def __repr__(self):
       return repr(selfcontained)


Here is some random code to test if -- at least superficially -- Container delegates to DataFrames properly and returns Containers:

df = pandasDataFrame(
    [(1  2)  (1  3)  (1  4)  (2  1) (2 2 )]  columns=['col1'  'col2'])
df = Container(df)
df['col1'][3] = 0
print(df)
#    col1  col2
# 0     1     2
# 1     1     3
# 2     1     4
# 3     2     1
# 4     2     2
gp = dfgroupby('col1')aggregate(npcount_nonzero)
print(gp)
#       col2
# col1      
# 1        3
# 2        2
print(type(gp))
# <class '__main__Container'>

print(type(gp[gpcol2 > 2]))
# <class '__main__Container'>

tf = gp[gpcol2 > 2]reset_index()
print(type(tf))
# <class '__main__Container'>

result = df[dfcol1 == tfcol1]
print(type(result))
# <class '__main__Container'>
",190597.0,,190597.0,,2012-11-19 21:27:36,2012-11-19 21:27:36,6.0
13808212,2,13807758,2012-12-10 19:57:30,2,"Use pandasDataFramereset_index()  the option drop=True will do what you are looking for 

In [14]: df = pdDataFrame(nprandomrandn(5 4))

In [15]: dfix[::3] = npnan

In [16]: df
Out[16]:
          0         1         2         3
0       NaN       NaN       NaN       NaN
1  1895803  0532464  1879883 -1802606
2  0078928  0053323  0672579 -1188414
3       NaN       NaN       NaN       NaN
4 -0766554 -0419646 -0606505 -0162188

In [17]: df = dfdropna()

In [18]: dfreset_index(drop=True)
Out[18]:
          0         1         2         3
0  1895803  0532464  1879883 -1802606
1  0078928  0053323  0672579 -1188414
2 -0766554 -0419646 -0606505 -0162188
",484596.0,,1240268.0,,2012-12-10 20:02:32,2012-12-10 20:02:32,1.0
12910278,2,12910187,2012-10-16 08:16:51,5,"use dfix[x:y] where x and y are datetime objects

Example:

In [117]: frameindexsummary()
Out[117]: 'DatetimeIndex: 6312960 entries  2000-04-05 00:01:00 to 2012-04-06 00:00:00\nFreq: T'


In [118]: x=datetime(2001  4  5  0  1)

In [119]: y=datetime(2001  4  5  0  5)

In [120]: print frameix[x:y]
                     radiation      tamb
2001-04-05 00:01:00  67958873  8077386
2001-04-05 00:02:00  50801294  0731453
2001-04-05 00:03:00  16042035  6944998
2001-04-05 00:04:00   5678343  9728967
2001-04-05 00:05:00  72551601  7652942


you can also do this:

In [121]: print frameix[x]
radiation    67958873
tamb          8077386
Name: 2001-04-05 00:01:00
",1199589.0,,1199589.0,,2012-10-16 08:55:17,2012-10-16 08:55:17,
13316106,2,13133458,2012-11-09 21:12:37,0,"The fast parser code only handles standard ISO-8601 with dashes and colons-- and as you can see it's super fast when the strings are the right format If you can be persuaded the code is on GitHub and could definitely be improved to handle more cases (preferably without slowing down the standard format too much) 

As a partially satisfying workaround you could use datetimestrptime to convert the strings to datetimedatetime  then pass that result to to_datetime:

In [4]: paste
rng = date_range('1/1/2000'  periods=2000000  freq='ms')
strings = [xstrftime('%Y%m%dT%H%M%S%f') for x in rng]

## -- End pasted text --

In [5]: iso_strings = [xstrftime('%Y-%m-%d %H:%M:%S%f') for x in rng]

In [6]: %timeit result = to_datetime(iso_strings)
1 loops  best of 3: 479 ms per loop

In [7]: f = lambda x: datetimestrptime(x  '%Y%m%dT%H%M%S%f')

In [8]: f(strings[0])
Out[8]: datetimedatetime(2000  1  1  0  0)

In [9]: %time result = to_datetime(map(f  strings))
CPU times: user 4847 s  sys: 001 s  total: 4848 s
Wall time: 4854 s


It's 100x different but much better than 1000+% slower I bet to_datetime could be improved to use the C version of strptime that would be much faster Exercise left to the reader  I guess

A todo for someday: http://githubcom/pydata/pandas/issues/2213",776560.0,,,,,2012-11-09 21:12:37,1.0
13787487,2,13786209,2012-12-09 12:49:00,4,"I took another look at this and realized that my previous answer fit poorly since it didn't include an intercept  I've updated my answer

The segfault comes from trying to us the Datetime index as the exogenous variable Instead try:

import datetime
import matplotlibpyplot as plt
import statsmodelsapi as sm
import pandas
from pandasiodata import DataReader

sp500 = DataReader(""AGG""  ""yahoo""  start=datetimedatetime(2000  1  1)) # returns a DataFrame
sp500[""regression""] = smOLS(sp500[""Adj Close""] 
    smadd_constant(range(len(sp500index)) 
    prepend=True))fit()fittedvalues


Notice that you don't need to call statsmodels' fittedvalues as a function
If your data points are all equally space this model will give the same results as using the actual index

For your second question  pandas as a built in exponentially weighted moving average that you may want to look in to: pandasewma here",1889400.0,,1889400.0,,2012-12-09 15:07:38,2012-12-09 15:07:38,3.0
13803365,2,13786209,2012-12-10 14:54:44,1,You're going to need to post more information - version numbers and how you built pandas - maybe a traceback would also help narrow things down I am unable to replicate a segfault with pandas 091 on 64-bit linux in IPython with or without pylab You might also want to report bugs on github issues rather than stack overflow Easier to get developers' attentions there,535665.0,,,,,2012-12-10 14:54:44,4.0
13925150,2,13924972,2012-12-18 01:49:55,4,"Suppose you have this setup:

import pandas as pd
import numpy as np
import datetime as DT

nan = npnan

trades = pdDataFrame({'ticker' : ['IBM'  'MSFT'  'GOOG'  'AAPL'] 
                       'date' : pddate_range('1/1/2000'  periods = 4)  
                       'cusip' : [nan  nan  100  nan]
                       })
trades = tradesset_index(['ticker'  'date'])
print(trades)
#                    cusip
# ticker date             
# IBM    2000-01-01    NaN
# MSFT   2000-01-02    NaN
# GOOG   2000-01-03    100  # <-- We do not want to overwrite this
# AAPL   2000-01-04    NaN

config = pdDataFrame({'ticker' : ['IBM'  'MSFT'  'GOOG'  'AAPL'] 
                       'date' : pddate_range('1/1/2000'  periods = 4) 
                       'cusip' : [1 2 3 nan]})
config = configset_index(['ticker'  'date'])

# Let's permute the index to show `DataFrameupdate` correctly matches rows based on the index  not on the order of the rows
new_index = sorted(configindex)
config = configreindex(new_index)    
print(config)
#                    cusip
# ticker date             
# AAPL   2000-01-04    NaN
# GOOG   2000-01-03      3
# IBM    2000-01-01      1
# MSFT   2000-01-02      2


Then you can update NaN values in trades with values from config using the DataFrameupdate method Note that DataFrameupdate matches rows based on indices (which is why set_index was called above)

tradesupdate(config  join = 'left'  overwrite = False)
print(trades)

#                    cusip
# ticker date             
# IBM    2000-01-01      1
# MSFT   2000-01-02      2
# GOOG   2000-01-03    100 # If overwrite = True  then 100 is overwritten by 3
# AAPL   2000-01-04    NaN
",190597.0,,190597.0,,2012-12-18 02:06:33,2012-12-18 02:06:33,
14079919,2,14079766,2012-12-29 08:11:53,2,"You could create a DataFrame from each user's data Then store them all in a Panel  Pandas will line them all up based on the timestamp and you can use the forward fill method  ffill  to propagate the values:

In [62]: df1 = DataFrame([45 34 32]  index=[5 6 8]  columns=['value'])

In [63]: df2 = DataFrame([35 32 32]  index=[5 7 9]  columns=['value'])

In [64]: p = Panel({'user1': df1  'user2': df2})

In [75]: pffill()to_frame()unstack()
Out[75]:
       user1  user2
minor  value  value
major
5         45     35
6         34     35
7         34     32
8         32     32
9         32     32


Or  you could do the same thing using just Series and DataFrames  I guess it depends on what you want to do with it:

In [78]: s1 = Series([45 34 32]  index=[5 6 8])

In [79]: s2 = Series([35 32 32]  index=[5 7 9])

In [80]: df = DataFrame([s1 s2])

In [81]: df
Out[81]:
    5   6   7   8   9
0  45  34 NaN  32 NaN
1  35 NaN  32 NaN  32

In [82]: df = DataFrame([s1 s2])T

In [83]: df
Out[83]:
    0   1
5  45  35
6  34 NaN
7 NaN  32
8  32 NaN
9 NaN  32

In [84]: dfffill()
Out[84]:
    0   1
5  45  35
6  34  35
7  34  32
8  32  32
9  32  32
",919872.0,,919872.0,,2012-12-29 08:35:45,2012-12-29 08:35:45,1.0
14359050,2,14356577,2013-01-16 12:58:14,1,"I had a quick look and there appear to be a number of situations in which the ipython magic
is not getting the conversion right I have to get in touch with them regarding rmagic and
more magic

In the meantime  you should be able to cook up what you need to progress from the snippet of code below:

import pandas
orders = pandasread_csv(""dellcsv""  sep="" "")
%load_ext rmagic

import rpy2robjects
d = dict()
for i  (k v) in enumerate(ordersiteritems()):
    print(""%s (type: %s - %i/%i)"" %(k  vdtypekind  i  ordersshape[1]))
    if vdtypekind == 'O':
      v = rpy2robjectsvectorsStrVector(v)
    d[k] = rpy2robjectsconversionpy2ri(v)
df = rpy2robjectsDataFrame(d)

def print_rsummary(x):
    print(rpy2robjectsbaseenv['summary'](x))

print_rsummary(df)
",294017.0,,294017.0,,2013-01-16 14:41:23,2013-01-16 14:41:23,
9608269,2,9597681,2012-03-07 20:04:31,1,"You can pass integers (0  1  2) instead of the names From the docstring:

converters : dict optional
    Dict of functions for converting values in certain columns Keys can either
    be integers or column labels
",776560.0,,,,,2012-03-07 20:04:31,1.0
10511230,2,10511024,2012-05-09 07:00:40,1,"With your import matplotlibpyplot as plt just add

pltshow()


and it will show all stored plots",449449.0,,,,,2012-05-09 07:00:40,1.0
10511545,2,10511024,2012-05-09 07:24:51,4,"Ok  It seems the answer is to start ipython notebook with --pylab=inline 
so ipython notebook --pylab=inline
This has it do what I saw earlier and what I wanted it to do 
Sorry about the vague original question ",442158.0,,,,,2012-05-09 07:24:51,2.0
11354319,2,11350488,2012-07-06 00:21:14,1,"I think working with dictionaries (and a list of dictionaries) is a good way to get familiar with working with data in python To format your data like this  you'll want to read in your text files and define variables line by line

To start:

for line in infile:
    if linestartswith(""Class""):
        temp class_var = linesplit(' ')
        class_var = class_varreplace(':' '')   
    elif linestartswith(""Subject""):
        temp subject = linesplit(' ')
        subject = subjectreplace(':' '')


This will create variables that correspond to the current class and current subject Then  you want to read in your numeric variables A good way to just read in those values is through a try statement  which will try to make them into integers

    else:
        line = linesplit("" "")
        try:
            keys = ['posX' 'posY' 'posZ' 'x_perc' 'y_perc']
            values = [int(item) for item in line]
            entry = dict(zip(keys values))
            entry['class'] = class_var
            entry['subject'] = subject
            outputListappend(entry)
        except ValueError:
            pass


This will put them into dictionary form  including the earlier defined class and subject variables  and append them to an outputList You'll end up with this:

[{'posX': 0  'x_perc': 81  'posZ': 0  'y_perc': 72  'posY': 2  'class': '1'  'subject': 'A'} 
{'posX': 0  'x_perc': 63  'posZ': 180  'y_perc': 38  'posY': 2  'class': '1'  'subject': 'A'}  ]


etc

You can then average/take SD by subsetting the list of dictionaries (applying rules like excluding posZ=180 etc) Here's for averaging by Class:

classes = ['1' '2']
print ""By Class:""
print ""Class"" ""Avg X"" ""Avg Y"" ""X SD"" ""Y SD""
for class_var in classes:   

    x_m = npmean([item['x_perc'] for item in output if item['class'] == class_var and item['posZ'] != 180])
    y_m = npmean([item['y_perc'] for item in output if item['class'] == class_var and item['posZ'] != 180])
    x_sd = npstd([item['x_perc'] for item in output if item['class'] == class_var and item['posZ'] != 180])
    y_sd = npstd([item['y_perc'] for item in output if item['class'] == class_var and item['posZ'] != 180])

    print class_var x_m y_m x_sd y_sd


You'll have to play around printed output to get exactly what you want  but this should get you started",721432.0,,,,,2012-07-06 00:21:14,1.0
11769819,2,11761884,2012-08-02 01:25:38,0,"It's a bug Opened an issue here:

http://githubcom/pydata/pandas/issues/1719

(Please use GitHub for pandas bug reports)",776560.0,,,,,2012-08-02 01:25:38,1.0
12152759,2,12152716,2012-08-28 04:25:10,2,"pandas has a replace method too:

In [25]: df = DataFrame({1: [2 3 4]  2: [3 4 5]})

In [26]: df
Out[26]: 
   1  2
0  2  3
1  3  4
2  4  5

In [27]: df[2]
Out[27]: 
0    3
1    4
2    5
Name: 2

In [28]: df[2]replace(4  17)
Out[28]: 
0     3
1    17
2     5
Name: 2

In [29]: df[2]replace(4  17  inplace=True)
Out[29]: 
0     3
1    17
2     5
Name: 2

In [30]: df
Out[30]: 
   1   2
0  2   3
1  3  17
2  4   5


or you could use numpy-style advanced indexing:

In [47]: df[1]
Out[47]: 
0    2
1    3
2    4
Name: 1

In [48]: df[1] == 4
Out[48]: 
0    False
1    False
2     True
Name: 1

In [49]: df[1][df[1] == 4]
Out[49]: 
2    4
Name: 1

In [50]: df[1][df[1] == 4] = 19

In [51]: df
Out[51]: 
    1   2
0   2   3
1   3  17
2  19   5
",487339.0,,487339.0,,2012-08-28 04:45:23,2012-08-28 04:45:23,5.0
13969237,2,13968520,2012-12-20 09:45:35,3,"I am not aware of predetermined schemes I usually use a few colours for publication plots I mostly take two things into consideration when choosing colours:

Colour-blindness: this page on wikipedia has lots of good info about choosing colours that are distinguishable to most color-blind people If you notice on the ""tips for editors"" section  once you take the guidelines into account there are only a few sets of colours available (A good rule of thumb is to never mix red and green!) You can also use the linked colour-blind simulators to see if your plot would be well visible
Luminance: most of the journals in my field will publish in B&W by default Even though most people read the papers online  I still like to make sure that the plots can be understood when printed in grayscale So I take care to use colours that have different luminances To test  a good way is to just desaturate the image produced  and you'll have a good idea of how it looks when printed in grayscale In many cases (particularly line or scatter plots)  I also use other things than colour to distinguish between sets (eg line styles  different markers) 
If no colours are specified in matplotlib plots  it has a default set of colours that it cycles through This answer has a good explanation on how to change that default set of colours You can customise that to your preferred set of colours  so the plots would use them in turn",1580351.0,,,,,2012-12-20 09:45:35,
13983684,2,13968520,2012-12-21 02:55:08,3,I would suggest the cubehelix color map  It is designed to have correct luminosity ordering in both color and gray-scale,380231.0,,,,,2012-12-21 02:55:08,
14218930,2,14218728,2013-01-08 15:55:00,4,"You could create a function: and applymap it to every entry in the dataframe:

powers = {'B': 1000000000  'M': 1000000  'T': 1000000000000}
# add some more to powers as necessary

def f(s):
    try:
        power = s[-1]
        return int(s[:-1]) * powers[power]
    except TypeError:
        return s

dfapplymap(f)
",1240268.0,,,,,2013-01-08 15:55:00,2.0
14449068,2,14446744,2013-01-21 23:27:54,0,"Have you tried using the parse_dates keyword in read_csv ?
http://pandaspydataorg/pandas-docs/stable/iohtml#specifying-date-columns",442158.0,,,,,2013-01-21 23:27:54,1.0
14616921,2,14616809,2013-01-31 01:19:05,0,"I wonder if there is a simpler  builtin way of doing this in pandas

import csv
f=open('CP2006RA_2006POAtxt' ""r"")
reader=csvreader(f delimiter=' ')
data = [l for l in reader]
fclose()
colnames = pdread_csv('CP2006RA_2006POAtxt')columns
df = DataFrame(columns=colnames  data=data[1:])


Result:

In [42]: df[:5]
Out[42]: 
  RA_2006_CODE               RA_2006_NAME POA_2006 POA_20061 RATIO PERCENT
0           10  Major Cities of Australia     2000       2000     1     100
1           10  Major Cities of Australia     2006       2006     1     100
2           10  Major Cities of Australia     2007       2007     1     100
3           10  Major Cities of Australia     2008       2008     1     100
4           10  Major Cities of Australia     2009       2009     1     100
",1479269.0,,1479269.0,,2013-01-31 02:49:04,2013-01-31 02:49:04,
14617055,2,14616809,2013-01-31 01:35:35,0,"Pandas provides a quick and easy way to import csv files using read_csv() (documentation here)  so in this case it's as easy as

df = pandasread_csv(""CP2006RA_2006POAtxt"")


Just a note: in your comment you had header=False  but this particular dataset does have a header- the column names are present as the first line of the text file",1222578.0,,,,,2013-01-31 01:35:35,2.0
12298872,2,12297759,2012-09-06 11:16:56,1,"If you would use the timestamps of the events as index of the series instead of the data  resample can do this In the example below  the index of series s are the timestamps and data is the event_id  basically the index of your series

In [47]: s
Out[47]:
                      event_id
timestamp
2012-09-05 19:28:52          0
2012-09-05 19:28:52          1
2012-09-05 19:44:37          2
2012-09-05 19:44:37          3
2012-09-05 20:04:53          4
2012-09-05 20:04:53          5
2012-09-05 20:12:59          6
2012-09-05 20:13:15          7
2012-09-05 20:13:15          8
2012-09-05 20:13:15          9


resample (this method can also be used on a DataFrame) will give a new series  with in this case 15min periods  the end time of a bucket (period) is used to refer to it (you can control this with the label arg)

In [48]: sresample('15Min'  how=len)
Out[48]:
                      event_id
timestamp
2012-09-05 19:30:00          2
2012-09-05 19:45:00          2
2012-09-05 20:00:00          0
2012-09-05 20:15:00          6
",1548051.0,,,,,2012-09-06 11:16:56,2.0
12570410,2,12569730,2012-09-24 18:05:26,2,"I found the answer

Using the data  dataframe from the question:

from pandas import *

P1Channels = datafilter(regex=""P1"")
P1Sum = P1Channelssum(axis=1)
",865662.0,,,,,2012-09-24 18:05:26,
12915331,2,12907231,2012-10-16 13:02:38,0,"You can simply join the dataFrame with your daily bar and use fillna(method=""ffill"") to forward fill the previous value  in your example you have adjustment factors for a range 

#suppose sbux is an ohlc daily bar and adj is your adjustment factors  
adj  = pandasDataFrame({""adj"":pandasSeries(values index = start_dates)})
sbux_with_adj  = sbuxjoin(adj)
sbux_with_adj[""Adj""] =    sbux_with_adj[""Adj""]fillna(method=""ffill"")
ohlc = sbux_with_adj[[""Open"" ""High"" ""Low"" ""Close""]] * sbux_with_adj[""adj""]


I would approach adjusting with more common adjustment factors (eg 985 for a 15% dividend) in the following manner:

sbux_with_adj  = sbuxjoin(adj)
sbux_with_adj[""Adj""] =    sbux_with_adj[""Adj""]fillna(10)
#now reverse cumulated adjustment
cumulated_adj = sbux_with_adj[""Adj""]cumprod()values[::-1] 
ohlc = sbux_with_adj[[""Open"" ""High"" ""Low"" ""Close""]] * cumulated_adj
",1064197.0,,1064197.0,,2012-10-16 18:44:55,2012-10-16 18:44:55,
13261966,2,13261737,2012-11-07 01:23:02,1,"concatenated = pdconcat([bb  cc])

concatenated

                      0         1
class  sample                    
2      22      0730631  0656266
       33      0871282  0942768
3      44      0081831  0714360
       55      0600095  0770108
5      66      0128320  0970817
       66      0160488  0969077
       77      0919263  0008597
6      77      0811914  0123960
       88      0639887  0262943
       88      0312303  0660786


Answer To Your Edited Question

So to answer your edited question  the problem lies with your column names having duplicates

 cols = ['d1']*2 + ['d2']*2  # <-- this creates ['d1'  'd1'  'd2'  'd2']


and your dataframes end up having what-is-considered duplicated columns  ie

In [62]: aa
Out[62]: 
                 d1        d1        d2        d2
class idx                                        
0     00   0805445  0442059  0296162  0041271
      11   0384600  0723297  0997918  0006661
1     22   0685997  0794470  0541922  0326008
      33   0117422  0667745  0662031  0634429


and 

In [64]: bb
Out[64]: 
                 d1        d1        d2        d2
class idx                                        
2     44   0465559  0496039  0044766  0649145
      55   0560626  0684286  0929473  0607542
3     66   0526605  0836667  0608098  0159471
      77   0216756  0749625  0096782  0547273
4     88   0619338  0032676  0218736  0684045
      99   0987934  0349520  0346036  0926373


pandasappend() (or concat() method) can only append correctly if you have unique column names

Try this and you will not get any error:-

cols2 = ['d1'  'd2'  'd3'  'd4']

cc = pandasDataFrame(x1  index=y1  columns=cols2)
ccindexnames = names

dd = pandasDataFrame(x2  index=y2  columns=cols2)
ccindexnames = names


Now

In [70]: ccappend(dd)
Out[70]: 
                 d1        d2        d3        d4
class idx                                        
0     00   0805445  0442059  0296162  0041271
      11   0384600  0723297  0997918  0006661
1     22   0685997  0794470  0541922  0326008
      33   0117422  0667745  0662031  0634429
2     44   0465559  0496039  0044766  0649145
      55   0560626  0684286  0929473  0607542
3     66   0526605  0836667  0608098  0159471
      77   0216756  0749625  0096782  0547273
4     88   0619338  0032676  0218736  0684045
      99   0987934  0349520  0346036  0926373
",482506.0,,482506.0,,2012-11-07 14:59:29,2012-11-07 14:59:29,5.0
12652397,2,12651618,2012-09-29 11:41:02,1,"I suspect this is a bug so I added it here

In the mean time (if you excuse the pun)  you could use the agg method:

test_gagg([npmean npmedian])
        A             B        
     mean  median  mean  median
 C                             
 1  1150    115   25     25
 2  3825    340   65     65
",1240268.0,,,,,2012-09-29 11:41:02,4.0
12652419,2,12651618,2012-09-29 11:44:59,2,"The reason is quite funny Probably some pandas specialists would want to chime in  but it comes down to a ping-pong between numpy and pandas Note that the documentation says:


  Function to use for aggregating groups If a function  must either
  work when passed a DataFrame or when passed to DataFrameapply If
  pass a dict  the keys must be DataFrame column names


The first thing is a 2D (array_like) the second method comes down to 1D array_likes being passed to the function you give in

This means aggregate passes first the 2D series in In the first case (npmean)  numpy knows that arrays have a mean attribute  so it does what it always does it calls this However it calls it with axis=None (default for numpy) This makes Pandas throw an Exception (it wants axis to be 0 or 1 and never None) and it goes to the second step  which passes it as 1D and is foolproof

However  when you give in npmedian numpy arrays do not have the median attribute  so it does the normal numpy machinery  which is to flatten the array (ie  typically axis=None)

The workaround would be to use test_gaggregate([npmedian  npmedian]) to force it to always take the second path or what would work too: test_gaggregate(npmedian  axis=0) which passes the axis=0 on into npmedian and thus tells numpy how to handle it correctly In generally I wonder if pandas should not at least throw a warning  afterall broadcasting the result to both columns should be almost never what is wanted",455221.0,,455221.0,,2012-09-29 11:52:13,2012-09-29 11:52:13,1.0
12657739,2,12651618,2012-09-30 00:42:15,1,"Also as a workaround  please note that pandas has shortcut methods for common operations:

In [12]: testgroupby('C')mean()
Out[12]: 
       A    B
C            
1  1150  25
2  3825  65

In [13]: testgroupby('C')median()
Out[13]: 
      A    B
C           
1  115  25
2  340  65


For things like sum  mean  median  max  min  first  last  std  you can call the method directly and not have to worry about the apply-to-DataFrame-but-failover-to-each-column mechanism in the GroupBy engine",1306530.0,,,,,2012-09-30 00:42:15,
13003683,2,13000427,2012-10-22 00:42:23,1,"There is a melt in pandascorereshape:

In [52]: melted = reshapemelt(df  id_vars=['DateTime'  'ColumnName'])

In [53]: meltedset_index(['DateTime'  'ColumnName'  'variable'])valueunstack([1  2])
Out[53]: 
ColumnName                  Percent_Used  Current_Count      Max  Percent_Used  Current_Count      Max  Percent_Used  Current_Count      Max
variable                             Min            Min      Min           Avg            Avg      Avg           Max            Max      Max
DateTime                                                                                                                                    
2012-10-14 11:29:23810000            24         254503  1048576            24         254503  1048576            24         254503  1048576
2012-10-14 11:34:23813000            24         254116  1048576            24         254116  1048576            24         254116  1048576


The columns end up being a MultiIndex  but if that's a deal breaker for you just concat the names and make it a regular Index",1306530.0,,,,,2012-10-22 00:42:23,6.0
11312776,2,11298097,2012-07-03 14:11:33,1,"If the Series are in a dict data  you need only do:

frame = DataFrame(data)

That puts things into a DataFrame and unions all the dates If you want to fill values forward  you can call frame = framefillna(method='ffill')",776560.0,,,,,2012-07-03 14:11:33,1.0
12287627,2,11697887,2012-09-05 18:28:43,0,From the Django perspective (I'm not familiar with pandas) this is fine My only concern is that if you have a very large number of records  you may run into memory problems If this were the case  something along the lines of this memory efficient queryset iterator would be necessary (The snippet as written might require some rewriting to allow for your smart use of values()),18950.0,,,,,2012-09-05 18:28:43,
12030465,2,12030398,2012-08-19 22:20:15,2,"How about something like this?

>>> from pandas import *
>>> df = DataFrame({'A':['a' 'b' 'c']  'B':['d' 'e' 'f']  'C':['concat' 'me' 'yo']  'D':['me' 'too' 'tambien']})
>>> df
   A  B       C        D
0  a  d  concat       me
1  b  e      me      too
2  c  f      yo  tambien
>>> dfcolumns[2:]
Index([C  D]  dtype=object)
>>> df[dfcolumns[2:]]
        C        D
0  concat       me
1      me      too
2      yo  tambien
>>> [' 'join(row) for row in df[dfcolumns[2:]]values]
['concat me'  'me too'  'yo tambien']
>>> df[""new""] = [' 'join(row) for row in df[dfcolumns[2:]]values]
>>> df
   A  B       C        D         new
0  a  d  concat       me   concat me
1  b  e      me      too      me too
2  c  f      yo  tambien  yo tambien


If you have None objects floating around  you could handle that too  For example:

>>> df[""C""][1] = None
>>> df
   A  B       C        D
0  a  d  concat       me
1  b  e    None      too
2  c  f      yo  tambien
>>> rows = df[dfcolumns[2:]]values


In near-English:

>>> new = [' 'join(word for word in row if word is not None) for row in rows]
>>> new
['concat me'  'too'  'yo tambien']


Using filter:

>>> new = [' 'join(filter(None  row)) for row in rows]
>>> new
['concat me'  'too'  'yo tambien']


etc  You could do it in one line but I think it's clearer to separate it",487339.0,,487339.0,,2012-08-19 22:36:21,2012-08-19 22:36:21,4.0
12207352,2,12207326,2012-08-31 00:14:46,6,"Maybe value_counts()?

>>> import pandas
>>> my_series = pandasSeries([1 2 2 3 3 3  ""fred""  18  18])
>>> my_series
0       1
1       2
2       2
3       3
4       3
5       3
6    fred
7     18
8     18
>>> counts = my_seriesvalue_counts()
>>> counts
3       3
2       2
18     2
fred    1
1       1
>>> len(counts)
5
>>> sum(counts)
9
>>> counts[""fred""]
1
>>> dict(counts)
{18: 2  2: 2  3: 3  1: 1  'fred': 1}
",487339.0,,487339.0,,2012-09-26 20:23:30,2012-09-26 20:23:30,1.0
12393132,2,12391758,2012-09-12 17:05:39,2,"Usually  if you mind memory usage  it's better to use generators instead of creating a list ahead
Something like:

dir_path = ospathjoin(data_dir  'master_*dat')
master_all = pdconcat(pdread_table(data_file  delimiter='|'  header=0)
                                     for data_file in globglob(dir_path))


Or you can write a generator function for a more verbose version

Anyway this wont solve the problem if the RAM is not enough to contain the final result + some temp space for at list a complete file(and probably more it depends on how the garbage collector works)",510937.0,,510937.0,,2012-11-13 17:01:47,2012-11-13 17:01:47,2.0
12549057,2,12548349,2012-09-23 01:33:09,2,"The problem with your original filter is it checks for 'NaN' rather than numpynan  which is what empty strings are parsed as by default
If you want to filter all the columns so you only get rows where no element is 'X' or 'XX'  do something like this:

In [45]: names = ['a'  'b'  'c'  'd'  'e'  'f'  'g'  'h'  'i'  'j'  'k'  'l']

In [46]: df = pdread_csv(StringIO(data)  header=None  names=names)

In [47]: mask = dfapplymap(lambda x: x in ['X'  'XX'  None  npnan])

In [48]: df[-maskany(axis=1)]
Out[48]: 
<class 'pandascoreframeDataFrame'>
Int64Index: 5 entries  0 to 9
Data columns:
a    5  non-null values
b    5  non-null values
c    5  non-null values
d    5  non-null values
e    5  non-null values
f    5  non-null values
g    5  non-null values
h    5  non-null values
i    5  non-null values
j    4  non-null values
k    5  non-null values
l    5  non-null values
dtypes: float64(6)  int64(1)  object(5)
",1306530.0,,,,,2012-09-23 01:33:09,1.0
5558929,2,5558607,2011-04-05 21:44:36,0,"Did you try it? At least in the version of pandas I tried  DataMatrix inherits from DataFrame

>>> type(dm)
<class 'pandascorematrixDataMatrix'>
>>> dmsort()
                       compound_ret    
2011-02-08 00:00:00   -06986         
2011-02-09 00:00:00    01846         
2011-02-10 00:00:00    02312         
2011-02-11 00:00:00    1844          
2011-02-12 00:00:00    03662         
2011-02-13 00:00:00    01331         
2011-02-14 00:00:00    05166         
2011-02-15 00:00:00    137           
2011-02-16 00:00:00    09346         

>>> dmsort(ascending=False)                                                    
                       compound_ret    
2011-02-16 00:00:00    09346         
2011-02-15 00:00:00    137           
2011-02-14 00:00:00    05166         
2011-02-13 00:00:00    01331         
2011-02-12 00:00:00    03662         
2011-02-11 00:00:00    1844          
2011-02-10 00:00:00    02312         
2011-02-09 00:00:00    01846         
2011-02-08 00:00:00   -06986         
",6372.0,,,,,2011-04-05 21:44:36,4.0
5560792,2,5558607,2011-04-06 02:23:06,0,"I definitely tried it I know that DataMatrix inherits from DataFrame and I assumed sort would too

>>>dm

<class 'pandascorematrixDataMatrix'>


Then when I run it

Traceback (most recent call last):
  File ""pandasreturnspy""  line 194  in &lt;module&gt;
    returns = Returns(dm)
  File ""pandasreturnspy""  line 43  in __init__
    print data_matrixsort()
AttributeError: 'DataMatrix' object has no attribute 'sort'
",687739.0,,,,,2011-04-06 02:23:06,
6468964,2,5558607,2011-06-24 14:08:42,6,"Indeed between 02 and 03 I renamed sortUp/sortDown to the single sort methods Sorry about that

I definitely recommend keeping up on the bleeding edge of pandas if you can ( https://githubcom/wesm/pandas )! Also  consider using IPython for all your interactive work ( http://ipythonscipyorg )-- I find that having tab completion and easy introspection of objects helps a great deal for finding methods and exploring docstrings",776560.0,,,,,2011-06-24 14:08:42,
10149202,2,10133021,2012-04-13 22:40:25,5,"I have an open issue to work on problems like these:

https://githubcom/pydata/pandas/issues/388

Here is a solution First a simple (and not very efficient) function to get the group ordinal number:

def group_position(*args):
    """"""
    Get group position
    """"""
    from collections import defaultdict
    table = defaultdict(int)

    result = []
    for tup in zip(*args):
        resultappend(table[tup])
        table[tup] += 1

    return nparray(result)


ie

In [49]: group_position(df['Z']  df['A'])
Out[49]: array([0  1  2  0  1  0  1  2  0  0])


Now use this as an auxiliary index variable and unstack:

In [52]: df
Out[52]: 
     Z    A    beta2      M  shell
0  100  200  031120  1972 -4213
1  100  200 -041970  2020 -1143
2  100  200  003205  2030  0000
3  100  201  029670  1910 -4434
4  100  201 -048930  1961 -4691
5  100  202  030840  1834 -4134
6  100  202 -048730  1882 -4750
7  100  202 -024830  1884 -1106
8  100  203  030690  1771 -4355
9  101  203 -049560  1825 -5217

In [53]: df['pos'] = group_position(df['Z']  df['A'])

In [54]: dfset_index(['Z'  'A'  'pos'])unstack('pos')
Out[54]: 
          beta2                       M                shell              
pos           0       1        2      0      1      2      0      1      2
Z   A                                                                     
100 200  03112 -04197  003205  1972  2020  2030 -4213 -1143  0000
    201  02967 -04893      NaN  1910  1961    NaN -4434 -4691    NaN
    202  03084 -04873 -024830  1834  1882  1884 -4134 -4750 -1106
    203  03069     NaN      NaN  1771    NaN    NaN -4355    NaN    NaN
101 203 -04956     NaN      NaN  1825    NaN    NaN -5217    NaN    NaN


Final munging to get it exactly like you showed:

In [61]: result = dfset_index(['Z'  'A'  'pos'])unstack('pos')

In [62]: resultrename(columns=lambda x: '%s[%d]' % (x[0]  x[1]+1))reset_index()
Out[62]: 
     Z    A  beta2[1]  beta2[2]  beta2[3]   M[1]   M[2]   M[3]  shell[1]  shell[2]  shell[3]
0  100  200    03112   -04197   003205  1972  2020  2030    -4213    -1143     0000
1  100  201    02967   -04893       NaN  1910  1961    NaN    -4434    -4691       NaN
2  100  202    03084   -04873  -024830  1834  1882  1884    -4134    -4750    -1106
3  100  203    03069       NaN       NaN  1771    NaN    NaN    -4355       NaN       NaN
4  101  203   -04956       NaN       NaN  1825    NaN    NaN    -5217       NaN       NaN
",776560.0,,,,,2012-04-13 22:40:25,1.0
14472166,2,14471515,2013-01-23 03:52:18,1,"I acheived the above usign below code Let me know if this is the best way to do it Note  each row is a list of dictionaries in the below code

frames=[]
for row in selfrowResults:
    framesappend(pandasDataFrame(row))
selfresults = pandasconcat(frames)
",899811.0,,,,,2013-01-23 03:52:18,
14624704,2,14623935,2013-01-31 11:31:38,0,"With df being the name of your dataframe and c is your constant  try:

df['Premium'] = dfPremium / ( 1 + dfRate ) ** (c - dfQtr)
df['Claim'] = dfClaim / ( 1 + dfRate ) ** ( c - dfQtr )


Update for comment  I am sure there is a more pythonic way of doing this  but this works:

columns = dfcolumns
subset_cols = columnsdrop(['Rate' 'Qtr'])
for col in subset_cols:
    df[col] = df[col] / ( 1 + dfRate ) ** (c- dfQtr)


2nd update  you could extract the calculation into a function and carry out the process in a list comprehension

def calc(df  col  c):
    df[col] = df[col] / ( 1 + dfRate ) ** ( c - dfQtr )
[calc(df  col  c) for col in dfcolumnsdrop(['Rate' 'Qtr'])]
",1135883.0,,1135883.0,,2013-01-31 16:27:19,2013-01-31 16:27:19,4.0
9557224,2,9556892,2012-03-04 17:35:40,2,"I created several issues just now to address some features / conveniences that I think would be nice to have: GH-856  GH-857  GH-858

We're currently working on a revamp of the time series capabilities and doing alignment to secondly resolution is possible now (though not with duplicates  so would need to write some functions for that) I also want to support duplicate timestamps in a better way However  this is really panel (3D) data  so one way that you might alter things is the following:

In [29]: dfpivot('Symbol'  'TimeStamp')stack()
Out[29]: 
                   M1    M2   Price   Volume
Symbol TimeStamp                            
FUEL   9:58:40 AM   6  105  38544   100116
       9:58:47 AM   7  107  38599   102116
       9:59:09 AM   8  111  39099   105265
       9:59:11 AM   9  115  39490   109674
GBR    9:57:52 AM   2  034  37500    47521
       9:58:20 AM   3  045  38000    63211
       9:58:24 AM   4  046  38300    64251
MPET   9:57:52 AM   3  026  14200    44600
ORBC   9:59:02 AM   2  022  34000    10509
SUNH   9:59:09 AM   6  009  43700    24394
TBET   9:59:05 AM   2  803  21800  1121629
       9:59:14 AM   3  805  21900  1124179
XRA    9:58:08 AM   3  012  36167    42310


note that this created a MultiIndex Another way I could have gotten this:

In [32]: dfset_index(['Symbol'  'TimeStamp'])
Out[32]: 
                    Price  M1    M2   Volume
Symbol TimeStamp                            
TBET   9:59:14 AM  21900   3  805  1124179
FUEL   9:59:11 AM  39490   9  115   109674
SUNH   9:59:09 AM  43700   6  009    24394
FUEL   9:59:09 AM  39099   8  111   105265
TBET   9:59:05 AM  21800   2  803  1121629
ORBC   9:59:02 AM  34000   2  022    10509
FUEL   9:58:47 AM  38599   7  107   102116
       9:58:40 AM  38544   6  105   100116
GBR    9:58:24 AM  38300   4  046    64251
       9:58:20 AM  38000   3  045    63211
XRA    9:58:08 AM  36167   3  012    42310
GBR    9:57:52 AM  37500   2  034    47521
MPET   9:57:52 AM  14200   3  026    44600

In [33]: dfset_index(['Symbol'  'TimeStamp'])sortlevel(0)
Out[33]: 
                    Price  M1    M2   Volume
Symbol TimeStamp                            
FUEL   9:58:40 AM  38544   6  105   100116
       9:58:47 AM  38599   7  107   102116
       9:59:09 AM  39099   8  111   105265
       9:59:11 AM  39490   9  115   109674
GBR    9:57:52 AM  37500   2  034    47521
       9:58:20 AM  38000   3  045    63211
       9:58:24 AM  38300   4  046    64251
MPET   9:57:52 AM  14200   3  026    44600
ORBC   9:59:02 AM  34000   2  022    10509
SUNH   9:59:09 AM  43700   6  009    24394
TBET   9:59:05 AM  21800   2  803  1121629
       9:59:14 AM  21900   3  805  1124179
XRA    9:58:08 AM  36167   3  012    42310


you can get this data in a true panel format like so:

In [35]: dfset_index(['TimeStamp'  'Symbol'])sortlevel(0)to_panel()
Out[35]: 
<class 'pandascorepanelPanel'>
Dimensions: 4 (items) x 11 (major) x 7 (minor)
Items: Price to Volume
Major axis: 9:57:52 AM to 9:59:14 AM
Minor axis: FUEL to XRA

In [36]: panel = dfset_index(['TimeStamp'  'Symbol'])sortlevel(0)to_panel()

In [37]: panel['Price']
Out[37]: 
Symbol        FUEL   GBR  MPET  ORBC  SUNH  TBET     XRA
TimeStamp                                               
9:57:52 AM     NaN  375  142   NaN   NaN   NaN     NaN
9:58:08 AM     NaN   NaN   NaN   NaN   NaN   NaN  36167
9:58:20 AM     NaN  380   NaN   NaN   NaN   NaN     NaN
9:58:24 AM     NaN  383   NaN   NaN   NaN   NaN     NaN
9:58:40 AM  38544   NaN   NaN   NaN   NaN   NaN     NaN
9:58:47 AM  38599   NaN   NaN   NaN   NaN   NaN     NaN
9:59:02 AM     NaN   NaN   NaN   34   NaN   NaN     NaN
9:59:05 AM     NaN   NaN   NaN   NaN   NaN  218     NaN
9:59:09 AM  39099   NaN   NaN   NaN  437   NaN     NaN
9:59:11 AM  39490   NaN   NaN   NaN   NaN   NaN     NaN
9:59:14 AM     NaN   NaN   NaN   NaN   NaN  219     NaN


you can then generate some plots from that data 

note here that the timestamps are still as strings-- I guess they could be converted to Python datetimetime objects and things might be a bit easier to work with I don't have many plans to provide a lot of support for raw times vs timestamps (date + time) but if enough people need it I suppose I can be convinced :)

If you have multiple observations on a second for a single symbol then some of the above methods will not work But I want to build in better support for that in upcoming releases of pandas  so knowing your use cases will be helpful to me-- consider joining the mailing list (pystatsmodels)",776560.0,,15061.0,,2012-04-12 18:14:41,2012-04-12 18:14:41,2.0
10377863,2,10376647,2012-04-30 02:30:34,5,"argh  you've got two pythons in your path that are the same version?  don't do that

pip  easy-install  etc are associated with a particular python install and will use that python by default  so if you have a system-provided python and a system-provided easy_install (or installed easy_install yourself using the system python) then easy_install will  by default  install packages for the system python

the best way to avoid this mess  imho  is to use use system python for that version (27 probably) and  for other versions  use make alt-install when installing  which will give you executables like python31 and the like  if you really need to replace the version provided by the system  uninstall it

once you've done that each python will have a distinct name (ending in the version) and python will remain the system one

next  when you install easy_install  you'll notice that there are version-specific versions (easy_install-27 for example)  use those  if one is missing  then install distutils with the appropriate python (eg use python31 and you'll get an easy_install-31)  unfortunately  each time you do this (iirc) you overwrite the un-versioned easy_install  so never use that - always use the versioned one

alternatively  you could not install easy_install or pip for anything other than the system version  then always use virtualenv  virtualenv will let you specify a python version (so you can use the system virtualenv for all pythons installed) and then installs easy_install/pip for the python you use  so once you're inside the virtual environment  everything just works

and i just realised i haven't much experience with pip  so i can't actually help with that (except to note that virtualenv does provide it) (about which is preferable: it used to be that pip was better maintained; i think these days the latest distutils/easy_install is as good as pip  but pip has a few more features that i have never used)

disclaimer: the above is from experience gained developing lepl  which runs on 26 to 32  so i need to test it on all those  as far as i know  what i describe above works for me  but i have no deep knowledge of python/easy_install/pip so i may have some mistakes in rationalising/describing things (in other words  i'm writing all this in case it helps  but i'm a bit worried i have an error - please  someone correct me if so)",181772.0,,181772.0,,2012-04-30 15:59:40,2012-04-30 15:59:40,
10982198,2,10982089,2012-06-11 14:35:02,4,"In [18]: a
Out[18]: 
   x1  x2
0   0   5
1   1   6
2   2   7
3   3   8
4   4   9

In [19]: ax2 = ax2shift(1)

In [20]: a
Out[20]: 
   x1  x2
0   0 NaN
1   1   5
2   2   6
3   3   7
4   4   8
",449449.0,,,,,2012-06-11 14:35:02,0.0
14617791,2,14616809,2013-01-31 03:05:21,0,"This is WAY better:

df = pdread_csv('CP2006RA_2006POAtxt'  lineterminator='\r')

In [11]: df[:5]
Out[11]: 
   RA_2006_CODE               RA_2006_NAME  POA_2006  POA_20061  RATIO  PERCENT\r\r
0            10  Major Cities of Australia      2000        2000      1          100
1            10  Major Cities of Australia      2006        2006      1          100
2            10  Major Cities of Australia      2007        2007      1          100
3            10  Major Cities of Australia      2008        2008      1          100
4            10  Major Cities of Australia      2009        2009      1          100
",1479269.0,,,,,2013-01-31 03:05:21,
9526061,2,9516422,2012-03-01 23:52:59,0,"NAs / NaN is not supported in integer dtype columns unfortunately (http://pandaspydataorg/pandas-docs/stable/gotchashtml#support-for-integer-na) But the second thing you describe is a bug Creating a GitHub issue about it:

https://githubcom/pydata/pandas/issues/846",776560.0,,,,,2012-03-01 23:52:59,
10256600,2,10256229,2012-04-21 05:50:57,1,"OK  quick and dirty You can improve it:

from datetime import datetime as dt
from collections import defaultdict

dd = defaultdict(list)

with open('testtxt') as f:
    for line in f:
        lines = linesplit('\t')
        dd[lines[0]]append(lines)

def mydate(line):
    return dtstrptime(line[2]  ""%B %d  %Y"")

keys = sorted(ddkeys())

my_list = []
for key in keys:
    dd[key]sort(key=mydate)
    my_listextend(dd[key])

for item in my_list:
    print item


this produces:

['AX'  '2313'  'April 19  2009'  '2'  '3'  '40'  'hi there\n']
['AX'  '4532'  'December 19  2010'  '6'  '2'  '80'  'nice tie\n']
['AX'  '0123'  'December 20  2010'  '1'  '2'  '80'  'hello this\n']
['AX'  '1244'  'January 10  2011'  '3'  '4'  '80'  'king tale\n']
['BX'  '0114'  'February 9  2003'  '4'  '9'  '40'  'his brought\n']
['BX'  '3214'  'September 1  2006'  '1'  '3'  '30 is great\n']
['BX'  '0214'  'September 10  2009'  '2'  '3'  '90 this king\n']
['MG'  '246'  'May 8  2005'  '5'  '1'  '21'  'make goat']
['MG'  '980'  'April 20  2007'  '2'  '4'  '71'  'not available\n']


then you only need to stringjoin() all the lists

text_lines = []
for item in my_list:
    text_linesappend('\t'join(item))

full_text = ''join(text_lines)
",308903.0,,,,,2012-04-21 05:50:57,1.0
10257695,2,10256229,2012-04-21 09:19:28,0,"pandas is a python library designed for analysing data sets with different datatypes

If your data is in datatxt  you can read it with pandasread_csv() and than sort the resulting DataFrame

>>> import datetime
>>> import pandas as pd

>>> def date_converter(date_string):
     return datetimedatetimestrptime(datestring  '%B %d  %Y')date()
>>> df = pdread_csv('datatxt'  sep='\t'  header=None 
                  converters={2:date_converter})
>>> print df
  X1   X2         X3  X4  X5  X6            X7
0  AX   123  2010-12-20    1    2  80     hello this
1  AX  2313  2009-04-19    2    3  40       hi there
2  AX  4532  2010-12-19    6    2  80       nice tie
3  AX  1244  2011-01-10    3    4  80      king tale
4  BX   214  2009-09-10    2    3  90      this king
5  BX   114  2003-02-09    4    9  40    his brought
6  BX  3214  2006-09-01    1    3  30       is great
7  MG   980  2007-04-20    2    4  71  not available
8  MG   246  2005-05-08    5    1  21      make goat

>>> df = dfset_index(['X1'  'X3'])  # using a hierarchical index
>>> df = dfsort_index()
>>> print df
                 X2  X4  X5  X6            X7
X1 X3                                           
AX  2009-04-19  2313    2    3  40       hi there
    2010-12-19  4532    6    2  80       nice tie
    2010-12-20   123    1    2  80     hello this
    2011-01-10  1244    3    4  80      king tale
BX  2003-02-09   114    4    9  40    his brought
    2006-09-01  3214    1    3  30       is great
    2009-09-10   214    2    3  90      this king
MG  2005-05-08   246    5    1  21      make goat
    2007-04-20   980    2    4  71  not available


As it is numpy based  it should be the right choice for large data sets",1301710.0,,1301710.0,,2012-04-26 17:11:18,2012-04-26 17:11:18,
13552178,2,13550940,2012-11-25 14:57:37,0,"This won't tell you what to do  except try on a 64-bit computer or contact pandas developers (or patch the problem yourself) But at any rate  this seems to be your problem:

The problem is that DataFrame does not understand unsigned int 64 bit  at least on a 32-bit machine

I changed the values of your data_score to better be able to track what was happening:

data_scores = [(2**31 + 1  273)  (2 ** 31 - 1  23)  (2 ** 32 + 1  45)  (2 ** 63 - 1  270)  (2 ** 63 + 1  273)]


Then I tried:

In [92]: datadtype
Out[92]: dtype([('uid'  '<u8')  ('score'  '<u8')])

In [93]: data
Out[93]: 
array([(2147483649L  273L)  (2147483647L  23L)  (4294967297L  45L) 
       (9223372036854775807L  270L)  (9223372036854775809L  273L)]  
      dtype=[('uid'  '<u8')  ('score'  '<u8')])

In [94]: df = DataFrame(data  dtype='uint64')

In [95]: dfvalues
Out[95]: 
array([[2147483649                   273] 
       [2147483647                    23] 
       [4294967297                    45] 
       [9223372036854775807                   270] 
       [-9223372036854775807                   273]]  dtype=int64)


Notice how the dtype of DataFrame doesn't match the one requested in row 94 Also as I wrote in the comment above  the numpy array works perfectly Further  if you specify uint32 in row 94 it still specifies a dtype of int64 for the DataFrame values However it doesn't give you negative overflows  probably because uint32 fits inside the positive values of the int64",1099682.0,,1099682.0,,2012-11-25 15:03:25,2012-11-25 15:03:25,2.0
13553955,2,13550940,2012-11-25 18:16:52,1,"Yes-- it's a present limitation of pandas-- we do plan to add support for unsigned integer dtypes in the future An error message would be much better:

http://githubcom/pydata/pandas/issues/2355

For now you can make the column dtype=object as a workaround 

EDIT 2012-11-27

Detecting overflows now  though will become dtype=object for now until DataFrame has better support for unsigned data types

In [3]: df_crawls
Out[3]: 
                    uid  score
0   6311132704823138710    273
1   2685045978526272070     23
2   8921811264899370420     45
3  17019687244989530680    270
4   9930107427299601010    273

In [4]: df_crawlsdtypes
Out[4]: 
uid      object
score     int64
",776560.0,,776560.0,,2012-11-28 02:43:37,2012-11-28 02:43:37,
13690416,2,13690122,2012-12-03 19:39:17,0,"from StringIO import StringIO
import pandas as pd

a = ['Venezuela'  'N/A'  'President'  '10/7/12'  'Hugo Rafael Chavez Frias'  'Hugo Ch\xc3\xa1vez'  'Hugo Ch\xc3\xa1vez'  'Hugo Chavez'  'Hugo Ch\xc3\xa1vez Fr\xc3\xadas'  'Hugo Chavez'  'Hugo Ch\xc3\xa1vez']

pdread_csv(StringIO('\t'join(a))  delimiter='\t')


works here can upload the head of your data so I can test",239007.0,,,,,2012-12-03 19:39:17,
13693097,2,13690122,2012-12-03 22:42:58,1,"This is a bug  I think because csv reader was passing back an extra empty line in the beginning It worked for me on Python 273 and pandas 091 if I do:

In [36]: pdread_csv(BytesIO(fhread()decode('UTF-16')encode('UTF-8'))  sep='\t'  header=0)
Out[36]: 
<class 'pandascoreframeDataFrame'>
Int64Index: 50 entries  0 to 49
Data columns:
Country                             43  non-null values
State/City                          43  non-null values
Title                               43  non-null values
Date                                43  non-null values
Catalogue                           43  non-null values
Wikipedia Election Page             43  non-null values
Wikipedia Individual Page           43  non-null values
Electoral Institution in Country    43  non-null values
Twitter                             43  non-null values
CANDIDATE NAME 1                    43  non-null values
CANDIDATE NAME 2                    16  non-null values
dtypes: object(11)


I reported the bug here: https://githubcom/pydata/pandas/issues/2418
On github master it unfortunately causes a segfault in the c-parser We'll fix it

Now  interestingly: http://programmersstackexchangecom/questions/102205/should-utf-16-be-considered-harmful ;)",1306530.0,,1306530.0,,2012-12-03 23:28:27,2012-12-03 23:28:27,3.0
13930121,2,13929884,2012-12-18 09:43:16,1,"You can use plot_date:

plot_date(dfindex  dfdata)
",1240268.0,,,,,2012-12-18 09:43:16,2.0
14198133,2,14198103,2013-01-07 14:43:03,3,"Check asof

pricesasof('2013-01-01 13:37:42')


returns the value for the previous available datetime:

prices['2013-01-01 13:00:00']


To make calculations  you can use:

pricesasof(amountsindex) * amounts


which returns a Series with amount's Index and the respective values:

>>> prices
2013-01-01 12:00:00    0943607
2013-01-01 13:00:00   -1019452
2013-01-01 14:00:00   -0279136
2013-01-01 15:00:00    1013548
2013-01-01 16:00:00    0929920

>>> pricesasof(amountsindex)
2013-01-01 13:37:00   -1019452
2013-01-01 13:57:00   -1019452
2013-01-01 14:05:00   -0279136

>>> pricesasof(amountsindex) * amounts
2013-01-01 13:37:00   -1019452
2013-01-01 13:57:00   -2038904
2013-01-01 14:05:00   -0558272
",449449.0,,449449.0,,2013-01-07 14:56:38,2013-01-07 14:56:38,3.0
9300391,2,8270129,2012-02-15 19:58:06,1,"Sorry I missed this question some time ago Could you have a look at the new concat function in pandas 070 and see if it meets your needs:

http://pandaspydataorg/pandas-docs/stable/merginghtml#concatenating-objects

I recently spent a great deal of time on the join and concatenation methods",776560.0,,,,,2012-02-15 19:58:06,1.0
10218573,2,10215259,2012-04-18 21:50:13,1,"Try:

result = fdfgroupby(['row' col'])['percent']sum()
resultunstack('col')values


Alternately:

fdfpivot_table('percent'  rows='row'  cols='col'  aggfunc='sum')values
",776560.0,,,,,2012-04-18 21:50:13,
10795379,2,10791661,2012-05-29 08:13:14,2,"You can use some kind of broadcasting:

    In [58]: df
    Out[58]:
           a    b  c
    0    one  02  0
    1    two  04  1
    2    two  09  0
    3  three  01  2
    4    one  00  4
    5    two  02  5

    In [41]: (dfavalues[: numpynewaxis] == dfaunique())astype(int)
    Out[41]:
    array([[1  0  0] 
           [0  1  0] 
           [0  1  0] 
           [0  0  1] 
           [1  0  0] 
           [0  1  0]])

    In [54]: ((0 <= dfbvalues[: numpynewaxis]) & (dfbvalues[: numpynewaxis] < 02))astype(int)
    Out[54]:
    array([[0] 
           [0] 
           [0] 
           [1] 
           [1] 
           [0]])

    In [59]: (dfcvalues[: numpynewaxis] == dfcunique())astype(int)
    Out[59]:
    array([[1  0  0  0  0] 
           [0  1  0  0  0] 
           [1  0  0  0  0] 
           [0  0  1  0  0] 
           [0  0  0  1  0] 
           [0  0  0  0  1]])


And then join all the pieces together with pandasconcat or similar",1063605.0,,,,,2012-05-29 08:13:14,2.0
11005432,2,10791661,2012-06-12 21:52:56,7,"Note that I have implemented new cut and qcut functions for discretizing continuous data: 

http://pandaspydataorg/pandas-docs/dev/basicshtml#discretization-and-quantiling",776560.0,,,,,2012-06-12 21:52:56,
11287278,2,11285613,2012-07-02 02:43:02,4,"The column names (which are strings) cannot be sliced in the manner you tried

Here you have a couple of options If you know from context which variables you want to slice out  you can just return a view of only those columns by passing a list into the __getitem__ syntax (the []'s)

df1 = df[['a' 'b']]


Alternatively  if it matters to index them numerically and not by their name (say your code should automatically do this without knowing the names of the first two columns) then you can do this instead:

df1 = dfix[: 0:2] # Remember that Python does not slice inclusive of the ending index


Additionally  you should familiarize yourself with the idea of a view into a Pandas object vs a copy of that object The first of the above methods will return a new copy in memory of the desired sub-object (the desired slices) 

Sometimes  however  there are indexing conventions in Pandas that don't do this and instead give you a new variable that just refers to the same chunk of memory as the sub-object or slice in the original object This will happen with the second way of indexing  so you can modify it with the copy() function to get a regular copy When this happens  changing what you think is the sliced object can sometimes alter the original object Always good to be on the look out for this

df1 = dfix[0 0:2]copy() # To avoid the case where changing df1 also changes df
",567620.0,,567620.0,,2012-07-02 02:48:05,2012-07-02 02:48:05,5.0
10972557,2,10972410,2012-06-10 21:38:40,2,"Try this:

pandasconcat([df['foo']dropna()  df['bar']dropna()])reindex_like(df)


If you want that data to become the new column bar  just assign the result to df['bar']",1427416.0,,,,,2012-06-10 21:38:40,3.0
11297642,2,11295147,2012-07-02 16:31:03,0,"Edit: sorry  misunderstood the question!

You're correct that this would be wrong for most types; however pandasDataFrame has special support for setting values using a Boolean mask; it will select the corresponding values from the RHS with the corresponding time value  Under the hood it's using npputmask

You can check this for yourself:

>>> df = pandasDataFrame(nplinspace(00  10  10)reshape(5  2))
>>> df[df > 05] = -df
>>> df
          0         1
0  0000000  0111111
1  0222222  0333333
2  0444444 -0555556
3 -0666667 -0777778
4 -0888889 -1000000
",567292.0,,567292.0,,2012-07-02 19:56:42,2012-07-02 19:56:42,3.0
11672736,2,11672403,2012-07-26 15:28:46,1,Adding DataFrame metadata or per-column metadata is on the roadmap but hasn't been implemented yet I'm open to ideas about what the API should look like  though,776560.0,,,,,2012-07-26 15:28:46,3.0
12100543,2,12100497,2012-08-23 21:54:44,3,"I'm not sure I follow you  but do you use DataFrameix to select/set individual elements:

In [79]: M
Out[79]: 
        one       two     three      four
a -0277981  1500188 -0876751 -0389292
b -0705835  0108890 -1502786 -0302773
c  0880042 -0056620 -0550164 -0409458
d  0704202  0619031  0274018 -1755726

In [75]: Mix[0]
Out[75]: 
one     -0277981
two      1500188
three   -0876751
four    -0389292
Name: a

In [78]: Mix[0 0]
Out[78]: -027798082190723405

In [81]: Mix[0 0] = 10

In [82]: M
Out[82]: 
        one       two     three      four
a  1000000  1500188 -0876751 -0389292
b -0705835  0108890 -1502786 -0302773
c  0880042 -0056620 -0550164 -0409458
d  0704202  0619031  0274018 -1755726

In [84]: Mix[(0 1) (0 1)] = 1

In [85]: M
Out[85]: 
        one       two     three      four
a  1000000  1000000 -0876751 -0389292
b  1000000  1000000 -1502786 -0302773
c  0880042 -0056620 -0550164 -0409458
d  0704202  0619031  0274018 -1755726


You can also slice by indices:

In [98]: Mix[""a"":""c"" ""one""] = 20

In [99]: M
Out[99]: 
        one       two     three      four
a  2000000  1000000 -0876751 -0389292
b  2000000  1000000 -1502786 -0302773
c  2000000 -0056620 -0550164 -0409458
d  0704202  0619031  0274018 -1755726
",1443118.0,,1240268.0,,2012-12-30 18:18:41,2012-12-30 18:18:41,4.0
12273646,2,12269528,2012-09-05 02:14:39,3,"In order to parse a multi-column date  you need to tell pandas which columns should be combined into a single date  so you need to say parse_dates=['Year' 'Day' 'Hour' 'Min' 'Sec']

You also need to define your own parser that takes a element from each column you specified in parse_dates:

In [1]: import pandas as pd

In [2]: from datetime import datetime  timedelta

In [3]: from cStringIO import StringIO

In [4]: data = """"""\
Year Day Hour Min Sec P1'S1
 2003   1  0  0 1222  0541
 2003   1  1  0 2069  0708
 2003   1  2  0  495  0520
 2003   1  3  0 1342  0539
""""""

In [5]: def parse(yr  doy  hr  min  sec):
    yr  doy  hr  min = [int(x) for x in [yr  doy  hr  min]]
    sec = float(sec)
    mu_sec = int((sec - int(sec)) * 1e6)
    sec = int(sec)
    dt = datetime(yr - 1  12  31)
    delta = timedelta(days=doy  hours=hr  minutes=min  seconds=sec 
                      microseconds=mu_sec)
    return dt + delta
   : 

In [6]: pdread_csv(StringIO(data)  parse_dates={'datetime':      
           ['Year' 'Day' 'Hour' 'Min' 'Sec']}  
           date_parser=parse  index_col='datetime')
Out[6]: 
                            P1'S1
datetime                         
2003-01-01 00:00:12220000  0541
2003-01-01 01:00:20690000  0708
2003-01-01 02:00:04950000  0520
2003-01-01 03:00:13419999  0539
",1306530.0,,,,,2012-09-05 02:14:39,1.0
13280871,2,13279690,2012-11-08 00:55:25,0,"Do you actually mean to use append()?

Try:-

a2append(a1)


combine_first is not actually an append operation  See - http://pandaspydataorg/pandas-docs/dev/basicshtml?highlight=combine_first#combining-overlapping-data-sets:-


  A problem occasionally arising is the combination of two similar data
  sets where values in one are preferred over the other An example
  would be two data series representing a particular economic indicator
  where one is considered to be of higher quality However  the lower
  quality series might extend further back in history or have more
  complete data coverage As such  we would like to combine two
  DataFrame objects where missing values in one DataFrame are
  conditionally filled with like-labeled values from the other
  DataFrame


while append is http://pandaspydataorg/pandas-docs/dev/generated/pandasDataFrameappendhtml?highlight=append


  Append columns of other to end of this frames columns and index 
  returning a new object Columns not in this frame are added as new
  columns
",482506.0,,,,,2012-11-08 00:55:25,1.0
13288184,2,13279690,2012-11-08 11:28:24,1,"The combine_first function uses indexunion to combine and sort the indexes The indexunion docstring states that it only sorts if possible  so combine_first is not necessarily going to return sorted results by design

For non-monotonic indexes  the indexunion tries to sort  but returns unsorted results if there is an exception I don't know if this is a bug or not  but indexunion does not even attempt to sort monotonic indexes like the datetime index in your example

I've opened an issue on GitHub  but I guess you should do a2combine_first(a1)sort_index() for any datetime indexes for now

Update:  This bug is now fixed on GitHub",1452002.0,,1452002.0,,2012-11-09 18:26:21,2012-11-09 18:26:21,1.0
13560819,2,13557559,2012-11-26 08:02:47,1,"A CSV doesnt contain any information about the structure of your pandas Series Specifying some extra arguments might help Getting the data back as normal is possible with:

pdread_csv('scsv'  index_col=0  header=None)


But that adds default column and index names to it If you just want to save your Series/DF for later use its better to use the save() and pdload() methods",1755432.0,,,,,2012-11-26 08:02:47,
13567861,2,13557559,2012-11-26 15:24:59,4,"In [3]: sto_csv('/home/wesm/tmp/sfoocsv')

In [4]: Seriesfrom_csv('/home/wesm/tmp/sfoocsv')
Out[4]: 
a    1
b    2


You can also pass header=None  index_col=0  squeeze=True to read_csv similar to what Rutger Kassies suggested",776560.0,,,,,2012-11-26 15:24:59,
13726633,2,13726573,2012-12-05 15:27:55,0,"You can use the resample method (http://pandaspydataorg/pandas-docs/dev/timeserieshtml#up-and-downsampling) if you have a time series (if the time is used as the index):

dfresample('1min')


EDIT:

Something like:

rng1 = date_range('2012-11-11'  '2012-11-12'  freq='1min')
rng2 = date_range('2012-11-16'  '2012-11-17'  freq='1min')
rng = rng1 + rng2

dfreindex(rng)
",653364.0,,653364.0,,2012-12-05 22:50:14,2012-12-05 22:50:14,2.0
13937497,2,13937097,2012-12-18 16:38:37,3,"With SQL  you really want to avoid just inserting your value into the query You normally leave that to the database adapter  which has specialized knowledge about how to avoid creating dangerous SQL from your values (SQL quotation escaping  aka SQL injection attacks)

Unfortunately  the pandasiosql module has only half-heartedly implemented parameter support

Instead of using frame_query  just use DataFramefrom_records() directly

First  generate the SQL query with parameters The format of the SQL parameters differs from database adapter to database adapter  since the Python DB API standard allows for a few variants I'll assume you are using MySQL here  which uses %s for positional parameters  echoing Python's syntax:

sql = ""select * from dataBase where cus IN ({0})""format('  'join(['%s'] * len(cus2))


That creates enough parameters for each of the values in cus2 Then query the database:

cur = psqlexecute(sql  con  params=cus2)
rows = curfetchall()
columns = [col_desc[0] for col_desc in curdescription]
curclose()

result = DataFramefrom_records(rows  columns=columns  coerce_float=True)


Since you appear to be using the Sybase module module for your connection  you'll have to adjust this for the (somewhat non-standard) SQL parameter syntax that library uses It only accepts named parameters  which use the form @name:

params = dict(('@param{0}'format(i)  v) for i  v in enumerate(cus2))
sql = ""select * from dataBase where cus IN ({0})""format(
    '  'join(sorted(paramskeys())))

cur = psqlexecute(sql  con  params=params)
rows = curfetchall()
columns = [col_desc[0] for col_desc in curdescription]
curclose()

result = DataFramefrom_records(rows  columns=columns  coerce_float=True)
",100297.0,,100297.0,,2012-12-18 18:21:09,2012-12-18 18:21:09,11.0
11385335,2,11285613,2012-07-08 17:55:12,4,"In [39]: df
Out[39]: 
   index  a  b  c
0      1  2  3  4
1      2  3  4  5

In [40]: df1 = df[['b'  'c']]

In [41]: df1
Out[41]: 
   b  c
0  3  4
1  4  5
",776560.0,,,,,2012-07-08 17:55:12,
13165753,2,11285613,2012-10-31 18:57:33,1,"Assuming your column names (dfcolumns) are ['index' 'a' 'b' 'c']  then the data you want is in the 
3rd & 4th columns If you don't know their names when your script runs  you can do this

newdf = df[dfcolumns[2:4]] # Remember  Python is 0-based! The ""3nd"" entry is at slot 2


As EMS points out in his answer  dfix slices columns a bit more concisely  but this might be a more intuitive interface for you",623735.0,,623735.0,,2012-10-31 20:06:18,2012-10-31 20:06:18,2.0
11637456,2,11637384,2012-07-24 18:54:43,1,"It sounds like maybe you want pandasconcat?  merge and join do  well  joins  which means they will give you something based around the Cartesian product of the two inputs  but it sounds like you just want to paste them together into one big table

Edit: did you try concat with axis=1?  It seems to do what you're asking for:

>>> print x
          A         B         C
0  0155614 -0252148  0861163
1  0973517  1156465 -0458846
2  2504356 -0356371 -0737842
3  0012994  1785123  0161667
4  0574578  0123689  0017598
>>> print y
         A2        B2        C2
0 -0280993  1278750 -0704449
1  0140282  1955322 -0953826
2  0581997 -0239829  2227069
3 -0876146 -1955199 -0155030
4 -0518593 -2630978  0333264
>>> print pandasconcat([x  y]  axis=1)
          A         B         C        A2        B2        C2
0  0155614 -0252148  0861163 -0280993  1278750 -0704449
1  0973517  1156465 -0458846  0140282  1955322 -0953826
2  2504356 -0356371 -0737842  0581997 -0239829  2227069
3  0012994  1785123  0161667 -0876146 -1955199 -0155030
4  0574578  0123689  0017598 -0518593 -2630978  0333264
",1427416.0,,1427416.0,,2012-07-24 19:15:57,2012-07-24 19:15:57,7.0
11639358,2,11637384,2012-07-24 21:10:36,3,"Does your index have duplicates xindexis_unique? If so would explain the behavior you're seeing:

In [16]: left
Out[16]: 
            a
2000-01-01  1
2000-01-01  1
2000-01-01  1
2000-01-02  2
2000-01-02  2
2000-01-02  2

In [17]: right
Out[17]: 
            b
2000-01-01  3
2000-01-01  3
2000-01-01  3
2000-01-02  4
2000-01-02  4
2000-01-02  4

In [18]: leftjoin(right)
Out[18]: 
            a  b
2000-01-01  1  3
2000-01-01  1  3
2000-01-01  1  3
2000-01-01  1  3
2000-01-01  1  3
2000-01-01  1  3
2000-01-01  1  3
2000-01-01  1  3
2000-01-01  1  3
2000-01-02  2  4
2000-01-02  2  4
2000-01-02  2  4
2000-01-02  2  4
2000-01-02  2  4
2000-01-02  2  4
2000-01-02  2  4
2000-01-02  2  4
2000-01-02  2  4
",776560.0,,,,,2012-07-24 21:10:36,1.0
12092339,2,12091967,2012-08-23 13:09:42,0,"I fixed it

dfindexget_level_values('RPT_Date')

array([20120331  20111231  20110930  20110630  20110331  20101231 
       20100930  20100630  20100331  20091231  20090930  20090630 
       20090331  20081231  20080930  20080630  20080331  20071231 
       20070930  20070630  20070331  20061231  20060930  20060630 
       20060331]  dtype=object)


I find the syntax quite ugly  why does the developer of Pandas not accept the straightforward way of df['RPT_Date']?",1072888.0,,1240268.0,,2012-12-17 10:38:41,2012-12-17 10:38:41,
12342951,2,12091967,2012-09-09 21:01:52,2,"That would work if you did:

df2 = dfreset_index()
df2['RPT_Date']
",776560.0,,,,,2012-09-09 21:01:52,
12262719,2,12260124,2012-09-04 11:39:03,2,"You seem to have swapped x and y as first argument of Rectangle((x y)  ) Rectangle((22395760  0)  ) 

Instead of using a patch  plotaxvspan() seems a better match for what you want to do

pltgca()axvspan(date date+2*pandasdatetoolsMinute(15) facecolor='green' edge color='green' alpha=03)
",1548051.0,,,,,2012-09-04 11:39:03,
12737349,2,12734320,2012-10-04 23:13:55,0,"One alternative is to merge on 'Name' and 'L1':

In [36]: df
Out[36]: 
   a         b         c
0  k -0787279  1431643
1  m  1278970  2294351
2  n  0793787 -2337330

In [37]: df2
Out[37]: 
   x         y         z
2  k -2419514  1178166
3  m -0827535 -0113485
4  n  0135814 -0612922

In [38]: dfmerge(df2  left_on='a'  right_on='x')
Out[38]: 
   a         b         c  x         y         z
0  k -0787279  1431643  k -2419514  1178166
1  m  1278970  2294351  m -0827535 -0113485
2  n  0793787 -2337330  n  0135814 -0612922


Another is to call DataFramereset_index first before you call merge:

In [50]: dfreset_index()merge(df2reset_index()  left_index=True  right_index=True)
Out[50]: 
   index_x  a         b         c  index_y  x         y         z
0        0  k -0787279  1431643        2  k -2419514  1178166
1        1  m  1278970  2294351        3  m -0827535 -0113485
2        2  n  0793787 -2337330        4  n  0135814 -0612922
",1306530.0,,,,,2012-10-04 23:13:55,4.0
13086305,2,13085709,2012-10-26 11:43:09,5,"dfhead(n) returns a DataFrame holding the first n rows of df
Now to display a DataFrame pandas checks by default the width of the terminal  if this is too small to display the DataFrame a summary view will be shown Which is what you get in the second case

Could you increase the size of your terminal  or disable autodetect on the columns by pdset_printoptions(max_columns=10)?",1548051.0,,,,,2012-10-26 11:43:09,7.0
13728364,2,13728208,2012-12-05 16:55:39,2,"Concatenating a list of Series returns a DataFrame  Thus  serie_4 is a DataFrame serie_3 is a Series Concatenating a DataFrame with a Series raises the exception

You could use

serie_5 = pandasconcat([serie_1  serie_2  serie_3]  join='outer'  axis = 1)


instead

Another way is to use join:

serie_3name = 2
serie_5 = serie_4join(serie_3  how = 'outer')
",190597.0,,190597.0,,2012-12-05 20:03:45,2012-12-05 20:03:45,3.0
13900044,2,13899914,2012-12-16 09:31:09,1,"One way it to use the DataFrame's select method:

In [4]: hdfselect(lambda x: x[1] in ['two'  'three'])
Out[4]: 
                  A         B         C
foo two   -0015483  1630099  0085461
    three  1396752 -0272583 -0760000
bar two   -1657258 -1271384 -2429598
baz two    1124609  0138720 -1994984
    three  0124298 -0127099 -0409736
qux two    0287724  0461041 -0268918
    three -0259649  0226574 -0558334
",1240268.0,,,,,2012-12-16 09:31:09,
14351185,2,13899914,2013-01-16 04:04:05,0,"Note you can also do:

In [9]: hdfindexget_level_values(1)isin(['two'  'three'])
Out[9]: array([False   True   True  False   True   True   True  False   True   True]  dtype=bool)


There really ought to be a better syntax for this",776560.0,,,,,2013-01-16 04:04:05,
14060360,2,14025879,2012-12-27 19:46:05,7,"I pushed a fix for this yesterday Here's the new behavior on github master:

In [1]: paste
from pandas import DataFrame
f = DataFrame({'a': ['1' '2' '3']  'b': ['2' '3' '4']})
fcolumns = [['level1 item1'  'level1 item2'] [''  'level2 item2']  ['level3 item1'  'level3 item2']]
f

## -- End pasted text --
Out[1]: 
  level1 item1 level1 item2
               level2 item2
  level3 item1 level3 item2
0            1            2
1            2            3
2            3            4

In [2]: f['level1 item1']
Out[2]: 
  level3 item1
0            1
1            2
2            3
",1306530.0,,,,,2012-12-27 19:46:05,
14290688,2,14281871,2013-01-12 05:52:47,2,"I think I would probably do this in one go (without importing division):

1 * dfsexvalue_counts() / len(dfsex)


or perhaps  remembering you want a percentage:

100 * dfsexvalue_counts() / len(dfsex)


Much of a muchness really  your way looks fine too",1240268.0,,,,,2013-01-12 05:52:47,1.0
14179036,2,14178913,2013-01-06 03:38:56,1,"applymap() can be used to apply a function to every element of a dataframe

In [1]: df = DataFrame([[True  True  False] [False  False  True]])T

In [2]: df
Out[2]:
       0      1
0   True  False
1   True  False
2  False   True

In [3]: dfapplymap(lambda x: 1 if x else npnan)
Out[3]:
    0   1
0   1 NaN
1   1 NaN
2 NaN   1


You can also use a dict:

In [4]: d = {True:1  False:npnan}

In [5]: dfapplymap(lambda x: d[x])
Out[5]:
    0   1
0   1 NaN
1   1 NaN
2 NaN   1


Addressing DSM's comment from below  I misread the OP and assumed the datetime was an index  If it's not an index this worked for me:

In [6]: dfapplymap(lambda x: dget(x x))
Out[6]:
    0   1                    2
0   1 NaN  2012-01-01 00:00:00
1 NaN   1  2012-01-01 00:00:00
",919872.0,,919872.0,,2013-01-06 05:04:55,2013-01-06 05:04:55,5.0
14184308,2,14178913,2013-01-06 16:43:29,1,"try this where works because the first use by default nans out the not-found entries (eg anything that is not == 'T')  then 2nd replaces the non-found entries with the 1

In [48]: df = pdDataFrame([ 'T'  'T'  'T'  'F'  'F' ]  columns=['value'] index=pddate_range('20010101' periods=5))

In [49]: df
Out[49]: 
           value
2001-01-01     T
2001-01-02     T
2001-01-03     T
2001-01-04     F
2001-01-05     F

In [50]: dfwhere(df=='T')where(df!='T' 1)
Out[50]: 
           value
2001-01-01     1
2001-01-02     1
2001-01-03     1
2001-01-04   NaN
2001-01-05   NaN
",644898.0,,,,,2013-01-06 16:43:29,
14474271,2,14474093,2013-01-23 07:12:14,2,"In [36]: rng = date_range('1/1/2011'  periods=5  freq='H')

In [37]: df = DataFrame({'price':[1 2 3 4 5]} index = rng)

In [38]: df
Out[38]: 
                     price
2011-01-01 00:00:00      1
2011-01-01 01:00:00      2
2011-01-01 02:00:00      3
2011-01-01 03:00:00      4
2011-01-01 04:00:00      5

In [39]: dfindex
Out[39]: 
<class 'pandastseriesindexDatetimeIndex'>
[2011-01-01 00:00:00    2011-01-01 04:00:00]
Length: 5  Freq: H  Timezone: None

In [40]: dfindexvalues
Out[40]: 
array([1970-01-15 104:00:00  1970-01-15 105:00:00  1970-01-15 106:00:00 
       1970-01-15 107:00:00  1970-01-15 108:00:00]  dtype=datetime64[ns])
",1199589.0,,1199589.0,,2013-01-23 07:24:41,2013-01-23 07:24:41,
10485626,2,10484424,2012-05-07 16:26:42,0,I don't think underlying mathematics apply that sum of interpolation equal to interpolation of sum it only holds at special case,1377107.0,,,,,2012-05-07 16:26:42,
10485998,2,10484424,2012-05-07 16:59:02,1,"It might be a bug Looking at the source  Seriesinterpolate doesn't look at the index values while doing interpolation It assumes they are equally spaced and just uses len(serie) for indexes Maybe this is the intention and it's not a bug I'm not sure

I modified the Seriesinterpolate method and came up with this interpolate function This will do what you want

import numpy as np
from pandas import *

def interpolate(serie):
    try:
        inds = nparray([float(d) for d in serieindex])
    except ValueError:
        inds = nparange(len(serie))

    values = serievalues

    invalid = isnull(values)
    valid = -invalid

    firstIndex = validargmax()
    valid = valid[firstIndex:]
    invalid = invalid[firstIndex:]
    inds = inds[firstIndex:]

    result = valuescopy()
    result[firstIndex:][invalid] = npinterp(inds[invalid]  inds[valid] 
                                             values[firstIndex:][valid])

    return Series(result  index=serieindex  name=seriename)
",843822.0,,,,,2012-05-07 16:59:02,3.0
10963104,2,10934323,2012-06-09 17:33:57,1,"Not sure you can constrain a DataFrame  but your helper function could be a lot simpler (something like)

mismatch = set(cols)difference(set(dfcols))
    if mismatch:
        raise SystemExit('Unknown column(s): {}'format(' 'join(mismatch)))
",1252759.0,,,,,2012-06-09 17:33:57,
10716007,2,10715965,2012-05-23 08:14:43,5,You could use pandasconcat() or DataFrameappend() For details and examples  see Merge  join  and concatenate,367273.0,,1064197.0,,2012-10-17 19:38:13,2012-10-17 19:38:13,1.0
11230582,2,11230071,2012-06-27 16:07:34,0,"If you want NaN's in the places where there is no data  you can just use Minute() located in datetools (as of pandas 07x) 

from pandascoredatetools import day  Minute
tseriesasfreq(Minute())


That should provide an evenly spaced time series with 1 minute differences with NaNs as the series values where there is no data ",1443118.0,,1240268.0,,2012-12-12 22:37:15,2012-12-12 22:37:15,3.0
11972498,2,11971381,2012-08-15 15:40:50,1,"I think you want:

dfgb['bar']value_counts()unstack()fillna(0)
",1306530.0,,,,,2012-08-15 15:40:50,1.0
12250416,2,12250024,2012-09-03 15:12:35,3,"you can use the xlrd library and open the workbook with the ""on_demand=True"" flag  so that the sheets won't be loaded automaticaly

Than you can retrieve the sheet names in a similar way to pandas:

import xlrd
xls = xlrdopen_workbook(r'<path_to_your_excel_file>'  on_demand=True)
print xlssheet_names() # <- remeber: xlrd sheet_names is a function  not a property


-Colin-",1643773.0,,,,,2012-09-03 15:12:35,1.0
12391989,2,12390336,2012-09-12 15:48:52,2,pandas has the fillna method that offer many possible ways of filling gaps  as explained in the documentation,1571826.0,,,,,2012-09-12 15:48:52,
13297472,2,12390336,2012-11-08 20:42:12,1,"You need to construct your full index  and then use the reindex method of the dataframe Like so

import pandas
import StringIO
datastring = StringIOStringIO(""""""\
C1 C2 C3 C4
A A1 20 30
A A2 20 30
A A5 20 30
B B2 20 30
B B4 20 30"""""")

dataframe = pandasread_csv(datastring  index_col=['C1'  'C2'])
full_index = [('A'  'A1')  ('A'  'A2')  ('A'  'A3')  
              ('A'  'A4')  ('A'  'A5')  ('B'  'B1')  
              ('B'  'B2')  ('B'  'B4')  ('B'  'B4')]
new_df = dataframereindex(full_index)
new_df
      C3  C4
A A1  20  30
  A2  20  30
  A3 NaN NaN
  A4 NaN NaN
  A5  20  30
B B1 NaN NaN
  B2  20  30
  B4  20  30
  B4  20  30


And then you can use the fillna method to set the NaNs to whatever you want",1552748.0,,,,,2012-11-08 20:42:12,
12662547,2,12662223,2012-09-30 15:43:03,2,"First -- you need to get out of the habit of starting integers with 0s  This means they'll be interpreted as octal (base-8) constants  which leads to confusion:

>>> 10
10
>>> 010
8


As for why they give two different answers  there are two reasons:

(1) You're not comparing the week numbers  As the documentation says  isocalendar ""[r]eturn[s] a 3-tuple containing ISO year  week number  and weekday""  So isocalendar()[2] will give you the ISO weekday  not the week

(2) ISO dates are defined a little differently than you might expect (explained here)  For example  in the most severe case:

>>> dtdatetime(2010 1 1)isocalendar()
(2009  53  5)
>>> pdTimestamp(dtdatetime(2010 1 1))week
1


so there will often be a difference",487339.0,,,,,2012-09-30 15:43:03,5.0
13002993,2,13002850,2012-10-21 22:51:15,3,"This is actually deliberate Arithmetic operations do data alignment  but comparisons do not I considered changing it in the past but found that it caused too many problems (especially when passing Series to functions expecting NumPy arrays  as an example  numpydiff)

EDIT: to get alignment  you can do the alignment by hand:

In [10]: numpyequal(*aalign(b))
Out[10]: 
2000-01-03    False
2000-01-04     True
2000-01-05     True
2000-01-06     True
2000-01-07     True
2000-01-10    False
Freq: B
",776560.0,,776560.0,,2012-10-23 20:16:36,2012-10-23 20:16:36,2.0
11384667,2,11348183,2012-07-08 16:16:39,2,"If you want to add the legend manually  you have to ask the subplot for the elements of the bar plot:

In [17]: ax = xplot(kind='bar'  legend=False)

In [18]: patches  labels = axget_legend_handles_labels()

In [19]: axlegend(patches  labels  loc='best')
Out[19]: <matplotliblegendLegend at 0x10b292ad0>


Also  pltlegend(loc='best') or axlegend(loc='best') should ""just work""  because there are already ""links"" to the bar plot patches set up when the plot is made  so you don't have to pass a list of axis labels

I'm not sure if the version of pandas you're using returns a handle to the subplot (ax = ) but I'm fairly certain that 073 does You can always get a reference to it with pltgca()",776560.0,,,,,2012-07-08 16:16:39,2.0
11708610,2,11707805,2012-07-29 10:47:08,5,"Note: when you load the data with pandas it will create a DataFrame object where each column has an homogeneous datatype for all the rows but 2 columns can have distinct datatypes (eg integer  dates  strings)

When you pass a DataFrame instance to a scikit-learn model it will first allocate a homogeneous 2D numpy array with dtype npfloat32 or npfloat64 (depending on the implementation of the models) At this point you will have 2 copies of your dataset in memory

To avoid this you could write / reuse a CSV parser that directly allocates the data in the internal format / dtype expected by the scikit-learn model You can try numpyloadtxt for instance (have a look at the docstring for the parameters)

Also if you data is very sparse (many zero values) it will be better to use a scipysparse datastructure and a scikit-learn model that can deal with such an input format (check the docstrings to know) However the CSV format itself is not very well suited for sparse data and I am not sure there exist a direct CSV-to-scipysparse parser

Edit: for reference KNearestNeighborsClassifer allocate temporary distances array with shape (n_samples_predict  n_samples_train) which is very wasteful when only (n_samples_predict  n_neighbors) is needed instead This issue can be tracked here:

https://githubcom/scikit-learn/scikit-learn/issues/325",163740.0,,163740.0,,2012-07-30 12:59:14,2012-07-30 12:59:14,3.0
12140128,2,12138126,2012-08-27 10:20:06,1,"Looks like a bug to me I created an issue for this

Note that by using the *index_col* argument it is possible to set the index

In [15]: df = pdread_csv(StringIO(data) parse_dates=[0]  index_col=0)

In [15]: dfindex
<class 'pandastseriesindexDatetimeIndex'>
[2012-07-31 02:00:00    2012-07-31 02:30:00]
Length: 3  Freq: None  Timezone: None
",1548051.0,,,,,2012-08-27 10:20:06,
12141611,2,12138126,2012-08-27 12:02:14,0,"If parse_dates=True  the reader will attempt to parse the index as datetime (see documentation: http://pandaspydataorg/pandas-docs/stable/iohtml) And since you didn't set the index in the original call  it didn't try to parse it

This will work:

In [237]: df1 = pdread_csv(StringIO(data) parse_dates=True  index_col=0)

In [238]: df1
Out[238]: 
                      c1
date                    
2012-07-31 02:00:00  11
2012-07-31 02:15:00  22
2012-07-31 02:30:00  33

In [239]: df1index
Out[239]: 
<class 'pandastseriesindexDatetimeIndex'>
[2012-07-31 02:00:00    2012-07-31 02:30:00]
Length: 3  Freq: None  Timezone: None
",1306530.0,,,,,2012-08-27 12:02:14,3.0
12286958,2,12286607,2012-09-05 17:42:37,1,"You want matplotlibpcolor:

import numpy as np 
from pandas import *
import matplotlibpyplot as plt

Index= ['aaa' 'bbb' 'ccc' 'ddd' 'eee']
Cols = ['A'  'B'  'C' 'D']
df = DataFrame(abs(nprandomrandn(5  4))  index= Index  columns=Cols)

pltpcolor(df)
pltyticks(arange(05  len(dfindex)  1)  dfindex)
pltxticks(arange(05  len(dfcolumns)  1)  dfcolumns)
pltshow()
",1648033.0,,1648033.0,,2012-09-05 18:39:45,2012-09-05 18:39:45,2.0
12519180,2,12519103,2012-09-20 19:02:55,3,set_index() is a method of DataFrames  not a standalone function Are you calling it as a method eg dfset_index(idx)? Is it still giving you that NameError if you are doing it that way?,1662369.0,,,,,2012-09-20 19:02:55,4.0
12862196,2,12860421,2012-10-12 15:19:00,2,"Do you mean something like this?

In [39]: df2pivot_table(values='X'  rows='Y'  cols='Z'  
                         aggfunc=lambda x: len(xunique()))
Out[39]: 
Z   Z1  Z2  Z3
Y             
Y1   1   1 NaN
Y2 NaN NaN   1


Note that using len assumes you don't have NAs in your DataFrame You can do xvalue_counts()count() or len(xdropna()unique()) otherwise",1306530.0,,,,,2012-10-12 15:19:00,1.0
12862240,2,12860421,2012-10-12 15:21:39,0,"You can construct a pivot table for each distinct value of X In this case  

for xval  xgroup in g:
    ptable = pdpivot_table(xgroup  rows='Y'  cols='Z'  
        margins=False  aggfunc=numpysize)


will construct a pivot table for each value of X You may want to index ptable using the xvalue With this code  I get (for X1)

     X        
Z   Z1  Z2  Z3
Y             
Y1   2   1 NaN
Y2 NaN NaN   1
",536801.0,,,,,2012-10-12 15:21:39,1.0
13226352,2,13226029,2012-11-05 05:22:05,2,"Hierarchical indexing (also referred to as multi-level indexing) was introduced in the pandas 04 release 

This opens the door to some quite sophisticated data analysis and manipulation  especially for working with higher dimensional data In essence  it enables you to effectively store and manipulate arbitrarily high dimension data in a 2-dimensional tabular structure (DataFrame)  for example 

Imagine constructing a dataframe using MultiIndex like this:-

import pandas as pd
import numpy as np

nparrays = [['one' 'one' 'one' 'two' 'two' 'two'] [1 2 3 1 2 3]]

df = pdDataFrame(nprandomrandn(6 2) index=pdMultiIndexfrom_tuples(zip(*nparrays)) columns=['A' 'B'])

df  # This is the dataframe we have generated

          A         B
one 1 -0732470 -0313871
    2 -0031109 -2068794
    3  1520652  0471764
two 1 -0101713 -1204458
    2  0958008 -0455419
    3 -0191702 -0915983


This df is simply a data structure of two dimensions 

dfndim

2


But we can imagine it  looking at the output  as a 3 dimensional data structure

one with 1 with data -0790620  0229276 
one with 2 with data 0751353 -1623718 
one with 3 with data 2174736 -0144823
Aka: ""effectively store and manipulate arbitrarily high dimension data in a 2-dimensional tabular structure""

This is not just a ""pretty display"" It has the benefit of easy retrieval of data since we now have a hierarchal index

For example

In [44]: dfix[""one""]
Out[44]: 
          A         B
1 -0732470 -0313871
2 -0031109 -2068794
3  1520652  0471764


will give us a new data frame only for the group of data belonging to ""one""

And we can narrow down our data selection further by doing this:-

In [45]: dfix[""one""]ix[1]
Out[45]: 
A   -0732470
B   -0313871
Name: 1


And of course  if we want a specific value  here's an example:-

In [46]: dfix[""one""]ix[1][""A""]
Out[46]: -073247029752040727


So if we have even more indexes (besides the 2 indexes shown in the example above)  we can essentially drill down and select the data set we are really interested in without a need for groupby

We can even grab a cross-section (either rows or columns) from our dataframe

By rows:-

In [47]: dfxs('one')
Out[47]: 
          A         B
1 -0732470 -0313871
2 -0031109 -2068794
3  1520652  0471764


By columns:-

In [48]: dfxs('B'  axis=1)
Out[48]: 
one  1   -0313871
     2   -2068794
     3    0471764
two  1   -1204458
     2   -0455419
     3   -0915983
Name: B
",482506.0,,482506.0,,2012-11-05 05:50:24,2012-11-05 05:50:24,1.0
13925665,2,13924801,2012-12-18 03:04:50,0,"In [659]: price = Series(index = ['Missouri'  'Ohio']  data = [200  250])

In [660]: test1 = testitems

In [661]: test1mul(price  axis='index')
Out[661]: 
year      2000  2001  2002  All
All        NaN   NaN   NaN  NaN
Missouri     0     0   200  200
Nevada     NaN   NaN   NaN  NaN
Ohio       250   250   250  750
",1479269.0,,,,,2012-12-18 03:04:50,
14093125,2,14092339,2012-12-30 17:57:46,1,"You could use the index method get_loc  and then slice:

d = pdto_datetime('2008-06-06')
loc = sindexget_loc(d)

In [12]: loc
Out[12]: 4

In [13]: s[loc-2:loc+3]
Out[13]: 
2008-06-04    12440
2008-06-05    12689
2008-06-06    12284
2008-06-09    12314
2008-06-10    12253
Name: SPY




If you were just interested in those within two days:

In [14]: dt = datetimetimedelta(1)

In [15]: s[d - 2*dt:d + 2*dt]
Out[15]: 
2008-06-04    12440
2008-06-05    12689
2008-06-06    12284
Name: SPY
",1240268.0,,,,,2012-12-30 17:57:46,1.0
10167466,2,10158060,2012-04-16 00:39:08,2,"In the upcoming pandas 080 release (available current in git master  please report any bugs)  you can do:

In [52]: tsresample('T'  how='ohlc')
Out[52]: 
                         open       low     high     close
2012-03-15 09:45:00  1398000  1398000  139800  1398000
2012-03-15 09:46:00  1397900  1397850  139810  1398000
2012-03-15 09:47:00  1398100  1398000  139850  1398200
2012-03-15 09:48:00  1398200  1398200  139868  1398600
2012-03-15 09:49:00  1398620  1398197  139862  1398200
2012-03-15 09:50:00  1398000  1398000  139860  1398600
2012-03-15 09:51:00  1398600  1398600  139930  1399300
2012-03-15 09:52:00  1399290  1399100  139937  1399370
2012-03-15 09:53:00  1399386  1399386  139980  1399800
2012-03-15 09:54:00  1399900  1399700  139990  1399884
2012-03-15 09:55:00  1399900  1399850  140010  1399850
2012-03-15 09:56:00  1399900  1399890  140009  1399900
2012-03-15 09:57:00  1399900  1399900  140040  1400200
2012-03-15 09:58:00  1400300  1400200  140040  1400200
2012-03-15 09:59:00  1400400  1400300  140050  1400500
2012-03-15 10:00:00  1400400  1400200  140040  1400200
2012-03-15 10:01:00  1400100  1400100  140090  1400700
2012-03-15 10:02:00  1400800  1399800  140080  1399800
2012-03-15 10:03:00  1399300  1399300  140020  1399300
2012-03-15 10:04:00  1399300  1399200  139970  1399600
2012-03-15 10:05:00  1399700  1399100  139970  1399100
2012-03-15 10:06:00  1398900  1398900  139990  1399700
2012-03-15 10:07:00  1399800  1399800  140040  1400300
2012-03-15 10:08:00  1400400  1399893  140040  1399900
2012-03-15 10:09:00  1400100  1399800  140010  1400000
2012-03-15 10:10:00  1399710  1399614  139971  1399614
2012-03-15 10:11:00  1399700  1399100  139970  1399400
2012-03-15 10:12:00  1399800  1398800  140000  1398800
2012-03-15 10:13:00  1399000  1399000  139930  1399200
2012-03-15 10:14:00  1399300  1399300  139970  1399600
2012-03-15 10:15:00  1399700  1399400  140030  1400300


For an OHLC plot  take a look here: http://matplotlibsourceforgenet/examples/pylab_examples/finance_demohtml  I plan to add a nice method for this in pandas eventually",776560.0,,776560.0,,2012-04-25 03:17:16,2012-04-25 03:17:16,2.0
10859883,2,10857924,2012-06-02 04:52:48,8,"Yes  dropna See http://pandaspydataorg/pandas-docs/stable/missing_datahtml and the DataFramedropna docstring:

Definition: DataFramedropna(self  axis=0  how='any'  thresh=None  subset=None)
Docstring:
Return object with labels on given axis omitted where alternately any
or all of the data are missing

Parameters
----------
axis : {0  1}
how : {'any'  'all'}
    any : if any NA values are present  drop that label
    all : if all values are NA  drop that label
thresh : int  default None
    int value : require that many non-NA values
subset : array-like
    Labels along other axis to consider  eg if you are dropping rows
    these would be a list of columns to include

Returns
-------
dropped : DataFrame


The specific command to run would be:

df=dfdropna(axis=1 how='all')
",776560.0,,905596.0,,2012-07-24 14:30:10,2012-07-24 14:30:10,2.0
12497443,2,12497402,2012-09-19 15:03:40,0,"I am not going to give you the whole answer (I don't think you're looking for the parsing and writing to file part anyway)  but a pivotal hint should suffice: use python's set() function  and then sorted() or sort() coupled with reverse():

>>> a=sorted(set([10 60 30 10 50 20 60 50 60 10 30]))
>>> a
[10  20  30  50  60]
>>> areverse()
>>> a
[60  50  30  20  10]
",711017.0,,711017.0,,2012-09-19 15:10:29,2012-09-19 15:10:29,2.0
12497577,2,12497402,2012-09-19 15:10:56,3,"Try this:

dfgroupby(['A'])max()
",449449.0,,,,,2012-09-19 15:10:56,3.0
13059751,2,12497402,2012-10-25 00:10:02,1,"This takes the last Not the maximum though:

In [10]: dfdrop_duplicates(cols='A'  take_last=True)
Out[10]: 
   A   B
1  1  20
3  2  40
4  3  10


You can do also something like:

In [12]: dfgroupby('A'  group_keys=False)apply(lambda x: xix[xBidxmax()])
Out[12]: 
   A   B
A       
1  1  20
2  2  40
3  3  10
",776560.0,,,,,2012-10-25 00:10:02,
12798599,2,12797352,2012-10-09 10:58:45,1,"You can specify hierarchical index using keys:

In [288]: concatenated = concat([df df]  keys=['first'  'second'])

In [289]: print concatenatedT
   first      second    
       0   1       0   1
a      1   1       1   1
b     10  20      10  20
c     41  42      41  42

In [290]: print concatenatedTto_dict()values()
[{'a': 1  'c': 41  'b': 10}  {'a': 1  'c': 41  'b': 10}  {'a': 1  'c': 42  'b': 20}  {'a': 1  'c': 42  'b': 20}]
",1199589.0,,,,,2012-10-09 10:58:45,
13167701,2,13166842,2012-10-31 21:28:37,0,"Why not create your own dataframe tile function:

def tile_df(df  n  m):
    dfn = dfT
    for _ in range(1  m):
        dfn = dfnappend(dfT  ignore_index=True)
    dfm = dfnT
    for _ in range(1  n):
        dfm = dfmappend(dfnT  ignore_index=True)
    return dfm


Example:

df = pandasDataFrame([[1 2] [3 4]])
tile_df(df  2  3)
#    0  1  2  3  4  5
# 0  1  2  1  2  1  2
# 1  3  4  3  4  3  4
# 2  1  2  1  2  1  2
# 3  3  4  3  4  3  4


However  the docs note: ""DataFrame is not intended to be a drop-in replacement for ndarray as its indexing semantics are quite different in places from a matrix"" Which presumably should be interpreted as ""use numpy if you are doing lots of matrix stuff""",1240268.0,,1240268.0,,2012-11-01 21:18:53,2012-11-01 21:18:53,4.0
13216688,2,13166842,2012-11-04 06:52:40,2,"This can be accomplished quite simply with the DataFrame method apply

In[1]: import pandas as pd; import numpy as np

In[2]: df = pdDataFrame(nparange(40)reshape((8  5))  columns=list('abcde')); df
Out[2]: 
        a   b   c   d   e
    0   0   1   2   3   4
    1   5   6   7   8   9
    2  10  11  12  13  14
    3  15  16  17  18  19
    4  20  21  22  23  24
    5  25  26  27  28  29
    6  30  31  32  33  34
    7  35  36  37  38  39

In[3]: ser = pdSeries(nparange(8) * 10); ser
Out[3]: 
    0     0
    1    10
    2    20
    3    30
    4    40
    5    50
    6    60
    7    70


Now that we have our DataFrame and Series we need a function to pass to apply

In[4]: func = lambda x: npasarray(x) * npasarray(ser)


We can pass this to dfapply and we are good to go 

In[5]: dfapply(func)
Out[5]:
          a     b     c     d     e
    0     0     0     0     0     0
    1    50    60    70    80    90
    2   200   220   240   260   280
    3   450   480   510   540   570
    4   800   840   880   920   960
    5  1250  1300  1350  1400  1450
    6  1800  1860  1920  1980  2040
    7  2450  2520  2590  2660  2730


dfapply acts column-wise by default  but it can can also act row-wise by passing axis=1 as an argument to apply

In[6]: ser2 = pdSeries(nparange(5) *5); ser2
Out[6]: 
    0     0
    1     5
    2    10
    3    15
    4    20

In[7]: func2 = lambda x: npasarray(x) * npasarray(ser2)

In[8]: dfapply(func2  axis=1)
Out[8]: 
       a    b    c    d    e
    0  0    5   20   45   80
    1  0   30   70  120  180
    2  0   55  120  195  280
    3  0   80  170  270  380
    4  0  105  220  345  480
    5  0  130  270  420  580
    6  0  155  320  495  680
    7  0  180  370  570  780


This could be done more concisely by defining the anonymous function inside apply

In[9]: dfapply(lambda x: npasarray(x) * npasarray(ser))
Out[9]: 
          a     b     c     d     e
    0     0     0     0     0     0
    1    50    60    70    80    90
    2   200   220   240   260   280
    3   450   480   510   540   570
    4   800   840   880   920   960
    5  1250  1300  1350  1400  1450
    6  1800  1860  1920  1980  2040
    7  2450  2520  2590  2660  2730

In[10]: dfapply(lambda x: npasarray(x) * npasarray(ser2)  axis=1)
Out[10]:
       a    b    c    d    e
    0  0    5   20   45   80
    1  0   30   70  120  180
    2  0   55  120  195  280
    3  0   80  170  270  380
    4  0  105  220  345  480
    5  0  130  270  420  580
    6  0  155  320  495  680
    7  0  180  370  570  780
",1742701.0,,,,,2012-11-04 06:52:40,
12525836,2,12525722,2012-09-21 07:14:15,4,"In [92]: df
Out[92]:
           a         b          c         d
A  -0488816  0863769   4325608 -4721202
B -11937097  2993993 -12916784 -1086236
C  -5569493  4672679  -2168464 -9315900
D   8892368  0932785   4535396  0598124

In [93]: df_norm = (df - dfmean()) / (dfmax() - dfmin())

In [94]: df_norm
Out[94]:
          a         b         c         d
A  0085789 -0394348  0337016 -0109935
B -0463830  0164926 -0650963  0256714
C -0158129  0605652 -0035090 -0573389
D  0536170 -0376229  0349037  0426611

In [95]: df_normmean()
Out[95]:
a   -2081668e-17
b    4857226e-17
c    1734723e-17
d   -1040834e-17

In [96]: df_normmax() - df_normmin()
Out[96]:
a    1
b    1
c    1
d    1
",1548051.0,,,,,2012-09-21 07:14:15,1.0
12867369,2,12867178,2012-10-12 21:29:18,2,"male_tripscount()


doesnt work?
http://pandaspydataorg/pandas-docs/stable/generated/pandasDataFramecounthtml",541038.0,,,,,2012-10-12 21:29:18,2.0
12867693,2,12867178,2012-10-12 22:00:59,1,"how long would this take:

df = male_tripsgroupby('start_station_id')sum()",190894.0,,,,,2012-10-12 22:00:59,1.0
12872059,2,12867178,2012-10-13 10:05:26,1,"edit: after seeing in the answer above that isin and value_counts exist (and value_counts even comes with its own entry in pandascorealgorithm and also isin isn't simply npin1d) I updated the three methods below

male_tripsstart_station_id[male_tripsstart_station_idisin(stationid)]value_counts()

You could also do an inner join on stationsid:
pdmerge(male_trips  station  left_on='start_station_id'  right_on='id') followed by value_counts
Or:

male_tripsset_index('start_station_id  inplace=True)
stationset_index('id  inplace=True)
male_tripsix[male_tripsindexintersection(stationindex)]reset_index()start_station_idvalue_counts()

If you have the time I'd be interested how this performs differently with a huge DataFrame",942591.0,,942591.0,,2012-10-14 14:18:27,2012-10-14 14:18:27,6.0
12874054,2,12867178,2012-10-13 14:29:37,8,"I'd do like Vishal but instead of using sum() using size() to get a count of the number of rows allocated to each group of 'start_station_id' So:

df = male_tripsgroupby('start_station_id')size()
",1743507.0,,1743507.0,,2012-10-13 22:45:36,2012-10-13 22:45:36,
12874135,2,12867178,2012-10-13 14:39:56,0,"My answer below works in Pandas 073 Not sure about the new releases

This is what the pandasSeriesvalue_counts method is for:

count_series = male_tripsstart_station_idvalue_counts()


It should be straight-forward to then inspect count_series based on the values in stations['id'] However  if you insist on only considering those values  you could do the following:

count_series = (
                male_trips[male_tripsstart_station_idisin(stationsidvalues)]
                    start_station_id
                    value_counts()
               )


and this will only give counts for station IDs actually found in stationsid",567620.0,,,,,2012-10-13 14:39:56,
13261865,2,13261691,2012-11-07 01:08:22,1,"Figured it out myself

df = read_csv(""data/Workbook1csv"")

df

     name team      date  score
0    John    A    3/9/12    100
1    John    B    3/9/12     99
2    Jane    B    4/9/12    102
3   Peter    A    9/9/12    103
4   Josie    C   11/9/12    111
5  Rachel    A  30/10/12     98
6    Kate    B  31/10/12    103
7   David    C   1/11/12    104

df2 = dfpivot('team'  'name')stack()

df2

                 date  score
team name                   
A    John      3/9/12    100
     Peter     9/9/12    103
     Rachel  30/10/12     98
B    Jane      4/9/12    102
     John      3/9/12     99
     Kate    31/10/12    103
C    David    1/11/12    104
     Josie    11/9/12    111
",482506.0,,,,,2012-11-07 01:08:22,
14665875,2,14665828,2013-02-02 19:57:16,1,"columns = ['DealID'  'PropId'  'LoanId'  'ServicerId'  'ServicerPropId']

d = [('A'  [ 'BAC98765'  '15'  '000015'  '30220144'  '010-002-001']) 
     ('B'  [ 'BAC98765'  '16'  '000016'  '30220092'  '010-003-001']) 
     ('C'  [ 'BAC98765'  '45'  '000045'  '30220155'  '010-045-001']) 
     ('D'  [ 'BAC98765'  '48'  '000048'  '30220157'  '010-048-001']) ]

D =  pandasDataFramefrom_items(d  orient='index'  columns=columns)

criterion1 = D['DealID']map(lambda x: x == 'BAC98765' )
criterion2 = D['ServicerId']map(lambda x: x == '30220144')

res = D[criterion1 & criterion2]['ServicerPropId']


Using the map lets you put in any condition you want  in this case you can do this more simply (as pointed out in the comments by DSM)

res = D[(D['DealID'] == ""BAC98765"") & (D[""ServicerId""] == ""30220144"")]['ServicerPropId']


Which gives

In [35]: print res
A    010-002-001
Name: ServicerPropId

In [36]: type(res)
Out[36]: pandascoreseriesSeries


(doc)",380231.0,,380231.0,,2013-02-02 20:22:10,2013-02-02 20:22:10,2.0
10064458,2,9962114,2012-04-08 16:39:40,2,Can you provide some references / examples of what you need? StatsModels has some of the things (like Loess) that you need,776560.0,,2683.0,,2012-04-08 21:09:16,2012-04-08 21:09:16,1.0
10763488,2,9962114,2012-05-26 04:06:49,1,"Just found this package that has not been updated in a while  but works so far in Python 273 (on 64-bit Windows 7  using pretty up-to-date supporting packages):

In [1]: import spc
In [2]: import matplotlibpyplot as plt
In [3]: x = [25 19 14 17 25 39 49 6 11 19 13 26 24 32 14 19]
In [4]: cc = spcSpc(x  spcCHART_X_MR_X)
In [5]: ccget_chart()
In [6]: pltshow()




Looks like the 6th point is outside the upper control limit

In [7]: ccget_violating_points()
Out[7]: {'1 beyond 3*sigma': [6]}


The package is basically a single initpy file that is only a few hundred source lines  and looks to implement more than a dozen charts  including CUSUM 

Lastly  there is a github project worth keeping an eye on: https://githubcom/bwghughes/controlchart",233334.0,,233334.0,,2012-05-26 04:17:13,2012-05-26 04:17:13,1.0
10636141,2,10636024,2012-05-17 12:55:30,0,"I'm not a Pandas user myself  but a quick search for ""pandas gui"" turns up the Pandas project's GSOC 2012 proposal:


  Currently the only way to interact with these objects is through the API This project proposes to add a simple Qt or Tk GUI with which to view and manipulate these objects


So  there's no GUI  but if you'd write one using Qt or Tk  the project might be interested in your code",166749.0,,,,,2012-05-17 12:55:30,1.0
10657954,2,10636024,2012-05-18 18:32:32,1,I'd really like to build something like that in / for pandas but I'm not sure where it will fall on the roadmap ,776560.0,,,,,2012-05-18 18:32:32,3.0
10746190,2,10636024,2012-05-24 22:29:32,0,"It seems there is no easy solution So  below is a little function to open a dataframe in Excel It's probably not production quality code  but it works for me!

def open_in_excel(df  index=True  excel_path=""excelexe""  tmp_path=''):
    """"""Open dataframe df in excel

    excel_path - path to your copy of excel
    index=True - export the index of the dataframe as the first columns
    tmp_path    - directory to save the file in


    This creates a temporary file name  exports the dataframe to a csv of that file name 
    and then tells excel to open the file (in read only mode) (It uses dfto_csv instead
    of to_excel because if you don't have excel  you still get the csv)

    Note - this does NOT delete the file when you exit 
    """"""

    f=tempfileNamedTemporaryFile(delete=False  dir=tmp_path  suffix='csv'  prefix='tmp_')
    tmp_name=fname
    fclose()

    dfto_csv(tmp_name  index=index)
    cmd=[excel_path  '/r'  '/e'  tmp_name]
    try:
        ret_val=subprocessPopen(cmd)pid
    except:
        print ""open_in_excel(): failed to open excel""
        print ""filename = ""  tmp_name
        print ""command line = ""  cmd
        print ""Unexpected error:""  sysexc_info()[0]

    return
",1400991.0,,,,,2012-05-24 22:29:32,
13015576,2,13002850,2012-10-22 16:21:32,0,"I found the answer to my own question It may be useful to someone

In [38]:
dbcloseshift(periods=1)head() - dbclosehead()

Out[38]:
datetime
2012-06-28 23:58:00        NaN
2012-06-28 23:59:00   -000020
2012-06-29 00:00:00    000006
2012-06-29 00:01:00    000019
2012-06-29 00:02:00    000001
Freq: T  Name: close


Unfortunately  it is 2-3x slower than normal arithmetic operations

In [40]:

%timeit dbcloseshift(periods=1) - dbclose
1000 loops  best of 3: 164 ms per loop

%timeit dbclose - dbopen
1000 loops  best of 3: 700 us per loop
",1763885.0,,,,,2012-10-22 16:21:32,
13295801,2,13295735,2012-11-08 18:54:27,5,"I believe DataFramefillna() will do this for you

Link to Docs for a dataframe and for a Series 

Example: 

In [7]: df
Out[7]: 
          0         1
0       NaN       NaN
1 -0494375  0570994
2       NaN       NaN
3  1876360 -0229738
4       NaN       NaN

In [8]: dffillna(0)
Out[8]: 
          0         1
0  0000000  0000000
1 -0494375  0570994
2  0000000  0000000
3  1876360 -0229738
4  0000000  0000000


To fill the NaNs in only one column  select just that column in this case I'm using inplace=True to actually change the contents of df 

In [12]: df[1]fillna(0  inplace=True)
Out[12]: 
0    0000000
1    0570994
2    0000000
3   -0229738
4    0000000
Name: 1

In [13]: df
Out[13]: 
          0         1
0       NaN  0000000
1 -0494375  0570994
2       NaN  0000000
3  1876360 -0229738
4       NaN  0000000
",484596.0,,484596.0,,2012-11-08 19:11:12,2012-11-08 19:11:12,
13576876,2,13576164,2012-11-27 03:19:03,0,"Provisional solution:

In [255]: a = areset_index()

In [256]: a
Out[256]: 
   id1 id2  col1  to_merge_on
0    1   a     1            2
1    1   b     3            4
2    2   a     1            2
3    2   b     3            4

In [271]: c = pdmerge(a  b  how=""left"")

In [272]: c
Out[272]: 
   id1 id2  col1  to_merge_on  col2
0    1   a     1            2   NaN
1    2   a     1            2   NaN
2    2   b     3            3     2
3    1   b     3            4   NaN

In [273]: c = cset_index(['id1' 'id2'])

In [274]: c
Out[274]: 
         col1  to_merge_on  col2
id1 id2                         
1   a       1            2   NaN
2   a       1            2   NaN
    b       3            3     2
1   b       3            4   NaN
",1479269.0,,,,,2012-11-27 03:19:03,
13729448,2,13728896,2012-12-05 17:56:22,1,"Your example dataframes do not match the statement ""bindex is a subset of aindex  however b has real values""  The dates are not overlapping  However:

This works for integer indexes  don't know about ts indexes:

aix[aindex] = msetix[aindex]
",1876739.0,,,,,2012-12-05 17:56:22,
13938831,2,13938704,2012-12-18 18:03:13,4,"Something like this should work:

>>> from pandas import DataFrame
>>> 
>>> df = DataFrame({""A"": [8 9 5 4]  ""B"": [3 4 4 8]  ""C"": [5 0 3 5]  ""D"": [8 4 8 1]})
>>> dfmax()
A    9
B    8
C    5
D    8
>>> (df * 10)/dfmax()
          A      B    C      D
0  0888889  0375  10  1000
1  1000000  0500  00  0500
2  0555556  0500  06  1000
3  0444444  1000  10  0125


Note that I multiplied df by 10 so that it didn't consists of ints anymore (astype(float) would have worked too) to avoid integer division and a resulting DataFrame full of 0s and 1s",487339.0,,,,,2012-12-18 18:03:13,
14181629,2,14181519,2013-01-06 11:28:28,2,"Instead of modifying the DataFrame  perhaps just change the matplotlib xtick labels:

import pandas as pd
import matplotlibpyplot as plt

df = pdread_csv('tmpcsv'  sep = '\s')
dfplot(kind='bar')
locs  labels = pltxticks()
pltxticks(locs  [d if d==d else '' for d in dfindex]  rotation = 25)
pltshow()


",190597.0,,,,,2013-01-06 11:28:28,3.0
11005048,2,10934323,2012-06-12 21:20:16,2,"How about:

In [3]: df1[['a'  'c']]
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
/home/wesm/code/pandas/<ipython-input-3-2349e89f1bb5> in <module>()
----> 1 df1[['a'  'c']]

/home/wesm/code/pandas/pandas/core/framepy in __getitem__(self  key)
   1582             if com_is_bool_indexer(key):
   1583                 key = npasarray(key  dtype=bool)
-> 1584             return self_getitem_array(key)
   1585         elif isinstance(selfcolumns  MultiIndex):
   1586             return self_getitem_multilevel(key)

/home/wesm/code/pandas/pandas/core/framepy in _getitem_array(self  key)
   1609             mask = indexer == -1
   1610             if maskany():
-> 1611                 raise KeyError(""No column(s) named: %s"" % str(key[mask]))
   1612             result = selfreindex(columns=key)
   1613             if resultcolumnsname is None:

KeyError: 'No column(s) named: [c]'
",776560.0,,,,,2012-06-12 21:20:16,1.0
11141606,2,11105728,2012-06-21 15:31:58,0,"Well  you have:

In [17]: B_pto_csv(sysstdout)
0 not you you
hey 50 40
not hey 60 20

In [18]: B_pto_csv(sysstdout  index=False)
not you you
50 40
60 20


But I assume you want the row names Setting the index name to None (B_pindexname = None) gives a leading comma:

In [20]: B_pto_csv(sysstdout)
 not you you
hey 50 40
not hey 60 20


This roughly matches (ignoring quoted strings) what R writes in writecsv when rownames=TRUE:

"""" ""a"" ""b""
""foo"" 0720538259472741 -0848304940318957
""bar"" -064266667412325 -0442441171401282
""baz"" -0419181615269841 -0658545964124229
""qux"" 0881124313748992 036383198969179
""bar2"" -135613767310069 -0124014006180608


Any of these help?

EDIT: Added the index_label=False option today which does what you want:

In [2]: df
Out[2]: 
       A  B
one    1  4
two    2  5
three  3  6

In [3]: dfto_csv('foocsv'  index_
index_exp     index_label=  index_name=   

In [3]: dfto_csv('foocsv'  index_name=False)

In [4]: 
11:24 ~/code/pandas  (master)$ R

R version 2140 (2011-10-31)
Copyright (C) 2011 The R Foundation for Statistical Computing
ISBN 3-900051-07-0
Platform: x86_64-unknown-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY
You are welcome to redistribute it under certain conditions
Type 'license()' or 'licence()' for distribution details

  Natural language support but running in an English locale

R is a collaborative project with many contributors
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications

Type 'demo()' for some demos  'help()' for on-line help  or
'helpstart()' for an HTML browser interface to help
Type 'q()' to quit R

[Previously saved workspace restored]

re> readcsv('foocsv')
      A B
one   1 4
two   2 5
three 3 6
",776560.0,,776560.0,,2012-09-18 15:32:35,2012-09-18 15:32:35,2.0
11617682,2,11615504,2012-07-23 17:45:03,4,"If you pass a list to index_col  it means you want to create a hierarchical index out of the columns in the list

In addition  the parse_dates keyword can be set to either True or a list/dict If True  then it tries to parse individual columns as dates  otherwise it combines columns to parse a single date column

In summary  what you want to do is:

from datetime import datetime
import pandas as pd
parse = lambda x: datetimestrptime(x  '%Y%m%d %H')
pdread_csv(""\\filecsv""   parse_dates = [['YYYYMMDD'  'HH']]  index_col = 0  
            date_parser=parse)
",1306530.0,,,,,2012-07-23 17:45:03,1.0
12190932,2,12190874,2012-08-30 06:18:39,0,"You first need to convert the DataFrame to a Numpy array before you use it with sklearn SVM module

data = numpyasarray(data) 
",314278.0,,,,,2012-08-30 06:18:39,2.0
11065664,2,11057222,2012-06-16 17:38:48,1,"I would suggest using groupby for iteration:

In [25]: for exp  group in dfgroupby(level=0  axis=1):
   :     print exp  group
   :     
IWWGCW Experiment           IWWGCW       
Lead Time                24     48
2010-11-27 12:00:00   0997  0991
2010-11-28 12:00:00   0998  0987
2010-11-29 12:00:00   0997  0992
2010-11-30 12:00:00   0997  0987
2010-12-01 12:00:00   0996  0986
IWWGDW Experiment           IWWGDW       
Lead Time                24     48
2010-11-27 12:00:00   0998  0990
2010-11-28 12:00:00   0997  0990
2010-11-29 12:00:00   0997  0992
2010-11-30 12:00:00   0997  0987
2010-12-01 12:00:00   0996  0986


However  I see that this doesn't drop the top level as you're looking for Ideally you would be able to write something like:

dfgroupby(level=0  axis=1)sub(df['IWWGCW'])

and have that do the pair-wise subtraction  but because df['IWWGCW'] drops the level  the column names don't line up This works  though:

In [29]: dfgroupby(level=0  axis=1)sub(df['IWWGCW']values)
Out[29]: 
Experiment           IWWGCW      IWWGDW       
Lead Time                24  48      24     48
2010-11-27 12:00:00       0   0   0001 -0001
2010-11-28 12:00:00       0   0  -0001  0003
2010-11-29 12:00:00       0   0   0000  0000
2010-11-30 12:00:00       0   0   0000  0000
2010-12-01 12:00:00       0   0   0000  0000


I'll think a bit more about this",776560.0,,,,,2012-06-16 17:38:48,1.0
11538850,2,11538574,2012-07-18 10:00:21,4,"Shouldn't the groupby function of pandas work with sum ?

from pandas import *

fich = 'D://ENERGIE//test2csv'
df = read_csv(fich  delimiter="";""  index_col=""Date"")
sumValue = dfgroupby(""Heure"")sum()
print sumValue 
",874042.0,,,,,2012-07-18 10:00:21,1.0
11538971,2,11538574,2012-07-18 10:07:04,1,"If you want to sum all the values in the ""Heure"" column  just do:

$> dfHeuresum()
",1063605.0,,,,,2012-07-18 10:07:04,
11539007,2,11538574,2012-07-18 10:09:29,1,"Or  without pandas  the csv module is pretty handy:

import csv
fich = 'D://ENERGIE//test2csv'

with open(fich) as f:
    d = csvDictReader(f  delimiter=';')
    print sum(float(row['Heure']) for row in d)
",21945.0,,,,,2012-07-18 10:09:29,
12504527,2,12504493,2012-09-20 00:09:18,2,"You could use fill_value:

>>> import pandas as pd
>>> Sr1 = pdSeries([1 2 3 4]  index = ['A'  'B'  'C'  'D'])
>>> Sr2 = pdSeries([5 6]  index = ['A'  'C'])
>>> Sr1+Sr2
A     6
B   NaN
C     9
D   NaN
>>> Sr1add(Sr2  fill_value=0)
A    6
B    2
C    9
D    4
",487339.0,,,,,2012-09-20 00:09:18,1.0
13013548,2,13013351,2012-10-22 14:27:45,0,"Turns out groupby is what is needed to make this clear and concise (and probably more efficient too):

counts = dataframegroupby(level=[""Sample""  ""Detector""])
counts = countssize()unique()

if countssize != 1:
    raise ValueError(""Detectors not equal for all samples"")
",241515.0,,,,,2012-10-22 14:27:45,
13221579,2,13221218,2012-11-04 18:35:32,0,"A regular DatetimeIndex allows to use between_time method

In [12]: data = """"""\
 20090102 04:51:00 899900 899900 899900 899900 100
 20090102 05:36:00 900100 900100 900100 900100 200
 20090102 05:44:00 901400 901400 901400 901400 100
 20090102 05:50:00 900500 900500 900500 900500 500
 20090102 05:56:00 901000 901000 901000 901000 300
 20090102 05:57:00 901000 901000 901000 901000 200
""""""

In [13]: singledf = pdDataFramefrom_csv(StringIO(data)  header=None  parse_dates=[[0 1]])

In [14]: singledf
Out[14]:
                        X2     X3     X4     X5   X6
X0_X1
2009-01-02 04:51:00  8999  8999  8999  8999  100
2009-01-02 05:36:00  9001  9001  9001  9001  200
2009-01-02 05:44:00  9014  9014  9014  9014  100
2009-01-02 05:50:00  9005  9005  9005  9005  500
2009-01-02 05:56:00  9010  9010  9010  9010  300
2009-01-02 05:57:00  9010  9010  9010  9010  200

In [15]: singledfbetween_time('5:30:00'  '5:45:00')
Out[15]:
                        X2     X3     X4     X5   X6
X0_X1
2009-01-02 05:36:00  9001  9001  9001  9001  200
2009-01-02 05:44:00  9014  9014  9014  9014  100
",1548051.0,,,,,2012-11-04 18:35:32,2.0
13315962,2,13166842,2012-11-09 21:01:11,3,"What's wrong with

result = dataframemul(series  axis=0)


?

http://pandaspydataorg/pandas-docs/stable/basicshtml#flexible-binary-operations",776560.0,,,,,2012-11-09 21:01:11,1.0
13362096,2,13361326,2012-11-13 14:02:05,1,"I assume this is a bug in pandas - the dtype is changed to a float after the groupby

dt3 = dt2groupby(by=""file"")aggregate(""first"")
dt3dtypes


Gives me:

datetime    float64
atn         float64


To change the dtype back to datetime64 you can do:

dt3['datetime'] = pdSeries(dt3['datetime']  dtype='datetime64[ns]')


I have created a new issue on GitHub",1452002.0,,,,,2012-11-13 14:02:05,2.0
13362808,2,13361326,2012-11-13 14:46:32,0,"Looks like a bug but at this moment  not specifying parse_dates=True will give me the expected result

My ipython results - no parse_dates=True:-

In [29]: dt2 = pdread_clipboard(sep="" ""  index_col=0  
                           names=[""datetime""  ""atn""  ""file""])

In [30]: dt2
Out[30]: 
                           atn  file
datetime                            
2012-10-08 14:00:00  23007462     1
2012-10-08 14:30:00  27045666     1
2012-10-08 15:00:00  31483825     1
2012-10-08 15:30:00  37540651     2
2012-10-08 16:00:00  43564573     2
2012-10-08 16:00:00  48589852     2
2012-10-08 16:00:00  55289452     2

In [31]: dt2reset_index()groupby(by=""file"")aggregate(""first"")
Out[31]: 
                 datetime        atn
file                                
1     2012-10-08 14:00:00  23007462
2     2012-10-08 15:30:00  37540651

In [32]: 


My ipython results  with parse_dates=True:-

In [33]: dt = pdread_clipboard(sep="" ""  parse_dates=True  index_col=0  
                           names=[""datetime""  ""atn""  ""file""])
KeyboardInterrupt

In [33]: dt = pdread_clipboard(sep="" ""  parse_dates=True  index_col=0  
                           names=[""datetime""  ""atn""  ""file""])

In [34]: dtreset_index()groupby(by=""file"")aggregate(""first"")
Out[34]: 
          datetime        atn
file                         
1     1349705e+18  23007462
2     1349710e+18  37540651


Explicitly checking dtypes:-

In [40]: new_dt = dtreset_index()groupby(by=""file"")aggregate(""first"")

In [41]: new_dt
Out[41]: 
          datetime        atn
file                         
1     1349705e+18  23007462
2     1349710e+18  37540651

In [42]: new_dtdtypes
Out[42]: 
datetime    float64
atn         float64

In [43]: new_dt2 = dt2reset_index()groupby(by=""file"")aggregate(""first"")

In [44]: new_dt2dtypes
Out[44]: 
datetime     object
atn         float64
",482506.0,,,,,2012-11-13 14:46:32,2.0
13371097,2,13361326,2012-11-14 00:11:06,0,I believe this is fixed and will be in 091 release,776560.0,,,,,2012-11-14 00:11:06,
13635365,2,13630035,2012-11-29 21:49:00,2,"Try setting the maximum rows displayed in a DataFrame using set_printoptions:

pdset_printoptions(max_columns=101)


This should allow you to see all of the columns in the (summarized) DataFrame


  The max_rows and max_columns control how many rows and columns of DataFrame objects are shown by default
",1240268.0,,1240268.0,,2012-11-29 22:14:14,2012-11-29 22:14:14,3.0
13652339,2,13630035,2012-11-30 20:03:55,0,"when the dataframe is large it does not display in ipython notebook 
I just force it to:

from IPythondisplay import HTML
HTML(dfhead()to_html())


make sure to use head :)",239007.0,,,,,2012-11-30 20:03:55,1.0
13385921,2,13385860,2012-11-14 19:29:04,1,"Well  the whitespace is in your data  so you can't read in the data without reading in the whitespace  However  after you've read it in  you could strip out the whitespace by doing  eg  df[""Make""] = df[""Make""]map(strstrip) (where df is your dataframe)",1427416.0,,,,,2012-11-14 19:29:04,
13386025,2,13385860,2012-11-14 19:35:40,4,"You could use converters:

import pandas as pd

def strip(text):
    try:
        return textstrip()
    except AttributeError:
        return text

def make_int(text):
    return int(textstrip('"" '))

table = pdread_table(""datacsv""  sep=r' ' 
                      names=[""Year""  ""Make""  ""Model""  ""Description""] 
                      converters = {'Description' : strip 
                                    'Model' : strip 
                                    'Make' : strip 
                                    'Year' : make_int})
print(table)


yields

   Year     Make   Model              Description
0  1997     Ford    E350                     None
1  1997     Ford    E350                     None
2  1997     Ford    E350   Super  luxurious truck
3  1997     Ford    E350  Super ""luxurious"" truck
4  1997     Ford    E350    Super luxurious truck
5  1997     Ford    E350                     None
6  1997     Ford    E350                     None
7  2000  Mercury  Cougar                     None
",190597.0,,,,,2012-11-14 19:35:40,
13518184,2,13517281,2012-11-22 18:18:39,1,"Here is a reasonable solution  with bin upper limits stored in bins

import numpy as np
min_bin_upper=0
max_bin_upper=100
bin_step=05

bins = nparange(min_bin_upper max_bin_upper bin_step)
counts = npzeros(len(bins))
i=0
for e in data:
    if e[0]>= bins[i]: i+=1
    if i>=len(bins): break
    counts[i]+=e[1]

print counts


I have tested it with 

data = [(01  3)  (02  1) (03  10)]
min_bin_upper = 0
max_bin_upper = 1
bin_step = 02


It returned 

[  0   3  11   0   0]


I hope this is what you need ",1031191.0,,,,,2012-11-22 18:18:39,
13518290,2,13517281,2012-11-22 18:27:04,3,"An approach with pandas:

In [74]: df = pdDataFramefrom_records(dat)set_index(0)

In [75]: counts = dfgroupby(lambda x: floor(x / 05) * 05)count()

In [76]: counts
Out[76]: 
       1
00   12
05    3
25    3
55    4
85    2
185   1
190   1
210   1
220   1


You can fill the intervals with zero counts:

In [77]: countsreindex(nparange(0  22  05))fillna(0)
Out[73]: 
       1
00   12
05    3
10    0
15    0
20    0
25    3
30    0
35    0
40    0

etc 
",1548051.0,,,,,2012-11-22 18:27:04,1.0
13518292,2,13517281,2012-11-22 18:27:07,0,"Here is a way to do it using Python standard lib:

import math

step_size = 05
result = {}
i = 0

for intval in [x * step_size for x in range(int(mathceil(max(dat)[0]*2)+1))]:
    result[intval] = 0
    for n  count in dat[i:]:
        if n > intval:
            break
        result[intval] += count
        i += 1


print sorted(resultitems()  key=lambda x:x[0])


[(00  0)  (05  46)  (10  75)  (15  0)  (20  0)  (25  0)  (30  439)  (35  0)
  (40  0)  (45  0)  (50  0)  (55  0)  (60  25)  (65  0)  (70  0)  (75  0) 
(80  0)  (85  0)  (90  8)  (95  0)  (100  0)  (105  0)  (110  0)  (115  0) 
 (120  0)  (125  0)  (130  0)  (135  0)  (140  0)  (145  0)  (150  0)  (155
  0)  (160  0)  (165  0)  (170  0)  (175  0)  (180  0)  (185  0)  (190  8) 
(195  0)  (200  0)  (205  0)  (210  2)  (215  0)  (220  4)]
",853611.0,,,,,2012-11-22 18:27:07,
13636662,2,13636592,2012-11-29 23:27:02,0,"By using sort()

dfsort(['Peak'  'Weeks']  ascending=[True  False])


Will sort into ascending order of peak position  then within that descending order of length in charts",1252759.0,,,,,2012-11-29 23:27:02,11.0
13535114,2,13517451,2012-11-23 20:03:39,1,"Updating to a newer version of numpy (and rebuilding pandas) seems to have fixed the problem 


  
    
      numpyversion
      '180dev-fd78546'
      pandasversion
      '091dev-85d982d'
    
  


I posted and closed the issue here: https://githubcom/pydata/pandas/issues/2329

Feel free to close this question on stackoverflow if you think it is superfluous  though who know maybe someone else will run into it too",287238.0,,,,,2012-11-23 20:03:39,
13688030,2,13638865,2012-12-03 16:56:40,2,"Series based it works with level:

df[""C01""]mul(s  level=1)

major       minor
2007-09-14  OC       0745391
2007-09-15  CC       0480887
            CO       0325674
            OC       0753075
            OO       0505361


Then you can insert it again into your DataFrame But that should work with DataFrames too  maybe you can suggest it",1798300.0,,,,,2012-12-03 16:56:40,1.0
13867534,2,13867294,2012-12-13 20:03:11,1,"Try to convert the 'sales' string to an int  if it is well formed then it goes on  if it is not it will raise a ValueError which we catch and replace with the place holder

bad_lines = []

with open(fname 'rb') as f:
    header = freadline()
    for j l in enumerate(f):
        country count sales = lsplit()
        try:
            sales_count = int(sales)
        except ValueError:
            sales_count = 'NaN'
            bad_linesappend(j)
        # shove in to your data structure
        print country count sales_count


you might need to edit the line that splits the line (as your example copied out as spaces  not tabs)  Replace the print line  with what ever you want to do with the data  You probably need to relpace 'NaN' with the pandas NaN as well",380231.0,,380231.0,,2012-12-13 20:25:42,2012-12-13 20:25:42,4.0
13867757,2,13867294,2012-12-13 20:17:04,4,"import os
import numpy as np
import pandas as PD

filename = ospathexpanduser('~/tmp/datacsv')
df = PDDataFrame(
        npgenfromtxt(
            filename  delimiter = '\t'  names = True  dtype = '|O4 <i4 <f8'))
print(df)


yields

  Country  Count  Sales
0     USA      1  65000
1      UK      3   4000
2     IND      8    NaN
3     SPA      3   9000
4     NTH      5  80000


and to find the country with NaN sales  you could compute

print(y['Country'][npisnan(y['Sales'])])


which yields the pandasSeries:

2    IND
Name: Country
",190597.0,,,,,2012-12-13 20:17:04,1.0
13868278,2,13867294,2012-12-13 20:49:59,3,"Assuming you've already successfully created a DataFrame using read_csv  I would use apply to convert those strings which are numbers into integers (using strisdigit):

import pandas as pd
import numpy as np
df = pdread_csv('citycsv'  sep='\s+')
# df = pdDataFrame({'Count': {0: 1  1: 3  2: 8  3: 3  4: 5}  'Country': {0: 'USA'  1: 'UK'  2: 'IND'  3: 'SPA'  4: 'NTH'}  'Sales': {0: '65000'  1: '4000'  2: 'g'  3: '9000'  4: '80000'}})

In [5]: df
Out[5]: 
  Country  Count  Sales
0     USA      1  65000
1      UK      3   4000
2     IND      8      g
3     SPA      3   9000
4     NTH      5  80000

In [6]: df['Sales'] = df['Sales']apply(lambda x: int(x) 
                                                  if strisdigit(x)
                                                  else npnan)

In [7]: df
Out[7]: 
  Country  Count  Sales
0     USA      1  65000
1      UK      3   4000
2     IND      8    NaN
3     SPA      3   9000
4     NTH      5  80000


Using pandasisnull  you can select only those rows with NaN in the 'Sales' column  or the 'Country' series:

In [8]: df[pdisnull(df['Sales'])]
Out[8]: 
  Country  Count  Sales
2     IND      8    NaN

In [9]: df[pdisnull(df['Sales'])]['Country']
Out[9]: 
2    IND
Name: Country
",1240268.0,,1240268.0,,2012-12-14 11:14:32,2012-12-14 11:14:32,2.0
13870664,2,13867294,2012-12-13 23:56:51,1,"filename = open('filecsv')
filenamereadline()

for line in filename:
    currentline = linesplit(' ')
    try:
        int(currentline[2][:-1])
    except:
        print currentline[0]  currentline[2][:-1]


IND g",1902440.0,,,,,2012-12-13 23:56:51,
13870726,2,13867294,2012-12-14 00:03:12,1,"I propose to use a regex:

import re

ss = '''Country  Count  Sales
USA          3    65000
UK           3     4000
IND          8        g
SPA         ju     9000
NTH          5    80000
XSZ        rob       k3'''

with open('fofotxt' 'w') as f:
    fwrite(ss)

print ss
print

delimiter = ' '

regx = recompile('(+?(?:{0}))'
                  '(( *\d+?)| *+?)'
                  '( *(?:{0}))'
                  '(( *\d+?)| *+?)'
                  '( *\r?\n?)$'format(delimiter))

def READ(filepath  regx = regx):
    with open(filepath 'rb+') as f:
        yield freadline()
        for line in f:
            if None in regxmatch(line)group(3 6):
                g2 g3 g5 g6 = regxmatch(line)group(2 3 5 6)
                tr = ('%%%ds' % len(g2) % 'NaN' if g3 is None else g3 
                      '%%%ds' % len(g5) % 'NaN' if g6 is None else g6)
                modified_line = regxsub(('\g<1>%s\g<4>%s\g<7>' % tr) line)
                print ('------------------------------------------------\n'
                       '%r with aberration\n'
                       '%r modified line'
                       % (line modified_line))
                yield modified_line
            else:
                yield line

with open('modifiedtxt' 'wb') as g:
    gwritelines(x for x in READ('fofotxt'))


result

Country  Count  Sales
USA          3    65000
UK           3     4000
IND          8        g
SPA         ju     9000
NTH          5    80000
XSZ        rob       k3

------------------------------------------------
'IND          8        g\r\n' with aberration
'IND          8      NaN\r\n' modified line
------------------------------------------------
'SPA         ju     9000\r\n' with aberration
'SPA        NaN     9000\r\n' modified line
------------------------------------------------
'XSZ        rob       k3' with aberration
'XSZ        NaN      NaN' modified line
",551449.0,,,,,2012-12-14 00:03:12,
14077305,2,14075855,2012-12-29 00:00:31,3,"In [24]: df
Out[24]: 
  sequence  time
0        a     1
1        b     1
2        a     3
3        a     5
4        b     2

In [25]: df['nexttime'] = dfgroupby('sequence')timeshift(-1)fillna(999)

In [26]: df
Out[26]: 
  sequence  time  nexttime
0        a     1         3
1        b     1         2
2        a     3         5
3        a     5       999
4        b     2       999
",243434.0,,,,,2012-12-29 00:00:31,2.0
14289790,2,14286807,2013-01-12 03:06:57,1,"It sounds like you want an 'outer' join:

df1join(df2  how='outer')
",1240268.0,,,,,2013-01-12 03:06:57,1.0
14567693,2,14567210,2013-01-28 17:28:30,1,"Once you've got a group  you can iterate row by row using the iterrows() method  It gives you a row index and the row itself:

In [33]: for row_number  row in groupiterrows():
   :     print row_number
   :     print row
   :     
676
Tag                        black fabric
User          http://stevenl/user_1002
Quality               usefulness-useful
Cluster_id                            1
Name: 676
708
Tag                          blond wood
User          http://stevenl/user_1002
Quality               usefulness-useful
Cluster_id                            1
Name: 708
[etc]


and each of these rows can be indexed into like a dictionary  for example:

In [48]: row
Out[48]: 
Tag                       rocking chair
User          http://stevenl/user_1002
Quality       problematic-misperception
Cluster_id                            1
Name: 3650

In [49]: row[""User""]
Out[49]: 'http://stevenl/user_1002'

In [50]: row[""Tag""]
Out[50]: 'rocking chair'


And so you can write your loop like

good = 0
bad = 0
for row_number  row in groupiterrows():
    if row['Quality'] == 'usefulness-useful':
        good += 1
    else:
        bad += 1
print 'good'  good  'bad'  bad


which gives

good 3 bad 4


That's a perfectly fine way to do it if it makes sense to you  Another way is to work directly from the counts in the Quality column:

In [54]: counts = group[""Quality""]value_counts()

In [55]: counts
Out[55]: 
usefulness-useful            3
problematic-misperception    2
usefulness-not_useful        1
problematic-misspelling      1

In [56]: counts['usefulness-useful']
Out[56]: 3


and since bad = total - good  we have

In [57]: countssum() - counts['usefulness-useful']
Out[57]: 4
",487339.0,,,,,2013-01-28 17:28:30,2.0
12036847,2,10636024,2012-08-20 11:30:18,1,"I use QTableWidget from PyQt to display a DataFrame I create a QTableWidgetObject and then populate with QTableWidgetItems created with DataFrame values
Following is the snippet of code that reads a CSV file  create a DataFrame  then display in a GUI:

df  = read_csv(filename  index_col = 0 header = 0)
selfdatatable = QtGuiQTableWidget(parent=self)
selfdatatablesetColumnCount(len(dfcolumns))
selfdatatablesetRowCount(len(dfindex))
for i in range(len(dfindex)):
    for j in range(len(dfcolumns)):
        selfdatatablesetItem(i j QtGuiQTableWidgetItem(str(dfiget_value(i  j))))
",1319128.0,,789649.0,,2012-08-20 13:28:26,2012-08-20 13:28:26,1.0
13446194,2,13445973,2012-11-19 00:37:32,0,"Maybe something like this?  Admittedly this leans a little more to the Python side than the pandas side  but it should work:

>>> import pandas as pd
>>> df = pdDataFrame({""City"": ""Brooklyn Astoria Astoria Ridgewood Ridgewood""split() 
                    ""Borough"": ""Brooklyn Queens Unspecified Unspecified Queens""split()})
>>> df
       Borough       City
0     Brooklyn   Brooklyn
1       Queens    Astoria
2  Unspecified    Astoria
3  Unspecified  Ridgewood
4       Queens  Ridgewood
>>>
>>> unspecified = df[""Borough""] == 'Unspecified'
>>> known = df[~unspecified]
>>> known_dict = {c: set(tuple(b[""Borough""]values)) for c  b in knowngroupby(""City"")}
>>> 
>>> known_dict
{'Brooklyn': set(['Brooklyn'])  'Astoria': set(['Queens'])  'Ridgewood': set(['Queens'])}
>>> 
>>> # sanity check
 if not all(len(known_val) == 1 for known_val in known_dictvalues()):
     raise Exception(""ambiguity!"")
 
>>> known_dict = {c: max(b) for c  b in known_dictitems()}
>>> 
>>> df[""Borough""][unspecified] = df[""City""][unspecified]apply(known_dictget)
>>> df
    Borough       City
0  Brooklyn   Brooklyn
1    Queens    Astoria
2    Queens    Astoria
3    Queens  Ridgewood
4    Queens  Ridgewood
",487339.0,,,,,2012-11-19 00:37:32,4.0
13446322,2,13445973,2012-11-19 00:54:59,2,"Here's one way:

>>> d
         City      Borough
0   Brooklyn     Brooklyn
1    Astoria       Queens
2    Astoria  Unspecified
3  Ridgewood  Unspecified
4  Ridgewood       Queens
>>> realData = d[dBorough != ""Unspecified""]
>>> realData = pandasSeries(data=realDataBoroughvalues  index=realDataCity)
>>> d['Borough'] = dCitymap(realData)
>>> d
         City   Borough
0   Brooklyn  Brooklyn
1    Astoria    Queens
2    Astoria    Queens
3  Ridgewood    Queens
4  Ridgewood    Queens


This assumes that every City has exactly one non-unspecified Borough value  (If a city has no value but Unspecified  the borough will show up as NA)

Edit: If you've already created your dict as in your edited post  just use d['Borough'] = dCitymap(paired['Borough']) to map each city to the borough from your dict  map is a useful method to know about  It can map values either with a Pandas series  with a dict  or with a function that returns the mapped value given the key",1427416.0,,1427416.0,,2012-11-20 07:51:20,2012-11-20 07:51:20,8.0
11362056,2,11361985,2012-07-06 12:18:26,6,"There is too much data to be displayed on the screen  therefore a summary is displayed instead

If you want to output the data anyway (it won't probably fit on a screen and does not look very well):

print paramdatavalues


converts the dataframe to its numpy-array matrix representation

paramdatacolumns


stores the respective column names and

paramdataindex


stores the respective index (row names)",449449.0,,,,,2012-07-06 12:18:26,2.0
11366429,2,11361985,2012-07-06 16:50:17,0,you can also use DataFramehead(x) / tail(x) to display the first / last x rows of the DataFrame,1342094.0,,1301710.0,,2012-07-22 15:53:26,2012-07-22 15:53:26,
13866133,2,13630035,2012-12-13 18:29:17,2,"DataFrameinfo unfortunately is hacked to not display the full summary unless there are fewer than 100 columns (look at the source code) We'll get it fixed for 010:

https://githubcom/pydata/pandas/issues/2524",776560.0,,,,,2012-12-13 18:29:17,
13829474,2,13824973,2012-12-11 22:19:48,0,Can you just use dfBreindex(T)iterrows()?,1306530.0,,,,,2012-12-11 22:19:48,2.0
13958624,2,13950053,2012-12-19 18:20:51,1,"I would suggest converting your dates to timestamps  and then using a custom formatter (doc) to convert the seconds to date format of your choice  You will probably have to play with the locator (doc) a bit to get it to look good (in terms of spacing/labels fitting)

import datetime
def tmp_f(dt x=None):
    return datetimedatetimefromtimestamp(dt)isoformat()
mf = matplotlibtickerFuncFormatter(tmp_f)

ax = gca()
axget_xaxis()set_major_formatter(mf)
draw()
",380231.0,,,,,2012-12-19 18:20:51,2.0
13969867,2,13950053,2012-12-20 10:17:25,1,"I ended up using the following code:

idx = mpldatesdate2num(dfindex)
axxaxisset_major_formatter(mpldatesDateFormatter('%d-%m-%Y'))

axplot(idx  dfV * 01 + 4  'o-' color='k')
axquiver(idx  npones(size) * 35  dfUvalues  dfVvalues  pivot='mid')
axbarbs(idx  npones(size) * 65  dfUvalues  dfVvalues  length=8  pivot='middle')
",1755432.0,,,,,2012-12-20 10:17:25,
14247036,2,14246817,2013-01-09 21:42:50,3,"You were so close:

In [1]: dfgroupby('one')agg(lambda x: ""|""join(xtolist()))
Out[1]:
     two
one
1    x|y
2    y|z
3      z


Expanded answer to handle sorting and take only the set:

In [1]: df = DataFrame({'one':[1 1 2 2 3]  'two':list('xyyzz')  'three':list('eecba')}  index=list('abcde')  columns=['one' 'two' 'three'])

In [2]: df
Out[2]:
   one two three
a    1   x     e
b    1   y     e
c    2   y     c
d    2   z     b
e    3   z     a

In [3]: dfgroupby('one')agg(lambda x: ""|""join(xorder()unique()tolist()))
Out[3]:
     two three
one
1    x|y     e
2    y|z   b|c
3      z     a
",919872.0,,919872.0,,2013-01-09 22:06:38,2013-01-09 22:06:38,6.0
10781699,2,9826431,2012-05-28 08:41:04,1,"I have not tested the performance  but maybe you can use something like this:

Iterate thru the rows of the DataFrame  yielding a string representing a row (see below)
Convert this iterable in a stream  using for example Python: Convert an iterable to a stream?
Finally use psycopg's copy_from on this stream
To yield rows of a DataFrame efficiently use something like:

    def r(df):
            for idx  row in dfiterrows():
                    yield ' 'join(map(str  row))
",1063605.0,,,,,2012-05-28 08:41:04,
10975782,2,10975690,2012-06-11 07:10:08,1,"dfgroupby(get_day)


but I would convert the date strings to datetime objects first anyway

Another problem is that you're calling day which returns a day of month (number 1-31) You probably want to call date():

def get_day(date_string):
    return datetimestrptime(date_string  '%m/%d/%Y')date()


or directly

dfgroupby(lambda x: datetimestrptime(date_string  '%m/%d/%Y')date())
",449449.0,,449449.0,,2012-06-11 08:42:29,2012-06-11 08:42:29,2.0
11201223,2,11193212,2012-06-26 05:28:27,2,"You should only have to wrap the first import pandas with npset_printoptions(foo) since python will cache it See below: 

import numpy
>>> numpyarray([12345  006])
array([  123450000e+02    600000000e-02])
>>> import pandas
>>> numpyarray([12345  006])
array([ 12345     006])
>>> numpyset_printoptions(edgeitems=3 infstr='inf'  linewidth=75  nanstr='nan'  precision=8  suppress=False  threshold=1000)
>>> numpyarray([12345  006])
array([  123450000e+02    600000000e-02])
>>> import pandas
>>> numpyarray([12345  006])
array([  123450000e+02    600000000e-02])
",1337342.0,,,,,2012-06-26 05:28:27,1.0
9971908,2,9938130,2012-04-02 06:43:11,1,"Recently I have programmed a function to do something very similar Here you have a simplified version:

from matplotlibbackendsbackend_agg import FigureCanvasAgg as FigureCanvas
from matplotlibfigure import Figure
from matplotlibcolors import colorConverter
import matplotliblines as mlines
import matplotlib

def _add_legend(axes):
    'It adds the legend to the plot'
    box = axesget_position()
    axesset_position([boxx0  boxy0  boxwidth * 09  boxheight])

    handles  labels = axesget_legend_handles_labels()

    # sort by the labels
    handel_lables = sorted(zip(handles  labels)  key=operatoritemgetter(1))
    handles  labels = zip(*handel_lables)

    axeslegend(handles  labels  bbox_to_anchor=(105  1)  loc=2 
                borderaxespad=0  prop={'size':LEGEND_FONT_SIZE} 
                fancybox=True  numpoints=1)


def stacked_bars(matrix  fhand  bar_colors=None):
    'It draws stacked columns'
    bar_width = 1
    fig = Figure(figsize=FIGURE_SIZE)
    canvas = FigureCanvas(fig)
    axes = figadd_subplot(111)
    nrows  ncols = matrixshape

    bar_locs = range(0  nrows)
    cum_heights = numpyzeros(nrows)
    for col_index  (col_name  column) in enumerate(matrixiteritems()):
        color = bar_colors[col_index] if bar_colors is not None else None
        values = columnvalues
        axesbar(bar_locs  values  color=color  bottom=cum_heights 
                 width=bar_width  label=col_name)
        cum_heights += values
    min_y  max_y = axesget_ylim()

    #bar labels
    axesset_xticks([l + bar_width * 04 for l in bar_locs])
    labels = axesset_xticklabels([str(l) + '  ' for l in matrixindexvalues] 
                                  fontsize=AXIS_LABELS_FONT_SIZE)
    for label in labels:
        labelset_rotation('vertical')

    _add_legend(axes)

    canvasprint_figure(fhand  format=_get_format_from_fname(fhandname))
fhandflush()


I hope it helps you to get an idea",1242647.0,,,,,2012-04-02 06:43:11,2.0
10064444,2,9938130,2012-04-08 16:37:48,3,"I've just implemented a stacked bar plot function in the git repository for pandas  will be part of the upcoming 073 release:

In [7]: df
Out[7]: 
          a         b         c
0  0425199  0564161  0727342
1  0174849  0071170  0679178
2  0224619  0331846  0468959
3  0654766  0189413  0868011
4  0617331  0715088  0387540
5  0444001  0069016  0417990
6  0203908  0689652  0227135
7  0382930  0874078  0571042
8  0658687  0493955  0245392
9  0758986  0385871  0455357

In [8]: dfplot(kind='barh'  stacked=True)




It properly handles positive and negative values (stacking negative values below the origin and positive values above)",776560.0,,,,,2012-04-08 16:37:48,
10872241,2,10841538,2012-06-03 17:02:25,2,You can use pivotcolumnstolist(),,user1433796,,,,2012-06-03 17:02:25,1.0
11038086,2,11037895,2012-06-14 17:11:45,4,"I might suggest using pandasconcat along with its keys argument to glue together Series DataFrames to create a MultiIndex in the columns:

In [20]: data
Out[20]: 
{'a': 2012-04-16    0
2012-04-17    1
2012-04-18    2
2012-04-19    3
2012-04-20    4
2012-04-21    5
2012-04-22    6
2012-04-23    7
2012-04-24    8
2012-04-25    9
Freq: D 
 'b': 2012-04-16    0
2012-04-17    1
2012-04-18    2
2012-04-19    3
2012-04-20    4
2012-04-21    5
2012-04-22    6
2012-04-23    7
2012-04-24    8
2012-04-25    9
Freq: D 
 'c': 2012-04-16    0
2012-04-17    1
2012-04-18    2
2012-04-19    3
2012-04-20    4
2012-04-21    5
2012-04-22    6
2012-04-23    7
2012-04-24    8
2012-04-25    9
Freq: D}

In [21]: df = pdconcat(data  axis=1  keys=['a'  'b'  'c'])

In [22]: df
Out[22]: 
            a  b  c
2012-04-16  0  0  0
2012-04-17  1  1  1
2012-04-18  2  2  2
2012-04-19  3  3  3
2012-04-20  4  4  4
2012-04-21  5  5  5
2012-04-22  6  6  6
2012-04-23  7  7  7
2012-04-24  8  8  8
2012-04-25  9  9  9

In [23]: df2 = pdconcat([df  df]  axis=1  keys=['group1'  'group2'])

In [24]: df2
Out[24]: 
            group1        group2      
                 a  b  c       a  b  c
2012-04-16       0  0  0       0  0  0
2012-04-17       1  1  1       1  1  1
2012-04-18       2  2  2       2  2  2
2012-04-19       3  3  3       3  3  3
2012-04-20       4  4  4       4  4  4
2012-04-21       5  5  5       5  5  5
2012-04-22       6  6  6       6  6  6
2012-04-23       7  7  7       7  7  7
2012-04-24       8  8  8       8  8  8
2012-04-25       9  9  9       9  9  9


You have then:

In [25]: df2['group2']
Out[25]: 
            a  b  c
2012-04-16  0  0  0
2012-04-17  1  1  1
2012-04-18  2  2  2
2012-04-19  3  3  3
2012-04-20  4  4  4
2012-04-21  5  5  5
2012-04-22  6  6  6
2012-04-23  7  7  7
2012-04-24  8  8  8
2012-04-25  9  9  9


or even

In [27]: df2xs('b'  axis=1  level=1)
Out[27]: 
            group1  group2
2012-04-16       0       0
2012-04-17       1       1
2012-04-18       2       2
2012-04-19       3       3
2012-04-20       4       4
2012-04-21       5       5
2012-04-22       6       6
2012-04-23       7       7
2012-04-24       8       8
2012-04-25       9       9


You can have arbitrarily many levels:

In [29]: pdconcat([df2  df2]  axis=1  keys=['tier1'  'tier2'])
Out[29]: 
             tier1                       tier2                    
            group1        group2        group1        group2      
                 a  b  c       a  b  c       a  b  c       a  b  c
2012-04-16       0  0  0       0  0  0       0  0  0       0  0  0
2012-04-17       1  1  1       1  1  1       1  1  1       1  1  1
2012-04-18       2  2  2       2  2  2       2  2  2       2  2  2
2012-04-19       3  3  3       3  3  3       3  3  3       3  3  3
2012-04-20       4  4  4       4  4  4       4  4  4       4  4  4
2012-04-21       5  5  5       5  5  5       5  5  5       5  5  5
2012-04-22       6  6  6       6  6  6       6  6  6       6  6  6
2012-04-23       7  7  7       7  7  7       7  7  7       7  7  7
2012-04-24       8  8  8       8  8  8       8  8  8       8  8  8
2012-04-25       9  9  9       9  9  9       9  9  9       9  9  9
",776560.0,,,,,2012-06-14 17:11:45,3.0
11420594,2,11418192,2012-07-10 19:35:29,1,"Suppose I had a DataFrame as follows:

In [39]: df
Out[39]: 
      mass1     mass2  velocity
0  1461711 -0404452  0722502
1 -2169377  1131037  0232047
2  0009450 -0868753  0598470
3  0602463  0299249  0474564
4 -0675339 -0816702  0799289


I can use sin and DataFrameprod to create a boolean mask:

In [40]: mask = (npsin(dfvelocity) / dfix[:  0:2]prod(axis=1)) > 0

In [41]: mask
Out[41]: 
0    False
1    False
2    False
3     True
4     True


Then use the mask to select from the DataFrame:

In [42]: df[mask]
Out[42]: 
      mass1     mass2  velocity
3  0602463  0299249  0474564
4 -0675339 -0816702  0799289
",1306530.0,,,,,2012-07-10 19:35:29,1.0
11475486,2,11418192,2012-07-13 17:33:12,4,"You can do this using DataFrameapply  which applies a function along a given axis 

In [3]: df = pandasDataFrame(nprandomrandn(5  3)  columns=['a'  'b'  'c'])

In [4]: df
Out[4]: 
          a         b         c
0 -0001968 -1877945 -1515674
1 -0540628  0793913 -0983315
2 -1313574  1946410  0826350
3  0015763 -0267860 -2228350
4  0563111  1195459  0343168

In [6]: df[dfapply(lambda x: x['b'] > x['c']  axis=1)]
Out[6]: 
          a         b         c
1 -0540628  0793913 -0983315
2 -1313574  1946410  0826350
3  0015763 -0267860 -2228350
4  0563111  1195459  0343168
",128580.0,,,,,2012-07-13 17:33:12,
11975948,2,11970614,2012-08-15 19:32:35,1,"You can switch to engineering notation using ""set_eng_float_format()""  see also http://pandaspydataorg/pandas-docs/stable/basicshtml#console-output-formatting",1548051.0,,,,,2012-08-15 19:32:35,
12443013,2,12436979,2012-09-15 22:37:31,0,"You probably have another Numpy version installed on your system  
try to query your numpy version and retrieve it if your distribution does not support it
aka debian/unbuntu/Mint version can query mostly from dpkg package manger :
dpkg --get-selections | egrep -i ""numpy""  you can see actual Numpy version

Some having apt can either asking to removing it by doing this: apt-get remove numpy
Some having distribution like Fedora  RedHat and any compatible release under RedHat model can use rpm as well to query the installation 
This is happening by telling to Numpy installer to install itself in current /usr/local/lib/python[VERSION]/dist-packages over Linux env and c:[]\python[VERSION]\site-packages for windows Having probably One version of Numpy installed in /usr/local/python[VERSION]/dist-packages  this one will be instantiated first 
pth file hold information about path location of specific python module  but erasing a component from packages may corrupt it 
Be careful  and you will have to remove the package and all it's dependency really painful in some case 

Visiting lunchadnet may save you time sometimes they had new versions from some packages",1674965.0,,944181.0,,2012-09-18 09:28:33,2012-09-18 09:28:33,1.0
12975518,2,12436979,2012-10-19 13:39:55,9,"Don't know if you solved the problem but if anyone has this problem in future

$python
>>import numpy
>>print(numpy)


Go to the location printed and delete the numpy installation found there You can then use pip or easy_install",1448346.0,,,,,2012-10-19 13:39:55,1.0
13601503,2,12436979,2012-11-28 09:11:53,1,"If you are using a version of enthought python (EPD) you might want to go directly to your site-packages and reinstall numpy
Then try to install pandas with pip You will have to modify your installation prefix for that 

If the problem persists (as it did with me) try downloading pandas tar ball  unpack it in your site packages and run setuppy install from your pandas directory

If you got your dependencies right you can import pandas and check it imports smoothly",1094031.0,,,,,2012-11-28 09:11:53,
12192021,2,12190874,2012-08-30 07:36:18,2,"What version of pandas are you using? For me your code works fine (i`m on git master)

Another approach could be:

In [117]: import pandas

In [118]: import random

In [119]: df = pandasDataFrame(nprandomrandn(100  4)  columns=list('ABCD'))

In [120]: rows = randomsample(dfindex  10)

In [121]: df_10 = dfix[rows]

In [122]: df_90 = dfdrop(rows)
",1548051.0,,,,,2012-08-30 07:36:18,1.0
12687827,2,12687742,2012-10-02 09:32:04,1,"Use join:

$ cat in1
2012-08-10 11
2012-08-11 12
2012-08-12 18
2012-08-14 14
2012-08-15 17
2012-08-17 16
2012-08-18 11
$ cat in2
2012-08-10 11
2012-08-11 12
2012-08-13 11
2012-08-15 13
2012-08-16 11
2012-08-17 12
2012-08-18 11
$ join in1 in2
2012-08-10 11 11
2012-08-11 12 12
2012-08-15 17 13
2012-08-17 16 12
2012-08-18 11 11


Edit: If you want to split both files again  do this:

$ join in1 in2 | awk '{print $1  $2}' > out1
$ join in1 in2 | awk '{print $1  $3}' > out2
$ cat out1
2012-08-10 11
2012-08-11 12
2012-08-15 17
2012-08-17 16
2012-08-18 11
$ cat out2
2012-08-10 11
2012-08-11 12
2012-08-15 13
2012-08-17 12
2012-08-18 11


I love small tools :)",,user647772,,user647772,2012-10-02 09:53:49,2012-10-02 09:53:49,3.0
12688408,2,12687742,2012-10-02 10:13:08,2,"In [52]: s1align(s2  join='inner')
Out[52]:
(2012-08-10    11
2012-08-11    12
2012-08-15    17
2012-08-17    16
2012-08-18    11 
 2012-08-10    11
2012-08-11    12
2012-08-15    13
2012-08-17    12
2012-08-18    11)
",1548051.0,,,,,2012-10-02 10:13:08,1.0
13032528,2,13031803,2012-10-23 14:17:06,2,"In [61]: df
Out[61]:
     A       B
1  abc     463
6  abc    4341
0  abc   13123
3  def      45
2  def    1231
5  def    4839
4  def  142131

In [62]: df['C'] =  dfgroupby('A')['A']transform(lambda x: pdSeries(range(1  len(x)+1)  index=xindex))

In [63]: df
Out[63]:
     A       B  C
1  abc     463  1
6  abc    4341  2
0  abc   13123  3
3  def      45  1
2  def    1231  2
5  def    4839  3
4  def  142131  4
",1548051.0,,1548051.0,,2012-10-23 14:45:50,2012-10-23 14:45:50,4.0
13032896,2,13031803,2012-10-23 14:35:22,0,"And for comparison  the correct datatable syntax is :

df[  C := 1:N  by=A]


This adds a new column C by reference to df The := operator is part of the datatable package for R It allows you to add and remove columns and assign to subsets of datatable  by group  by reference with no copy at all",403310.0,,,,,2012-10-23 14:35:22,
13033334,2,13031803,2012-10-23 14:57:02,0,"Index magic seems to be another way:

df['C']=dfsort(['A' 'B'] inplace=True)groupby('A')reset_index()indexlabels[1]
",54567.0,,,,,2012-10-23 14:57:02,
13236277,2,13236098,2012-11-05 16:30:58,1,"There's no default way to do this right now I would suggest chunking the file and iterating over it and discarding the columns you don't want
So something like pdconcat([xix[:  cols_to_keep] for x in pdread_csv(  chunksize=200)]) ",1306530.0,,,,,2012-11-05 16:30:58,
13319614,2,13236098,2012-11-10 05:41:17,3,Ian  I implemented a usecols option which does exactly what you describe It will be in upcoming pandas 010; development version will be available soon ,776560.0,,,,,2012-11-10 05:41:17,1.0
13446268,2,13445174,2012-11-19 00:47:38,2,"freq='M' is for month-end frequencies (see here) But you can use shift to shift it by any number of days (or any frequency for that matter):

pddate_range(start  end  freq='M')shift(15  freq=pddatetoolsday)
",1452002.0,,,,,2012-11-19 00:47:38,1.0
13455624,2,13445174,2012-11-19 14:16:57,0,"There actually is no ""day of month"" frequency (eg ""DOMXX"" like ""DOM09"")  but I don't see any reason not to add one 

http://githubcom/pydata/pandas/issues/2289

I don't have a simple workaround for you at the moment because resample requires passing a known frequency rule I think it should be augmented to be able to take any date range to be used as arbitrary bin edges  also Just a matter of time and hacking",776560.0,,,,,2012-11-19 14:16:57,
14487598,2,14487562,2013-01-23 19:17:24,2,"Somewhat confusingly  reindex does not mean ""create a new index""  To create a new index  just assign to the index attribute  So at your last step just do sample_mean_seriesindex = range(len(sample_mean_series))",1427416.0,,,,,2013-01-23 19:17:24,
14487693,2,14487562,2013-01-23 19:24:09,1,"Here's a one-liner:

In [1]: s
Out[1]:
0   -0942184
1    0397485
2   -0656745
3    1415797
4    1123858
5   -1890870
6    0401715
7   -0193306
8   -1018140
9    0262998

In [2]: s2 = sdrop([5])reset_index()ix[: 1]

In [3]: s2
Out[3]:
0   -0942184
1    0397485
2   -0656745
3    1415797
4    1123858
5    0401715
6   -0193306
7   -1018140
8    0262998
Name: 0
",919872.0,,,,,2013-01-23 19:24:09,
10519094,2,10518803,2012-05-09 15:20:45,1,"Read the docs

DF type = df1align(df2  join='inner'  axis=0)
DFto_csv(path)
",555315.0,,1322401.0,,2012-05-09 15:55:19,2012-05-09 15:55:19,3.0
10658446,2,10518803,2012-05-18 19:10:37,0,"align returns aligned versions of the left and right DataFrames (as a tuple):

http://pandaspydataorg/pandas-docs/stable/basicshtml#aligning-objects-with-each-other-with-align",776560.0,,,,,2012-05-18 19:10:37,
10943545,2,10943478,2012-06-08 05:34:25,9,"It sounds like you don't want reindex  Somewhat confusingly reindex is not for defining a new index  exactly; rather  it looks for rows that have the specified indices  So if you have a DataFrame with index [0  1  2]  then doing a reindex([2  1  0]) will return the rows in reverse order  Doing something like reindex([8  9  10]) does not make a new index for the rows; rather  it will return a DataFrame with NaN values  since there are no rows with indices 8  9  or 10

It seems like what you want is to just keep the same rows  but make a totally new index for them  For that you can just assign to the index directly  So try doing dfindex = df['dtstamp']",1427416.0,,,,,2012-06-08 05:34:25,2.0
11182373,2,11136006,2012-06-25 00:49:26,2,"The quick answer is that what you indicate as the fastest way to parse your date/time strings into a datetime-type index  is indeed the fastest way I timed some of your approaches and some others and this is what I get 

First getting an example DataFrame to work with:

import datetime
from pandas import *

start = datetime(2000  1  1)
end = datetime(2012  12  1)
d = DateRange(start  end  offset=datetoolsHour())
t_df = DataFrame({'field_1': nparray(['OFF'  'ON'])[nprandomrandom_integers(0  1  dsize)]  'field_2': nprandomrandom_integers(0  1  dsize)}  index=d)


Where:

In [1]: t_dfhead()
Out[1]: 
                    field_1  field_2
2000-01-01 00:00:00      ON        1
2000-01-01 01:00:00     OFF        0
2000-01-01 02:00:00     OFF        1
2000-01-01 03:00:00     OFF        1
2000-01-01 04:00:00      ON        1
In [2]: t_dfshape
Out[2]: (113233  2)


This is an approx 32MB file if you dump it on disk We now need to drop the DataRange type of your Index and make it a list of str to simulate how you would parse in your data:

t_dfindex = t_dfindexmap(str)


If you use parse_dates = True when reading your data into a DataFrame using read_table you are looking at 95sec mean parse time:

In [3]: import numpy as np
In [4]: import timeit
In [5]: t_dfto_csv('datatsv'  sep='\t'  index_label='date_time')
In [6]: t = timeitTimer(""from __main__ import read_table; read_table('datatsv'  sep='\t'  index_col=0  parse_dates=True)"")
In [7]: npmean(trepeat(10  number=1))
Out[7]: 95226533889770515


The other strategies rely on parsing your data into a DataFrame first (negligible parsing time) and then converting your index to an Index of datetime objects:

In [8]: t = timeitTimer(""from __main__ import t_df  dateutil; map(dateutilparserparse  t_dfindexvalues)"")
In [9]: npmean(trepeat(10  number=1))
Out[9]: 76590064525604244
In [10]: t = timeitTimer(""from __main__ import t_df  dateutil; t_dfindexmap(dateutilparserparse)"")
In [11]: npmean(trepeat(10  number=1))
Out[11]: 78106775999069216
In [12]: t = timeitTimer(""from __main__ import t_df  datetime; t_dfindexmap(lambda x: datetimestrptime(x  \""%Y-%m-%d %H:%M:%S\""))"")
Out[12]: 20389052629470825
In [13]: t = timeitTimer(""from __main__ import t_df  np; map(npdatetime_  t_dfindexvalues)"")
In [14]: npmean(trepeat(10  number=1))
Out[14]: 38656840562820434
In [15]: t = timeitTimer(""from __main__ import t_df  np; map(npdatetime64  t_dfindexvalues)"")
In [16]: npmean(trepeat(10  number=1))
Out[16]: 39244711160659791


And now for the winner:

In [17]: def f(s):
   :         return datetime(int(s[0:4])  
   :                     int(s[5:7])  
   :                     int(s[8:10])  
   :                     int(s[11:13])  
   :                     int(s[14:16])  
   :                     int(s[17:19]))
   : t = timeitTimer(""from __main__ import t_df  f; t_dfindexmap(f)"")
   : 
In [18]: npmean(trepeat(10  number=1))
Out[18]: 033927145004272463


When working with numpy  pandas or datetime-type approaches  there definitely might be more optimizations to think of but it seems to me that staying with CPython's standard libraries and converting each date/time str into a tupple of ints and that into a datetime instance is the fastest way to get what you want",696023.0,,,,,2012-06-25 00:49:26,1.0
13866073,2,13636592,2012-12-13 18:24:51,1,"On pandas 091 and higher this should work (this is with 0100b1):

In [23]: songssort_index(by=['Peak'  'Weeks']  ascending=[True  False])
Out[23]: 
                                      Song  Peak  Weeks
10                           She Loves You     1     36
118                               Hey Jude     1     27
20                I Want To Hold Your Hand     1     24
22                       Can't Buy Me Love     1     17
56                                   Help!     1     17
76                        Paperback Writer     1     16
109                   All You Need Is Love     1     16
45                             I Feel Fine     1     15
29                      A Hard Day's Night     1     14
48                          Ticket To Ride     1     14
85                           Eleanor Rigby     1     14
87                        Yellow Submarine     1     14
173            The Ballad Of John And Yoko     1     13
60                             Day Tripper     1     12
61                      We Can Work It Out     1     12
117                           Lady Madonna     1      9
8                           From Me To You     1      7
115                          Hello Goodbye     1      7
155                               Get Back     1      6
2                         Please Please Me     2     20
107                   Magical Mystery Tour     2     16
176                              Let It Be     2     14
93                              Penny Lane     2     13
92               Strawberry Fields Forever     2     12
0                               Love Me Do     4     26
166                          Come Together     4     10
157                              Something     4      9
58                               Yesterday     8     21
135                   Back In The USSR    19      3
164                     Here Comes The Sun    58     19
96   Sgt Pepper's Lonely Hearts Club Band    63     12
105     With A Little Help From My Friends    63      7
",776560.0,,,,,2012-12-13 18:24:51,
13865831,2,13865176,2012-12-13 18:08:12,1,"by setting index_col=[0 2 4] you are creating a MultiIndex that's why you get that output 

For the output you want read_csv will not be able to do this on the fly Just read single and merge the dataframes",239007.0,,,,,2012-12-13 18:08:12,1.0
13865838,2,13865176,2012-12-13 18:08:32,1,"I think your confusion is due to a misunderstanding about the index_col argument When you pass a list of columns to index_col  pandas is attempting to create a multi-index  that is  a dataframe with more than one column as index  like a multi-dimensional table It is NOT trying to create a single index by concatenating multiple columns 

One strategy that would work is to create three dataframes with the appropriate pairs of columns from your input file  and then concatenate them 

X1  Y1  X2  Y2  X3  Y3 --> Dataframe of (X1  Y1) + Dataframe of (X2  Y2) + Dataframe of (X3  Y3)

If you are using the latest development version of Pandas  or are willing to  this is simplified by using the new parse_cols argument in read_csv() Or you can read in all the data  extract the three dataframes you need  and then concatenate them 

Finally  you can dftruncate with before and after arguments to get the DateRange you need More simply  you could use dropna() to omit dates with missing values 

Hope this helps Do let us know what version of pandas you are using",484596.0,,1897966.0,,2012-12-13 18:18:33,2012-12-13 18:18:33,2.0
14025598,2,14025549,2012-12-24 20:51:27,4,"Pass the axis option to the apply function:

In [265]: fapply(clean  axis=1)
Out[265]:
  level1
   item1 item2
0   1000  2000
1   2000  3000
2   3000  4000


When both axes have hierarchical indices here's a workaround:

In [316]: findex = [[1 2 3] [1 2 3]]

In [317]: f
Out[317]:
    level1
     item1  item2
1 1  1 000  2 000
2 2  2 000  3 000
3 3  3 000  4 000

In [314]: fapply(clean  axis=1)reindex(findex)
Out[314]:
    level1
     item1 item2
1 1   1000  2000
2 2   2000  3000
3 3   3000  4000
",919872.0,,919872.0,,2012-12-25 00:08:06,2012-12-25 00:08:06,8.0
13413842,2,13404468,2012-11-16 09:34:21,5,"it depends what sort of t-test you want to do (one sided or two sided dependent or independent) but it should be as simple as:

from scipystats import ttest_ind

cat1 = my_data[my_data['Category']=='cat1']
cat2 = my_data[my_data['Category']=='cat2']

ttest_ind(cat1['values']  cat2['values'])
>>> (14927289925706944  016970867501294376)


it returns a tuple with the t-statistic & the p-value

see here for other t-tests http://docsscipyorg/doc/scipy/reference/statshtml",240068.0,,,,,2012-11-16 09:34:21,
11368788,2,11361985,2012-07-06 19:49:50,0,"you can use sequence slicing syntax ie

paramdata[:5] # first five records
paramdata[-5:] # last five records
paramdata[:] # all records


sometimes the dataframe might not fit in the screen buffer in which case you are probably better off either printing a small subset or exporting it to something else  plot or (csv again)",240068.0,,,,,2012-07-06 19:49:50,
13237914,2,11361985,2012-11-05 18:13:42,2,"Use:

pandasset_printoptions(max_columns=7)


because you have 7 columns The default is max_columns=0  which takes into account the width of your console  which seems not enough for 7 columns ",1579844.0,,,,,2012-11-05 18:13:42,
11882354,2,11881165,2012-08-09 11:26:08,6,"In [36]: df
Out[36]:
   A  B  C  D
a  0  2  6  0
b  6  1  5  2
c  0  2  6  0
d  9  3  2  2

In [37]: rows
Out[37]: ['a'  'c']

In [38]: dfdrop(rows)
Out[38]:
   A  B  C  D
b  6  1  5  2
d  9  3  2  2

In [39]: df[~((dfA == 0) & (dfB == 2) & (dfC == 6) & (dfD == 0))]
Out[39]:
   A  B  C  D
b  6  1  5  2
d  9  3  2  2

In [40]: dfix[rows]
Out[40]:
   A  B  C  D
a  0  2  6  0
c  0  2  6  0

In [41]: df[((dfA == 0) & (dfB == 2) & (dfC == 6) & (dfD == 0))]
Out[41]:
   A  B  C  D
a  0  2  6  0
c  0  2  6  0
",1548051.0,,,,,2012-08-09 11:26:08,
12168857,2,12168648,2012-08-28 23:13:56,2,"How about this  

from pandas import *
idx = Int64Index([171  174  173])
df = DataFrame(index = idx  data =([1 2 3]))
print df

gives me 

     0
171  1
174  2
173  3


Is this what you are looking for?",1257953.0,,,,,2012-08-28 23:13:56,2.0
12170403,2,12168648,2012-08-29 03:13:24,1,"How about:

df['new_col'] = range(1  len(df) + 1)


Alternatively if you want the index to be the ranks and store the original index as a column:

df = dfreset_index()
",1306530.0,,,,,2012-08-29 03:13:24,
12329993,2,12329853,2012-09-08 10:41:23,0,"There may be an elegant built-in function (but I haven't found it yet) You could write one:

def set_column_sequence(dataframe  seq):
    '''Takes a dataframe and a subsequence of its columns  returns dataframe with seq as first columns'''
    cols = seq[:] # copy so we don't mutate seq
    for x in dataframecolumns:
        if x not in cols:
            colsappend(x)
    return dataframe[cols]


For your example: set_column_sequence(df  ['x' 'y']) would return the desired output",1240268.0,,,,,2012-09-08 10:41:23,2.0
12334295,2,12329853,2012-09-08 20:41:11,0,I would suggest you just write a function to do what you're saying probably using drop (to delete columns) and insert to insert columns at a position There isn't an existing API function to do what you're describing ,776560.0,,,,,2012-09-08 20:41:11,
12335447,2,12329853,2012-09-08 23:58:58,0,"Feel free to disregard this solution as subtracting a list from an Index does not preserve the order of the original Index  if that's important

In [61]: dfreindex(columns=pdIndex(['x'  'y'])append(dfcolumns - ['x'  'y']))
Out[61]: 
    x  y  a  b
0   3 -1  1  2
1   6 -2  2  4
2   9 -3  3  6
3  12 -4  4  8
",243434.0,,243434.0,,2012-09-11 08:44:21,2012-09-11 08:44:21,
12608873,2,12608718,2012-09-26 19:19:38,0,"You need to call it like this:

dfapply(discardValueLessThan  args=(01 ))

The way you're doing it the 01 is not passed as the argument to discardValueLessThan",1662369.0,,,,,2012-09-26 19:19:38,
12608947,2,12608718,2012-09-26 19:24:56,2,"The error message looks like a pandas bug to me  but I think there are two other problems

First  I think you have to either specify named parameters or use args to pass additional arguments to apply  Your second argument is probably being interpreted as an axis  But if you use 

dfapply(discardValueLessThan  args=(01 ))


or

dfapply(discardValueLessThan  threshold=01)


then you'll get

ValueError: ('The truth value of an array with more than one element is ambiguous Use aany() or aall()'  'occurred at index A')


because apply doesn't act elementwise  it acts on entire Series objects  Other approaches include using applymap or boolean indexing  ie

In [47]: df = DataFrame(nprandomrandn(3  3)  columns=['A'  'B'  'C'])

In [48]: df
Out[48]: 
          A         B         C
0 -0135336 -0274687  1480949
1 -1079800 -0618610 -0321235
2 -0610420 -0422112  0102703

In [49]: df1 = dfapplymap(lambda x: discardValueLessThan(x  01))

In [50]: df1
Out[50]: 
   A  B         C
0  0  0  1480949
1  0  0  0000000
2  0  0  0102703


or simply

In [51]: df[df < 01] = 0

In [52]: df
Out[52]: 
   A  B         C
0  0  0  1480949
1  0  0  0000000
2  0  0  0102703
",487339.0,,,,,2012-09-26 19:24:56,2.0
13665099,2,12436979,2012-12-02 00:07:29,1,"I had this exact problem 

The issue is that there is an old version of numpy in the default mac install  and that pip install pandas sees that one first and fails -- not going on to see that there is a newer version that pip herself has installed  

If you're on a default mac install  and you've done pip install numpy --upgrade to be sure you're up to date  but pip install pandas still fails due to an old numpy  try the following: 

$ cd /System/Library/Frameworks/Pythonframework/Versions/27/Extras/lib/python/
$ sudo rm -r numpy
$ pip install pandas


This should now install / build pandas

To check it out what we've done  do the following: start python  and import numpy and import pandas With any luck  numpy__version__ will be 162 (or greater)  and pandas__version__ will be 091 (or greater)  

If you'd like to see where pip has put (found!) them  just print(numpy) and print(pandas) ",1453172.0,,1453172.0,,2013-01-02 22:41:52,2013-01-02 22:41:52,2.0
13697962,2,12436979,2012-12-04 07:25:21,1,"If you're like me and you don't like the idea of deleting things that were part of the standard system installation (which others have suggested) then you might like the solution I ended up using:

Get Homebrew - it's a one-line shell script to install!
Edit your profile  or whatever is appropriate  and put /usr/local/bin at the start
of your PATH so that Homebrew binaries are found before system binaries
brew install python - this installs a newer version of python in /usr/local
pip install pandas
This worked for me in OS X 1082  and I can't see any reason it shouldn't work in 1068 ",1080717.0,,,,,2012-12-04 07:25:21,
13792725,2,12436979,2012-12-09 22:50:25,2,"This worked for me under 1075 with EPD_free-73-2 from Enthought:

Install EPD free  then follow the step in the following link to create bash_profile file 

http://redfinsolutionscom/blog/creating-bashprofile-your-mac

And add the following to the file 

PATH=""/Library/Frameworks/Pythonframework/Versions/Current/bin:$(PATH)}""
export PATH


Execute the following command in Terminal

$ sudo easy_install pandas


When finished  launch PyLab and type:

In [1]: import pandas

In [2]: plot(arange(10))


This should open a plot with a diagonal straight line ",1890270.0,,1890270.0,,2012-12-09 22:55:56,2012-12-09 22:55:56,
13674286,2,12945971,2012-12-02 21:53:14,3,"Both pandas and matplotlibdates use matplotlibunits for locating the ticks 

But while matplotlibdates has convenient ways to set the ticks manually  pandas seems to have the focus on auto formatting so far (you can have a look at the code for date conversion and formatting in pandas)

So for the moment it seems more reasonable to use matplotlibdates (as mentioned by @BrenBarn in his comment)

import numpy as np
import pandas as pd
import matplotlibpyplot as plt 
import matplotlibdates as dates

idx = pddate_range('2011-05-01'  '2011-07-01')
s = pdSeries(nprandomrandn(len(idx))  index=idx)

fig  ax = pltsubplots()
axplot_date(idxto_pydatetime()  s  'v-')
axxaxisset_minor_locator(datesWeekdayLocator(byweekday=(1) 
                                                interval=1))
axxaxisset_minor_formatter(datesDateFormatter('%d\n%a'))
axxaxisgrid(True  which=""minor"")
axyaxisgrid()
axxaxisset_major_locator(datesMonthLocator())
axxaxisset_major_formatter(datesDateFormatter('\n\n\n%b\n%Y'))
plttight_layout()
pltshow()




(my locale is German  so that Tuesday [Tue] becomes Dienstag [Di])  ",1301710.0,,,,,2012-12-02 21:53:14,1.0
11672789,2,11668446,2012-07-26 15:30:56,1,DataFrame is not a structured array and thus doesn't function in the same way with respect to data types There's no API equivalent (to my knowledge  at least) to the record array operation you show there,776560.0,,,,,2012-07-26 15:30:56,1.0
12193483,2,12193253,2012-08-30 09:09:01,2,"pdconcat takes a list of Series:

orig = pdconcat([pdSeries() for i in li]  axis=1)


(renamed your list to li)",449449.0,,,,,2012-08-30 09:09:01,2.0
12196549,2,12193253,2012-08-30 12:09:41,1,"I do something like this all the time but I use append like this:

orig = pdSeries(data  index=index)
for i in list:
    new = pdSeries()
    orig = origappend(new)


Can you verify that the index is unique?

http://pandassourceforgenet/merginghtml#concatenating-using-append

Can you paste the traceback? I would be happy to debug it for you",1064197.0,,128421.0,,2012-08-30 14:09:19,2012-08-30 14:09:19,
12726468,2,12725417,2012-10-04 11:41:00,6,"It`s a private method  but it will do the trick: source_get_numeric_data()

In [2]: import pandas as pd

In [3]: source = pdDataFrame({'A': ['foo'  'bar']  'B': [1  2]  'C': [(1 2)  (3 4)]})

In [4]: source
Out[4]:
     A  B       C
0  foo  1  (1  2)
1  bar  2  (3  4)

In [5]: source_get_numeric_data()
Out[5]:
   B
0  1
1  2
",1548051.0,,,,,2012-10-04 11:41:00,3.0
13066674,2,13063259,2012-10-25 10:33:04,1,"What error do you get with?

ave = datagroupby('Segment')median()


I think that should work  maybe there's something in your data causing the error  like nan's  im just guessing You could try applying your own median function to see if you can work around the cause of the error  something like:

def mymed(group):
    return npmedian(groupdropna())

ave = datagroupby('segment')['Metric']apply(mymed)


It would be easier if you could provide some sample data which replicates the error

Here is a different approach  you can add the median back to your original dataframe  the median for the metric column becomes:

data['metric_median'] = datagroupby('Segment')['Metric']transform('median')


Wether its useful to have the median of the group attached to each datapoint depends a bit what you want to do afterwards",1755432.0,,,,,2012-10-25 10:33:04,
13250102,2,13249135,2012-11-06 11:30:50,1,"I would recoment using macport or fink to install pandas:

Install XCode from App Store  this will install 3 compilers  clang  gcc (""apple"") and gcc (""normal"")
Install macports (wwwmacportsorg) or fink (wwwfinkprojectorg)
Never use your mac python again  and install all python modules trough the fink/macport and enjoy it taking care dependencies for you
Installing pandas in macports is as simple as:
    sudo port install py27-pandas

you usualy install macport in /opt/local and fink in /sw  I would advice (though this may be bad advice) you to symlink your fink/mac ports python to your system python as follows such that:
/usr/bin/python -> /opt/local/Library/Frameworks/Pythonframework/Versions/27/bin/python27",1356142.0,,,,,2012-11-06 11:30:50,
13252767,2,13249135,2012-11-06 14:09:43,1,You need to install XCode AND you need to make sure you install the command line tools for XCode so you can get gcc,1306530.0,,,,,2012-11-06 14:09:43,2.0
13445630,2,13445241,2012-11-18 23:15:17,3,"How about:

d = dapplymap(lambda x: npnan if isinstance(x  basestring) and xisspace() else x)


The applymap function applies a function to every cell of the dataframe",1427416.0,,,,,2012-11-18 23:15:17,2.0
13193256,2,13187778,2012-11-02 10:16:00,2,"You can use the to_records method  but have to play around a bit with the dtypes if they are not what you want from the get go In my case  having copied your DF from a string  the index type is string (represented by an object dtype in pandas):

In [102]: df
Out[102]: 
label    A    B    C
ID                  
1      NaN  02  NaN
2      NaN  NaN  05
3      NaN  02  05
4      01  02  NaN
5      01  02  05
6      01  NaN  05
7      01  NaN  NaN

In [103]: dfindexdtype
Out[103]: dtype('object')
In [104]: dfto_records()
Out[104]: 
recarray([(1  nan  02  nan)  (2  nan  nan  05)  (3  nan  02  05) 
       (4  01  02  nan)  (5  01  02  05)  (6  01  nan  05) 
       (7  01  nan  nan)]  
      dtype=[('index'  '|O8')  ('A'  '<f8')  ('B'  '<f8')  ('C'  '<f8')])
In [106]: dfto_records()dtype
Out[106]: dtype([('index'  '|O8')  ('A'  '<f8')  ('B'  '<f8')  ('C'  '<f8')])


Converting the recarray dtype does not work for me  but one can do this in Pandas already:

In [109]: dfindex = dfindexastype('i8')
In [111]: dfto_records()view([('ID'  '<i8')  ('A'  '<f8')  ('B'  '<f8')  ('C'  '<f8')])
Out[111]:
recarray([(1  nan  02  nan)  (2  nan  nan  05)  (3  nan  02  05) 
       (4  01  02  nan)  (5  01  02  05)  (6  01  nan  05) 
       (7  01  nan  nan)]  
      dtype=[('ID'  '<i8')  ('A'  '<f8')  ('B'  '<f8')  ('C'  '<f8')])


Note that Pandas does not set the name of the index properly (to ID) in the exported record array (a bug?)  so we profit from the type conversion to also correct for that 

At the moment Pandas has only 8-byte integers  i8  and floats  f8 (see this issue)",54567.0,,,,,2012-11-02 10:16:00,4.0
13349131,2,13347288,2012-11-12 18:12:34,1,"For regular frequencies pandas converts the underlying index into a PeriodIndex before plotting and uses a different set of tick formatters and locators for PeriodIndex

As a workaround for now  you can use rot to rotate the ticks so they don't overlap It won't be as pretty as the second plot but it'll at least be readable

I made an issue here on github (https://githubcom/pydata/pandas/issues/2235)",1306530.0,,,,,2012-11-12 18:12:34,2.0
12994302,2,12992958,2012-10-21 02:02:05,1,"This should answer both your questions:

Note: if you want the 704th element  you should use ""703"" as index starts form zero As you see df['A']argmin() also returns 1  that is the second row in the df

In [682]: print df
                   A         B         C
2000-01-01  1073247 -1784255  0137262
2000-01-02 -0797483  0665392  0692429
2000-01-03  0123751  0532109  0814245
2000-01-04  1045414 -0687119 -0451437
2000-01-05  0594588  0240058 -0813954
2000-01-06  1104193  0765873  0527262
2000-01-07 -0304374 -0894570 -0846679
2000-01-08 -0443329 -1437305 -0316648


In [683]: dfindex[3]
Out[683]: <Timestamp: 2000-01-04 00:00:00>

In [684]: df['A']argmin()
Out[684]: 1
",1199589.0,,1199589.0,,2012-10-21 12:14:10,2012-10-21 12:14:10,
13262240,2,13261855,2012-11-07 02:02:03,1,"Sorting is a big subject  and I'm sure there are many ways to do this Here's one 

First creating an example dataframe

In [31]: rndrange = pdDatetimeIndex(start='10/17/2012'  end='10/22/2012'  freq='D')    
In [32]: df = pdDataFrame(nprandomrandn(len(rndrange) 5) index=rndrange)
In [33]: df = dfapplymap(abs)  #Easier to see sorting if all vals are positive
In [34]: df
Out[34]: 
                   0         1         2         3         4
2012-10-17  1542735  1081290  2602967  0748706  0682501
2012-10-18  0058414  0148083  0094104  0716789  2482998
2012-10-19  2396277  0524733  2169018  1365622  0590767
2012-10-20  0513535  1542485  0186261  2138740  1173894
2012-10-21  0495713  1401872  0919931  0055136  1358439
2012-10-22  1010086  0350249  1116935  0323305  0506086


Sorting: 

In [35]: dfas_matrix()sort(1)    
In [36]: df
Out[36]: 
                   0         1         2         3         4
2012-10-17  0682501  0748706  1081290  1542735  2602967
2012-10-18  0058414  0094104  0148083  0716789  2482998
2012-10-19  0524733  0590767  1365622  2169018  2396277
2012-10-20  0186261  0513535  1173894  1542485  2138740
2012-10-21  0055136  0495713  0919931  1358439  1401872
2012-10-22  0323305  0350249  0506086  1010086  1116935
",484596.0,,,,,2012-11-07 02:02:03,2.0
13319544,2,13261855,2012-11-10 05:26:09,1,"Well  it's not too easy to do with pandas out of the box First  familiarize yourself with argsort:

In [8]: df
Out[8]: 
                   0         1         2         3         4
2012-10-17  1542735  1081290  2602967  0748706  0682501
2012-10-18  0058414  0148083  0094104  0716789  2482998
2012-10-19  2396277  0524733  2169018  1365622  0590767
2012-10-20  0513535  1542485  0186261  2138740  1173894
2012-10-21  0495713  1401872  0919931  0055136  1358439
2012-10-22  1010086  0350249  1116935  0323305  0506086

In [12]: inds = dfvaluesargsort(1)

In [13]: inds
Out[13]: 
array([[4  3  1  0  2] 
       [0  2  1  3  4] 
       [1  4  3  2  0] 
       [2  0  4  1  3] 
       [3  0  2  4  1] 
       [3  1  4  0  2]])


These are the indirect sort indices for each row Now you want to do something like:

new_values = npempty_like(df)
for i  row in enumerate(dfvalues):
    # sort in descending order
    new_values[i] = row[inds[i]][::-1]

sorted_df = DataFrame(new_values  index=dfindex)


Not that satisfying but it gets the job done:

In [15]: sorted_df
Out[15]: 
                   0         1         2         3         4
2012-10-17  2602967  1542735  1081290  0748706  0682501
2012-10-18  2482998  0716789  0148083  0094104  0058414
2012-10-19  2396277  2169018  1365622  0590767  0524733
2012-10-20  2138740  1542485  1173894  0513535  0186261
2012-10-21  1401872  1358439  0919931  0495713  0055136
2012-10-22  1116935  1010086  0506086  0350249  0323305


More generally you can do:

In [23]: dfapply(lambda x: npsort(xvalues)[::-1]  axis=1)
Out[23]: 
                   0         1         2         3         4
2012-10-17  2602967  1542735  1081290  0748706  0682501
2012-10-18  2482998  0716789  0148083  0094104  0058414
2012-10-19  2396277  2169018  1365622  0590767  0524733
2012-10-20  2138740  1542485  1173894  0513535  0186261
2012-10-21  1401872  1358439  0919931  0495713  0055136
2012-10-22  1116935  1010086  0506086  0350249  0323305


But you're responsible for assigning new columns yourself",776560.0,,,,,2012-11-10 05:26:09,1.0
14248948,2,14248706,2013-01-10 00:33:02,3,"You could use the value_counts method:

In [10]: servalue_counts()
Out[10]: 
two      3
one      1
three    1


and then plot this as a bar chart:

servalue_counts()plot(kind='bar')


Edit: I noticed that this doesn't keep the desired order If you have a list/Series for this ordering (in this case ser[:3] will do) you can reindex before plotting:

In [12]: servalue_counts()reindex(ser[:3])
Out[12]: 
one      1
two      3
three    1
",1240268.0,,1240268.0,,2013-01-10 00:43:33,2013-01-10 00:43:33,1.0
14514757,2,14514046,2013-01-25 03:13:41,3,"The key to doing this kind of thing in pandas is the stack() method:

dfstack(level=0)


However  I found getting to a place where you can use this  at least particular csv is tricky to say the least (there is almost certainly a nicer way to do this!):

df_data = pdread_csv('ecsv'  sep=' \s+'  header=None  skiprows=3)[range(7)]set_index(0)
df_cols = pdread_csv('ecsv'  sep=' \s+'  header=None  nrows=2)set_index(0)[:2] #interval causing problems    
df_ = df_colsappend(df_data)Tset_index(['DATES' 'UNITS' 'Interval'])T
df = df_stack(level=0)
df_dates = map(lambda x: pdto_datetime(' 'join(x[::-1]))  dfindex)
dfindex = df_dates

In [7]: df
Out[7]: 
UNITS                   Hz      MW        kV
2010-01-12 00:15:00  4982   3465  3373755
2010-02-12 00:15:00  4992  3633    339009
2010-01-12 00:30:00   499   3534   337722
2010-02-12 00:30:00  4989  3765    338382
2010-01-12 00:45:00  4994    335   338316
2010-02-12 00:45:00  5009  3741   3407745
2010-01-12 01:00:00  4986   3091  3394875
2010-02-12 01:00:00  5018  3611   3420945
2010-01-12 01:15:00  4997   2728   342243
2010-02-12 01:15:00  5011  3324    343596
2010-01-12 01:30:00  5002   2691   343332
2010-02-12 01:30:00  5012  3103     34452
2010-01-12 01:45:00  5001   3126   341286
2010-02-12 01:45:00     50  3886    339306
2010-01-12 02:00:00  5008   3496   339141
2010-02-12 02:00:00  5014  3831   3399165
2010-01-12 02:15:00  5007   3533  3384975
2010-02-12 02:15:00  5001  3978    339537
2010-01-12 02:30:00  4997   3363   340263
2010-02-12 02:30:00  5007  4148    338547


That's a little messy and  has commas in some columns!:

def clean(s):
    try: return float(sstrip(' '))
    except: return s

In [9]: dfapplymap(clean)
Out[9]: 
                        Hz     MW        kV
2010-01-12 00:15:00  4982  3465  3373755
2010-02-12 00:15:00  4992  3633  3390090
2010-01-12 00:30:00  4990  3534  3377220
2010-02-12 00:30:00  4989  3765  3383820
2010-01-12 00:45:00  4994  3350  3383160
2010-02-12 00:45:00  5009  3741  3407745
2010-01-12 01:00:00  4986  3091  3394875
2010-02-12 01:00:00  5018  3611  3420945
2010-01-12 01:15:00  4997  2728  3422430
2010-02-12 01:15:00  5011  3324  3435960
2010-01-12 01:30:00  5002  2691  3433320
2010-02-12 01:30:00  5012  3103  3445200
2010-01-12 01:45:00  5001  3126  3412860
2010-02-12 01:45:00  5000  3886  3393060
2010-01-12 02:00:00  5008  3496  3391410
2010-02-12 02:00:00  5014  3831  3399165
2010-01-12 02:15:00  5007  3533  3384975
2010-02-12 02:15:00  5001  3978  3395370
2010-01-12 02:30:00  4997  3363  3402630
2010-02-12 02:30:00  5007  4148  3385470
",1240268.0,,,,,2013-01-25 03:13:41,7.0
14528938,2,14514046,2013-01-25 19:23:33,0,"Another solution would be to 

Read the dates (first row of csv)
Read in the rest of the data  including intervals
Construct the index as desired and apply it to the dataframe 
Here's some sample code: 

In [1]: from StringIO import StringIO    
In [2]: import pandas as pd
In [3]: pd__version__
Out[3]: '0101'

In [4]: CSV_SAMPLE = """"""
DATES  01-12-2010  01-12-2010  01-12-2010  02-12-2010  02-12-2010  02-12-2010
UNITS  Hz  kV  MW   Hz  kV  MW
Interval           
00:15  4982  3373755  3465  4992  339009  3633 
00:30  499  337722  3534  4989  338382  3765 
00:45  4994  338316  335  5009  3407745  3741 
01:00  4986  3394875  3091  5018    3420945  3611 
01:15  4997  342243   2728   5011   343596  3324 
01:30   5002   343332  2691  5012   34452  3103 
""""""

#Create one dataframe from just the dates (and we'll grab the units  too)
In [6]: datesdf = pdread_csv(StringIO(CSV_SAMPLE)  nrows= 2)
In [7]: dates  units = datesdfindexdroplevel()

In [9]: dates  units
Out[9]:
((' 01-12-2010' 
  ' 01-12-2010' 
  ' 01-12-2010' 
  ' 02-12-2010' 
  ' 02-12-2010' 
  ' 02-12-2010') 
 (' Hz'  ' kV'  ' MW'  '  Hz'  ' kV'  ' MW'))

#Create a second dataframe from the rest of the data
In [11]: data = pdread_csv(StringIO(CSV_SAMPLE)  skiprows=3)
In [12]: data = dataicol([0 1 2])

#Note: Instead  in pandas 010  you can use the usecols paramater in read_csv()
#  to combine the above two steps into one 

In [14]: datacolumns = units[:3]
In [15]: print data
          Hz        kV     MW
00:15  4982  3373755  3465
00:30  4990  3377220  3534
00:45  4994  3383160  3350
01:00  4986  3394875  3091
01:15  4997  3422430  2728
01:30  5002  3433320  2691


Now create the desired index and apply it Here are several approaches for the index 

#We'll need to grab the intervals from this data df
In [16]: intervals = dataindextolist()  

In [17]: index1 = pdMultiIndexfrom_arrays([dates intervals])

#This is a multi-index
In [18]: print index1
MultiIndex
[( 01-12-2010  00:15)  ( 01-12-2010  00:30)  ( 01-12-2010  00:45)  ( 02-12-2010  01:00)  ( 02-12-2010  01:15)  ( 02-12-2
010  01:30)]

#This index is a tuple of date interval
In [21]: index2 = pdIndex(zip(dates  intervals))
In [22]: print index2
Index([( 01-12-2010  00:15)  ( 01-12-2010  00:30)  ( 01-12-2010  00:45)  ( 02-12-2010  01:00)  ( 02-12-2010  01:15)  ( 0
2-12-2010  01:30)]  dtype=object)

#This index is based on a string concat of date and interval
In [23]: def list_join(x y):
   :         joined = x + ' ' + y
   :         return joinedstrip()
   :

In [24]: index3 = pdIndex(map(list_join  dates  intervals))    
In [25]: print index3
Simple index:
Index([01-12-2010 00:15  01-12-2010 00:30  01-12-2010 00:45  02-12-2010 01:00  02-12-2010 01:15  02-12-2010 01:30]  dtyp
e=object)


Since third third type of index is what you had in your original request  I'll use that 

In [26]: dataindex = index3
In [27]: print data
                     Hz        kV     MW
01-12-2010 00:15  4982  3373755  3465
01-12-2010 00:30  4990  3377220  3534
01-12-2010 00:45  4994  3383160  3350
02-12-2010 01:00  4986  3394875  3091
02-12-2010 01:15  4997  3422430  2728
02-12-2010 01:30  5002  3433320  2691


You may have to modify the code above to handle the complete dataset if it complains about non-unique index values In that case  read in the Intervals csv column as a data column (not an index) and pull it out as an array to create the desired index  same as above ",484596.0,,,,,2013-01-25 19:23:33,2.0
12335161,2,11193212,2012-09-08 23:02:53,1,This is a bit irritating as I think that NumPy should have suppress=True as its default I'm changing pandas to not change it,776560.0,,,,,2012-09-08 23:02:53,1.0
11811425,2,11811392,2012-08-04 19:31:03,7,Simplest way is probably list(dtTitertuples()) (where dt is your dataframe)  This generates a list of tuples,1427416.0,,,,,2012-08-04 19:31:03,2.0
11812627,2,11811392,2012-08-04 23:02:21,0,"My naive approach would be using iteritems with 'll' as a list of lists and l as a single list

df = DataFrame({'one':[1 1]  'two':[2 1]  'three':[3 1]  'four':[3 1] })

ll = []

for idx row in dfiteritems():
    l = rowvaluestolist()
    linsert(0 idx)
    llappend(l)
",1521724.0,,,,,2012-08-04 23:02:21,
12413347,2,12411897,2012-09-13 19:23:13,0,If you know they're always in the same positions you can say something like parse_dates=[[0 1 2 3 4]],1306530.0,,,,,2012-09-13 19:23:13,1.0
12818364,2,12818103,2012-10-10 11:34:05,1,"use dfDate=dfDateapply(lambda x:xdate()) to convert the datetime column

>>> df=DataFrame({'Date':[datetime(2006 1 1)]})
>>> df
                  Date
0  2006-01-01 00:00:00
>>> dfDate=dfDateapply(lambda x:xdate())
>>> df
         Date
0  2006-01-01


EDIT:

to format DATE_LIST

dates=[datetimedatetimestrptime(i  ""%Y%m%d"")date() for i in DATE_LIST]


and finally use isin to filter out the rows that are in DATE_LIST:

df[df['Dates']isin(dates)]


in one go(given that you have stk_price dataframe that has a datecolumn called Date):

import datetime

stk_price = DataReader('600809ss'  'yahoo'  datetime(2006 1 1)  datetime(2012 12 31))reset_index()
stk_priceDate=stk_priceDateapply(lambda x:xdate())
dates=[datetimedatetimestrptime(i  ""%Y%m%d"")date() for i in DATE_LIST]
stk_price[stk_price['Dates']isin(dates)]
",1199589.0,,1199589.0,,2012-10-10 15:41:46,2012-10-10 15:41:46,6.0
12818681,2,12818103,2012-10-10 11:53:26,1,"Instead of converting stk_price dates to strings (which shows the wrong dates due to a numpy bug  see also github issue #1802) you can also convert the DATE_LIST to timestamps and use these directly to index stk_price Below an example on frame containing random data 

In [16]: DATE_LIST = [
 u'20090331'  u'20090630'  u'20090930'  u'20091231'  \
 u'20100331'  u'20100630'  u'20100930'  u'20101231'  \
 u'20110331'  u'20110630'  u'20110930'  u'20111231'  \
 u'20120331'  u'20120630'  u'20120930'  u'20121231'
 ]

In [17]: timestamps = [pdTimestamp(date) for date in DATE_LIST]

In [18]: df = pdDataFrame(nprandomrandn(100 3)  index=pddate_range('20090331'  periods=100))

In [19]: dfix[timestamps]dropna()
Out[19]:
                   0         1         2
2009-03-31  0520235  1158889 -0310227
2009-06-30  1036449  0528931 -0083166
",1548051.0,,,,,2012-10-10 11:53:26,
13184361,2,13183271,2012-11-01 19:44:30,1,"I know no easy solution to get to align to the closest and I find the current version quite logical But with label='left' you can achieve what you want with the current data  still it doesn't align to the closest  so overall you probably have to figure out something else (like using apply to change the dates so they would conform as you wish)

sresample('Q' label='left')    

Out[57]:
2011-12-31    100
2012-03-31    200
Freq: Q-DEC
",1199589.0,,,,,2012-11-01 19:44:30,1.0
13291523,2,13281305,2012-11-08 15:00:01,1,"CSV should contain rows with same count of fields If it just pairs of date-number without relations between pairs  it isnt CSV  but just file of pairs So  it should be parsed as file of pairs:

input = open(""sparsecsv"")read()split() # split by newlines and spaces
i = iter(input)
for date in i:
    if date != ""nan"":
        value = inext()
        # process pairs
",1692925.0,,,,,2012-11-08 15:00:01,1.0
13703721,2,13703720,2012-12-04 13:08:29,1,"One option is to use __str__  and then to_datetime (or similar):

In [11]: dt64__str__()
Out[11]: '2012-05-01T01:00:00000000+0100'

In [12]: pdto_datetime(dt64__str__())
Out[12]: datetimedatetime(2012  5  1  1  0  tzinfo=tzoffset(None  3600))


Note: it is not equal to dt because it's become ""offset-aware"":

In [13]: pdto_datetime(dt64__str__())replace(tzinfo=None)
Out[13]: datetimedatetime(2012  5  1  1  0)


This seems inelegant



Update: this can deal with the ""nasty example"":

In [21]: dt64 = numpydatetime64('2002-06-28T01:00:00000000000+0100')

In [22]: pdto_datetime(dt64__str__())replace(tzinfo=None)
Out[22]: datetimedatetime(2002  6  28  1  0)
",1240268.0,,1240268.0,,2012-12-04 18:01:40,2012-12-04 18:01:40,
13703930,2,13703720,2012-12-04 13:22:10,1,">>> dt64tolist()
datetimedatetime(2012  5  1  0  0)


For DatetimeIndex  the tolist returns a list of datetime objects For a single datetime64 object it returns a single datetime object",449449.0,,,,,2012-12-04 13:22:10,5.0
13704307,2,13703720,2012-12-04 13:42:08,3,"To convert numpydatetime64 to datetime object that represents time in UTC on numpy-18:

>>> from datetime import datetime
>>> import numpy as np
>>> dt = datetimeutcnow()
>>> dt
datetimedatetime(2012  12  4  19  51  25  362455)
>>> dt64 = npdatetime64(dt)
>>> ts = (dt64 - npdatetime64('1970-01-01T00:00:00Z')) / nptimedelta64(1  's')
>>> ts
13546506853624549
>>> datetimeutcfromtimestamp(ts)
datetimedatetime(2012  12  4  19  51  25  362455)
>>> np__version__
'180dev-7b75899'


The above example assumes that a naive datetime object is interpreted by npdatetime64 as time in UTC

To convert datetime to npdatetime64 and back (numpy-16):

>>> npdatetime64(datetimeutcnow())astype(datetime)
datetimedatetime(2012  12  4  13  34  52  827542)


It works both on a single npdatetime64 object and a numpy array of npdatetime64

Think of npdatetime64 the same way you would about npint8  npint16  etc and apply the same methods to convert beetween Python objects such as int  datetime and corresponding numpy objects

Your ""nasty example"" works correctly:

>>> from datetime import datetime
>>> import numpy 
>>> numpydatetime64('2002-06-28T01:00:00000000000+0100')astype(datetime)
datetimedatetime(2002  6  28  0  0)
>>> numpy__version__
'162' # current version available via pip install numpy


I can reproduce the long value on numpy-180 installed as:

pip install git+https://githubcom/numpy/numpygit#egg=numpy-dev


The same example:

>>> from datetime import datetime
>>> import numpy
>>> numpydatetime64('2002-06-28T01:00:00000000000+0100')astype(datetime)
1025222400000000000L
>>> numpy__version__
'180dev-7b75899'


It returns long because for numpydatetime64 type astype(datetime) is equivalent to astype(object) that returns Python integer (long) on numpy-18 

To get datetime object you could:

>>> dt64dtype
dtype('<M8[ns]')
>>> ns = 1e-9 # number of seconds in a nanosecond
>>> datetimeutcfromtimestamp(dt64astype(int) * ns)
datetimedatetime(2002  6  28  0  0)


To get datetime64 that uses seconds directly:

>>> dt64 = numpydatetime64('2002-06-28T01:00:00000000000+0100'  's')
>>> dt64dtype
dtype('<M8[s]')
>>> datetimeutcfromtimestamp(dt64astype(int))
datetimedatetime(2002  6  28  0  0)


The numpy docs say that the datetime API is experimental and may change in future numpy versions",4279.0,,4279.0,,2012-12-04 20:30:33,2012-12-04 20:30:33,7.0
13753918,2,13703720,2012-12-06 22:40:22,6,"Welcome to hell

You can just pass a datetime64 object to pandasTimestamp:

In [16]: Timestamp(numpydatetime64('2012-05-01T01:00:00000000'))
Out[16]: <Timestamp: 2012-05-01 01:00:00>


I noticed that this doesn't work right though in NumPy 161:

numpydatetime64('2012-05-01T01:00:00000000+0100')


Also  pandasto_datetime can be used (this is off of the dev version  haven't checked v091):

In [24]: pandasto_datetime('2012-05-01T01:00:00000000+0100')
Out[24]: datetimedatetime(2012  5  1  1  0  tzinfo=tzoffset(None  3600))
",776560.0,,,,,2012-12-06 22:40:22,
13937141,2,13937022,2012-12-18 16:18:48,4,"You nearly had it  but you have to use the ""bitwise or"" operator:

In [6]: df[(dfone == 1) | (dftwo == 7)]
Out[6]: 
   one  three  two
0    1      9    5
2    3     17    7

In [7]: df[(dfoneisin(checkList)) | (dftwoisin(checkList))]
Out[7]: 
   one  three  two
0    1      9    5
2    3     17    7
",1240268.0,,1240268.0,,2012-12-18 16:34:54,2012-12-18 16:34:54,3.0
14112100,2,14112020,2013-01-01 16:31:08,2,"In [1]: import pandas as pd

In [2]: idx1 = pddate_range('2010-01-01' '2010-12-31' freq='D')

In [3]: idx2 = pddate_range('2010-01-01' '2010-11-01' freq='D')

In [4]: idx3 = pddate_range('2010-01-01' '2010-12-31' freq='D')

In [5]: help(idx1equals)
Help on method equals in module pandastseriesindex:

equals(self  other) method of pandastseriesindexDatetimeIndex instance
    Determines if two Index objects contain the same elements


In [6]: print(idx1equals(idx2))
False

In [7]: print(idx1equals(idx3))
True
",190597.0,,,,,2013-01-01 16:31:08,1.0
14439783,2,14439060,2013-01-21 13:31:22,2,"Oh  I managed to do it with:

asciitableread(""1txt""  Reader=asciitableFixedWidthNoHeader 
                col_starts=( 8  16  22  28  36  44  49  55  60  67  74) 
                col_ends  =(14  21  25  34  42  48  53  58  65  73  80))


discarding first three rows

Edit:

As DSM notes -- same can be acheived with pandas:

pandasread_fwf(""1txt""  widths=[6  8  7  5  8  8  6  6  5  6  8  7]  skiprows=3)
",788700.0,,788700.0,,2013-01-28 15:11:45,2013-01-28 15:11:45,
14493658,2,14439060,2013-01-24 03:52:58,0,"what about:

import pandas as pd
pdread_table('exampletxt'  skiprows=3  sep=r'\s*' )


if theres only spaces in the middle I don't think its going to work but its hard to say without sample data",983191.0,,,,,2013-01-24 03:52:58,2.0
14603893,2,14602739,2013-01-30 12:21:44,4,"You can use join to do the combining:

>>> import pandas as pd
>>> df = pdDataFrame({""A"": [10 20 30]  ""B"": [20  30  10]})
>>> df
    A   B
0  10  20
1  20  30
2  30  10
>>> df * 2
    A   B
0  20  40
1  40  60
2  60  20
>>> dfjoin(df*2  rsuffix='1')
    A   B  A1  B1
0  10  20  20  40
1  20  30  40  60
2  30  10  60  20


where you could replace df*2 with dfapply(your_function) if you liked",487339.0,,487339.0,,2013-01-30 12:29:26,2013-01-30 12:29:26,1.0
14609910,2,14602739,2013-01-30 17:19:48,2,"I would skip the apply method and just define the columns directly

import pandas as pd
df = pdDataFrame({""A"": [10 20 30]  ""B"": [20  30  10]})
for col in dfcolumns:
    df[col+""1""] = df[col] * 2


Not as elegant as DSM's solution But for whatever reason I avoid apply unless I really need it",1552748.0,,,,,2013-01-30 17:19:48,2.0
10374456,2,10373660,2012-04-29 17:50:33,8,"g1 here is a DataFrame It has a hierarchical index  though:

In [19]: type(g1)
Out[19]: pandascoreframeDataFrame

In [20]: g1index
Out[20]: 
MultiIndex([('Alice'  'Seattle')  ('Bob'  'Seattle')  ('Mallory'  'Portland') 
       ('Mallory'  'Seattle')]  dtype=object)


Perhaps you want something like this?

In [21]: g1add_suffix('_Count')reset_index()
Out[21]: 
      Name      City  City_Count  Name_Count
0    Alice   Seattle           1           1
1      Bob   Seattle           2           2
2  Mallory  Portland           2           2
3  Mallory   Seattle           1           1


Or something like:

In [36]: DataFrame({'count' : df1groupby( [ ""Name""  ""City""] )size()})reset_index()
Out[36]: 
      Name      City  count
0    Alice   Seattle      1
1      Bob   Seattle      2
2  Mallory  Portland      2
3  Mallory   Seattle      1
",776560.0,,,,,2012-04-29 17:50:33,1.0
9918526,2,9892044,2012-03-29 02:57:43,3,"Works fine for me:

In [3]: read_csv('/home/wesm/tmp/footxt'  skiprows=3  index_col=6  parse_dates=True)
Out[3]: 
                     Est  dir  Vmed   raj  Vmin  desvpadro  Unnamed: 7
date                                                                     
2009-01-01 00:00:00  555  162   53  101   65          067         NaN
2009-01-01 00:10:00  555  135   61  109   64          067         NaN
2009-01-01 00:20:00  555  156   59  110   59          076         NaN
2009-01-01 00:30:00  555  178   69  109   53          096         NaN
2009-01-01 00:40:00  555  200   98  112   61          096         NaN
2009-01-01 00:50:00  555  100   97  114   57          096         NaN


What version of pandas are you using? Maybe there's a problem that only presents itself with the whole file?",776560.0,,,,,2012-03-29 02:57:43,
9986326,2,9892044,2012-04-03 02:26:39,0,"Try this:
Cut the file showing only a few rows  say 13 rows---three dead rows  then rerun the file You may have the correct data there The problem you are having is that the rows are 157968 Pandas cant show all those rows The output is a summary table  Or  its spyder doing the summary I have seen similar summary tables 

try this:

for x in dados: print x
",428862.0,,428862.0,,2012-04-04 21:02:27,2012-04-04 21:02:27,
10717773,2,10717504,2012-05-23 10:10:06,1,"When you get the row from the csvreader  and when you can be sure that the first element is a string  then you can use

if not row[0]startswith('TEST'):
    process(row)
",1346705.0,,,,,2012-05-23 10:10:06,
10717900,2,10717504,2012-05-23 10:17:15,0,"http://pandaspydataorg/pandas-docs/stable/generated/pandasioparsersread_csvhtml?highlight=read_csv#pandasioparsersread_csv


  skiprows : list-like or integer
  Row numbers to skip (0-indexed) or number of rows to skip (int)


Pass [0  6] to skip rows with ""TEST""",412080.0,,,,,2012-05-23 10:17:15,1.0
10718007,2,10717504,2012-05-23 10:23:48,2,"from cStringIO import StringIO
import pandas

s = StringIO()
with open('filecsv') as f:
    for line in f:
        if not linestartswith('TEST'):
            swrite(line)
sseek(0) # ""rewind"" to the beginning of the StringIO object

pandasread_csv(s) # with further parameters
",449449.0,,,,,2012-05-23 10:23:48,1.0
11039349,2,11004088,2012-06-14 18:39:53,3,"Reconstructing your DataFrame:

In [1]: index = MultiIndexfrom_tuples(zip([21 22 23] [45 45 46])  names=['A'  'B'])
In [2]: df = DataFrame({0:[001  030  045]  
                        1:[056  088  023]  
                        2:[023  053  090]  
                        'ref': [002  087  023]}  index=index)
In [3]: df
Out[3]: 
        0     1     2   ref
A  B                         
21 45  001  056  023  002
22 45  030  088  053  087
23 46  045  023  090  023


I would first get the absolute distance of columns0  1 and 2 from ref:

 In [4]: dist = df[[0 1 2]]sub(df['ref']  axis=0)apply(npabs)
 In [5]: dist
 Out[5]: 
         0     1     2
 A  B                   
 21 45  001  054  021
 22 45  057  001  034
 23 46  022  000  067


Given now dist you can determine the column with the min value by row using DataFrameidxmin:

In [5]: idx = distidxmin(axis=1)
In [5]: idx
Out[5]: 
A   B 
21  45    0
22  45    1
23  46    1


To now generate your new closest  then you simply need to use idx to index df: 

In [6]: df['closest'] = idxindexmap(lambda x: dfix[x][idxix[x]])
In [7]: df
Out[7]: 
        0     1     2   ref  closest
A  B                                  
21 45  001  056  023  002     001
22 45  030  088  053  087     088
23 46  045  023  090  023     023


For the last step  there might be a more elegant way to do it but I'm relatively new to Pandas and that's the best I can think of right now",696023.0,,,,,2012-06-14 18:39:53,
10891703,2,10883805,2012-06-05 04:47:09,1,"if you are sure that Date1 is subset of Date2 and Date2 contains no empty value  you can simply do 

df = read_csv('foocsv'  index_col=2  parse_dates=True)
df = df[[""rate1""  ""rate2""]]


but it will be complicated if Date2 has date which Date1 doesn't have I suggest you put date/rate pair in separate files with date as common header 

df1 = read_csv('foo1csv'  index_col=0  parse_dates=True)
df2 = read_csv('foo2csv'  index_col=0  parse_dates=True)
df1join(df2  how=""outer"")


EDIT:
This method doesn't look good so for your NaN in your datetime  you can do sth like

dateindex2 = map(lambda x: datetime(int(""20""+xsplit(""/"")[2])  int(xsplit(""/"")[0])  int(xsplit(""/"")[1]))  filter(notnull  df['Date2']values))
ts2 = Series(df[""Rate2""]dropna()  index=dateindex2)
#same for ts1
df2 = DataFrame({""rate1"":ts1  ""rate2"":ts2})


the thing is you have to make sure that there is case like date exists but not rate because dropna() will shift records and mismatch with index",1377107.0,,1377107.0,,2012-06-06 06:51:17,2012-06-06 06:51:17,1.0
10899744,2,10883805,2012-06-05 15:00:27,2,"I don't think splitting up the data into multiple files is necessary How about loading the file with read_csv and converting each date/rate pair into a separate time series? So your code would look like:

data = read_csv('foocsv')

ts1 = Series(data['rate1']  index=data['date1'])
ts2 = Series(data['rate2']  index=data['date2'])


Now  to join then together and align the data in a DataFrame  you can do:

frame = DataFrame({'rate1': ts1  'rate2': ts2})


This will form the union of the dates in ts1 and ts2 and align all of the data (inserting NA values where appropriate)

Or  if you have N time series  you could do:

all_series = {}
for i in range(N):
   all_series['rate%d' % i] = Series(data['rate%d' % i]  index=data['date%d' % i])

frame = DataFrame(all_series)


This is a very common pattern in my experience",776560.0,,,,,2012-06-05 15:00:27,3.0
11077060,2,11077023,2012-06-18 04:51:10,5,"Numpy is required by pandas (and by virtually all numerical tools for Python)  Scipy is not strictly required for pandas but is listed as an ""optional dependency""  I wouldn't say that pandas is an alternative to Numpy and/or Scipy  Rather  it's an extra tool that provides a more streamlined way of working with numerical and tabular data in Python  You can use pandas data structures but freely draw on Numpy and Scipy functions to manipulate them",1427416.0,,,,,2012-06-18 04:51:10,
11077215,2,11077023,2012-06-18 05:11:17,18,Indeed  pandas provides high level data manipulation tools built on top of NumPy NumPy by itself is a fairly low-level tool  and will be very much similar to using MATLAB pandas on the other hand provides rich time series functionality  data alignment  NA-friendly statistics  groupby  merge and join methods  and lots of other conveniences It has become very popular in recent years in financial applications I will have a chapter dedicated to financial data analysis using pandas in my upcoming book ,776560.0,,776560.0,,2012-06-18 19:32:13,2012-06-18 19:32:13,
11565306,2,11563435,2012-07-19 16:32:03,0,"It seems to work for me on 081dev Can you post a stack trace and/or what merged2 looks like? Also are you sure you're using pandas 08?

In [50]: import pandas as pd

In [51]: idx = pdIndex([16  25  32  9  22  7  33  13  28  29])

In [52]: idx
Out[52]: Int64Index([16  25  32   9  22   7  33  13  28  29])

In [53]: df = DataFrame(nprandomrandn(len(idx)  3)  idx  ['id'  1  2])

In [54]: df
Out[54]: 
          id         1         2
16  0351188  2082303 -0143037
25  0633243 -1731306  0749934
32 -0337893 -0264249 -0549856
9  -0728056  0786955  1103877
22  1131559 -0255439 -0397913
7  -1384519  0397626 -0421481
33  1356455  2863659 -2060498
13 -0355786 -0051383 -0609486
28 -0056607  0767800  1433946
29 -0288202 -0437992  0843746

In [55]: dfset_index('id')
Out[55]: 
                  1         2
id                           
 0351188  2082303 -0143037
 0633243 -1731306  0749934
-0337893 -0264249 -0549856
-0728056  0786955  1103877
 1131559 -0255439 -0397913
-1384519  0397626 -0421481
 1356455  2863659 -2060498
-0355786 -0051383 -0609486
-0056607  0767800  1433946
-0288202 -0437992  0843746

In [56]: pd__version__
Out[56]: '081dev-e2633d4'
",1306530.0,,,,,2012-07-19 16:32:03,1.0
11321950,2,11321243,2012-07-04 03:07:48,1,"Updated Answer:

It seems that I can replicate this using my work version of the various libraries I will check my home versions later to see if there is a difference in the docs for these functions

In the meantime  the following worked for me using your exact edited version:

In [35]: testDFaggregate(lambda x: stsem(x  axis=None))
Out[35]:
<class 'pandascoreframeDataFrame'>
Int64Index: 600 entries  0 to 599
Data columns:
A    600  non-null values
B    600  non-null values
C    600  non-null values
D    600  non-null values
E    600  non-null values
F    600  non-null values
G    600  non-null values
H    600  non-null values
I    600  non-null values
J    600  non-null values
dtypes: float64(10)


This makes me suspect that it has to do with the sem() axis conventions It defaults to 0  and the Pandas objects that this ultimately gets mapped to might have a weird 0-th axis or something When I used the option axis=None  it ravels the object that it gets applied to  and this made it work

Just as a sanity check  I did this and it worked too:

In [37]: testDFaggregate(lambda x: stsem(x  axis=1))
Out[37]:
<class 'pandascoreframeDataFrame'>
Int64Index: 600 entries  0 to 599
Data columns:
A    600  non-null values
B    600  non-null values
C    600  non-null values
D    600  non-null values
E    600  non-null values
F    600  non-null values
G    600  non-null values
H    600  non-null values
I    600  non-null values
J    600  non-null values
dtypes: float64(10)


But you should check to make sure this is actually the SEM values you wanted  probably on some smaller example data

Older answer:
Could this have to do with module issues with scipystats? When I use this module  I have to call it as from scipy import stats as st or something like that import scipystats doesn't work  and calling import scipy; scipystatssem gives an error saying that no module named ""stats"" exists 

Pandas appears to simply not be finding that function I think the error messages should be improved because this is not obvious

>>> from scipy import stats as st
>>> import pandas
>>> import numpy as np
>>> df_list = []
>>> for ii in range(10):
     df_listappend(pandasDataFrame(nprandomrand(10 3)  
     columns = ['A'  'B'  'C']))
 
>>> df_list
# Suppressed the output cause it was big

>>> testDF = (pandasconcat(df_list  axis=1  keys=range(len(df_list)))
     swaplevel(0  1  axis=1)
     sortlevel(axis=1)
     groupby(level=0  axis=1))
>>> testDF
<pandascoregroupbyDataFrameGroupBy object at 0x38524d0>
>>> testDFaggregate(npmean)
key_0         A         B         C
0      0660324  0408377  0374681
1      0459768  0345093  0432542
2      0498985  0443794  0524327
3      0605572  0563768  0558702
4      0561849  0488395  0592399
5      0466505  0433560  0408804
6      0561591  0630218  0543970
7      0423443  0413819  0486188
8      0514279  0479214  0534309
9      0479820  0506666  0449543
>>> testDFaggregate(npvar)
key_0         A         B         C
0      0093908  0095746  0055405
1      0075834  0077010  0053406
2      0094680  0092272  0095552
3      0105740  0126101  0099316
4      0087073  0087461  0111522
5      0105696  0110915  0096959
6      0082860  0026521  0075242
7      0100512  0051899  0060778
8      0105198  0100027  0097651
9      0082184  0060460  0121344
>>> testDFaggregate(stsem)
          A         B         C
0  0089278  0087590  0095891
1  0088552  0081365  0098071
2  0087968  0116361  0076837
3  0110369  0087563  0096460
4  0101328  0111676  0046567
5  0085044  0099631  0091284
6  0113337  0076880  0097620
7  0087243  0087664  0118925
8  0080569  0068447  0106481
9  0110658  0071082  0084928


Seems to work for me",567620.0,,567620.0,,2012-07-05 18:40:59,2012-07-05 18:40:59,5.0
11836385,2,11836286,2012-08-06 21:41:15,1,"It's always worth trying the obvious :^)

In [14]: olsresultparams
Out[14]: 
GNPDEFL           15061872
GNP               -0035819
UNEMP             -2020230
ARMED             -1033227
POP               -0051104
YEAR            1829151465
intercept   -3482258634597

In [15]: dict(olsresultparams)
Out[15]: 
{'ARMED': -10332268671737328 
 'GNP': -0035819179292614578 
 'GNPDEFL': 15061872271452557 
 'POP': -0051104105653539733 
 'UNEMP': -20202298038172479 
 'YEAR': 1829151464613984 
 'intercept': -34822586345966831}


See also the to_dict() method of Series objects",487339.0,,,,,2012-08-06 21:41:15,1.0
11843576,2,11836286,2012-08-07 10:04:04,2,"olsresultparams is a pandasSeries object which is dict like  maybe you don`t need to convert to a dict

In [12]: olsresultparamsget('GNP')
Out[12]: -0035819179292566283

In [13]: olsresultparams['GNP']
Out[13]: -0035819179292566283

In [14]: for key  value in olsresultparamsiteritems():
   :     print key  value
   :
GNPDEFL 150618722714
GNP -00358191792926
UNEMP -202022980382
ARMED -103322686717
POP -00511041056537
YEAR 182915146461
intercept -34822586346
",1548051.0,,,,,2012-08-07 10:04:04,1.0
12436963,2,12436895,2012-09-15 11:27:02,1,"To associate the first element to the other you can use itertoolsrepeat and zip  in this way:

>>> import itertools as it
>>> L = [['000002'  [u'20060331'  u'20060630']] 
      ['000005'  [u'20061231'  u'20070331'  u'20070630']]]
>>> couples = [zip(itrepeat(key)  rest) for key  rest in L]
>>> couples
[[('000002'  u'20060331')  ('000002'  u'20060630')] 
[('000005'  u'20061231')  ('000005'  u'20070331')  ('000005'  u'20070630')]]


It shouldn't be too hard to obtain a list like L from the Series object

To create a MultiIndex I belive you've to use the from_tuples method:

MultiIndexfrom_tuples(sum(couples  [])  names=('first'  'second'))


Since I'm not a pandas user I can't help much in the remaining tasks  even though they are probably easy It's a matter of iterating over the Series in the correct way",510937.0,,,,,2012-09-15 11:27:02,3.0
12842390,2,12841827,2012-10-11 14:50:59,1,"Last two are the correct syntax  but there is a (bug preventing to display the result

s = dfix[(22  45)]


works fine  but you can not display it",1548051.0,,,,,2012-10-11 14:50:59,
13187017,2,13185454,2012-11-01 23:21:40,1,"df1 = DataFrame({'a':a})
df2 = DataFrame({'aa':aa})
df3 = DataFrame({'apol':apol})
df=df1append([df2 df3])sort_index()
print dfresample('Q-APR' loffset='-1m')T


Output:

      2011-09-30  2011-12-31  2012-03-31
a           1728        1635        1733
aa          6419        5989        6006
apol        1100        1179         969
",1199589.0,,,,,2012-11-01 23:21:40,
13332682,2,13331518,2012-11-11 15:42:23,3,"How to add single item This is not very effective but follows what you are asking for:

x = pSeries()
N = 4
for i in xrange(N):
   x = xset_value(i  i**2)


produces x:

0    0
1    1
2    4
3    9


Obviously there are better ways to generate this series in only one shot  

For your second question check answer and references of SO question add one row in a pandasDataFrame ",308903.0,,,,,2012-11-11 15:42:23,
13456432,2,13331518,2012-11-19 15:03:18,0,"You can use the append function to add another element to it Only  make a series of the new element  before you append it:

testappend(pdSeries(200  index=[101]))
",170005.0,,,,,2012-11-19 15:03:18,
12184743,2,12178808,2012-08-29 18:57:05,1,"Suppose sec is an array of integers that represents the number of seconds since 1990:

In [26]: import pandas as pd

In [27]: pdIndex(datetime(1990  1  1) + sec * pdoffsetsSecond())
Out[27]:
<class 'pandastseriesindexDatetimeIndex'>
[1990-01-01 00:14:40    1990-04-26 17:26:52]
Length: 10000  Freq: None  Timezone: None
",1306530.0,,,,,2012-08-29 18:57:05,1.0
12540009,2,12539627,2012-09-22 01:40:16,2,"The syntax for a single row is:

dfix['rowlabel'] /= dfix['rowlabel']max()


If you want that done on every row in the dataframe  you can use apply (with axis=1 to select rows instead of columns):

dfapply(lambda x: x / xmax()  axis=1)
",1639821.0,,,,,2012-09-22 01:40:16,
13018496,2,13018134,2012-10-22 19:35:51,0,"It's a workaround  but it works without using periodindex:

from pandastseriesoffsets import *

In [164]: s
Out[164]: 
2012-01-20   -1266376
2012-07-31   -0865573

In [165]: sindex=sindex+MonthEnd(n=0)

In [166]: s
Out[166]: 
2012-01-31   -1266376
2012-07-31   -0865573
",1199589.0,,1199589.0,,2012-10-22 20:25:10,2012-10-22 20:25:10,2.0
13237875,2,13223360,2012-11-05 18:10:44,0,"Here's a way to take your input (as text) and group it the way you want The key is to use a dictionary for each grouping (date  then centre)

import collections
import datetime
import functools

def delta_totals_by_date_and_centre(in_file):
    # Use a defaultdict instead of a normal dict so that missing values are
    # automatically created by_date is a mapping (dict) from a tuple of (year  week)
    # to another mapping (dict) from centre to total delta time
    by_date = collectionsdefaultdict(functoolspartial(collectionsdefaultdict  int))

    # For each line in the input
    for line in in_file:
        # Parse the three fields of each line into date  int  int
        date  centre  delta = linesplit()
        date = datetimedatetimestrptime(date  ""%d/%m/%Y"")date()
        centre = int(centre)
        delta = int(delta)

        # Determine the year and week of the year
        year  week  weekday = dateisocalendar()
        year_and_week = year  week

        # Add the time delta
        by_date[year_and_week][centre] += delta

    # Yield each result  in order
    for year_and_week  by_centre in sorted(by_dateitems()):
        for centre  delta in sorted(by_centreitems()):
            yield year_and_week  centre  delta


For your sample input  it produces this output (where the first column is year-week_of_the_year)

2012-36     0      0
2012-36  2073 141208
2012-36  6078 171481
2012-36  7042  27129
2012-36  7569 124600
2012-36  8239  82153
2012-36  8273 154517
2012-36  8367 113339
2012-36  8959  82770
2012-36  9292 128089
2012-36  9532 137491
2012-36  9705 146321
2012-36 10085 151483
2012-36 10220  87496
2012-36 14573    186
2012-37     0  89522
2012-37  2073 113024
2012-37  6078 160871
2012-37  7042  35063
2012-37  7097  30866
2012-37  8239  61744
2012-37  8273 153898
2012-37  8367  93564
2012-37  8959 116727
2012-37  9292 132628
2012-37  9532 121462
2012-37  9705 139992
2012-37 10085 111229
2012-37 10220  91245
2012-38     0      6
2012-38  2073 169599
2012-38  6078 153976
2012-38  7097  34909
2012-38  7569 152958
2012-38  8239 122693
2012-38  8273 119536
2012-38  8367 116157
2012-38  8959  75579
2012-38  9292 128340
2012-38  9532 163278
2012-38  9705  95205
2012-38 10085  94284
2012-38 10220  92318
2012-38 14573    468
2012-39     0    161
2012-39  2073 170780
2012-39  6078 122972
2012-39  7042  34953
2012-39  7097  63475
2012-39  7569  92371
2012-39  8239 194048
2012-39  8273 123332
2012-39  8367 115365
2012-39  8959 104609
2012-39  9292 131369
2012-39  9532 143933
2012-39  9705 123107
2012-39 10085 129276
2012-39 10220 124681
",99377.0,,,,,2012-11-05 18:10:44,1.0
13243951,2,13223360,2012-11-06 03:32:14,0,"Perhaps group by CostCentre first  then use Series/DataFrame resample()?

In [72]: centers = {}

In [73]: for center  idx in dfgroupby(""CostCentre"")groupsiteritems():
   :     timediff = dfix[idx]set_index(""Date"")['TimeDifference']
   :     centers[center] = timediffresample(""W""  how=sum)

In [77]: pdconcat(centers  names=['CostCentre'])
Out[77]: 
CostCentre  Date      
0           2012-09-09         0
            2012-09-16     89522
            2012-09-23         6
            2012-09-30       161
2073        2012-09-09    141208
            2012-09-16    113024
            2012-09-23    169599
            2012-09-30    170780
6078        2012-09-09    171481
            2012-09-16    160871
            2012-09-23    153976
            2012-09-30    122972


Additional details:

When parse_dates is True for the pdread_* functions  index_col must also be set

In [28]: df = pdread_clipboard(sep=' +'  parse_dates=True  index_col=0 
   :                        dayfirst=True)

In [30]: dfhead()
Out[30]: 
              CostCentre  TimeDifference
DateOccurred                            
2012-09-03          2073           28138
2012-09-03          6078           34844
2012-09-03          8273           31215
2012-09-03          8367           28160
2012-09-03          8959           32037


Since resample() requires a TimeSeries-indexed frame/series  setting the index during creation eliminates the need to set the index for each group individually  GroupBy objects also have an apply method  which is basically syntactic sugar around the ""combine"" step done with pdconcat() above

In [37]: x = dfgroupby(""CostCentre"")apply(lambda df: 
   :         df['TimeDifference']resample(""W""  how=sum))

In [38]: xhead(12)
Out[38]: 
CostCentre  DateOccurred
0           2012-09-09           0
            2012-09-16       89522
            2012-09-23           6
            2012-09-30         161
2073        2012-09-09      141208
            2012-09-16      113024
            2012-09-23      169599
            2012-09-30      170780
6078        2012-09-09      171481
            2012-09-16      160871
            2012-09-23      153976
            2012-09-30      122972
",243434.0,,243434.0,,2012-11-06 16:18:33,2012-11-06 16:18:33,5.0
13434235,2,13434077,2012-11-17 19:54:12,0,"Shohei  

There could be a number of different things going on  depending on your environment and what exactly you are doing If you provide some more information  that will help us help you 

Consider using ipython  in particular  an ipython notebook  if possible Also  try widening your terminal or window width Try dfhead() and share with us what you get ",484596.0,,,,,2012-11-17 19:54:12,2.0
14506690,2,14506583,2013-01-24 17:00:50,1,"Assign the return value to a variable (which I call _ to indicate it's unused):

_ = plthist()
",367273.0,,,,,2013-01-24 17:00:50,2.0
9620832,2,9588331,2012-03-08 16:08:29,3,"Assuming that you have a file called 2010csv with contents

category value
AB 10000
AB 20000
AC 15000
AD 50000


Then  using the ability to apply multiple aggregation functions following a groupby  you can say:

import pandas
data_2010 = pandasread_csv(""/path/to/2010csv"")
data_2010groupby(""category"")agg([len  sum])


You should get a result that looks something like

          value     
            len  sum
category            
AB            2  300
AC            1  150
AD            1  500


Note that Wes will likely come by to point out that sum is optimized and that you should probably use npsum",171965.0,,,,,2012-03-08 16:08:29,1.0
9623878,2,9588331,2012-03-08 19:49:23,2,"Thanks  Jeff It is possible to do this using pivot_table for those interested:

In [8]: df
Out[8]: 
  category  value
0       AB    100
1       AB    200
2       AC    150
3       AD    500

In [9]: dfpivot_table(rows='category'  aggfunc=[len  npsum])
Out[9]: 
            len    sum
          value  value
category              
AB            2    300
AC            1    150
AD            1    500


Note that the result's columns are hierarchically indexed If you had multiple data columns  you would get a result like this:

In [12]: df
Out[12]: 
  category  value  value2
0       AB    100       5
1       AB    200       5
2       AC    150       5
3       AD    500       5

In [13]: dfpivot_table(rows='category'  aggfunc=[len  npsum])
Out[13]: 
            len            sum        
          value  value2  value  value2
category                              
AB            2       2    300      10
AC            1       1    150       5
AD            1       1    500       5


The main reason to use __builtin__sum vs npsum is that you get NA-handling from the latter Probably could intercept the Python built-in  will make a note about that now",776560.0,,,,,2012-03-08 19:49:23,
10565742,2,10565282,2012-05-12 17:03:18,1,"Here's an example that does what you want:

In [32]: from datetime import datetime as dt

In [33]: dr = pDateRange(dt(2009 1 1) dt(2010 12 31)  offset=pdatetoolsHour())

In [34]: hr = drmap(lambda x: xhour)

In [35]: dt = pDataFrame(rand(len(dr) 2)  dr)

In [36]: dt 

Out[36]: 
<class 'pandascoreframeDataFrame'>
DateRange: 17497 entries  2009-01-01 00:00:00 to 2010-12-31 00:00:00
offset: <1 Hour>
Data columns:
0    17497  non-null values
1    17497  non-null values
dtypes: float64(2)

In [37]: dt[(hr >= 10) & (hr <=16)]

Out[37]: 
<class 'pandascoreframeDataFrame'>
Index: 5103 entries  2009-01-01 10:00:00 to 2010-12-30 16:00:00
Data columns:
0    5103  non-null values
1    5103  non-null values
dtypes: float64(2)
",1209307.0,,1209307.0,,2012-05-12 17:56:29,2012-05-12 17:56:29,2.0
10567298,2,10565282,2012-05-12 20:46:40,2,"In upcoming pandas 080  you'll be able to write

hour = tsindexhour
selector = ((10 <= hour) & (hour <= 13)) | ((20 <= hour) & (hour <= 23))
data = ts[selector]
",776560.0,,,,,2012-05-12 20:46:40,
14063022,2,10565282,2012-12-28 00:16:44,1,"As it looks messy in my comment above  I decided to provide another answer which is a syntax update for pandas 0100 on Marc's answer  combined with Wes' hint:

import pandas as pd
from datetime import datetime as dt

dr = pddate_range(dt(2009 1 1) dt(2010 12 31) freq='H')
dt = pdDataFrame(rand(len(dr) 2) dr)
hour = dtindexhour
selector = ((10 <= hour) & (hour <= 13)) | ((20<=hour) & (hour<=23))
data = dt[selector]
",680232.0,,,,,2012-12-28 00:16:44,
11673081,2,11040626,2012-07-26 15:45:35,0,"Your code is not valid  reset_index has no inplace argument in my version of pandas (081) 
The following achieves what you want but there's probably a more elegant way  but you've not provided enough information as to why you are avoiding the reset_index

df2index = MultiIndexfrom_tuples([(x y df2['d']values[i]) for i (x y) in enumerate(df2indexvalues)])


HTH",521586.0,,,,,2012-07-26 15:45:35,
11856979,2,11040626,2012-08-08 02:50:06,0,We added an append option to set_index Try that,776560.0,,,,,2012-08-08 02:50:06,
11619888,2,11619144,2012-07-23 20:18:54,1,"what version of pandas are you using?

I just tried it on my end and the axis looks fine:

http://imgurcom/FpoR7

I am on pandas 081 and matplotlib 110 ",1306530.0,,,,,2012-07-23 20:18:54,6.0
11624916,2,11619144,2012-07-24 05:53:10,2,"I think this is a bug of pandas  it should be self_use_dynamic_x()  please post issue to pandas  to walk around this problem:

import pylab as pl
from pandas import *
import datetime
df = DataFrame([1 1]  index =  [ datetimedatetime(2012 1 1)  datetimedatetime(2012 9 1) ] )
dfplot()
plxticks(rotation=90)
",772649.0,,,,,2012-07-24 05:53:10,1.0
12001086,2,11994765,2012-08-17 06:54:29,3,"Using __repr__ or to_string columns are by default truncated at 50 chars This can be controlled using pandasset_printoptions()

In [64]: df
Out[64]:
                                                   A    B
a  this is a very long string  longer than the defau  bar
b                                                foo  baz

In [65]: pandasset_printoptions(max_colwidth=100)

In [66]: df
Out[66]:
                                                                      A    B
a  this is a very long string  longer than the default max_column width  bar
b                                                                   foo  baz
",1548051.0,,,,,2012-08-17 06:54:29,1.0
12170479,2,12167324,2012-08-29 03:27:07,0,"If you just want to do simple arithmetic operations  I think something like Adiv(B  level='date') should work

Alternatively  you can do something like Breindex(Aindex  level='date') to manually match the indices",1306530.0,,,,,2012-08-29 03:27:07,3.0
12962864,2,12962705,2012-10-18 20:07:06,1,"Use groupby and last()

In [279]: s
Out[279]: 
a    1
b    2
b    3
b    4
e    5

In [280]: grouped = sgroupby(level=0)

In [281]: groupedfirst()
Out[281]: 
a    1
b    2
e    5

In [282]: groupedlast()
Out[282]: 
a    1
b    4
e    5
",1199589.0,,1199589.0,,2012-10-19 06:01:02,2012-10-19 06:01:02,4.0
12989834,2,12962705,2012-10-20 15:20:44,1,BTW we plan on adding a drop_duplicates method to Series like DataFramedrop_duplicates in the near future,776560.0,,,,,2012-10-20 15:20:44,
13130357,2,13129618,2012-10-29 22:07:25,2,"You just need to use the histogram function of numpy:

import numpy as np
count division = nphistogram(serie)


where division is the automatically calculated border for your bins and count is the population inside each bin

If you need to fix a certain number of bins  you can use the argument bins and specify a number of bins  or give it directly the boundaries between each bin

count division = nphistogram(serie bins = [-201 -149 949 1001])


to plot the results you can use the matplotlib function hist  but if you are working in pandas each Series has its own handle to the hist function  and you can give it the chosen binning:

seriehist(bins=division)
",1784138.0,,,,,2012-10-29 22:07:25,1.0
13270595,2,13261175,2012-11-07 13:16:49,2,"pivot only supports using a single column to generate your columns You probably want to use pivot_table to generate a pivot table using multiple columns eg

pandastoolspivotpivot_table(your_dataframe  values='value'  rows='name'  cols=['type'  'date']  aggfunc='sum')


The hierarchical columns that are mentioned in the API reference and documentation for pivot relates to cases where you have multiple value fields rather than multiple categories 

Assuming 'type' and 'date' are categories  whose values should be used as the column names  then you should use pivot_table

However  if you want separate columns for different value fields for the same category (eg 'type')  then you should use pivot without specifying the value column and your category as the columns parameter

For example  suppose you have this DataFrame:

df = DataFrame({'name': ['A'  'B'  'A'  'B']  'type': [1  1  2  2]  'date': ['2012-01-01'  '2012-01-01'  '2012-02-01'  '2012-02-01']   'value': [1  2  3  4]})

pt = dfpivot_table(values='value'  rows='name'  cols=['type'  'date'])
p = dfpivot('name'  'type')


pt will be:

type           1           2
date  2012-01-01  2012-02-01
name                        
A              1           3
B              2           4


and p will be:

          date              value   
type           1           2      1  2
name                                  
A     2012-01-01  2012-02-01      1  3
B     2012-01-01  2012-02-01      2  4
",1452002.0,,1452002.0,,2012-11-07 14:47:42,2012-11-07 14:47:42,3.0
13572798,2,13572576,2012-11-26 20:41:45,1,"You need to use | instead of or  The and and or operators are special in Python and don't interact well with things like numpy and pandas that try to apply to them elementwise across a collection  So for these contexts  they've redefined the ""bitwise"" operators & and | to mean ""and"" and ""or""",1427416.0,,1427416.0,,2012-11-28 04:35:30,2012-11-28 04:35:30,1.0
13888546,2,13888468,2012-12-15 01:46:22,3,"One way is to use indexlevels:

In [11]: df
Out[11]: 
       C
A B     
0 one  3
1 one  2
2 two  1

In [12]: dfindexlevels[1]
Out[12]: Index([one  two]  dtype=object)
",1240268.0,,1240268.0,,2012-12-15 02:21:36,2012-12-15 02:21:36,1.0
14046244,2,14046006,2012-12-26 21:04:08,2,"Nope  you're not required to assign column names  nor do you need them to access any element

In [12]: df = pdDataFrame([0])

In [13]: dfix[0 0]
Out[13]: 0

In [14]: df[0][0]
Out[14]: 0


In fact  you can think of the column already having a name -- it is the integer 0 Look at what happens when you provide a name

In [15]: df    #Before naming the column
Out[15]:
   0
0  0

In [16]: dfcolumns = ['ColA']
In [17]: df    #Renamed
Out[17]:
   ColA
0     0

In [18]: df['ColA'][0]    #Now you can access the column using the new name
Out[18]: 0

In [19]: df[0][0]         # but trying the old name will not work
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)

KeyError: 'no item named 0'


You can still use DataFrameix just as before  though:

In [20]: dfix[0 0]
Out[20]: 0
",484596.0,,,,,2012-12-26 21:04:08,5.0
14235744,2,14235487,2013-01-09 12:58:03,2,"You can recast the dtype using the method astype:

In [11]: a = aastype(object)

In [12]: a['a'] = b

In [13]: a
Out[13]: 
a    [lol  lol  lol]
b                NaN
c                NaN


Alternatively (to using astype) when contructing a you can force the dtype to be object:

In [14]: a = Series(None  index=['a' 'b' 'c']  dtype=object)


The reason you are getting this error is because float64  doesn't allow a Series and similarly it doesn't allow strings - try to set a['a'] = 'lol' and you'll get a ValueError

In [21]: a = Series(None  index=['a' 'b' 'c'])

In [22]: adtype
Out[22]: dtype('float64')


You can read more about typecasting in the docs",1240268.0,,1240268.0,,2013-01-09 13:13:22,2013-01-09 13:13:22,3.0
14365475,2,14361634,2013-01-16 18:46:11,0,"The problem is that you cannot pass an Index to ols
Change it to a Series:

In [153]: ts
Out[153]: 
2011-01-01 00:00:00    19828763
2011-01-01 01:00:00    20112191
2011-01-01 02:00:00    19509116
Freq: H  Name: 1

In [158]: type(tsindex)
Out[158]: pandastseriesindexDatetimeIndex


In [154]: df = tsreset_index()

In [155]: df
Out[155]: 
                index          1
0 2011-01-01 00:00:00  19828763
1 2011-01-01 01:00:00  20112191
2 2011-01-01 02:00:00  19509116

In [160]: type(df['index'])
Out[160]: pandascoreseriesSeries


In [156]: model = pdols(y=df[1]  x=df['index']  intercept=True)

In [163]: model
Out[163]: 

-------------------------Summary of Regression Analysis-------------------------

Formula: Y ~ <x> + <intercept>

Number of Observations:         3
Number of Degrees of Freedom:   1

R-squared:        -00002
Adj R-squared:    -00002

Rmse:              03017

F-stat (1  2):       -inf  p-value:     10000

Degrees of Freedom: model 0  resid 2

-----------------------Summary of Estimated Coefficients------------------------
      Variable       Coef    Std Err     t-stat    p-value    CI 25%   CI 975%
--------------------------------------------------------------------------------
             x     00000     00000       000     09998    -00000     00000
     intercept     00000 766834934       000     10000 -1502996471 1502996471
---------------------------------End of Summary---------------------------------
",733291.0,,,,,2013-01-16 18:46:11,1.0
14669654,2,14663004,2013-02-03 05:02:23,0,"This is because of using integer indices (ix selects those by label over -3 rather than position  and this is by design: see integer indexing in pandas ""gotchas"")

You can use the irows DataFrame method to overcome this ambiguity:

In [11]: df1irow(slice(-3  None))
Out[11]: 
    STK_ID  RPT_Date  TClose   sales  discount
8      568  20080331   3875  12668       NaN
9      568  20080630   3009  21102       NaN
10     568  20080930   2600  30769       NaN


Note: Series has a similar iget method",1240268.0,,,,,2013-02-03 05:02:23,
10020126,2,10002181,2012-04-04 22:24:11,4,"The 3 main data structures are Series (1-dimensional)  DataFrame (2D)  and Panel (3D) (http://pandaspydataorg/pandas-docs/stable/dsintrohtml) A DataFrame is like a collection of Series while a Panel is like a collection of DataFrames In many problem domains (statistics  economics  social sciences  ) these are the 3 major kinds of data that are dealt with

http://pandaspydataorg/pandas-docs/stable/overviewhtml

Also  DataMatrix has been deprecated In pandas >= 040  DataMatrix is just an alias for DataFrame",776560.0,,776560.0,,2012-04-09 02:20:10,2012-04-09 02:20:10,
10793167,2,10771745,2012-05-29 04:43:14,1,"Solution:

acombine_first(b)


Thanks Wes",792849.0,,,,,2012-05-29 04:43:14,
11400509,2,11398688,2012-07-09 18:01:30,1,"from your code snippet it looks like the parsed date value should be the index and each DataFrame will have the values in a different column name right? In that case I think an iterative call to DataFramecombine_first should do the trick

Also  are you passing in ""keep_date_col=True"" as well? By default the parser should be throwing away the component date columns when parsing multiple date components into one (if not then that's a bug so please let me know)

Best 

Chang",1306530.0,,,,,2012-07-09 18:01:30,3.0
12034043,2,12031532,2012-08-20 07:55:49,1,The last field description of target in both files holds multiple words Since white space  is used as seperator  description of target is not treated as a single column by read_table Each word in this field is in a different column In AY942707 the first description of target holds more words than on all of the other lines  this is not the case in AY907538 read_table determines the number of columns from the first line and all following lines should have equal or less number of columns,1548051.0,,,,,2012-08-20 07:55:49,1.0
12369616,2,12369546,2012-09-11 12:22:39,2,"I'm not familiar with panda  but I suspect something is wrong with the dtype you're choosing for your fields It looks like you're using [us]  that is  microseconds from the epoch (1970-01-01)  when you probably should be using days from the epoch

Another possibility is to use basic datetime objects in your ndarray  by using dtype=object",1491200.0,,1491200.0,,2012-09-11 13:27:45,2012-09-11 13:27:45,8.0
12992323,2,12369546,2012-10-20 20:19:59,1,This is a bug Reported here: http://githubcom/pydata/pandas/issues/2095,776560.0,,,,,2012-10-20 20:19:59,
12594030,2,12593759,2012-09-26 03:07:11,0,Try DataFrameduplicated and DataFramedrop_duplicates,1306530.0,,,,,2012-09-26 03:07:11,3.0
13034577,2,13030402,2012-10-23 15:58:16,1,"This is definitely a bug I've created a report on github The reason is because internally  pandas converts a regular frequency DatetimeIndex to PeriodIndex to hook into formatters/locators in pandas  and currently PeriodIndex does NOT retain timezone information
Please stay tuned for a fix",1306530.0,,,,,2012-10-23 15:58:16,
13315888,2,13116394,2012-11-09 20:56:19,0,"Could you describe the pyodbc issues?

I created an issue here To get the ultimate perf you'd want to drop down into C or Cython and build the raw byte string yourself using C string functions Not very satisfying  I know At some point we should build a better-performing to_csv for pandas  too:

http://githubcom/pydata/pandas/issues/2210",776560.0,,,,,2012-11-09 20:56:19,1.0
13401681,2,13395725,2012-11-15 16:17:23,0,"How about this:

In [1]: import numpy as np; import pandas as pd

In [2]: df = pdDataFrame(nprandomrandn(50000  10))

In [3]: def shuffle(df  n):
   :     for i in n:
   :         nprandomshuffle(dfvalues)
   :     return df


In [4]: dfhead()
Out[4]:
          0         1         2         3         4         5         6         7         8         9
0  0329588 -0513814 -1267923  0691889 -0319635 -1468145 -0441789  0004142 -0362073 -0555779
1  0495670  2460727  1174324  1115692  1214057 -0843138  0217075  0495385  1568166  0252299
2 -0898075  0994281 -0281349 -0104684 -1686646  0651502 -1466679 -1256705  1354484  0626840
3  1158388 -1227794 -0462005 -1790205  0399956 -1631035 -1707944 -1126572 -0892759  1396455
4 -0049915  0006599 -1099983  0775028 -0694906 -1376802 -0152225  1413212  0050213 -0209760

In [5]: shuffle(df  1)head(5)
Out[5]:
          0         1         2         3         4         5         6         7         8         9
0  2044131  0072214 -0304449  0201148  1462055  0538476 -0059249 -0133299  2925301  0529678
1  0036957  0214003 -1042905 -0029864  1616543  0840719  0104798 -0766586 -0723782 -0088239
2 -0025621  0657951  1132175 -0815403  0548210 -0029291  0575587  0032481 -0261873  0010381
3  1396024  0859455 -1514801  0353378  1790324  0286164 -0765518  1363027 -0868599 -0082818
4 -0026649 -0090119 -2289810 -0701342 -0116262 -0674597 -0580760 -0895089 -0663331  0

In [6]: %timeit shuffle(df  100)
Out[6]:
1 loops  best of 3: 144 s per loop


This does what you need it to The only question is whether or not it is fast enough

Update

Per the comments by @Einar I have changed my solution 

In[7]: def shuffle2(df  n):
           ind = dfindex
           for i in range(n):
               sampler = nprandompermutation(dfshape[0])
               new_vals = dftake(sampler)values
               df = pdDataFrame(new_vals  index=ind)
           return df

In [8]: dfhead()
Out[8]: 
          0         1         2         3         4         5         6         7         8         9
0 -0175006 -0462306  0565517 -0309398  1100570  0656627  1207535 -0221079 -0933068 -0192759
1  0388165  0155480 -0015188  0868497  1102662 -0571818 -0994005  0600943  2205520 -0294121
2  0281605 -1637529  2238149  0987409 -1979691 -0040130  1121140  1190092 -0118919  0790367
3  1054509  0395444  1239756 -0439000  0146727 -1705972  0627053 -0547096 -0818094 -0056983
4  0209031 -0233167 -1900261 -0678022 -0064092 -1562976 -1516468  0512461  1058758 -0206019

In [9]: shuffle2(df  1)head()
Out[9]: 
          0         1         2         3         4         5         6         7         8         9
0  0054355  0129432 -0805284 -1713622 -0610555 -0874039 -0840880  0593901  0182513 -1981521
1  0624562  1097495 -0428710 -0133220  0675428  0892044  0752593 -0702470  0272386 -0193440
2  0763551 -0505923  0206675  0561456  0441514 -0743498 -1462773 -0061210 -0435449 -2677681
3  1149586 -0003552  2496176 -0089767  0246546 -1333184  0524872 -0527519  0492978 -0829365
4 -1893188  0728737  0361983 -0188709 -0809291  2093554  0396242  0402482  1884082  1373781

In [10]: timeit shuffle2(df  100)
1 loops  best of 3: 247 s per loop
",1742701.0,,1742701.0,,2012-11-16 18:20:02,2012-11-16 18:20:02,8.0
13873113,2,13871150,2012-12-14 05:19:58,2,"This may not be exactly it but should help you along  assuming ts is your timeseries:

hourly = tsresample('H')
hourlyindex = pdMultiIndexfrom_arrays([hourlyindexhour  hourlyindexnormalize()])
hourlyunstack()plot()


If you don't care about the day AT ALL  just hourlyindex = hourlyindexhour should work",1306530.0,,,,,2012-12-14 05:19:58,
14059783,2,14059094,2012-12-27 18:59:12,2,"You can use the DataFrame apply method:

order_df['Value'] = order_dfapply(lambda row: (row['Prices']*row['Amount']
                                               if row['Action']=='Sell'
                                               else -row['Prices']*row['Amount']) 
                                   axis=1)


It is usually faster to use these methods rather than over for loops",1240268.0,,,,,2012-12-27 18:59:12,1.0
14060625,2,14059094,2012-12-27 20:05:45,1,"If we're willing to sacrifice the succinctness of Hayden's solution  one could also do something like this:  

In [22]: orders_df['C'] = orders_dfActionapply(
               lambda x: (1 if x == 'Sell' else -1))

In [23]: orders_df   # New column C represents the sign of the transaction
Out[23]:
   Prices  Amount Action  C
0       3      57   Sell  1
1      89      42   Sell  1
2      45      70    Buy -1
3       6      43   Sell  1
4      60      47   Sell  1
5      19      16    Buy -1
6      56      89   Sell  1
7       3      28    Buy -1
8      56      69   Sell  1
9      90      49    Buy -1


Now we have eliminated the need for the if statement Using DataFrameapply()  we also do away with the for loop As Hayden noted  vectorized operations are always faster 

In [24]: orders_df['Value'] = orders_dfPrices * orders_dfAmount * orders_dfC

In [25]: orders_df   # The resulting dataframe
Out[25]:
   Prices  Amount Action  C  Value
0       3      57   Sell  1    171
1      89      42   Sell  1   3738
2      45      70    Buy -1  -3150
3       6      43   Sell  1    258
4      60      47   Sell  1   2820
5      19      16    Buy -1   -304
6      56      89   Sell  1   4984
7       3      28    Buy -1    -84
8      56      69   Sell  1   3864
9      90      49    Buy -1  -4410


This solution takes two lines of code instead of one  but is a bit easier to read I suspect that the computational costs are similar as well ",484596.0,,,,,2012-12-27 20:05:45,
14071265,2,14059094,2012-12-28 14:47:45,5,"I think a more elegant solution would be to use the where method (was introduced in version 09  but seems not to be documented in the API docs):

In [37]: values = dfPrices * dfAmount

In [38]: df['Values'] = valueswhere(dfAction == 'Sell'  other=-values)

In [39]: df
Out[39]: 
   Prices  Amount Action  Values
0       3      57   Sell     171
1      89      42   Sell    3738
2      45      70    Buy   -3150
3       6      43   Sell     258
4      60      47   Sell    2820
5      19      16    Buy    -304
6      56      89   Sell    4984
7       3      28    Buy     -84
8      56      69   Sell    3864
9      90      49    Buy   -4410


Further more this should be the fastest solution",1301710.0,,1301710.0,,2013-01-10 10:20:30,2013-01-10 10:20:30,1.0
14218626,2,14217581,2013-01-08 15:40:44,1,"You should use the to_excel DataFrame method:

# first convert Series to DataFrame
df_segmenti_t0 = DataFrame(segmenti_t0)

# save as excel spreadsheet
df_segmenti_t0to_excel('provaxls')
",1240268.0,,1240268.0,,2013-01-08 16:00:45,2013-01-08 16:00:45,16.0
14508237,2,14405544,2013-01-24 18:30:24,0,"I don't know how to do the broadcasting you want but for strict assignment this should do it:

dftst[(('GOOG'  'avg_close'))] = 7 


More specifically but still without broadcasting:

for tic in cols_1:
   dftst[(tic  'avg_close')] = pandasrolling_mean(dftst[(tic  'close')] 5) 
",1563557.0,,1563557.0,,2013-01-24 18:41:01,2013-01-24 18:41:01,1.0
14640453,2,14405544,2013-02-01 05:41:32,0,"for this particular problem  it seems like using a Panel object works  I did the following (taking dftst from my original post):

pn = dftstTto_panel()
print pn

Out[83]: 
<class 'pandascorepanelPanel'>
Dimensions: 12 (items) x 3 (major_axis) x 2 (minor_axis)
Items axis: 2009-03-01 06:29:59 to 2009-03-12 06:29:59
Major_axis axis: AAPL to GS
Minor_axis axis: close to rate


If I move the ('close'  'rate') to the Items by doing the following:

pn = pntranspose(2 0 1)
print pn

Out[91]: 
<class 'pandascorepanelPanel'>
Dimensions: 2 (items) x 12 (major_axis) x 3 (minor_axis)
Items axis: close to rate
Major_axis axis: 2009-03-01 06:29:59 to 2009-03-12 06:29:59
Minor_axis axis: AAPL to GS


Now I can do a time series operation and add it as a field in the Panel object:

pn['avg_close'] = pandasrolling_mean(pn['close']  5)
print pn

Out[93]: 
<class 'pandascorepanelPanel'>
Dimensions: 3 (items) x 12 (major_axis) x 3 (minor_axis)
Items axis: close to avg_close
Major_axis axis: 2009-03-01 06:29:59 to 2009-03-12 06:29:59
Minor_axis axis: AAPL to GS

print pn['avg_close']

Out[94]: 
ticker                   AAPL      GOOG        GS
2009-03-01 06:29:59       NaN       NaN       NaN
2009-03-02 06:29:59       NaN       NaN       NaN
2009-03-03 06:29:59       NaN       NaN       NaN
2009-03-04 06:29:59       NaN       NaN       NaN
2009-03-05 06:29:59  0303719 -0129300 -0037954
2009-03-06 06:29:59 -0006839  0206331  0336467
2009-03-07 06:29:59  0128299  0174935  0698275
2009-03-08 06:29:59  0471010 -0137343  0671049
2009-03-09 06:29:59 -0279855 -0033427  0848610
2009-03-10 06:29:59 -0516032  0260944  0373046
2009-03-11 06:29:59 -0456213  0164710  0910448
2009-03-12 06:29:59 -0799156  0544132  0862764


I am actually having some other problems with the Panel objects  but I will leave those to another post",1988295.0,,,,,2013-02-01 05:41:32,
14573561,2,14573453,2013-01-29 00:10:14,4,"In [6]: dfpivot_table(values='rt'  rows='mp'  cols='me'  aggfunc=sum)
Out[6]: 
me         1         2
mp                    
0   1987366  1955086
1   1769593  1729387
2   1416274  1490797
3   1650428  1546333
4   1882780  1933006
",243434.0,,,,,2013-01-29 00:10:14,1.0
8479296,2,8451327,2011-12-12 18:50:53,3,"I think all you need to do is:

data = DataFrame(dict(test1))

that will result in a DataFrame whose columns are the elements like
(11  175  -12118290962161304)

in pandas 061 (to be released soon) you'll also be able to do:

data = DataFramefrom_items(test1)",776560.0,,,,,2011-12-12 18:50:53,
10619525,2,10475488,2012-05-16 13:24:19,2,"To do this I ended up with the following  It is a vectorized version which is 150x faster than one that uses a loop

def cross(series  cross=0  direction='cross'):
    """"""
    Given a Series returns all the index values where the data values equal 
    the 'cross' value 

    Direction can be 'rising' (for rising edge)  'falling' (for only falling 
    edge)  or 'cross' for both edges
    """"""
    # Find if values are above or bellow yvalue crossing:
    above=seriesvalues > cross
    below=nplogical_not(above)
    left_shifted_above = above[1:]
    left_shifted_below = below[1:]
    x_crossings = []
    # Find indexes on left side of crossing point
    if direction == 'rising':
        idxs = (left_shifted_above & below[0:-1])nonzero()[0]
    elif direction == 'falling':
        idxs = (left_shifted_below & above[0:-1])nonzero()[0]
    else:
        rising = left_shifted_above & below[0:-1]
        falling = left_shifted_below & above[0:-1]
        idxs = (rising | falling)nonzero()[0]

    # Calculate x crossings with interpolation using formula for a line:
    x1 = seriesindexvalues[idxs]
    x2 = seriesindexvalues[idxs+1]
    y1 = seriesvalues[idxs]
    y2 = seriesvalues[idxs+1]
    x_crossings = (cross-y1)*(x2-x1)/(y2-y1) + x1

    return x_crossings

# Test it out:
time = [0  01  021  031  040  049  051  06  071  082  093]
voltage = [1   -1   11  -09     1    -1   09 -12  095  -11  111]
df = DataFrame(data=voltage  index=time  columns=['voltage'])
x_crossings = cross(df['voltage'])
y_crossings = npzeros(x_crossingsshape)
pltplot(time  voltage  '-ob'  x_crossings  y_crossings  'or')
pltgrid(True)


It was quite satisfying when this worked  Any improvements that can be made?",792849.0,,792849.0,,2012-05-16 13:29:47,2012-05-16 13:29:47,1.0
11056889,2,11012981,2012-06-15 19:18:50,1,"I'm sure there is a better way  but this did it in a loop:

for idx  eachRecord in reportAggregateDFTiteritems():
reportAggregateDF['PORT_WEIGHT']ix[idx] = reportAggregateDF['SEC_WEIGHT_RATE'][(reportAggregateDF['PORT_ID'] == portID) &            
    (reportAggregateDF['SEC_ID'] == 0) &            
    (reportAggregateDF['GROUP_LIST'] == "" "") &             
    (reportAggregateDF['START_DATE'] == reportAggregateDF['START_DATE']ix[idx]) &             
    (reportAggregateDF['END_DATE'] == reportAggregateDF['END_DATE']ix[idx])]sum()
",1452305.0,,,,,2012-06-15 19:18:50,
11495086,2,11495051,2012-07-15 19:49:30,12,"Looks like Python does not add an intercept by default to your expression  whereas R does when you use the formula interface

This means you did fit two different models Try

lm( y ~ x - 1  data)


in R to exclude the intercept  or in your case and with somewhat more standard notation

lm(num_rx ~ ridageyr - 1  data=demoq)
",143305.0,,143305.0,,2012-07-15 21:56:15,2012-07-15 21:56:15,2.0
11976153,2,11973741,2012-08-15 19:47:23,1,"Pivot table returns a DataFrame so you can simply filter by doing:

In [15]: pivoted = pivot_table(df  values='D'  rows=['A'  'B']  cols=['C'])

In [16]: pivoted[pivotedfoo < 0]
Out[16]: 
C             bar       foo
A     B                    
one   A -0412628 -1062175
three B       NaN -0562207
two   A       NaN -0007245


You can use something like 

pivotedix['one']


to select all A series groups

or 

pivotedix['one'  'A']


to select distinct A and B series groups",1306530.0,,776560.0,,2012-09-09 21:15:28,2012-09-09 21:15:28,3.0
12327090,2,12326641,2012-09-08 00:42:02,2,"Would something like this work?

In [99]: df
Out[99]: 
                            X         Y
2009-08-07 00:00:00 -0900602 -1107547
2009-08-07 01:00:00  0398914  1545534
2009-08-07 02:00:00 -0429100  2052242
2009-08-07 03:00:00  0857940 -0348118
2009-08-07 04:00:00  0394655 -1578197
2009-08-07 05:00:00 -0240995 -1474097
2009-08-07 06:00:00  0619148 -0040635
2009-08-07 07:00:00 -1403177 -0187540
2009-08-07 08:00:00 -0360626 -0399728
2009-08-07 09:00:00  0179741 -2709712

In [100]: df['Time'] = dfindexasi8

In [101]: dist = dfdiff()fillna(0)

In [102]: dist['Dist'] = npsqrt(distX**2 + distY**2)

In [103]: dist['Speed'] = distDist / (distTime / 1e9)

In [104]: dist
Out[104]: 
                            X         Y          Time      Dist     Speed
2009-08-07 00:00:00  0000000  0000000  0000000e+00  0000000       NaN
2009-08-07 01:00:00  1299516  2653081  3600000e+12  2954248  0000821
2009-08-07 02:00:00 -0828013  0506708  3600000e+12  0970752  0000270
2009-08-07 03:00:00  1287040 -2400360  3600000e+12  2723637  0000757
2009-08-07 04:00:00 -0463285 -1230079  3600000e+12  1314430  0000365
2009-08-07 05:00:00 -0635650  0104100  3600000e+12  0644118  0000179
2009-08-07 06:00:00  0860143  1433462  3600000e+12  1671724  0000464
2009-08-07 07:00:00 -2022324 -0146906  3600000e+12  2027653  0000563
2009-08-07 08:00:00  1042550 -0212188  3600000e+12  1063924  0000296
2009-08-07 09:00:00  0540367 -2309984  3600000e+12  2372345  0000659
",1306530.0,,1306530.0,,2012-09-08 00:55:08,2012-09-08 00:55:08,
12792511,2,12702423,2012-10-09 03:40:31,0,"Answer: 

startrng = startrng + pddatetoolsWeekOfMonth(week=1 weekday=4) - pddatetoolsBDay()
",1711722.0,,,,,2012-10-09 03:40:31,
13216292,2,13216087,2012-11-04 05:24:25,2,"Assuming your data source is in a csv file  

from pandasioparsers import read_csv
df = read_csv(""radar_datacsv"")

df  # shows what is in df

       loc  speed  time
0    A     63     0
1    B     61     0
2    C     63     0
3    D     65     0
4    A     73     5
5    B     73     5
6    C     75     5
7    D     75     5
8    A     67     0
9    B     68     0
10   C     68     0
11   D     70     0


Note that I did not set loc as the index yet so it uses an autoincrement integer index

panel = dfset_index(['loc'  'time'])sortlevel(0)to_panel()


However  if your data frame is already using loc as the index  we will need to append the time column into it so that we have a loc-time hierarchal index This can be done using the new append option in the set_index method  Like this:-

panel = dfset_index(['time']  append=True)sortlevel(0)to_panel()


In either case  we should arrive at this scenario:-

panel  # shows what panel is

<class 'pandascorepanelPanel'>
Dimensions: 1 (items) x 4 (major) x 2 (minor)
Items: speed to speed
Major axis: A to D
Minor axis: 0 to 5

panel[""speed""]  # <--- This is what you are looking for


time   0   5
loc         
A     63  67
B     73  61
C     68  73
D     63  68


Hope this helps",482506.0,,482506.0,,2012-11-04 05:34:19,2012-11-04 05:34:19,4.0
13216413,2,13216087,2012-11-04 05:53:01,3,"You can use the pivot method here:

In [71]: df
Out[71]: 
     speed  time
loc             
A       63     0
B       61     0
C       63     0
D       65     0
A       73     5
B       71     5
C       73     5
D       75     5

In [72]: dfreset_index()pivot('loc'  'time'  'speed')
Out[72]: 
time   0   5
loc         
A     63  73
B     61  71
C     63  73
D     65  75
",1306530.0,,,,,2012-11-04 05:53:01,7.0
13616324,2,13608748,2012-11-28 23:31:52,2,"Easy Dataframes (and matrices in general) make it easy to operate on multiple elements at one go

Define the function you want to apply

>>> def abs_diff(x  y):
>>>     return abs(x - y)


Then  apply it

>>> df['Diff'] = abs_diff(df['S1']  df['S2'])

>>> df

   S1  S2  Diff
A   1   4     3
B   5   5     0
C   0   6     6


And of course  if you just want to render the specific column:-

>>> df['Diff']

A    3
B    0
C    6
Name: Diff


(>>> is the python shell prompt of course)",482506.0,,482506.0,,2012-11-29 02:06:09,2012-11-29 02:06:09,1.0
13829026,2,13828891,2012-12-11 21:46:32,2,"You can write this neatly as follows:

ts = df1prices


Here's an example:

In [1]: df = pdread_csv('pricescsv' 
                  parse_dates={'datetime': [0 1]})set_index('datetime')

In [2]: df # dataframe
Out[2]: 
                    prices  duty
datetime                 
2012-11-12 10:00:00      1     0
2012-12-12 10:00:00      2     0
2012-12-12 10:00:00      3     1


In [3]: dfprices # timeseries
Out[3]: 
datetime
2012-11-12 10:00:00    1
2012-12-12 10:00:00    2
2012-12-12 11:00:00    3
Name: prices

In [4]: ts = dfprices


You can groupby date like so (similar to this example from the docs):

In [5]: key = lambda x: xdate()

In [6]: dfgroupby(key)sum()
Out[6]: 
            prices  duty
2012-11-12       1     0
2012-12-12       5     1

In [7]: tsgroupby(key)sum()
Out[7]: 
2012-11-12    1
2012-12-12    5


Where pricescsv contains:

date time prices duty
11/12/2012 10:00 1 0
12/12/2012 10:00 2 0
12/12/2012 11:00 3 1
",1240268.0,,1240268.0,,2012-12-11 23:58:06,2012-12-11 23:58:06,7.0
14000420,2,13999850,2012-12-22 05:46:53,4,"You could use strftime to save these as separate columns:

df['date'] = df['datetime']apply(lambda x: xstrftime('%d%m%Y'))
df['time'] = df['datetime']apply(lambda x: xstrftime('%H%M%S'))


and then be specific about which columns to export to csv:

df[['date'  'time'   ]]to_csv('dfcsv')
",1240268.0,,1240268.0,,2012-12-22 06:10:38,2012-12-22 06:10:38,
14177731,2,14174241,2013-01-05 23:51:48,1,"You can use an expression to select the indices you want without using select():

In [1]: df
Out[1]:
            A
time
2012-05-01  0
2012-05-02  1
2012-05-02  2

In [2]: dfindex
Out[2]:
<class 'pandastseriesindexDatetimeIndex'>

In [3]: dfindexis_unique
Out[3]: False

In [4]: df[dfindex > datetime(2012 5 1)]
Out[4]:
            A
time
2012-05-02  1
2012-05-02  2


Replicating your error using select:

In [5]: sel = lambda x: x > datetime(2012 5 1)

In [6]: dfselect(sel)
Exception: Reindexing only valid with uniquely valued Index objects
",919872.0,,,,,2013-01-05 23:51:48,3.0
14323422,2,14323299,2013-01-14 17:24:02,2,"You can use the Series method isin:

In [1]: df = pdread_csv(cuspcsv  sep='\s+')

In [2]: dfcusipisin(['XXXX'  'ZZZZ'])
Out[2]: 
0     True
1    False
2     True
3     True
4    False
5     True
Name: cusip

In [3]: df[dfcusipisin(['XXXX'  'ZZZZ'])]
Out[3]: 
         date cusip  value
0  2012-12-20  XXXX   423
2  2012-12-20  ZZZZ   812
3  2012-12-21  XXXX   578
5  2012-12-21  ZZZZ   909
",1240268.0,,,,,2013-01-14 17:24:02,1.0
14446240,2,14444916,2013-01-21 19:58:35,1,"Here's a one-liner:

In [1]: df
Out[1]:
    SEGM1 SEGM2  VAL
key
A       K     X    1
B       K     X    2
C       K     X    3
D       K     Y    4
E       K     Y    5
F       J     Y    6
G       J     Z    7
H       J     Z    8
I       J     Z    9


Use the DataFramediv function to divide two dataframes  The first dataframe is grouped by the ""inner levels"" for which you want to calculate shares and then summed  The second dataframe is grouped by the ""outer level"" which serves as the denominator for the share calculation You have to pass level=0 to the div function which refers to the multi-index level SEGM1

In [2]: dfgroupby(['SEGM1' 'SEGM2'])[['VAL']]sum()div(dfgroupby('SEGM1')sum() level=0)
Out[2]:
             VAL
SEGM1 SEGM2
J     Y      02
      Z      08
K     X      04
      Y      06


Numerator DataFrame:

In [1]: dfgroupby(['SEGM1' 'SEGM2'])[['VAL']]sum()
Out[1]:
             VAL
SEGM1 SEGM2
J     Y        6
      Z       24
K     X        6
      Y        9


Denominator DataFrame:

In [2]: dfgroupby('SEGM1')sum()
Out[2]:
       VAL
SEGM1
J       30
K       15
",919872.0,,,,,2013-01-21 19:58:35,1.0
7877795,2,7766400,2011-10-24 15:10:26,1,"So it turns out this was a Python path order issue By running syspathreverse() in my wsgi config file  the code now runs

Due to the order of Python path  the built in OS X numpy library must have been imported first over the virtual environment one causing the issue

'RuntimeError: module compiled against API version 6 but this version of numpy is 4' was the error line I missed in my original post which could have helped debug the answer",995182.0,,,,,2011-10-24 15:10:26,
10037032,2,10027719,2012-04-05 22:43:43,0,Is this the Python distro that came with ArcGIS or Pythonorg? As discussed on the mailing list this does not seem to be an issue with Python 25 per se but potentially your environment,776560.0,,,,,2012-04-05 22:43:43,1.0
11107627,2,11106823,2012-06-19 19:02:15,1,"If I understand you correctly  you want something like:

(xreindex_like(y)fillna(0) + yfillna(0))fillna(0)


This will give the sum of the two dataframes  If a value is in one dataframe and not the other  the result at that position will be that existing value  If a value is missing in both dataframes  the result at that position will be zero

>>> x
   A   B   C
0  1   2 NaN
1  3 NaN   4
>>> y
    A   B   C
0   8 NaN  88
1   2 NaN   5
2  10  11  12
>>> (xreindex_like(y)fillna(0) + yfillna(0))fillna(0)
    A   B   C
0   9   2  88
1   5   0   9
2  10  11  12
",1427416.0,,,,,2012-06-19 19:02:15,1.0
11112419,2,11106823,2012-06-20 03:28:12,5,How about xadd(y  fill_value=0)?,776560.0,,,,,2012-06-20 03:28:12,1.0
11415882,2,11415701,2012-07-10 14:44:48,2,"If my comment answered your question  my answer does not have to comment on it any more ;-)

pandasDataFrame(initialload  columns=list_of_column_names)
",449449.0,,,,,2012-07-10 14:44:48,
11763490,2,11763204,2012-08-01 16:21:23,2,"You can use the converters kw in read_csv Given /tmp/datacsv like this:

""x"" ""y""                                                                         
""one"" ""1234 56""                                                                
""two"" ""2000 00""   


you can do:  

In [20]: pandasread_csv('/tmp/datacsv'  converters={'y': lambda x: float(xreplace('' '')replace(' ' ''))})
Out[20]: 
     x        y
0  one  123456
1  two  200000
",1063605.0,,,,,2012-08-01 16:21:23,2.0
12452587,2,12429279,2012-09-17 02:45:53,1,There's not much overlap between pandas and pytables  but they are very good compliments of each other PyTables is all about storage and retrieval while pandas is about working with the data after retrieval and before storage Pandas has convenient interfaces to PyTables (Check out pandasiopytablesHDFStore) so you can easily store pandas Series/DataFrame using pytables,1306530.0,,,,,2012-09-17 02:45:53,
12989920,2,12950024,2012-10-20 15:31:46,6,"There definitely is a weakness in the API here but I'm not sure off the top of my head to make it easier to do what you're doing Here's one simple way around this  at least for your example:

In [20]: df
Out[20]: 
First     A                             B                         
Second  foo       bar       baz       foo       bar       baz     
Third   dog  cat  dog  cat  dog  cat  dog  cat  dog  cat  dog  cat
0         7    2    9    3    3    0    5    9    8    2    0    6
1         1    4    1    7    2    3    2    3    1    0    4    0
2         6    5    0    6    6    1    5    1    7    4    3    6
3         4    8    1    9    0    3    9    2    3    1    5    9
4         6    1    1    5    1    2    2    6    3    7    2    1

In [21]: rdf = dfstack(['First'  'Third'])

In [22]: rdf['new'] = rdffoo + rdfbar

In [23]: rdf
Out[23]: 
Second         bar  baz  foo  new
  First Third                    
0 A     cat      3    0    2    5
        dog      9    3    7   16
  B     cat      2    6    9   11
        dog      8    0    5   13
1 A     cat      7    3    4   11
        dog      1    2    1    2
  B     cat      0    0    3    3
        dog      1    4    2    3
2 A     cat      6    1    5   11
        dog      0    6    6    6
  B     cat      4    6    1    5
        dog      7    3    5   12
3 A     cat      9    3    8   17
        dog      1    0    4    5
  B     cat      1    9    2    3
        dog      3    5    9   12
4 A     cat      5    2    1    6
        dog      1    1    6    7
  B     cat      7    1    6   13
        dog      3    2    2    5

In [24]: rdfunstack(['First'  'Third'])
Out[24]: 
Second  bar                 baz                 foo                 new               
First     A         B         A         B         A         B         A         B     
Third   cat  dog  cat  dog  cat  dog  cat  dog  cat  dog  cat  dog  cat  dog  cat  dog
0         3    9    2    8    0    3    6    0    2    7    9    5    5   16   11   13
1         7    1    0    1    3    2    0    4    4    1    3    2   11    2    3    3
2         6    0    4    7    1    6    6    3    5    6    1    5   11    6    5   12
3         9    1    1    3    3    0    9    5    8    4    2    9   17    5    3   12
4         5    1    7    3    2    1    1    2    1    6    6    2    6    7   13    5


And you can of course rearrange to your heart's content:

In [28]: rdfunstack(['First'  'Third'])reorder_levels(['First'  'Second'  'Third']  axis=1)sortlevel(0  axis=1)
Out[28]: 
First     A                                       B                                   
Second  bar       baz       foo       new       bar       baz       foo       new     
Third   cat  dog  cat  dog  cat  dog  cat  dog  cat  dog  cat  dog  cat  dog  cat  dog
0         3    9    0    3    2    7    5   16    2    8    6    0    9    5   11   13
1         7    1    3    2    4    1   11    2    0    1    0    4    3    2    3    3
2         6    0    1    6    5    6   11    6    4    7    6    3    1    5    5   12
3         9    1    3    0    8    4   17    5    1    3    9    5    2    9    3   12
4         5    1    2    1    1    6    6    7    7    3    1    2    6    2   13    5
",776560.0,,,,,2012-10-20 15:31:46,1.0
14169034,2,13175251,2013-01-05 05:45:19,0,"In [46]: firstdays = dfgroupby('ob')dayfirst()

In [47]: firstdays
Out[47]: 
ob
a     2012-09-03
b     2012-09-04
c     2012-09-03
d     2012-09-03
e     2012-09-01
f     2012-09-02
Name: day

In [48]: dfapply(lambda row: (row['day'] - firstdays[row['ob']])days + 1  axis=1)
Out[48]: 
343340    1
343341    1
343342    1
343343    5
343344    1
343345    1
343349    1
343350    1
343351    1
343352    1
343353    5
343354    8
",243434.0,,,,,2013-01-05 05:45:19,
13415772,2,13411544,2012-11-16 11:33:47,1,"It's good practice to always use the [] notation  one reason is that attribute notation (dfcolumn_name) does not work for numbered indices:

In [1]: df = DataFrame([[1  2  3]  [4  5  6]])

In [2]: df[1]
Out[2]: 
0    2
1    5
Name: 1

In [3]: df1
  File ""<ipython-input-3-e4803c0d1066>""  line 1
    df1
       ^
SyntaxError: invalid syntax
",1240268.0,,,,,2012-11-16 11:33:47,1.0
13485766,2,13411544,2012-11-21 03:12:31,1,It's difficult to make del dfcolumn_name work simply as the result of syntactic limitations in Python del df[name] gets translated to df__delitem__(name) under the covers by Python,776560.0,,,,,2012-11-21 03:12:31,
13647445,2,13647222,2012-11-30 14:47:52,1,"Here's one way to obtain the DataFrame in which you can plot more easily: 

In [5]: df2 = DataFrame(dict(
                 (L  df[df['series'] == L]['value']values)
                       for L in df['series']unique()))

In [6]: df2
Out[6]: 
   A  B  C
0  0  0  5
1  1  0  4
2  2  1  3
3  3  2  2
4  4  4  1


If you set the index name to 'step' you can plot as desired:

In [7]: df2indexname = 'step'
Out[7]: 
      A  B  C
step         
0     0  0  5
1     1  0  4
2     2  1  3
3     3  2  2
4     4  4  1

In [8]: df2plot()


Which gives the desired figure",1240268.0,,1240268.0,,2012-12-02 13:05:29,2012-12-02 13:05:29,0.0
13986115,2,13914077,2012-12-21 07:38:15,2,"If you're interested in MultiIndexes  check out 
dfpivot_table()  It will create a MultiIndex automatically when
multiple keys are passed in the rows and/or cols parameters

For example  say you want to pivot the data so there are separate columns for
each weekend and non-weekend 30-minute block of the day; you could do that by adding 
Day  Weekend  and TOD (time-of-day) columns to the DataFrame  and then passing
those column names to pivot_table as follows

pivot = dfpivot_table(values='Usage'  rows='Day'  cols=['TOD'  'Weekend'])


In this format  pdrolling_mean() (or
a function of your creation) can easily be applied to the columns of pivot  pdrolling_mean()  like all rolling/moving functions in pandas  even accepts a center parameter for centered sliding windows

pdrolling_mean(pivot  90  center=True  min_periods=1)
",243434.0,,243434.0,,2012-12-21 08:13:07,2012-12-21 08:13:07,1.0
14086002,2,14085517,2012-12-29 21:17:32,7,"import io
import pandas as pd
import numpy as np

text = '''\
SEGM1\tDESC\tDistribuzione Ponderata\tRotazioni a volume
AD\tACCADINAROLO\t74040\t140249693409
AD\tZYMIL AMALAT Z\t90085\t321529053570
FUN\tSPECIALMALAT S\t88650\t120711182177
NORM\tSTD INNAROLO\t49790\t162259216710
NORM\tSTD PNAROLO\t52125\t1252174695695
NORM\tSTD PLNAROLO\t54230\t213257829615
NORM\tBONTA' MALAT B\t79280\t520454366419
NORM\tDA STD RILGARD\t35290\t554927497875
NORM\tOVANE VTMANTO\t15040\t466232639628
NORM\tWEIGHT MALAT W\t79170\t118628572692
'''

df = pdread_csv(ioBytesIO(text)  delimiter = '\t' 
                 index_col = (0 1) )

key1 = dfindexlabels[0]
key2 = df['Distribuzione Ponderata']rank(ascending=False)
sorter = nplexsort((key2  key1))

sorted_df = dftake(sorter)
print(sorted_df)


yields

                      Distribuzione Ponderata  Rotazioni a volume
SEGM1 DESC                                                       
AD    ZYMIL AMALAT Z                   90085       321529053570
      ACCADINAROLO                     74040       140249693409
FUN   SPECIALMALAT S                   88650       120711182177
NORM  BONTA' MALAT B                   79280       520454366419
      WEIGHT MALAT W                   79170       118628572692
      STD PLNAROLO                     54230       213257829615
      STD PNAROLO                     52125      1252174695695
      STD INNAROLO                     49790       162259216710
      DA STD RILGARD                   35290       554927497875
      OVANE VTMANTO                   15040       466232639628


I learned this trick here The key idea is to use numpylexsort",190597.0,,190597.0,,2013-01-08 14:34:55,2013-01-08 14:34:55,2.0
14279543,2,14276661,2013-01-11 13:51:12,0,"I found a compact solution by creating a class inheriting file:

import pandas as pd

class FileWrapper(file):
    def __init__(self  comment_literal  *args):
        super(FileWrapper  self)__init__(*args)
        self_comment_literal = comment_literal

    def next(self):
        while True:
            line = super(FileWrapper  self)next()
            if not linestartswith(self_comment_literal):
                return line

df = pdread_table(FileWrapper(""#""  ""14276661txt""  ""r"")  delimiter="" ""  header=None)


Atm  pandas (081) only uses the next()-method to iterate over file-like objects We can overload this method and only return those lines that do not start with the dedicated comment-literal  in my example ""#"" 

For input file:

176792 -230523 0430772 32016 1 1 2 
# 177042 -187729 0430562 32016 1 1 1
177047 -154957 0431853 31136 1 1 1
177403 -0657246 0432905 31152 1 1 1


we get

>>> df
       X1       X2       X3    X4  X5  X6  X7
0  176792 -2305230  0430772  32016    1    1    2
1  177047 -1549570  0431853  31136    1    1    1
2  177403 -0657246  0432905  31152    1    1    1


and for 

176792 -230523 0430772 32016 1 1 2 
177042 -187729 0430562 32016 1 1 1
177047 -154957 0431853 31136 1 1 1
177403 -0657246 0432905 31152 1 1 1


we get

>>> df
       X1       X2       X3    X4  X5  X6  X7
0  176792 -2305230  0430772  32016    1    1    2
1  177042 -1877290  0430562  32016    1    1    1
2  177047 -1549570  0431853  31136    1    1    1
3  177403 -0657246  0432905  31152    1    1    1


Instead of inheritance you could also use delegation  it is up to your flavor

EDIT
I tried many other ways to improve on the performance It's a tough job  though I tried

threading: Read file ahead in one thread with low-level io-operation and large chunks  split it into lines  enqueue these and only get from queue on next()
same with multiprocessing
similar  multithreaded approach but using readlines(size_hint)
mmap for reading from file
The first three approaches surprisingly were slower  so no benefit Using a mmap significantly improved the performance Here is the code:

class FileWrapper(file):
    def __init__(self  comment_literal  *args):
        super(FileWrapper  self)__init__(*args)
        self_comment_literal = comment_literal
        self_in_comment = True
        self_prepare()

    def __iter__(self):
        return self

    def next(self):
        if self_in_comment:
            while True:
                line = self_get_next_line()
                if line == """":
                    raise StopIteration()
                if not line[0] == self_comment_literal:
                    self_in_comment = False
                    return line
        line = self_get_next_line()
        if line == """":
            raise StopIteration()
        return line

    def _get_next_line(self):
        return super(FileWrapper  self)next()

    def _prepare(self):
        pass

class MmapWrapper(file):
    def __init__(self  fd  comment_literal = ""#""):
        self_mm = mmapmmap(fd  0  prot=mmapPROT_READ)
        self_comment_literal = comment_literal
        self_in_comment = True

    def __iter__(self):
        return self #iter(self_mmreadline  """")#self

    def next(self):
        if self_in_comment:
            while True:
                line = self_mmreadline()
                if line == """":
                    raise StopIteration()
                if not line[0] == self_comment_literal:
                    self_in_comment = False
                    return line
        line = self_mmreadline()
        if line == """":
            raise StopIteration()
        return line

if __name__ == ""__main__"":
    t0 = timetime()    
    for i in range(10):    
        with open(""1gram-d_1txt""  ""r+b"") as f:
            df1 = pdread_table(MmapWrapper(ffileno())  delimiter=""\t""  header=None)
    print ""mmap:""  timetime()-t0

    t0 = timetime()    
    for i in range(10):    
        df2 = pdread_table(FileWrapper(""#""  ""1gram-d_1txt""  ""r"")  delimiter=""\t""  header=None)
    print ""Unbuffered:""  timetime()-t0

    print (df1==df2)mean()


gives as output

mmap: 353251504898
Unbuffered: 413274121284
X1    1
X2    1
X3    1
X4    1


I also implemented comment-checking only until the first non-comment line is found This complies with your solution and further improves the performance

For mmaps  there exist some restrictions  though If file sizes are huge  be sure to have enough RAM If you're working on a 32bit OS  you won't be able to read files larger than 2GB",1156006.0,,1156006.0,,2013-01-14 10:39:11,2013-01-14 10:39:11,7.0
14280694,2,14276661,2013-01-11 14:57:15,0,"read_csv and read_table have a comment option that will skip bytes starting from a comment character until the end of a line If an entire line needs to be skipped  this isn't quite right because the parser will think that it's seen a line with no fields in it  then eventually see a valid data line and get confused

I'd suggest using your workaround to determine the number of rows to skip manually in the file It would be nice to have an option that enables automatically skipping lines when the entire line is a comment:

https://githubcom/pydata/pandas/issues/2685

Implementing this well would require dipping into the C tokenizer code It's not as bad as it might sound",776560.0,,,,,2013-01-11 14:57:15,1.0
14590847,2,14590638,2013-01-29 19:38:42,3,"Actually I think you should not modify the data itself  but to take a view of the data in the desired interval to plot This view would be the actual datapoints to be  plotted

A naive approach would be  for a computer screen for example  to calculate how many points are in your interval  and how many pixels you have available Thus  for plotting a dataframe with 10000 points in a window 1000 pixels width  you take a slice with a STEP of 10  using this syntax (whole_data would be a 1D array just for the example):

data_to_plot = whole_data[::10]


This might have undesired effects  specifically masking short peaks that might ""escape invisible"" from the slicing operation An alternative would be to split your data into bins  then calculating one datapoint (maximum value  for example) for each bin I feel that these operations might actually be fast due to numpy/pandas efficient array operations

Hope this helps!",401828.0,,,,,2013-01-29 19:38:42,2.0
14591748,2,14590638,2013-01-29 20:35:41,1,"You could use the pandasqcut method on the index to divide the index into equal quantiles The value you pass to qcut could be actual_size/SIZE_LIMIT

In [1]: from pandas import *

In [2]: df = DataFrame({'a':range(10000)})

In [3]: dfhead()

Out[3]:
   a
0  0
1  1
2  2
3  3
4  4


Here  grouping the index by qcut(dfindex 5) results in 5 equally binned groups  I then take the mean of each group

In [4]: dfgroupby(qcut(dfindex 5))mean()

Out[4]:
                       a
[0  19998]        9995
(19998  39996]  29995
(39996  59994]  49995
(59994  79992]  69995
(79992  9999]    89995
",919872.0,,,,,2013-01-29 20:35:41,
9762084,2,9758450,2012-03-18 20:39:19,5,"How about:

subset = data_set[['data_date'  'data_1'  'data_2']]
tuples = [tuple(x) for x in subsetvalues]
",776560.0,,,,,2012-03-18 20:39:19,1.0
13731128,2,9758450,2012-12-05 19:42:50,0,"A generic way:

[tuple(x) for x in data_setto_records(index=False)]
",1876739.0,,,,,2012-12-05 19:42:50,
12961196,2,11005012,2012-10-18 18:21:53,0,"The problem is that when you use an arbitrary number of white spaces as a separator  you end up with different numbers of columns for each row

In this case  this field in the header 2010/03/23 17:01:00  will be converted to two columns Now that the number of columns is differs between the header and the data  pandas assumes that it must index the first column This is why you get the error To fix it  you can try to specify a set number of white spaces as the delimiter You can also use a comma or a tab or something else that will lead to unambiguous field delineation",899470.0,,,,,2012-10-18 18:21:53,
11893669,2,11714768,2012-08-09 23:59:21,0,"Try to join on outer

When I am working with a number of stocks  I would usually have a frame titled ""open high low close etc"" with column as a ticker If you want one data structure  I would use Panels for this

for Yahoo data  you can use pandas:

import pandasiodata as data
spy = dataDataReader(""SPY"" ""yahoo"" ""1991/1/1"")
",1064197.0,,1064197.0,,2012-10-13 18:18:28,2012-10-13 18:18:28,
13593882,2,11714768,2012-11-27 21:57:44,1,"It is possible to read the data with pandas and to concatenate it

First import the data

In [449]: import pandasiodata as web

In [450]: nab = webget_data_yahoo('NABAX'  start='2009-05-25' 
                                   end='2009-06-05')[['Close'  'Volume']]

In [451]: cba = webget_data_yahoo('CBAAX'  start='2009-05-26' 
                                   end='2009-06-08')[['Close'  'Volume']]

In [453]: nab
Out[453]: 
            Close    Volume
Date                       
2009-05-25  2115   9685100
2009-05-26  2164   8541900
2009-05-27  2174   9042900
2009-05-28  2163   9701000
2009-05-29  2202  14665700
2009-06-01  2252   6782000
2009-06-02  2280  10473400
2009-06-03  2311   9931400
2009-06-04  2221  17869000
2009-06-05  2195   8214300

In [454]: cba
Out[454]: 
            Close    Volume
Date                       
2009-05-26  3545   4529600
2009-05-27  3513   4521500
2009-05-28  3395   7945400
2009-05-29  3514  12548500
2009-06-01  3616   4509400
2009-06-02  3633   4304900
2009-06-03  3680   4845400
2009-06-04  3679   4592300
2009-06-05  3651   4417500
2009-06-08  3651         0


Than concatenate it:

In [455]: keys = ['CBAAX' 'NABAX']

In [456]: pdconcat([cba  nab]  axis=1  keys=keys)
Out[456]: 
            CBAAX            NABAX          
             Close    Volume   Close    Volume
Date                                          
2009-05-25     NaN       NaN   2115   9685100
2009-05-26   3545   4529600   2164   8541900
2009-05-27   3513   4521500   2174   9042900
2009-05-28   3395   7945400   2163   9701000
2009-05-29   3514  12548500   2202  14665700
2009-06-01   3616   4509400   2252   6782000
2009-06-02   3633   4304900   2280  10473400
2009-06-03   3680   4845400   2311   9931400
2009-06-04   3679   4592300   2221  17869000
2009-06-05   3651   4417500   2195   8214300
2009-06-08   3651         0     NaN       NaN
",1301710.0,,,,,2012-11-27 21:57:44,
12022895,2,12022823,2012-08-18 23:00:00,0,Ok  I can create an empty dict  insert values and create a DataFrame,1583620.0,,,,,2012-08-18 23:00:00,
12022964,2,12022823,2012-08-18 23:12:28,2,"Your idea of creating the dict first is probably the best way:

>>> from pandas import *
>>> DataFrame({c: [1 2] for c in 'sp'})
   p  s
0  1  1
1  2  2


(here using dictionary comprehensions  available in Python 27)  Just for completeness  though  you could -- inefficiently -- use join or concat to get a column-by-column approach to work:

>>> df = DataFrame()
>>> for c in 'sp':
     df = concat([df  DataFrame({c: [1 2]})]  axis=1)
 
>>> print df
   s  p
0  1  1
1  2  2
>>> 
>>> df = DataFrame()
>>> for c in 'sp':
     df = dfjoin(DataFrame({c: [1 2]})  how='outer')
 
>>> print df
   s  p
0  1  1
1  2  2


[You can see the difference in column order]  But your idea of building the dict and then constructing the DataFrame from the constructed dict is a much better approach",487339.0,,,,,2012-08-18 23:12:28,
12228008,2,12227483,2012-09-01 13:05:59,2,"The RandomForestClassifier is copying the dataset several times in memory  especially when n_jobs is large We are aware of those issues and it's a priority to fix them:

I am currently working on a subclass of the multiprocessingPool class of the standard library that will do no memory copy when numpymemmap instances are passed to the subprocess workers This will make it possible to share the memory of the source dataset + some precomputed datastructures between the workers Once this is fixed I will close this issue on the github tracker
There is an ongoing refactoring that will further decrease the memory usage of RandomForestClassifier by two However the current state of the refactoring is twice as slow as the master  hence further work is still required
However none of those fixes will make it to 012 release that is scheduled for release next week Most probably they will be done for 013 (planned for release in 3 to 4 months) but offcourse will be available in the master branch a lot sooner",163740.0,,,,,2012-09-01 13:05:59,6.0
12499884,2,12498809,2012-09-19 17:29:42,0,"Installed pandas latest version 

I am able to read now 

>>> import pandas
>>> pandas__version__
'081'
",1649335.0,,,,,2012-09-19 17:29:42,
12741168,2,12741092,2012-10-05 07:01:18,5,"If I understand you right  you're looking for the applymap method

>>> print df
   A  B  C
0 -1  0  0
1 -4  3 -1
2 -1  0  2
3  0  3  2
4  1 -1  0
>>> print dfapplymap(lambda x: x>1)
       A      B      C
0  False  False  False
1  False   True  False
2  False  False   True
3  False   True   True
4  False  False  False
",1427416.0,,,,,2012-10-05 07:01:18,1.0
12959490,2,12957593,2012-10-18 16:37:35,3,"It looks like what you may be looking for is a hierarchical indexed dataframe [link]

Would something like this work?

#build a sample dataframe
a=['P_1']*9
b=[106 106 106 135 135 135 17 17 17]
c = nprandomrandint(1 100 9)
df = pandasDataFrame(data=zip(a b c)  columns=['sample' 'detector' 'cq'])

#add a repetition number column
df['rep_num']=[1 2 3]*( len(df)/3 )

#Convert to a multi-indexed DF
df_multi = dfset_index(['sample' 'detector' 'rep_num'])

#--------------Resulting Dataframe---------------------

                             cq
sample detector rep_num    
P_1    106      1        97
                2        83
                3        81
       135      1        46
                2        92
                3        89
       17       1        58
                2        26
                3        75
",1159497.0,,,,,2012-10-18 16:37:35,1.0
13062410,2,13061478,2012-10-25 05:54:10,2,"In [13]: df
Out[13]:
  ORG1  ORG2
0    A  ESBL
1    B     P
2    C     Q
3    D     R
4    E  ESBL

In [14]: cond = dfORG2 == 'ESBL'

In [15]: dfORG1[cond] = dfORG2[cond]

In [16]: df
Out[16]:
   ORG1  ORG2
0  ESBL  ESBL
1     B     P
2     C     Q
3     D     R
4  ESBL  ESBL
",1548051.0,,,,,2012-10-25 05:54:10,1.0
13401482,2,13400938,2012-11-15 16:06:02,3,"you can group by multiple columns:


  dfgroupby(['col1'  'col2'])apply(lambda x: x['col1'] == x['col2']  axis=1)


you can also use a mask:


  df[dfcol1==dfcol2]
",239007.0,,239007.0,,2012-11-15 18:56:30,2012-11-15 18:56:30,3.0
13433631,2,13400938,2012-11-17 18:45:48,2,"Note  if you goal is to do group computations  you can do

dfgroupby(dfcol1 == dfcol2)apply(f)


and the result will be keyed by True/False ",776560.0,,,,,2012-11-17 18:45:48,
13636218,2,13630269,2012-11-29 22:50:07,0,"I don't know of any straightforward way to cast a pandas dataframe into a gviz data table

Here's how I do it:-

from gviz_data_table import Table   # using https://bitbucketorg/charlie_x/gviz-data-table though you should probably use the official google one
import numpy as np
import pandas as pd

df = pdDataFrame({
    ""time"" : [1 2 3 4 5] 
    ""temp"" : nprandomrand(5)
})

table = Table()
tableadd_column('temp'  float  'Temp')
tableadd_column('time'  float  'Time')

for row in dfiterrows():
    tableappend(row[1]tolist())

tablerows

[OrderedDict([('temp'  <gviz_data_tablecellCell object at 0x104a64aa0>)  ('time'  <gviz_data_tablecellCell object at 0x104a64a50>)]) 
 OrderedDict([('temp'  <gviz_data_tablecellCell object at 0x104a64af0>)  ('time'  <gviz_data_tablecellCell object at 0x104a64b40>)]) 
 OrderedDict([('temp'  <gviz_data_tablecellCell object at 0x104a64b90>)  ('time'  <gviz_data_tablecellCell object at 0x104a64be0>)]) 
 OrderedDict([('temp'  <gviz_data_tablecellCell object at 0x104a64c30>)  ('time'  <gviz_data_tablecellCell object at 0x104a64c80>)]) 
 OrderedDict([('temp'  <gviz_data_tablecellCell object at 0x104a64cd0>)  ('time'  <gviz_data_tablecellCell object at 0x104a64d20>)])]
",482506.0,,,,,2012-11-29 22:50:07,
14360369,2,14360261,2013-01-16 14:12:59,2,"If your period is a fraction of a minute  you can try something like this:

index = pddate_range(start='1952'  periods=10**6  freq='s')
big = pdSeries(npones(len(index))*97  index)
small = pdSeries(npones(len(index))*2  index)

alternating = big[bigindexsecond % 10 >= 5]combine_first(small)


alternating looks then exactly as you asked and is calculated within 150ms",449449.0,,,,,2013-01-16 14:12:59,6.0
14554094,2,14552482,2013-01-28 00:59:39,2,"These lines seem weird to me:

if groupindex[groupTYPE=='start_codon']:
    start_exon = groupindex[groupTYPE=='exon'][0]


The first  I'm guessing  is simply trying to check to see whether the group has a start codon marker  But that doesn't make sense for two reasons  

(1) If there's only one start_codon entry and it's the first  then the condition is actually false!

In [8]: groupTYPE == 'start_codon'
Out[8]: 
0     True
1    False
2    False
3    False
4    False
5    False
6    False
7    False
Name: TYPE

In [9]: groupindex[groupTYPE == 'start_codon']
Out[9]: Int64Index([0]  dtype=int64)

In [10]: bool(groupindex[groupTYPE == 'start_codon'])
Out[10]: False


Maybe you want any(groupTYPE == 'start_codon')  or (groupTYPE == 'start_codon')any() or sum(groupTYPE == 'start_codon') == 1 or something?  But that can't be right either  because

(2) Your code only works if start_exon is set  If it isn't  then it'll either give a NameError or fall back on whatever value it happened to be last time  and you've got no guarantee that's going to be in a sensible order

If I simply use start_exon = groupindex[groupTYPE=='exon'][0] by itself  then I get

In [28]: for c in corrected:
   :     print c
   :     
       GENE_ID         TYPE   POS1   POS2 STRAND  POSA  POSB
8   PITG_00003  start_codon  38775  38777      +     1     3
9   PITG_00003   stop_codon  39069  39071      +   295   297
10  PITG_00003         exon  38775  39071      +     1   297
11  PITG_00003          CDS  38775  39068      +     1   294
      GENE_ID         TYPE   POS1   POS2 STRAND  POSA  POSB
0  PITG_00002  start_codon  10520  10522      -     1    -1
1  PITG_00002   stop_codon  10097  10099      -  -422  -424
2  PITG_00002         exon  10474  10522      -     1   -47
3  PITG_00002          CDS  10474  10522      -     1   -47
4  PITG_00002         exon  10171  10433      -   -88  -350
5  PITG_00002          CDS  10171  10433      -   -88  -350
6  PITG_00002         exon  10097  10114      -  -407  -424
7  PITG_00002          CDS  10100  10114      -  -407  -421


I have no idea if those values are meaningful  but it doesn't seem to be skipping anything",487339.0,,,,,2013-01-28 00:59:39,3.0
9966022,2,9962822,2012-04-01 16:21:23,4,"I would suggest:

months = cdiffDATEmap(lambda x: xmonth)
pivot_table(cdiff  values='COUNT'  rows=[months  'LOCATION'] 
            aggfunc=npsum)


To get a month name  pass a different function or use the built-in calendarmonth_name To get the data in the format you want  you should call reset_index on the result  or you could also do:

cdiffgroupby([months  'LOCATION']  as_index=False)sum()",776560.0,,,,,2012-04-01 16:21:23,1.0
10738385,2,10723847,2012-05-24 13:23:19,0,"As I wrote in my comments above  the best answer is:

newdf = df1dropna()[['S'  'JEXP']]


that dropan's from a slice of the original df  keeping just the series of interest 
Karmel has suggested:

newdf = df1dropna(subset=['S'  'JEXP'])


which also works and dropan's based on the subset list  however  keeping all others series - it duplicates your dataset",1289107.0,,,,,2012-05-24 13:23:19,
11401158,2,11400181,2012-07-09 18:46:31,0,"How are you calling DataFramepivot and what datatype is your dates column?

Suppose I have a DataFrame that's similar to yours  the dates columns contains datetime objects:

In [52]: df
Out[52]: 
       data                dates loc
0  0870900  2000-01-01 00:00:00   A
1  0344999  2000-01-02 00:00:00   A
2  0001729  2000-01-03 00:00:00   A
3  1565684  2000-01-01 00:00:00   B
4 -0851542  2000-01-02 00:00:00   B


In [53]: dfpivot('dates'  'loc'  'data')
Out[53]: 
loc                A         B
dates                         
2000-01-01  0870900  1565684
2000-01-02  0344999 -0851542
2000-01-03  0001729       NaN
",1306530.0,,,,,2012-07-09 18:46:31,2.0
11415288,2,11400181,2012-07-10 14:15:59,1,"If you have multiple data columns  calling pivot without the values columns should give you a pivoted frame with a MultiIndex as the columns:

In [3]: df
Out[3]: 
  columns     data1     data2 index
0       a -0602398 -0982524     x
1       a  0880927  0818551     y
2       b -0238849  0766986     z
3       b -1304346  0955031     x
4       c -0094820  0746046     y
5       c -0835785  1123243     z

In [4]: dfpivot('index'  'columns')
Out[4]: 
            data1                         data2                    
columns         a         b         c         a         b         c
index                                                              
x       -0602398 -1304346       NaN -0982524  0955031       NaN
y        0880927       NaN -0094820  0818551       NaN  0746046
z             NaN -0238849 -0835785       NaN  0766986  1123243
",1306530.0,,,,,2012-07-10 14:15:59,1.0
11418688,2,11400181,2012-07-10 17:28:46,0,"Just answered my own question I was using an old Sybase module to import data and I think it used an old DateTimeType object from mxDatetime In that module  a datetime of Jan 01 2011 would not necessarily equal another datetime of Jan 01 2011 (eg each datetime was unique) Hence the dataframe pivot treated each column value as unique in the index

Thanks for the help",1170240.0,,,,,2012-07-10 17:28:46,
11858532,2,11858472,2012-08-08 06:03:51,4,"df['bar'] = dfbarmap(str) + "" is "" + dffoo",1427416.0,,,,,2012-08-08 06:03:51,1.0
11874590,2,11858472,2012-08-08 23:15:47,2,"The problem in your code is that you want to apply the operation on every row The way you've written it though takes the whole 'bar' and 'foo' columns  converts them to strings and gives you back one big string You can write it like:

dfapply(lambda x:'%s is %s' % (x['bar'] x['foo']) axis=1)


It's longer than the other answer but is more flexible",243238.0,,,,,2012-08-08 23:15:47,
12083600,2,12082568,2012-08-23 01:34:04,0,These are specified in the matplotlib documentation The whiskers are some multiple (15 by default) of the interquartile range,1306530.0,,,,,2012-08-23 01:34:04,1.0
12292141,2,12290844,2012-09-06 02:23:44,0,"You can use multidimensional slicing conveniently:

import numpy as np

# just creating a random 2d array
a = (nprandomrandom((10  5)) * 100)astype(int)
print a
print

# select by the values of the 3rd column  selecting out more than 50
b = a[a[:  2] > 50]

# showing the rows for which the 3rd column value is > 50
print b


Another example  closer to what you are asking in the comment (?):

import numpy as np

# just creating a random 2d array
a = nprandomrandom((10000  5)) * 100
print a
print

# select by the values of the 3rd column  selecting out more than 50
b = a[a[:  2] > 500]
b = b[b[:  2] <= 502]

# showing the rows for which the 3rd column value is > 50
print b


This selects out rows for which the 3rd column values are (50  502]",515392.0,,515392.0,,2012-09-06 08:00:27,2012-09-06 08:00:27,2.0
12293372,2,12290844,2012-09-06 05:17:26,1,"You can use pandas for that task and more specifically the groupby method of DataFrame Here's some example code:

import numpy as np
import pandas as pd

# generate a random 20x5 DataFrame
x=nprandomrandint(0 10 100)
xshape=(20 5)
df=pdDataFrame(x)

# group by the values in the 1st column
g=dfgroupby(0)

# make a dict with the numbers from the 1st column as keys and
# the slice of the DataFrame corresponding to each number as
# values of the dict
d={k:v for (k v) in g}


Some sample output:

In [74]: d[3]
Out[74]: 
    0  1  2  3  4
2   3  2  5  4  3
5   3  9  4  3  2
12  3  3  9  6  2
16  3  2  1  6  5
17  3  5  3  1  8
",243238.0,,,,,2012-09-06 05:17:26,
12491287,2,12490657,2012-09-19 08:49:25,3,"You can use figtight_layout()

fig  ax = subplots(1 1 1)
axplot(nprandomrandn(5))
axset_xticklabels(['this is a very long label'  'b'  'c'  'd'  'e']  rotation=90)
figtight_layout()
figsavefig('testpdf')
",1548051.0,,,,,2012-09-19 08:49:25,1.0
12576728,2,12576313,2012-09-25 05:35:09,0,"This will at least clean it up for additional processing

import csv
reader = csvReader(open(<csv_file_name>)
data = []
keys = readernext()
for row in reader():
    r = dict(zip(keys row))
    if not r['measurements'] or not r['Error']:
        continue
    for key in ['SampleID'  'OtherInfo'  'Notes']:
        if not r[key]:
            index = -1
            while True:
                if data[index][key]:
                    r[key] = data[index][key]
                    break
                index -= 1
    dataappend(r)
",918447.0,,,,,2012-09-25 05:35:09,1.0
12577614,2,12576313,2012-09-25 06:52:35,2,"Looks like your file has fixed width columns  for which read_fwf() can be used

In [145]: data = """"""\
SampleID    OtherInfo    Measurements    Error    Notes                   
sample1     stuff                                 more stuff              
                         36              6
                         26              7
                         37              8
sample2     newstuff                              lots of stuff           
                         25              6
                         27              7
""""""

In [146]: df = pandasread_fwf(StringIO(data)  widths=[12  13  14  9  15])


Ok  now we have the data  just a little bit of extra work and you have a frame on which you can use set_index() to create a MultiLevel index

In [147]: df[['Measurements'  'Error']] = df[['Measurements'  'Error']]shift(-1)

In [148]: df[['SampleID'  'OtherInfo'  'Notes']] = df[['SampleID'  'OtherInfo'  'Notes']]fillna()

In [150]: df = dfdropna()

In [151]: df
Out[151]:
  SampleID OtherInfo  Measurements  Error          Notes
0  sample1     stuff            36      6     more stuff
1  sample1     stuff            26      7     more stuff
2  sample1     stuff            37      8     more stuff
4  sample2  newstuff            25      6  lots of stuff
5  sample2  newstuff            27      7  lots of stuff
",1548051.0,,,,,2012-09-25 06:52:35,1.0
12930541,2,12924264,2012-10-17 08:53:09,0,"import tables as pt
import pandas as pd
import numpy as np

# the content is junk but we don't care
grades = npempty((10 2)  dtype=(('name'  'S20')  ('grade'  'u2')))

# write to a PyTables table
handle = ptopenFile('/tmp/test_pandash5'  'w')
handlecreateTable('/'  'grades'  grades)
print handlerootgrades[:]dtype # it is a structured array

# load back as a DataFrame and check types
df = pdDataFramefrom_records(handlerootgrades[:])
dfdtypes


Beware that your u2 (unsigned 2-byte integer) will end as an i8 (integer 8 byte)  and the strings will be objects  because Pandas does not yet support the full range of dtypes that are available for Numpy arrays",54567.0,,,,,2012-10-17 08:53:09,7.0
13455190,2,13454909,2012-11-19 13:50:47,0,"The problem is that you don't have any columns of length 6 (the longest is 5)  I don't think there is a keyword in read_csv to overcome this

One solution is to be more explicit:

In [1]: df = pdread_csv('lin-nandat'  names=list('abcde')  index_col=0  skiprows=1)

In [2]: df['f'] = npnan

In [3]: df
Out[3]: 
        b    c     d    e   f
a                            
150  48  NaN   63  NaN NaN
160  52  65   72  NaN NaN
170  55  66   83  57 NaN
180  61  67   97  62 NaN
190  71  68  111  67 NaN
200  NaN  68  125  73 NaN
208  NaN  NaN   NaN  78 NaN
210  NaN  72   NaN  NaN NaN
220  NaN  80   NaN  NaN NaN
230  NaN  87   NaN  NaN NaN
240  NaN  92   82  NaN NaN
",1240268.0,,,,,2012-11-19 13:50:47,
13798506,2,13794560,2012-12-10 09:52:31,1,"The bdate_range function was introduced in pandas version 080 So this ought to work fine if you upgrade to pandas >= 080 (and I would recommend using the latest stable release)

Note: The pandas website allows you to search the docs by version number (select your versions's docs on the right-hand side of the main page) In version 073 there are no search results for bdate_range

For the latest features and bug-fixes  keep your favourite Data Analysis library up-to-date!",1240268.0,,,,,2012-12-10 09:52:31,0.0
13929347,2,13927267,2012-12-18 08:53:51,2,"How about:

def group_func(group):
    return (group / groupshift(1))

partition_functionsgroupby(level='atomic_number')apply(group_func)dropna()


Which results in:

                            values
atomic_number ion_number          
14            1           0519563
              2           0171204
              3           1991396
26            1           1121464
              2           0421345
              3           0233061
",1755432.0,,,,,2012-12-18 08:53:51,1.0
14431417,2,14429793,2013-01-21 01:40:20,2,"Ok  I think this does what you want:

Make a dictionary of your regional weights:

In [1]: weights = {'east':1 'west':2 'south':3}


The following function maps values from a Series to the value found in the weights dictionary  x is the row value of region and w is the region series after it has been mapped to the weights dict

In [2]: def f(x):
   :     w = xmap(weights)
   :     return w / wsum()astype(float)


Here  we groupby ['item' 'price'] and apply the function above  The output is a series of relative weights for the unique combinations of item and price

In [3]: dfgroupby(['item' 'price'])regionapply(f)
Out[3]:
0    0333333
1    0666667
2    1000000
3    1000000
4    0333333
5    0666667
6    1000000
7    1000000


Finally  you can multiply dfquantity by the above series to calculate your weight-adjusted quantities

In [4]: df['wt_quant'] = dfgroupby(['item' 'price'])regionapply(f) * dfquantity

In [5]: df
Out[5]:
    item  price  quantity region  wt_quant
0    one     50         3   east  1000000
1    one     50         3   west  2000000
2    two     12         4  south  4000000
3  three     35         5   west  5000000
4    two     10        12   east  4000000
5    two     10        14   west  9333333
6    one     12         3   east  3000000
7  three     12         8   west  8000000
",919872.0,,,,,2013-01-21 01:40:20,
14605286,2,14593304,2013-01-30 13:31:49,1,"You can get a list of colors from any colormap defined in Matplotlib  and even custom colormaps  by:

>>> import matplotlibpyplot as plt
>>> colors = pltcmPaired(nplinspace(0 1 60))


Plotting an example with these colors:

>>> pltscatter( range(60)  [0]*60  color=colors )
<matplotlibcollectionsPathCollection object at 0x04ED2830>
>>> pltaxis(""off"")
(-100  700  -00015  00015)
>>> pltshow()




I found the ""Paired"" colormap to be especially useful for this kind of things  but you can use any other available or custom colormap",1156006.0,,,,,2013-01-30 13:31:49,
5977496,2,5955695,2011-05-12 11:49:35,1,"So I have not found a tutorial but in the io module (pandasio) there is a pytablespy file (https://githubcom/wesm/pandas/blob/master/pandas/io/pytablespy) which contains a HDFStore method The comments in the code layout a simple (four line) example:

>>> store = HDFStore('testh5')
>>> store['foo'] = bar   # write to HDF5
>>> bar = store['foo']   # retrieve
>>> storeclose()


The 'bar' variable is a pandas DataMatrix object Basically  create a file 'testh5' through pytables and then create an empty group  'foo' When you set 'bar' to 'foo'  the root node of testh5 has a group 'foo' populated with three tables One with the column names  one with the indices  and one with the values

I am continuing to experiment and will post more as I get it",687739.0,,,,,2011-05-12 11:49:35,
6178993,2,5955695,2011-05-30 17:07:06,16,I'm the primary pandas developer (and not a frequent StackOverflow user  hence why I missed this for a while) There are unfortunately few docs about the HDFStore object and I'd love to dedicate some more time to building it out (or to get some help) The source code is the best source of information-- there is not much magic to using PyTables Some people have submitted patches to me for HDFStore class that I have not got around to applying them yet,776560.0,,,,,2011-05-30 17:07:06,3.0
11597273,2,5955695,2012-07-22 02:04:09,3,I found Wes tutorial at this years pycon invaluable http://wwwyoutubecom/watch?v=w26x-z-BdWQ,630752.0,,,,,2012-07-22 02:04:09,
10201415,2,10198295,2012-04-18 01:07:15,0,"I added a github issue for this recently: https://githubcom/pydata/pandas/issues/1043

In the meantime  you could do:

def get_last_valid(series):
    return seriesdropna()iget(-1)

dfgroupby('b')['a']apply(get_last_valid)
",776560.0,,,,,2012-04-18 01:07:15,1.0
10730159,2,10728757,2012-05-24 01:55:06,2,"I think this is pretty close to what you need I'm not sure I interpreted interval and period correctly  but I think I got it write within some constant factor 

import numpy as np

def aggregate(signal  time  period  interval):
    assert (period % interval) == 0
    ipp = period / interval

    midpoint = npr_[time[0]  (time[1:] + time[:-1])/2  time[-1]]
    cumsig = npr_[0  (npdiff(midpoint) * signal)cumsum()]
    grid = nplinspace(0  time[-1]  npfloor(time[-1]/period)*ipp + 1)
    cumsig = npinterp(grid  midpoint  cumsig)
    return npdiff(cumsig)reshape(-1  ipp)sum(0) / period
",1004096.0,,1004096.0,,2012-05-24 02:43:50,2012-05-24 02:43:50,3.0
10730365,2,10728757,2012-05-24 02:30:59,3,"I would strongly recommend using Pandas  Here I'm using version 08 (soon to be released) I think this is close to what you want

import pandas as p
import numpy as np
import matplotlib as plt

# Make up some data:
time = pdate_range(start='2011-05-23'  end='2012-05-23'  freq='min')
watts = nplinspace(0  314 * 365  timesize)
watts = 38 * (15 + npsin(watts)) + 8 * npsin(5 * watts)

# Create a time series
ts = pSeries(watts  index=time  name='watts')

# Resample down to 15 minute pieces  using mean values
ts15 = tsresample('15min'  how='mean')
ts15plot()


Pandas can easily do many other things with your data (like determine your average weekly energy profile) Check out pread_csv() for reading in your data",792849.0,,1301710.0,,2012-05-29 12:34:03,2012-05-29 12:34:03,5.0
11363382,2,10728757,2012-07-06 13:42:29,1,"I worked out a function that does exactly what I wanted based on the previous answers and on pandas  

def aggregate_by_time(signal  time  period=86400  interval=900  label='left'):
""""""
Function to calculate the aggregated average of a timeseries by 
period (typical a day) in bins of interval seconds (default = 900s)

label = 'left' or 'right'  'Left' means that the label i contains data from 
i till i+1  'right' means that label i contains data from i-1 till i    

Returns an array with period/interval values  one for each interval
of the period 

Note: the period has to be a multiple of the interval

""""""

def make_datetimeindex(array_in_seconds  year):
    """"""
    Create a pandas DateIndex from a time vector in seconds and the year
    """"""

    start = pandasdatetime(year  1  1)
    datetimes = [start + pandasdatetoolstimedelta(t/86400) for t in array_in_seconds]

    return pandasDatetimeIndex(datetimes)

interval_string = str(interval) + 'S'    
dr = make_datetimeindex(time  2012)
df = pandasDataFrame(data=signal  index=dr  columns=['signal'])
df15min = dfresample(interval_string  closed=label  label=label)

# now create bins for the groupby() method
time_s = df15minindexasi8/1e9
time_s -= time_s[0]
df15min['bins'] = npmod(time_s  period)

df_aggr = df15mingroupby(['bins'])mean()

# if you only need the numpy array: take df_aggrvalues
return df_aggr 
",566942.0,,,,,2012-07-06 13:42:29,
11246087,2,11232275,2012-06-28 13:46:31,2,"Works fine for me? Can you post the exact pivot method call you're using?

In [4]: dfpivot('name'  'id'  'x')
Out[4]: 
id    1  2
name      
john  0  0
mike  1  0
",776560.0,,,,,2012-06-28 13:46:31,3.0
13833239,2,11232275,2012-12-12 05:14:29,3,"Try this 

dfdrop_duplicates(['foo' 'bar'])
dfpivot('foo' 'bar' 'baz')
",1896737.0,,603744.0,,2012-12-12 05:32:09,2012-12-12 05:32:09,
11621042,2,11620721,2012-07-23 21:46:55,2,"something like:

import pandas as pd
pdisnull(frame)any()


Is probably what you're looking for to look for missing data

fillna currently does not take lambda functions though that's in the works as an open issue on github

You can use DataFrameapply to do custom filling for now Though can you be a little more specific on what you need to do to fill the data? Just curious what the use case is",1306530.0,,1306530.0,,2012-07-23 21:51:56,2012-07-23 21:51:56,3.0
11994944,2,11991627,2012-08-16 20:26:37,1,"dfix[my_list_of_dates] should work just fine

In [193]: df
Out[193]:
            A  B  C  D
2012-08-16  2  1  1  7
2012-08-17  6  4  8  6
2012-08-18  8  3  1  1
2012-08-19  7  2  8  9
2012-08-20  6  7  5  8
2012-08-21  1  3  3  3
2012-08-22  8  2  3  8
2012-08-23  7  1  7  4
2012-08-24  2  6  0  6
2012-08-25  4  6  8  1

In [194]: row_pos = [2  6  9]

In [195]: dfix[row_pos]
Out[195]:
            A  B  C  D
2012-08-18  8  3  1  1
2012-08-22  8  2  3  8
2012-08-25  4  6  8  1

In [196]: dates = [dfindex[i] for i in row_pos]

In [197]: dfix[dates]
Out[197]:
            A  B  C  D
2012-08-18  8  3  1  1
2012-08-22  8  2  3  8
2012-08-25  4  6  8  1
",1548051.0,,,,,2012-08-16 20:26:37,6.0
12255235,2,12255179,2012-09-03 23:59:57,2,"How about something like this:

Make the dataframe:

In [82]: v = [
   :     (1  ""000010101001010101011101010101110101""  ""aaa"") 
   :     (0  ""111101010100101010101110101010111010""  ""bb"") 
   :     (0  ""100010110100010101001010101011101010""  ""ccc"") 
   :     (1  ""000010101001010101011101010101110101""  ""ddd"") 
   :     (1  ""110100010101001010101011101010111101""  ""eeee"") 
   :     ]

In [83]: 

In [83]: df = pandasDataFrame(v)


We can use fromiter or array to get an ndarray:

In [84]: d =""000010101001010101011101010101110101""

In [85]: npfromiter(d  int) # better: npfromiter(d  int  count=len(d))
Out[85]: 
array([0  0  0  0  1  0  1  0  1  0  0  1  0  1  0  1  0  1  0  1  1  1  0 
       1  0  1  0  1  0  1  1  1  0  1  0  1])

In [86]: nparray(list(d)  int)
Out[86]: 
array([0  0  0  0  1  0  1  0  1  0  0  1  0  1  0  1  0  1  0  1  1  1  0 
       1  0  1  0  1  0  1  1  1  0  1  0  1])


There might be a slick vectorized way to do this  but I'd just apply the obvious per-entry function to the values and get on with my day:

In [87]: df[1]
Out[87]: 
0    000010101001010101011101010101110101
1    111101010100101010101110101010111010
2    100010110100010101001010101011101010
3    000010101001010101011101010101110101
4    110100010101001010101011101010111101
Name: 1

In [88]: df[1] = df[1]apply(lambda x: npfromiter(x  int)) # better with count=len(x)

In [89]: df
Out[89]: 
   0                                                  1     2
0  1  [0 0 0 0 1 0 1 0 1 0 0 1 0 1 0 1 0 1 0 1 1 1 0 1    aaa
1  0  [1 1 1 1 0 1 0 1 0 1 0 0 1 0 1 0 1 0 1 0 1 1 1 0     bb
2  0  [1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 0 0 1 0 1 0    ccc
3  1  [0 0 0 0 1 0 1 0 1 0 0 1 0 1 0 1 0 1 0 1 1 1 0 1    ddd
4  1  [1 1 0 1 0 0 0 1 0 1 0 1 0 0 1 0 1 0 1 0 1 0 1 1   eeee

In [90]: df[1][0]
Out[90]: 
array([0  0  0  0  1  0  1  0  1  0  0  1  0  1  0  1  0  1  0  1  1  1  0 
       1  0  1  0  1  0  1  1  1  0  1  0  1])
",487339.0,,487339.0,,2012-09-04 00:11:08,2012-09-04 00:11:08,4.0
12262022,2,12255179,2012-09-04 10:55:12,4,"Since you asked primarily for a way to convert a string of ones and zeros into a numpy array  I'll offer my solution as follows:

d = '0101010000' * 2000 # create a 20 000 long string of 1s and 0s
d_array = npfromstring(d  'int8') - 48 # 48 is ascii 0 ascii 1 is 49


This compares favourable to @DSM's solution in terms of speed:

In [21]: timeit numpyfromstring(d  dtype='int8') - 48
10000 loops  best of 3: 358 us per loop

In [22]: timeit numpyfromiter(d  dtype='int'  count=20000)
100 loops  best of 3: 857 ms per loop
",709852.0,,,,,2012-09-04 10:55:12,1.0
12483361,2,12483121,2012-09-18 19:05:32,0,"The problem is that the header argument is not a True/False type argument  Rather  it specifies the row number for the header  Since it is specified as 1 it is using the 2nd row for the header and considering that this row contains actual data  the values are not necissarily unique

Changing the command to

read_csv(""filecsv"")


or 

read_csv(""filecsv""  header=0)


fixes the problem  It is such a ""duh"" moment but being used to R I mistakenly thought header=1 was specifying to read a header  Ugh

For future reference  the Exception 

Reindexing only valid with uniquely valued Index objects


relates to the header values not being unique",154508.0,,,,,2012-09-18 19:05:32,2.0
12727919,2,12726432,2012-10-04 13:04:01,9,"Pandas has exponentially weighted moving moment functions

http://pandaspydataorg/pandas-docs/dev/computationhtml?highlight=exponential#exponentially-weighted-moment-functions

By the way  there shouldn't be any functionality leftover in the scikitstimeseries package that is not also in pandas",535665.0,,,,,2012-10-04 13:04:01,0.0
12745537,2,12726432,2012-10-05 11:41:39,4,For triple I found on web this http://adorio-researchorg/wordpress/?p=1230,1719510.0,,1719510.0,,2012-10-30 06:57:05,2012-10-30 06:57:05,
13070405,2,12726432,2012-10-25 14:05:14,1,"Somehow some questions got merged or deleted  so I'll post my answer here

Exp smoothing in Python natively

'''
simple exponential smoothing
go back to last N values
y_t = a * y_t + a * (1-a)^1 * y_t-1 + a * (1-a)^2 * y_t-2 +  + a*(1-a)^n * y_t-n
'''
from random import random randint

def gen_weights(a N):
    ws = list()
    for i in range(N):
        w = a * ((1-a)**i)
        wsappend(w)
    return ws

def weighted(data ws):
    wt = list()
    for i x in enumerate(data):
        wtappend(x*ws[i])
    return wt

N = 10
a = 05
ws = gen_weights(a N)
data = [randint(0 100) for r in xrange(N)]
weighted_data = weighted(data ws)
print 'data: ' data
print 'weights: ' ws
print 'weighted data: ' weighted_data
print 'weighted avg: ' sum(weighted_data)
",1460614.0,,,,,2012-10-25 14:05:14,
13073655,2,13073045,2012-10-25 17:04:23,0,"The command plfigure() makes a new matplotlib figure The figure size is set at instantiation You do want to set the figure size  but you already have a figure So you were on the right track  but try this instead:

def drawHeatMap(df  city  province  collector  classtype  color  titleposy):
    try:
        fig = plfigure(fig_size=())
        ax = figadd_subplot(111)
        axmatshow(dfvalues  cmap='PuBuGn')
        plcolorbar()
        aTitle = classtype + ' Composition Changes Over Time in ' + city + '  ' + province + '\n' + collector + ' collector ' + 'rs100'
        axset_title(aTitle  x=05  y=titleposy  style='oblique'  weight='bold')
        axset_xlabel('Collection Time')
        axset_xticks(range(len(dfcolumns))  dfcolumns  rotation=90)
        axset_yticks(range(len(dfindex))  dfindex)
        fileName = classtype + '-' + city + '-' + province + '-' + collector + 'png'
        figsavefig(fileName)
    except ZeroDivisionError:
        errorMessage = 'No Data Avaiable for ' + city + '  ' + province + ' with ' + collector + ' collector'
        print errorMessage
",1732770.0,,,,,2012-10-25 17:04:23,3.0
13073666,2,13073045,2012-10-25 17:05:20,0,"Short: You just need to call plfigure(figsize=) before you call the plcolorbar (and all the other stuff)

Explanation:
plfigure creates a new figure (with given size)  on which all pl* methods will act in the following
So plsavefig just saves the last created figure  which is empty if you created a new one in the preceeding line",1768121.0,,,,,2012-10-25 17:05:20,7.0
13354845,2,13354725,2012-11-13 02:37:29,1,"Hopefully this helps you get started?

import sys  os

def regex_match(line) :
  return 'LOOPS' in line

my_hash = {}

for fd in oslistdir(sysargv[1]) :           # for each file in this directory 
  for line in open(sysargv[1] + '/' + fd) :  # get each line of the file
    if regex_match(line) :                    # if its a line I want
      linerstrip('\n')split('\t')           # get the data I want
      my_hash[line[1]] = line[2]              # store the data

for key in my_hash : # data science can go here?
  do_something(key  my_hash[key] * 12)

# plots


ps make the first line 

#!/usr/bin/python


(or whatever which python returns ) to run as an executable",1214880.0,,,,,2012-11-13 02:37:29,9.0
13359605,2,13354725,2012-11-13 11:02:25,0,"To glob your files  use the built-in glob module in Python

To read your csv files after globbing them  the read_csv function that you can import using from pandasioparsers import read_csv will help you do that

As for MultiIndex feature in the pandas dataframe that you instantiate after using read_csv  you can then use them to organize your data and slice them anyway you want

3 pertinent links for your reference

Understanding MultiIndex dataframes in pandas - understanding MultiIndex and Benefits of panda's multiindex?
Using glob in a directory to grab and manipulate your files - extract values/renaming filename in python
",482506.0,,,,,2012-11-13 11:02:25,2.0
13391684,2,13354725,2012-11-15 04:45:07,2,"You are getting the following:

NameError: name 'MultiIndex' is not defined


because you are not importing MultiIndex directly when you import Series and DataFrame

You have -

from pandas import Series  DataFrame


You need -

from pandas import Series  DataFrame  MultiIndex


or you can instead refer to MultiIndex using pdMultiIndex since you are importing pandas as pd",738785.0,,,,,2012-11-15 04:45:07,
13581700,2,13580875,2012-11-27 10:03:44,2,"a = [2  3  4  5  6  7  8  9  10  15  20  25  30  34  40  45  46  50  55]

ans = [(a[0]  a[1]-a[0])]
for i in range(1  len(a)-1):
    if a[i+1] - a[i] - a[i] + a[i-1] is not 0:
        ansappend((a[i]  a[i+1]-a[i]))

print ans


Output: 

[(2  1)  (10  5)  (30  4)  (34  6)  (40  5)  (45  1)  (46  4)  (50  5)]


Is it what you want ? ",1426056.0,,,,,2012-11-27 10:03:44,
13581753,2,13580875,2012-11-27 10:06:26,2,"How about this generator:

L = [2  3  4  5  6  7  8  9  10  15  20  25  30  34  40  45  46  50  55]

def differences_gen(L  differences):
    previous = L[0]
    differences = iter(differences + [None])
    next_diff = next(differences)
    for i  n in enumerate(L[1:]):
        current_diff = n - previous
        while next_diff is not None and current_diff >= next_diff:
            yield (previous  next_diff)
            next_diff = next(differences)
        previous = n

list(differences_gen(L  [1 5]))
# [(2  1)  (10  5)]


There's probably a cleaner way to iterate over partition  but using generators should keep it efficient for longer L and differences",1240268.0,,,,,2012-11-27 10:06:26,1.0
13582115,2,13580875,2012-11-27 10:24:32,2,"In Python:

a = [2  3  4  5  6  7  8  9  10  15  20  25  30  34  40  45  46  50  55]
# zip() creates tuples of two consecutive values 
# (it zips lists of different length by truncating longer list(s))
# then tuples with first value and difference are placed in 'diff' list
diff = [(x  y-x) for x  y in zip(a  a[1:])]
# now pick only elements with changed difference 
result = []
for pair in diff:
    if not len(result) or result[-1][1]!=pair[1]: # -1 to take last element
        resultappend(pair)
",1108941.0,,,,,2012-11-27 10:24:32,1.0
13582975,2,13580875,2012-11-27 11:14:09,2,"I'm a fan of using a window function via islice  it's very useful and I find myself reusing it a lot:

from itertools import islice

def window(seq  n=2):
    """"""
    Returns a sliding window (of width n) over data from the iterable
    s -> (s0 s1 s[n-1])  (s1 s2  sn)                     
    """"""
    it = iter(seq)
    result = tuple(islice(it  n))
    if len(result) == n:
        yield result
    for elem in it:
        result = result[1:] + (elem )
        yield result

# Main code:
last_diff = None
results = []
for v1  v2 in window(a):
    diff = abs(v1 - v2)
    if diff != last_diff:
        resultsappend((v1  diff))
    last_diff = diff


Result:

[(2  1)  (10  5)  (30  4)  (34  6)  (40  5)  (45  1)  (46  4)  (50  5)]
",24718.0,,,,,2012-11-27 11:14:09,1.0
13584007,2,13580875,2012-11-27 12:13:10,2,"You could use numpy or pandas like so (the ""pandas version""):

In [256]: s = pdSeries([2  3  4  5  6  7  8  9  10  15  20  25  30  35 
                             40  45  50  55  65  75  85  86  87  88])

In [257]: df = pdDataFrame({'time': s 
                             'time_diff': sdiff()shift(-1)})set_index('time')

In [258]: df[dftime_diff - dftime_diffshift(1) != 0]dropna()
Out[258]: 
      time_diff
time           
2             1
10            5
55           10
85            1


If you only want to look at the first occurrence of every time step you could also use:

In [259]: dfdrop_duplicates()dropna() # set take_last=True if you want the last
Out[259]: 
      time_diff
time           
2             1
10            5
55           10


However with pandas you would normally use a DatetimeIndex to use the built in time series functionality:

In [44]: a = [2  3  4  5  6  7  8  9  10  15  20  25  30  35 
              40  45  50  55  65  75  85  86  87  88]

In [45]: start_time = datetimedatetimenow()

In [46]: times = [start_time + datetimetimedelta(seconds=int(x)) for x in a]

In [47]: idx = pdDatetimeIndex(times)

In [48]: df = pdDataFrame({'data1': nprandomrand(idxsize)  
                            'data2': nprandomrand(idxsize)} 
                           index=idx)

In [49]: dfresample('5S') # resample to 5 Seconds
Out[49]: 
                        data1     data2
2012-11-28 07:36:35  0417282  0477837
2012-11-28 07:36:40  0536367  0451494
2012-11-28 07:36:45  0902018  0457873
2012-11-28 07:36:50  0452151  0625526
2012-11-28 07:36:55  0816028  0170319
2012-11-28 07:37:00  0169264  0723092
2012-11-28 07:37:05  0809279  0794459
2012-11-28 07:37:10  0652836  0615056
2012-11-28 07:37:15  0508318  0147178
2012-11-28 07:37:20  0261157  0509014
2012-11-28 07:37:25  0609685  0324375
2012-11-28 07:37:30       NaN       NaN
2012-11-28 07:37:35  0736370  0551477
2012-11-28 07:37:40       NaN       NaN
2012-11-28 07:37:45  0839960  0118619
2012-11-28 07:37:50       NaN       NaN
2012-11-28 07:37:55  0697292  0394946
2012-11-28 07:38:00  0351824  0420454


From my point of view  for working with time series Pandas is by far the best library available in the Python ecosystem Not sure what you really want to do  but I would give pandas a try",1301710.0,,1301710.0,,2012-11-29 21:15:34,2012-11-29 21:15:34,6.0
13779725,2,13779308,2012-12-08 17:07:12,1,"I am not quite sure if this is what you are looking for:

s = 'date|o|h|l|c|e|f~07-12-2012 09:15|59340000|59455000|59340000|59386500|1749606|1749606~07-12-2012 09:16|59391000|59418000|59363500|59418000|1064557|2814163'

rows = ssplit ('~')
d = {}
keys = rows [0]split ('|')
for key in keys: d [key] = []
for row in rows [1:]:
    for idx  value in enumerate (rowsplit ('|') ):
        d [keys [idx] ]append (value)

print (d)
",763505.0,,,,,2012-12-08 17:07:12,1.0
13780423,2,13779308,2012-12-08 18:21:22,1,"I'm looking to make this much simpler to do with read_table  ie:

df = read_table(path  sep='|'  lineterminator='~')


Look out for the next pandas release:

http://githubcom/pydata/pandas/issues/2457

EDIT: this is done and works in pandas 010",776560.0,,776560.0,,2012-12-12 15:54:22,2012-12-12 15:54:22,1.0
13941149,2,13940753,2012-12-18 20:41:08,5,"Use the DataFrame div method and pass matchkey for the multi-index you want to broadcast across:

From the documentation for div:

level : int or name
    Broadcast across a level  matching Index values on the
    passed MultiIndex level

In [39]: concentrationdiv(weight  level='Land Use')
Out[39]:
                                    1E          1N           1S           2
Land Use    Parameter
Airfield    BOD5 (mg/l)       0818004    5198238     3668831    2697970
            Ortho P (mg/l)    0003914    0044053     0016234    0005076
            TSS (mg/l)        3469667  505286344    13993506    0464467
            Zn (mg/l)         0001957    0044053     0000804    0002538
Commercial  BOD5 (mg/l)      72000000    0245892          NaN    2957746
            Cu (mg/l)         0087400    0000431          NaN    0003662
            O&G (mg/l)       77000000    0745305          NaN    2469484
Open Space  TSS (mg/l)      463750000  602000000  6045000000  367500000
            Zn (mg/l)        15875000    1380000    66000000   17500000
Parking Lot BOD5 (mg/l)       2800000    0129961    10329365   18654971
            O&G (mg/l)        3090909    0289883     5345238   10994152
Rooftops    BOD5 (mg/l)       1666667   35714286   661176471  103333333
",919872.0,,,,,2012-12-18 20:41:08,1.0
14236239,2,14235984,2013-01-09 13:25:30,3,If you want to have multiple DataFrames on the same sheet how would you combine them? Instead merge  join  or concatenate them into a single DataFrame beforehand as pandas gives you multiple ways of doing so And then do your export,1199589.0,,1199589.0,,2013-01-09 13:31:43,2013-01-09 13:31:43,
14236242,2,14235984,2013-01-09 13:25:37,2,"I think you are going to be better doing a concat of these DataFrames before exporting (via to_excel) That is:

pdconcat(list_dfs)to_excel(xls_path)
",1240268.0,,,,,2013-01-09 13:25:37,4.0
14512542,2,14511752,2013-01-24 23:09:21,2,"Access the subplot in question and change its settings like so

axes = pdscatter_matrix(df  diagonal='kde')
ax = axes[2  2] # your bottom-right subplot
axxaxisset_visible(True)
draw()


You can inspect how the scatter_matrix function goes about labeling at the link below If you find yourself doing this over and over  consider copying the code into file and creating your own custom scatter_matrix function

https://githubcom/pydata/pandas/blob/master/pandas/tools/plottingpy#L160

Edit  in response to a rejected comment:

The obvious extensions of this  doing ax[0  0]xaxisset_visible(True) and so forth  do not work For some reason  scatter_matrix seems to set up ticks and labels for axes[2  2] without making them visible  but it does not set up ticks and labels for the rest If you decide that it is necessary to display ticks and labels on other subplots  you'll have to dig deeper into the code linked above

Specifically  change the conditions on the if statements to:

if i == 0
if i == n-1
if j == 0
if j == n-1


respectively I haven't tested that  but I think it will do the trick",1221924.0,,1221924.0,,2013-01-25 04:54:03,2013-01-25 04:54:03,1.0
9555766,2,9555635,2012-03-04 14:42:12,12,"If you are one Windows  I can advise pythonxy for an easy and painless installation of Python and the core scientific libraries 

It is quite large and contains a lot of packages  which you maybe do not need  but at the installation  you can opt to choose which libraries to install",653364.0,,653364.0,,2012-03-04 14:52:37,2012-03-04 14:52:37,2.0
9557319,2,9555635,2012-03-04 17:47:48,6,"On MacOSX  there is ScipySuperpack

On Linux  there are Linux distributions :) If you want recent builds on Debian and Ubuntu I recommend: http://neurodebiannet/",163740.0,,,,,2012-03-04 17:47:48,
9558852,2,9555635,2012-03-04 21:11:31,4,Sage It doesn't have the GUI tools of Enthought but otherwise contains a full scientific python stack ,238882.0,,238882.0,,2012-03-04 21:19:01,2012-03-04 21:19:01,
9577305,2,9555635,2012-03-06 02:58:07,10,"Have you seen EPD free?

From the enthought website:


  Our new lightweight distribution of scientific Python essentials:
  SciPy  NumPy  IPython  matplotlib  Traits  & Chaco


it might be enough to get you started",1004096.0,,,,,2012-03-06 02:58:07,1.0
9677723,2,9555635,2012-03-13 02:45:55,4,"You might at first exhale ""what is he smoking?"" to my answer  but here it comes as an echo to ogrisel's answer:

The best Python distribution is Debian GNU/Linux -- it comes with multiple versions of Python supported  hundreds (if not thousands) of Python modules and extensions packaged so their installation is guaranteed to be flawless (in 99% of the cases) regardless how complex underlying software/extension is  majority of them are unit-tested against supported versions and 3rd party modules at package build-time guaranteeing lack of head-ache later on

Besides Python itself you can also choose there among a dozen of available Python IDEs (eg spyder  Eric  PIDA  and others)  Python-aware editors (vim  emacs etc)  alternative Python implementations (pypy)  compilers (Cython  nuitka)  etc  Debug build of Python (python-dbg) in tandem with gdb allow you right away debug your extensions while inspecting Python stack etc  And all of those Python-specific tools are available within the same software management framework as the rest of the system which carries thousands of generic and specialized software tools and resources
Depending on your demand you can choose between stable  testing and unstable Debian ""suites""

Now ""how"": virtualization if you cannot or just prefer not to dual-boot  In a matter of minutes you can have a full blown system work on your Windows or OS X box without any major performance hit (unless you need heavy 3D graphics)  and only need sufficient amount of RAM to share with your host OS needs -- you can easily access your host drive space within a virtual machine  see eg http://neurodebiannet/vmhtml  for an easy starting point

Ah right -- pandas  we provide backport builds from NeuroDebian repository  so you could easily use stable Debian and bleeding-edge pandas ",1265472.0,,,,,2012-03-13 02:45:55,
13713475,2,9555635,2012-12-04 22:57:19,1,"Also check out Anaconda by Continuum Analytics It includes numpy  scipy  pandas  and ""all the other goodness"" Available for Linux  Windows  and MacOS Anaconda Community Edition is free and active 

Here's a complete list of packages it includes (as of version 12): 
  http://docscontinuumio/anaconda/12/pkgshtml",484596.0,,,,,2012-12-04 22:57:19,
14546849,2,9555635,2013-01-27 11:25:23,1,"For Windows  there is also WinPython: ""WinPython is a free open-source portable distribution of the Python programming language for Windows XP/7/8  designed for scientists  supporting both 32bit and 64bit versions of Python 2 and Python 3""",918626.0,,,,,2013-01-27 11:25:23,
10592962,2,10575251,2012-05-15 01:09:03,1,Try calling read_fwf() with argument index_col=None to tell it that there are no index columns (which must be unique over all rows) -- it will automatically index rows with an integer starting at zero,243434.0,,,,,2012-05-15 01:09:03,1.0
10964938,2,10951341,2012-06-09 22:07:32,8,"Yes; use the apply() function  which will be called on each sub-DataFrame For example:

grouped = dfgroupby(keys)

def wavg(group):
    d = group['data']
    w = group['weights']
    return (d * w)sum() / wsum()

groupedapply(wavg)
",776560.0,,,,,2012-06-09 22:07:32,
11178233,2,11175213,2012-06-24 14:23:55,4,"it appears there may be a bug lurking here  so I've created an issue here  will take a look soon and let you know:

https://githubcom/pydata/pandas/issues/1518

EDIT: the bug you encountered has been fixed I'm going to fix the pre-1900 display issue right now  too",776560.0,,776560.0,,2012-06-25 22:15:31,2012-06-25 22:15:31,
11686295,2,11686272,2012-07-27 10:47:55,1,"Half a million is not a number you could not manage with a python dictionary

Read data for all sensors from database  fill a dictionary and then build a numpy array  or even better  convert it to pandasDataFrame:

import pandas as pd

inp1 = [(1316275620    1)  (1316275680    2)]
inp2 = [(1316275620   10)  (1316275740   20)]
inp3 = [(1316275680  100)  (1316275740  200)]

inps = [('s1'  inp1)  ('s2'  inp2)  ('s3'  inp3)]

data = {}
for name  inp in inps:
    d = datasetdefault(name  {})
    for timestamp  value in inp:
        d[timestamp] = value
df = pdDataFramefrom_dict(data)


df is now:

            s1  s2   s3
1316275620   1  10  NaN
1316275680   2 NaN  100
1316275740 NaN  20  200
",449449.0,,449449.0,,2012-07-27 12:02:01,2012-07-27 12:02:01,3.0
12393965,2,12393387,2012-09-12 18:09:32,2,"One workaround I can think of would be converting the Python function to numpyufunc with numpyfrompyfunc:

numpyfrompyfunc((lambda x: x[0:2])  1  1)


and use this in apply:

In [50]: dfrm_test
Out[50]:
     A
0  the
1  the
2  the
3  the
4  the
5  the
6  the
7  the
8  the
9  the

In [51]: dfrm_test[""A""]apply(npfrompyfunc((lambda x: x[0:2])  1  1))
Out[51]:
0    th
1    th
2    th
3    th
4    th
5    th
6    th
7    th
8    th
9    th
Name: A

In [52]: pandasversionversion
Out[52]: '073'

In [53]: dfrm_test[""A""]apply(lambda x: x[0:2])
Out[53]:
0    the
1    the
Name: A
",843822.0,,,,,2012-09-12 18:09:32,1.0
12396964,2,12393387,2012-09-12 22:25:19,0,Try dfrm_testAmap(lambda x: x[0:2]),1306530.0,,,,,2012-09-12 22:25:19,7.0
13053339,2,12393387,2012-10-24 16:19:17,0,"This works as of pandas 081:

In [47]: dfrm_testAstr[:2]
Out[47]: 
0    th
1    th
2    th
3    th
4    th
5    th
6    th
7    th
8    th
9    th
Name: A
",776560.0,,,,,2012-10-24 16:19:17,
12811078,2,12810801,2012-10-10 02:21:24,0,"I have been able to accomplish what I initially asked for
It did require a bit of massaging of column names however

Solution (using pseudo code):

Set up dataframes with the relevant data eg

left = (id1 id2 id3 id4 id5 id6 A B C)
right = (id1 id2 id3 id4 id5 id6 D E F)
middle = (id1 id2 id3 id4 id5 id6 G H I)


Note  here  that for me dataset this resulted in my having non-unique indexing keys for each of the ids That is  a key is present for each row in left and right

Rename the column names

col_names = [id1 id2 id3 id4 id5 id6 val1 val2 val3]
leftcolumns = col_names
rightcolumns = col_names
middlecolumns = col_names


Concatenate these

pieces = [left  right  middle]
new_df = concat(pieces)


Now  this will create a new dataframe which contains x unique indexing values and 3x entries This isn't quite ideal but it will do for now  the major shortfall of this is that you cannot uniquely access a single entry row anymore  they will come in triples To access the data you can create a new dataframe based on the unique id values

eg

check_df = new_df[(new_df[id1] == 'id1') & (new_df[id2] == 'id2')  etc])
print check_df

key  id1  id2  id3  id4  id5  id6  A  B  C
key  id1  id2  id3  id4  id5  id6  D  E  F
key  id1  id2  id3  id4  id5  id6  G  H  I


Now  this isn't quite ideal but it's the format I needed for some of my other analysis It may not be applicable for all parties

If anyone has a better solution please do share it  I'm relatively new to using pandas with python",1307298.0,,,,,2012-10-10 02:21:24,1.0
13083900,2,13078751,2012-10-26 09:02:36,4,"I believe this does what you are after:

dfgroupby(lambda x:x  axis=1)sum()


Alternatively  between 3% and 15% faster depending on the length of the df:

dfgroupby(dfcolumns  axis=1)sum()


EDIT: To extend this beyond sums  use agg() (short for aggregate()):

dfgroupby(dfcolumns  axis=1)agg(numpymax)
",54567.0,,54567.0,,2012-10-26 15:50:19,2012-10-26 15:50:19,
13270110,2,13269890,2012-11-07 12:47:36,3,"If you have a key that is repeated for each row  then you can produce a cartesian product using merge (like you would in SQL)

from pandas import DataFrame  merge
df1 = DataFrame({'key':[1 1]  'col1':[1 2] 'col2':[3 4]})
df2 = DataFrame({'key':[1 1]  'col3':[5 6]})

merge(df1  df2 on='key')[['col1'  'col2'  'col3']]


See here for the documentation: http://pandaspydataorg/pandas-docs/stable/merginghtml#brief-primer-on-merge-methods-relational-algebra",1452002.0,,,,,2012-11-07 12:47:36,1.0
