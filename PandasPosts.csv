Id,PostTypeId,AcceptedAnswerId,CreationDate,Score,ViewCount,Body,OwnerUserId,OwnerDisplayName,LastEditorUserId,LastEditDate,LastActivityDate,Title,Tags,AnswerCount,CommentCount,FavoriteCount,ClosedDate
13446791,1,,2012-11-19 02:12:10,1,74,"I have a pandas dataframe with 2 columns (snippet below) I'm trying to use the City column to infer the Borough (you'll notice some Unspecified values that need to be replaced) To do this  I'm trying to show for each city the highest occurring Borough and output to a dictionary where the key would be the city and the value would be the highest occurring borough for that city

City        Borough

Brooklyn    Brooklyn
Astoria     Queens
Astoria     Unspecified
Ridgewood   Unspecified
Ridgewood   Queens


So if Ridgewood is found to be paired with Queens 100 times  Brooklyn 4 times and Manhattan 1 time  the pair would be Ridgewood : Queens

So far I've tried this code:

specified = data[['Borough' 'City']][data['Borough']!= 'Unspecified']
paired = specifiedBoroughgroupby(specifiedCity)max()


At first glance  this seemed like the correct output  but after closer inspection  the output isn't correct at all Any ideas?

EDIT:

Tried the suggestion below:
    paired = specifiedgroupby('City')agg(lambda x: statsmode(x['Borough'])[0])

I noticed some of the Boroughs come out truncated as shown below:

pairedBoroughvalue_counts()

#[Out]# QUEENS           58
#[Out]# MANHATTAN         7
#[Out]# STATEN ISLAND     4
#[Out]# BRONX             4
#[Out]# BROOKLYN          3
#[Out]# MANHATTA          2
#[Out]# STATE             1
#[Out]# QUEEN             1
#[Out]# MANHA             1
#[Out]# BROOK             1


Of course I can just manually replace the truncated words  but I'm curious to know what the cause is?

PS - Here's the output of the DF specified FYI:

specified
#[Out]# 
#[Out]# Int64Index: 719644 entries  1 to 396225
#[Out]# Data columns:
#[Out]# Borough    719644  non-null values
#[Out]# City       651617  non-null values
#[Out]# dtypes: object(2)

specifiedBoroughvalue_counts()
#[Out]# QUEENS           215382
#[Out]# BROOKLYN         208565
#[Out]# MANHATTAN        150016
#[Out]# BRONX             94648
#[Out]# STATEN ISLAND     51033
",1438637,,1438637.0,2012-11-19 21:41:54,2012-11-19 21:41:54,Group Pandas dataframe based on highest occurring values,<python><pandas>,1.0,2.0,,
13457806,1,,2012-11-19 16:15:37,1,31,"I have been using pandas within my scripts for some time now  especially to store large data sets in an easily accessible way I have stumbled upon this problem a couple of days ago and have not been able to solve it so far

The problem is that after I store a huge data frame into an hdf5 file  when I later load it back  it sometimes has one or more columns (only from the object type columns) completely inaccessible and returning the 'NoneType object is not iterable' error

While I use the frame in memory there are no problems  even with moderately larger data sets than the example below It is worth mentioning that the frame contains either multiple datetime columns or multiple VMS timestamps (http://labshoffmanlabscom/node/735)  as well as string and char and integer columns All non-object columns can and do have missing values

At first I thought I was saving 'NA' values in one of the 'object type' columns Then I tried to update to latest pandas version (091) Nothing has worked so far

I have been able to reproduce the error with the following code:

import pandas as pd
import numpy as np
import datetime

# Get VMS timestamps for today
time_now = datetimedatetimetoday()
start_vms = datetimedatetime(1858  11  17)
t_delta = (time_now - start_vms)
vms_time = t_deltatotal_seconds() * 10000000

# Generate Test Frame (dense)
test_records = []
vms_time1 = vms_time
vms_time2 = vms_time
for i in range(2000000):
    vms_time1 += 15 * nprandomrandn()
    vms_time2 += 25 * nprandomrandn()
    vms_time_diff = vms_time2 - vms_time1
    string1 = 'XXXXXXXXXX'
    string2 = 'XXXXXXXXXX'
    string3 = 'XXXXX'
    string4 = 'XXXXX'
    char1 = 'A'
    char2 = 'B'
    char3 = 'C'
    char4 = 'D'
    number1 = nprandomrandint(1 10)
    number2 = nprandomrandint(1 100)
    number3 = nprandomrandint(1 1000)
    test_recordsappend((char1  string1  vms_time1  number1  char2  string2  vms_time2  number2  char3  string3  vms_time_diff  number3  char4  string4))

df = pdDataFrame(test_records  columns = [""column_1""  ""column_2""  ""column_3""  ""column_4""  ""column_5""  ""column_6""  ""column_7""  ""column_8""  ""column_9""  ""column_10""  ""column_11""  ""column_12""  ""column_13""  ""column_14""])

# Generate Test Frame (sparse)
test_records = []
vms_time1 = vms_time
vms_time2 = vms_time
count = 0
for i in range(2000000):
    if (count%23 == 0):
        vms_time1 += 15 * nprandomrandn()
        string1 = 'XXXXXXXXXX'
        string2 = ' '
        string3 = 'XXXXX'
        string4 = 'XXXXX'
        char1 = 'A'
        char2 = 'B'
        char3 = 'C'
        char4 = 'D'
        number1 = None
        number2 = nprandomrandint(1 100)
        number3 = nprandomrandint(1 1000)
        test_recordsappend((char1  string1  vms_time1  number1  char2  None  None  number2  char3  string3  None  number3  None  string4))
    else:
        vms_time1 += 15 * nprandomrandn()
        vms_time2 += 25 * nprandomrandn()
        vms_time_diff = vms_time2 - vms_time1
        string1 = 'XXXXXXXXXX'
        string2 = 'XXXXXXXXXX'
        string3 = 'XXXXX'
        string4 = 'XXXXX'
        char1 = 'A'
        char2 = 'B'
        char3 = 'C'
        char4 = 'D'
        number1 = nprandomrandint(1 10)
        number2 = nprandomrandint(1 100)
        number3 = nprandomrandint(1 1000)
        test_recordsappend((char1  string1  vms_time1  number1  char2  string2  vms_time2  number2  char3  string3  vms_time_diff  number3  char4  string4))
    count += 1

df1 = pdDataFrame(test_records  columns = [""column_1""  ""column_2""  ""column_3""  ""column_4""  ""column_5""  ""column_6""  ""column_7""  ""column_8""  ""column_9""  ""column_10""  ""column_11""  ""column_12""  ""column_13""  ""column_14""])

store_loc = ""Some Location for the file""
h5_store = pdHDFStore(store_loc )
h5_store['df1'] = df
h5_store['df2'] = df1
h5_storeclose()


When I try to load from this store now the 'df1' is behaving normally  but the 'df2' is producing the following error:

TypeError: 'NoneType' object is not iterable
",423825,,,,2012-11-19 16:15:37,Retrieving large frames with sparse data from hdf5 - 'NoneType' object is not iterable error,<pandas>,,1.0,,
13478597,1,13482165.0,2012-11-20 17:22:04,1,107,"(Python 27  Pandas 09)

This seems like a simple thing to do  but I can't figure out how to calculate the difference between two date columns in a dataframe using Pandas This dataframe already has an index  so making either column into a DateTimeIndex is not desirable 

To convert each date column from strings I used:

dataDate_Column = pdto_datetime(dataDate_Column)


From there  to get elapsed time between 2 columns  I do:

dataClosed_Date - dataCreated_Date 


which returns an error:

TypeError: %d format: a number is required  not a numpytimedelta64


Checking dtypes on both columns yields datetime64[ns] and the individual dates in the array are type timestamp

What am I missing?

EDIT:

Here's an example where I can create separate DateTimeIndex objects and accomplish what I want  but when I try to do it in the context of a dataframe  it fails

Created_Date = pdDatetimeIndex(data['Created_Date']  copy=True)
Closed_Date = pdDatetimeIndex(data['Closed_Date']  copy=True)

Closed_Dateday - Created_Dateday
[Out] array([ -3  -16    5      0    0    0])


Now the same but in a dataframe:

dataCreated_Date = pdDatetimeIndex(data['Created_Date']  copy=True)
dataClosed_Date = pdDatetimeIndex(dataClosed_Date  copy=True)

dataCreated_Dateday - dataCreated_Dateday

AttributeError: 'Series' object has no attribute 'day'


Here's some of the data if you want to play around with it:

data['Created Date'][0:10]to_dict()
{0: '1/1/2009 0:00' 
 1: '1/1/2009 0:00' 
 2: '1/1/2009 0:00' 
 3: '1/1/2009 0:00' 
 4: '1/1/2009 0:00' 
 5: '1/1/2009 0:00' 
 6: '1/1/2009 0:00' 
 7: '1/1/2009 0:00' 
 8: '1/1/2009 0:00' 
 9: '1/1/2009 0:00'}

data['Closed Date'][0:10]to_dict()
{0: '1/7/2009 0:00' 
 1: nan 
 2: '1/1/2009 0:00' 
 3: '1/1/2009 0:00' 
 4: '1/1/2009 0:00' 
 5: '1/12/2009 0:00' 
 6: '1/12/2009 0:00' 
 7: '1/7/2009 0:00' 
 8: '1/10/2009 0:00' 
 9: '1/7/2009 0:00'}
",1438637,,1438637.0,2012-11-20 19:16:38,2012-11-20 21:07:57,Arithmetic on date series (not an index) in Pandas,<python><pandas>,1.0,2.0,2.0,
13492226,1,,2012-11-21 11:23:14,0,44,"I'm having some problems when running a certain piece of code soon after update at version 091 of Pandas (under Python 27) from previous version
Basically  the code I run is the following:

myfunc = lambda x: makeDfCurve(frame x)
dates = Series(frameindex  index = frameindex) # new Time series filled temporarily 
# with dates taken from a certain dataframe 'frame' index
# and here's where the code crash:
frame['curve'] = datesapply(myfunc) 


I get the following kind of error:


  TypeError: ufunc 'subtract' not supported for the input types  and the inputs
         could not be safely coerced to any supported types according to the casting rule 'safe'


I tried to apply 'manually' the function recursively to see if some of the dates passed as the x parameter in the lambda definition where wrong  but managed to get correct results any time But the apply method just seem not to work anymore  and cannot understand why

Any help please?
thanks

PS I'd like to edit my question with the following  since  infact  after further investigation  I see that the cause of this error is due to the fact that with the new version of Pandas the index of a TimeSeries is of a ""class 'pandaslibTimestamp'"" type  thus creating a problem with my function that expects a datetime object instead",1529852,,1529852.0,2012-11-21 11:54:35,2012-11-21 17:03:47,problems with apply function in pandas after update,<python><pandas><apply>,1.0,2.0,,
13517281,1,13518290.0,2012-11-22 17:05:18,2,68,"I am having the x-value and corresponding counts in a file I read that as list of tuples in the following form

dat = [(002  1)  
(00211  1)  
(0021  1)  
(0023  1)  
(00251  1)  
(012  2)  
(0141  1)  
(014  3)  
(0171  1)  
(0462  9) 
(0467  10) 
(0478  15)  
(0804  20)  
(0815  31)  
(0815  24) 
(272  164)  
(278  147)  
(28  128) 
(578  6)  
(583  1)  
(58603  1) 
(594  17)  
(863  3)  
(887  5)   
(18601  1)  
(190  7)  
(210  2)  
(220  4)]


How to convert these into equal interval counts For example  an intervals with 02 increments

x    count
0    0
05  12
10  75
15  0
20  0
25  0
30  439
 
",1649335,,,,2012-11-22 18:27:07,python making list of tuples into definite sized intervals,<python><tuples><pandas><intervals>,3.0,,1.0,
13517451,1,,2012-11-22 17:18:44,0,60,"Is this a bug? It looks like pushing a datetime object into a dataframe  then indexing on the datetime column  does some scrambling of the date but produces no error (see line 97) Interestingly the view looks right so I'm guessing it's some sort of memory indexing thing

This was on a fairly recent build of pandas: pandas-091dev_85d982d-py27-linux-x86_64egg 

In [93]: import datetime  pandas
In [94]: df = pandasDataFrame([[datetimedatetimetoday()  121]]  columns=['Date'  'Value'])

In [95]: df = dfset_index('Date')

In [96]: df
Out[96]: 
                            Value
Date                             
2012-11-22 12:12:40905739   121    

In [97]: dfindex
Out[97]: 

[2190-12-31 02:18:44941732032]
Length: 1  Freq: None  Timezone: None

In [98]: df = dfreset_index()

In [99]: df
Out[99]: 
                           Date  Value
0 2190-12-31 02:18:44941732032   121
",287238,,287238.0,2012-11-22 17:27:11,2012-11-23 20:03:39,pandas datetime to index bug OR possible time travel discovery,<python><pandas>,1.0,1.0,,
13555607,1,13557050.0,2012-11-25 21:20:56,2,174,"I am trying to graph a Pandas dataframe using Matplotlib The dataframe contains four data columns composed of natural numbers  and an index of integers I would like to produce a single plot with line graphs for each of the four columns  as well as error bars for each point In addition  I would like to produce a legend providing labels for each of the four graphed lines

Graphing the lines and legend without error bars works fine When I introduce error bars  however  the legend becomes invalid -- the colours it uses no longer correspond to the appropriate lines If you compare a graph with error bars and a graph without  the legend and the shapes/positions of the curves remain exactly the same The colours of the curves get switched about  however  so that though the same four colours are used  they now correspond to different curves  meaning that the legend now assigns the wrong label to each curve

My graphing code is thus:

def plot_normalized(agged  show_errorbars  filename):
  combined = {}
  # ""agged"" is a dictionary containing Pandas dataframes Each dataframe
  # contains both a CPS_norm_mean and CPS_norm_std column By running the code
  # below  the single dataframe ""combined"" is created  which has integer
  # indices and a column for each of the four CPS_norm_mean columns contained
  # in agged's four dataframes
  for k in agged:
    combined[k] = agged[k]['CPS_norm_mean']
  combined = pandasDataFrame(combined)

  pltfigure()
  combinedplot()

  if show_errorbars:
    for k in agged:
      plterrorbar(
        x=agged[k]index 
        y=agged[k]['CPS_norm_mean'] 
        yerr=agged[k]['CPS_norm_std']
      )

  pltxlabel('Time')
  pltylabel('CPS/Absorbency')
  plttitle('CPS/Absorbency vs Time')
  pltsavefig(filename)


The full 100-line script is available on GitHub To run  download both graphpy and luxcsv  then run ""python2 graphpy"" It will generate two PNG files in your working directory -- one graph with error bars and one without

The graphs are thus:

Correct graph (with no error bars): iimgurcom/XE3oYpng
Incorrect graph (with error bars): iimgurcom/OQVQLpng
Observe that the graph without error bars is properly labelled; note that the graph with error bars is improperly labelled  as though the legend is identical  the line graphs' changed colours mean that each legend entry now refers to a different (wrong) curve 

Thanks for any help you can provide I've spent a number of extremely aggravating hours bashing my head against the wall  and I suspect that I'm making a stupid beginner's mistake For what it's worth  I've tried with the Matplotlib development tree  version 120  and 110  and all three have exhibited identical behaviour",1691611,,1691611.0,2012-11-25 21:35:58,2012-11-26 00:14:59,Adding error bars to Matplotlib-generated graph of Pandas dataframe creates invalid legend,<python><matplotlib><pandas>,1.0,4.0,,
13548721,1,13552256.0,2012-11-25 05:59:26,2,126,"I have a pandas dataframe that I filled with this:

import pandasiodata as web
test = webget_data_yahoo('QQQ')


The dataframe looks like this in iPython:

In [13]:  test
Out[13]:
    
    DatetimeIndex: 729 entries  2010-01-04 00:00:00 to 2012-11-23 00:00:00
    Data columns:
    Open         729  non-null values
    High         729  non-null values
    Low          729  non-null values
    Close        729  non-null values
    Volume       729  non-null values
    Adj Close    729  non-null values
    dtypes: float64(5)  int64(1)


When I divide one column by another  I get a float64 result that has a satisfactory number of decimal places  I can even divide one column by another column offset by one  for instance testOpen[1:]/testClose[:]  and get a satisfactory number of decimal places  When I divide a column by itself offset  however  I get just 1:

In [83]: testOpen[1:] / testClose[:]
Out[83]:

    Date
    2010-01-04         NaN
    2010-01-05    0999354
    2010-01-06    1005635
    2010-01-07    1000866
    2010-01-08    0989689
    2010-01-11    1005393

In [84]: testOpen[1:] / testOpen[:]
Out[84]:
    Date
    2010-01-04   NaN
    2010-01-05     1
    2010-01-06     1
    2010-01-07     1
    2010-01-08     1
    2010-01-11     1


I'm probably missing something simple  What do I need to do in order to get a useful value out of that sort of calculation?  Thanks in advance for the assistance",1850663,,,,2012-11-25 15:07:04,pandas DataFrame Dividing a column by itself,<python><data.frame><pandas>,2.0,3.0,,
13570178,1,13570851.0,2012-11-26 17:39:18,0,39,"I have some base info in a pandas data frame   I need to join it with some reference tables that I have access via a pyodbc connection   Is there any way to get the sql result set into a pandas datafraim without writing the result set out to a csv first

It just seems like a wast to have this extra step out to csv and into a Dataframe",1615371,,,,2012-11-26 18:23:36,itter from some odbc connection to pandas table with out a csv,<pandas><pyodbc>,1.0,,,
13572550,1,13574627.0,2012-11-26 20:23:17,0,29,"I have two CSV files with different number of columns and rows The first CSV file has M columns and N rows  the second has H columns and G rows Some of the columns have the same name

I'd like to combine the two into data frame with following properties:

N+G rows
Union of (M  H) columns
if column A is element of first CSV file but not of second  the data frame should contain the same values in first N entries of A as in first CSV  and for the rest (since there is no A data in second CSV) should be NA
Here is an example:

CSV1
City  Population 
Zagreb  700000 
Rijeka  142000

CSV2
City  Area 
Split  20000
Osijek  17100
Dubrovnik  14335


I'd like build a data frame that looks like this:

City      Population   Area
Zagreb    700000       NA
Rijeka    142000       NA
Split     NA           20000
Osijek    NA           17100
Dubrovnik NA           14335


Also what if instead two CSV files I had two data frames and wanted to do the same  for example if I loaded first csv to df1 and second one in df2 and then wanted to make a merge to df3 that would look like example above",761492,,,,2012-11-26 22:59:22,Merge of multiple data frames of different number of columns into one big data frame,<pandas>,1.0,,,
13572576,1,13572798.0,2012-11-26 20:25:03,1,55,"I have a dataframe of values and I would like to explore the rows that are outliers I wrote a function below that can be called with the groupby()apply() function and it works great for high or low values but when I want to combine them together i generate an error I am somehow messing up the boolean OR selection but I could only find documentation for selection criteria using &  Any suggestions would be appreciated

zach cp

df = DataFrame( {'a': [1 1 1 2 2 2 2 2 2 2]  'b': [5 5 6 9 9 9 9 9 9 20] } )

#this works fine
def get_outliers(group):
    x = mean(groupb)
    y = std(groupb)
    top_cutoff =    x + 2*y
    bottom_cutoff = x - 2*y
    cutoffs = group[groupb > top_cutoff]
    return cutoffs

#this will trigger an error
def get_all_ outliers(group):
    x = mean(groupb)
    y = std(groupb)
    top_cutoff =    x + 2*y
    bottom_cutoff = x -2*y
    cutoffs = group[(groupb > top_cutoff) or (groupb ",983191,,,,2012-11-28 04:35:30,using 'OR' to select data in pandas,<python><pandas>,1.0,,,
13454909,1,,2012-11-19 13:34:10,0,79,"my data:

a b c d e f
15 48  63
160 52 65 72
170 55 66 83 57
180 61 67 97 62
190 71 68 111 67
2  68 125 73
208    78
21  72
22  80
23  87
24  92 82


from pandas import read_csv
ds = read_csv ('lin-nandat'  index_col=0  sep=' ')

Traceback (most recent call last):
  File ""read_linpy""  line 7  in 
    ds = read_csv ('lin-nandat'  index_col=0  sep=' ')
  File ""/home/nbecker/local/lib/python27/site-packages/pandas/io/parserspy""  line 253  in read_csv
    return _read(TextParser  filepath_or_buffer  kdict)
  File ""/home/nbecker/local/lib/python27/site-packages/pandas/io/parserspy""  line 202  in _read
    return parserget_chunk()
  File ""/home/nbecker/local/lib/python27/site-packages/pandas/io/parserspy""  line 844  in get_chunk
    alldata = self_rows_to_cols(content)
  File ""/home/nbecker/local/lib/python27/site-packages/pandas/io/parserspy""  line 809  in _rows_to_cols
    raise ValueError(msg)
ValueError: Expecting 6 columns  got 5 in row 1
",354911,,1240268.0,2012-11-19 13:35:15,2012-11-19 13:50:47,missing data in pandas read_csv,<pandas>,1.0,,,
13574420,1,,2012-11-26 22:41:26,0,104,"When creating a DataFrame with MultiIndex columns it seems not possible to return a single column with a MultiIndex Instead  an object with an Index is returned:

import pandas as pd
import numpy as np

dates = npasarray(pddate_range('1/1/2000'  periods=8))
_metaInfo = pdMultiIndexfrom_tuples([('AA'  '[m]')  ('BB'  '[m]')  ('CC'  '[s]')  ('DD'  '[s]')]  names=['parameter' 'unit'])

df = pdDataFrame(nprandomrandn(8  4)  index=dates  columns=_metaInfo)
print dfget('AA')columns
# Index([[m]]  dtype=object)


where the 'parameter' info is missing
Is this a bug  is there a workaround?",1515250,,1240268.0,2012-11-26 23:03:42,2012-11-27 08:10:53,MultiIndex and columns in a DataFrame,<pandas>,1.0,2.0,,
13576164,1,13576876.0,2012-11-27 01:40:07,1,72,"A similar question was asked in How to keep index when using pandas merge  but it will not work with MultiIndexes  ie 

a = DataFrame(nparray([1 2 3 4 1 2 3 3])reshape((4 2))  columns=['col1' 'to_merge_on']  index=['a' 'b' 'a' 'b'])
id = pdMultiIndexfrom_arrays([[1 1 2 2] ['a' 'b' 'a' 'b']]  names =['id1' 'id2'])
aindex = id

In [207]: a
Out[207]: 
         col1  to_merge_on
id1 id2                   
1   a       1            2
    b       3            4
2   a       1            2
    b       3            4

b=DataFrame(data={""col2"": [1 2 3]  'to_merge_on' : [1 3 5]})

In [209]: b
Out[209]: 
   col2  to_merge_on
0     1            1
1     2            3
2     3            5

areset_index()merge(b  how=""left"")set_index('index')

In [208]: areset_index()merge(b  how=""left"")set_index('index')
------------------------------------------------------------
Traceback (most recent call last):
  File """"  line 1  in 
  File ""C:\Python27\lib\site-packages\pandas\core\framepy""  line 2054  in set_index
    level = frame[col]
  File ""C:\Python27\lib\site-packages\pandas\core\framepy""  line 1458  in __getitem__
    return self_get_item_cache(key)
  File ""C:\Python27\lib\site-packages\pandas\core\genericpy""  line 294  in _get_item_cache
    values = self_dataget(item)
  File ""C:\Python27\lib\site-packages\pandas\core\internalspy""  line 625  in get
    _  block = self_find_block(item)
  File ""C:\Python27\lib\site-packages\pandas\core\internalspy""  line 715  in _find_block
    self_check_have(item)
  File ""C:\Python27\lib\site-packages\pandas\core\internalspy""  line 722  in _check_have
    raise KeyError('no item named %s' % str(item))
KeyError: 'no item named index'


How can one make the merge while preserving the MultiIndex in the left dataframe?",1479269,,1479269.0,2012-11-27 03:16:28,2012-11-27 03:19:03,How to keep MultiIndex when using pandas merge,<python><pandas>,1.0,,,
13492530,1,13518146.0,2012-11-21 11:42:53,3,112,"I have a pandas dataframe df contains two stocks' financial ratio data :

>>> df
                  ROIC    ROE
STK_ID RPT_Date              
600141 20110331  0012  0022
       20110630  0031  0063
       20110930  0048  0103
       20111231  0063  0122
       20120331  0017  0033
       20120630  0032  0077
       20120930  0050  0120
600809 20110331  0536  0218
       20110630  0734  0278
       20110930  0806  0293
       20111231  1679  0313
       20120331  0666  0165
       20120630  1039  0257
       20120930  1287  0359


And I try to plot the ratio 'ROIC' & 'ROE' of stock '600141' & '600809' together on the same 'RPT_Date' to benchmark their performance  

dfplot(kind='bar') gives below



The chart draws '600141' on the left side   '600809' on the right side It is somewhat inconvenience to compare  the 'ROIC' & 'ROE' of the two stocks on same report date 'RPT_Date' 

What I want is to put the 'ROIC' & 'ROE' bar indexed by same 'RPT_Date' in same group side by side ( 4 bar per group)  and x-axis only labels the 'RPT_Date'  that will clearly tell the difference of two stocks

How to do that ?

And if I dfplot(kind='line')    it only shows two lines  but it should be four lines (2 stocks * 2 ratios) :


Is it a bug  or what I can do to correct it ? Thanks

I am using Pandas 081 ",1072888,,,,2012-11-22 18:15:40,How to barplot Pandas dataframe columns aligning by sub-index?,<python><matplotlib><pandas>,1.0,,1.0,
13505843,1,,2012-11-22 04:21:20,3,115,"I've got a Pandas Panel with many DataFrames with the same rows/column labels  I want to make a new panel with DataFrames that fulfill certain criteria based on a couple columns

This is easy with dataframes and rows:  Say I have a df  zHe_compare  I can get the suitable rows with:

zHe_compare[(zHe_compare['zHe_calc'] > 100) & (zHe_compare['zHe_med'] > 100) | ((zHe_obs_lo_2s  min_num ] ]


I know the the inner boolean part  but how do I specify this for each dataframe in a panel?  Because I need multiple columns from each df  I haven't met success using the panelminor_xs slicing techniques

thanks!",1696130,,,,2012-11-22 04:21:20,Pandas Panel fancy indexing: How to return (index of) all DataFrames in Panel based on Boolean of multiple columns in each df,<python><indexing><panel><data.frame><pandas>,,2.0,,
13582449,1,13583024.0,2012-11-27 10:43:07,2,77,"I had a dataframe and did a groupby in FIPS and summed the groups that worked fine

kl = ksgroupby('FIPS')

klaggregate(npsum)


I just want a normal Dataframe back but I have a pandascoregroupbyDataFrameGroupBy object 

There is a question that sounds like this one but it is not the same ",1246428,,502950.0,2012-11-27 10:45:20,2012-11-27 11:16:51,Convert DataFrameGroupBy object to DataFrame pandas,<python><pandas>,1.0,,,
13606487,1,13606898.0,2012-11-28 13:43:40,0,75,"I just picked up pandas  thinking that it will enable me to do data analysis nicely in python Now I have a pandas data frame of the following form:

pandasDataFrame({""p1"": [1  1  2  2  3  3]*2 
                  ""p2"": [1]*6+[2]*6 
                  ""run"": [1  2]*6 
                  ""result"": xrange(12)})

    p1  p2  result  run
0    1   1       0    1
1    1   1       1    2
2    2   1       2    1
3    2   1       3    2
4    3   1       4    1
5    3   1       5    2
6    1   2       6    1
7    1   2       7    2
8    2   2       8    1
9    2   2       9    2
10   3   2      10    1
11   3   2      11    2


I would like to generate the frame that contains one entry for every set of parameters p1 and p2 with the average of all values of result for these parameters  that is 

   p1  p2  result
0   1   1     05
1   2   1     25
2   3   1     45
3   1   2     65
4   2   2     85
5   3   2    105


What is the pandas way to do this? I would try to copy the original table  drop columns that differ (result and run)  reindex that  combine both things again with the new index as multi-index and then run the mean method for that outer multi-index level Is that the way to do it  and if yes  how do I do these index things properly in code?",1274613,,1274613.0,2012-11-28 13:51:59,2012-11-28 14:04:03,Pandas: Calculate average for all entries that only differ in two columns,<python><pandas>,1.0,2.0,,
13596419,1,13637005.0,2012-11-28 01:42:21,1,125,"I am very new to Pandas (ie  less than 2 days) However  I can't seem to figure out the right syntax for combining two columns with an if/else condition 

Actually  I did figure out one way to do it using 'zip' This is what I want to accomplish  but it seems there might be a more efficient way to do this in pandas 

For completeness sake  I include some pre-processing I do to make things clear: 

records_data = pdread_csv(open('recordscsv'))

## pull out a year from column using a regex
source_years = records_data['source']map(extract_year_from_source) 

## this is what I want to do more efficiently (if its possible)
records_data['year'] = [s if s else y for (s y) in zip(source_years  records_data['year'])]
",215847,,,,2012-11-30 13:29:56,how to combine two columns with an if/else in python pandas?,<python><pandas>,2.0,5.0,1.0,
13608748,1,13616324.0,2012-11-28 15:37:36,1,102,"I have a pandas pivot_table that aggregates 2 data sets in 2 columns across several rows  I would like to add another column that is the difference between the aggregated values in the two existing columns by row  Is there a way to implement this directly in the pivot_table() call?  I know that the returned pivot is a dataframe so I can calculate it through other means  but just curious if there is a more efficient way  

Simple example of my data:

  Set     Type   Val
  S1       A     1
  S1       B     2
  S1       B     3
  S2       A     4
  S2       B     5
  S2       C     6


Using the following code where data is my df

piv=pivot_table(data 'Val' rows='Type' cols='Set' aggfunc=sum fill_value=00)


I get the below

    S1  S2
A   1   4
B   5   5
C   0   6


I would like the output to be

    S1  S2 Diff
A   1   4   3
B   5   5   0
C   0   6   6


or just 

   Diff
A   3
B   0
C   6
",1769851,,,,2012-11-29 02:06:09,Column Differences in Python Pivot-Table,<python><pandas><pivot-table>,1.0,,,
13611065,1,13616382.0,2012-11-28 17:34:35,1,131,"I have a scenario where a user wants to apply several filters to a Pandas DataFrame or Series object  Essentially  I want to efficiently chain a bunch of filtering (comparison operations) together that are specified at run-time by the user

The filters should be additive (aka each one applied should narrow results)

I'm currently using reindex() but this creates a new object each time and copies the underlying data (if I understand the documentation correctly)  So  this could be really inefficient when filtering a big Series or DataFrame

I'm thinking that using apply()  map()  or something similar might be better  I'm pretty new to Pandas though so still trying to wrap my head around everything

TL;DR

I want to take a dictionary of the following form and apply each operation to a given Series object and return a 'filtered' Series object

relops = {'>=': [1]  '>> df = pandasDataFrame({'col1': [0  1  2]  'col2': [10  11  12]})
>>> print df
>>> print df
   col1  col2
0     0    10
1     1    11
2     2    12

>>> from operator import le  ge
>>> ops ={'>=': ge  '>> apply_relops(df['col1']  {'>=': [1]})
col1
1       1
2       2
Name: col1
>>> apply_relops(df['col1']  relops = {'>=': [1]  '",1108031,,,,2012-11-29 09:44:10,Efficient way to apply multiple filters to pandas DataFrame or Series,<python><algorithm><pandas>,1.0,1.0,1.0,
13581517,1,13581784.0,2012-11-27 09:54:04,0,60,"When creating a DataFrame with MultiIndex columns it seems not possible to select / filter rows using syntax like df[df[""AA""]>00]
For example:

import pandas as pd
import numpy as np

dates = npasarray(pddate_range('1/1/2000'  periods=8))
_metaInfo = pdMultiIndexfrom_tuples([('AA'  '[m]')  ('BB'  '[m]')  ('CC'  '[s]')  ('DD'  '[s]')]  names=['parameter' 'unit'])

df = pdDataFrame(randn(8  4)  index=dates  columns=_metaInfo)
print df[df['AA']>00]


The result of df[""AA""]>00 is an indexed DataFrame iso a Timeseries This probably causes the crash

When using the same metaInfo as an index for the rows  the situation is different:

df1 = pandasDataFrame(nprandomrandn(4  6)  index=_metaInfo)
print df1[df1[""AA""]>00]


produces:

[ 113268106 -006887761  068535054  249431163 -029349413  034772553]


which are the elements of row AA larger than zero This gives only the values of row AA and not of the other columns of the DataFrame 

Is there a workaround? Am I trying to do something I shouldn't?",1515250,,1240268.0,2012-11-27 10:10:42,2012-11-27 10:10:42,Filtering rows with MultiIndex columns,<pandas>,1.0,,1.0,
13603181,1,13606221.0,2012-11-28 10:43:54,4,80,"I have GPS data of ice speed from three different GPS receivers The data are in a pandas dataframe with an index of julian day (incremental from the start of 2009)

This is a subset of the data (the main dataset is 3487235 rows):

                    R2          R7         R8
1235000000 116321959  100805197  96519977
1235000116 NaN         100771133  96234957
1235000231 NaN         100584559  97249262
1235000347 118823610  100169055  96777833
1235000463 NaN         99753551   96598350
1235000579 NaN         99338048   95283989
1235000694 113995003  98922544   95154067


The dataframe has form:


Index: 6071320 entries  12767291667 to 133851805556
Data columns:
R2    3487235  non-null values
R7    3875864  non-null values
R8    1092430  non-null values
dtypes: float64(3)


R2 sampled at a different rate to R7 and R8 hence the NaNs which appear systematically at that spacing

Trying dfplot() to plot the whole dataframe (or indexed row locations thereof) works fine in terms of plotting R7 and R8  but doesn't plot R2 Similarly  just doing dfR2plot() also doesn't work The only way to plot R2 is to do dfR2dropna()plot()  but this also removes NaNs which signify periods of no data (rather than just a coarser sampling frequency than the other receivers)

Has anyone else come across this? Any ideas on the problem would be gratefully received :)",1859488,,1240268.0,2012-11-28 12:21:02,2012-11-28 13:30:27,Plot pandas dataframe containing NaNs,<pandas><ipython><data-analysis>,1.0,1.0,1.0,
13614273,1,13615440.0,2012-11-28 20:58:19,0,50,"Here is a link to the data I'm working with

I try creating a group based on the columns I want (cuepos  targetpos and soa)  but when I list the groups  it seems to be creating groups with some of the other columns as well

groups = tgroupby(['cuepos'  'targetpos'  'soa'])
for name  _ in groups:
    print name


Output:

(-89  -89  -8941139261318807)
(-88  -88  -8844728835230345)
(-88  -88  -8820648583606493)
(-87  -87  -8777339640061896)
(-86  -86  -868637199297012)
(-85  -85  -8550514526170076)
(-83  -83  -8387935779179833)
(-83  -83  -8386953491773222)
(-81  -81  -8143570407709822)
(-80  -80  -8070639872201482)
(-80  -80  -8038454772926528)
(-79  -79  -7981516051155803)
(-75  -75  -7583933409447087)
(-74  -74  -7453528962061156)
(-73  -73  -7310397238440302)
(-70  -70  -7033208101764106)
(-64  -64  -6418024404177129)
(-61  -61  -61969216551968344)
(-61  -61  -6189154280549519)
(-61  -61  -6181223645812457)
(-61  -61  -6180055105439692)
(-59  -59  -5981551441456813)
(-57  -57  -5767934478380107)
(-53  -53  -5391038834185852)
(-51  -51  -5135605559139145)
(-48  -48  -4863443042074468)
(-48  -48  -48026567177299825)
(-44  -44  -4484750981999042)
(-44  -44  -4420816797871376)
(-43  -43  -4397185684796753)
(-39  -39  -3903132145644588)
(-37  -37  -3709246448040565)
(-37  -37  -3706406445785262)
(-36  -36  -3689551705610748)
(-34  -34  -3423312940622742)
(-33  -33  -33771084303661524)
(-31  -31  -31183030415916534)
(-29  -29  -29062383175092265)
(-22  -22  -221763042325164)
(-17  -17  -1751138905398824)
(-14  -14  -14673170146200675)
(-9  -9  -9389620131659427)
(-9  -9  -928109130634627)
(-8  -8  -8871025817651997)
(-8  -8  -847526860623043)
(-7  -7  -7484697635519495)
(-3  -3  -3265563116265213)
(-2  -2  -2842961251214575)
(1  1  -01)
(1  1  01)
(1  1  04)
(1  2  -01)
(1  2  04)
(2  1  -01)
(2  1  04)
(2  2  -01)
(2  2  01)
(2  2  04)
(6  6  6928400268960042)
(8  8  8476818809273727)
(11  11  11225720357570507)
(13  13  13949059199458294)
(17  17  17272663104264836)
(18  18  18548979295124248)
(21  21  21075945669054835)
(22  22  22101344720547228)
(22  22  2236405009971824)
(24  24  24658480906080996)
(27  27  27977154868918745)
(33  33  3375660016684323)
(49  49  4959296862775889)
(51  51  5109435632596291)
(52  52  52107845391762766)
(54  54  5422026217046835)
(54  54  5455461208382168)
(56  56  5692397800238861)
(57  57  5715634257840432)
(57  57  57490226928649264)
(57  57  5782030543311612)
(58  58  5820496727209113)
(58  58  5844217165553367)
(58  58  58591804845872765)
(58  58  5884514017314996)
(60  60  6015474896731822)
(60  60  6049526399943247)
(60  60  60621239605283456)
(61  61  6173542327989246)
(61  61  61882729155824705)
(63  63  6315716022529575)
(65  65  6562684954629724)
(67  67  6732622273875754)
(68  68  6872997170017184)
(71  71  7164012395084114)
(71  71  7187357582509455)
(71  71  7191237771102328)
(72  72  7287756472051248)
(73  73  7323547239962096)
(75  75  7520111322246554)
(76  76  7637312687962122)
(78  78  7839727821292199)
(79  79  7927674426299386)
(80  80  8022644745900354)
(82  82  8238562004739285)
(82  82  8275922122217577)
(85  85  8519181215842043)
(85  85  856896980533089)
(85  85  8584141277449113)
(87  87  8721598891172931)
(87  87  8760810304014197)
(87  87  8780910737578778)


The desired groups are in the middle (the ones that look like (1  1  -01))  What's this other stuff?  What am I doing wrong  here?",1156707,,,,2012-11-28 22:20:31,Why does this pandas groupby object have all these extra groups?,<python><pandas>,1.0,1.0,,
13630035,1,13866133.0,2012-11-29 16:18:08,0,88,"I have a large data frame Usually  when I have a data frame like this I get the summary for that data frame  where I get the info of how many non-NaN values in each column and column names However for this one I get an even shorter summary:


Index: 138289 entries  1993-07-23 to 2012-11-26
Columns: 101 entries  AAT to ZZT
dtypes: object(101)


I'd like to get a standard summary  with info about each column

I'm using ipython notebook and pandas 091 if that has anything to do with it",761492,,,,2012-12-13 18:29:17,Enable full summary on a huge data frame,<python><pandas>,3.0,,,
13636592,1,,2012-11-29 23:20:44,0,73,"I have the following DataFrame containing song names  their peak chart positions and the number of weeks they spent at position no 1:

                                          Song            Peak            Weeks
76                            Paperback Writer               1               16
117                               Lady Madonna               1                9
118                                   Hey Jude               1               27
22                           Can't Buy Me Love               1               17
29                          A Hard Day's Night               1               14
48                              Ticket To Ride               1               14
56                                       Help!               1               17
109                       All You Need Is Love               1               16
173                The Ballad Of John And Yoko               1               13
85                               Eleanor Rigby               1               14
87                            Yellow Submarine               1               14
20                    I Want To Hold Your Hand               1               24
45                                 I Feel Fine               1               15
60                                 Day Tripper               1               12
61                          We Can Work It Out               1               12
10                               She Loves You               1               36
155                                   Get Back               1                6
8                               From Me To You               1                7
115                              Hello Goodbye               1                7
2                             Please Please Me               2               20
92                   Strawberry Fields Forever               2               12
93                                  Penny Lane               2               13
107                       Magical Mystery Tour               2               16
176                                  Let It Be               2               14
0                                   Love Me Do               4               26
157                                  Something               4                9
166                              Come Together               4               10
58                                   Yesterday               8               21
135                       Back In The USSR              19                3
164                         Here Comes The Sun              58               19
96       Sgt Pepper's Lonely Hearts Club Band              63               12
105         With A Little Help From My Friends              63                7


I'd like to rank these songs in order of popularity  so I'd like to sort them according to the following criteria: songs that reached the highest position come first  but if there is a tie  the songs that remained in the charts for the longest come first

I can't seem to figure out how to do this in Pandas",1715271,,,,2012-12-13 18:24:51,How to sort a Pandas DataFrame according to multiple criteria?,<python><pandas>,2.0,,,
13638865,1,13688030.0,2012-11-30 04:07:44,2,121,"I have a MultiIndex DataFrame that contains these values:

                      AAPL
           minor          
2007-09-14 OC     0024436
2007-09-15 CC     0030293
           CO     0017518
           OC     0024688
           OO     0031835

# to_dict():

{'AAPL': {(  'OC'): 0024436265475779286 
  (  'CC'): 0030293017084353703 
  (  'CO'): 0017518449703066673 
  (  'OC'): 0024688182799779634 
  (  'OO'): 0031834725061579666}}


--

and a Series that contains these values:

CC    15874508
CO    18590320
OC    30503468
OO    15874508

# to_dict():

{'CC': 15874507866387544 
 'CO': 18590320061795602 
 'OC': 30503467646507644 
 'OO': 15874507866387544}


I'd like to multiply all of the minor index CC values by the CC value in the Series  and the same with the other values  I saw another question on here that gave me the mul method  but when I try that  even with the level='minor'  it tells me:


  TypeError: can only call with other hierarchical index objects


I've unstacked the minor index to make it columns  and specified level='minor'  axis='columns' with the same result

Finally  the end result is to be able to run this same calculation on a DataFrame where the major columns are several equities -- in that instance  would mul() work against each equity as well?

Thanks for your assistance!",1850663,,1850663.0,2012-11-30 14:42:28,2012-12-03 16:56:40,pandas: Multiply MultiIndex DataFrame with Series,<python><numpy><pandas>,1.0,2.0,,
13630269,1,,2012-11-29 16:29:58,0,39,"I've got a pandasDataFrame object that I'd like to cast into a gviz_apiDataTable which I'll then use in my google charts

Is there an easy way to do this?",1860470,,,,2012-11-29 22:50:07,how to cast pandas.DataFrame into gviz_api.DataTable,<api><google-charts><pandas><google-charts-api>,1.0,,,
13636848,1,13680953.0,2012-11-29 23:44:17,0,55,"I have two DataFrames which I want to merge based on a column However  due to alternate spellings  different number of spaces  absence/presence of diacritical marks  I would like to be able to merge as long as they are similar to one another

Any similarity algorithm will do (soundex  Levenshtein  difflib's) 

Say one DataFrame has the following data:

df1 = DataFrame([[1] [2] [3] [4] [5]]  index=['one' 'two' 'three' 'four' 'five']  columns=['number'])

       number
one         1
two         2
three       3
four        4
five        5

df2 = DataFrame([['a'] ['b'] ['c'] ['d'] ['e']]  index=['one' 'too' 'three' 'fours' 'five']  columns=['letter'])

      letter
one        a
too        b
three      c
fours      d
five       e


Then I want to get the resulting DataFrame

       number letter
one         1      a
two         2      b
three       3      c
four        4      d
five        5      e
",215847,,1240268.0,2012-12-03 10:08:10,2012-12-03 10:23:45,is it possible to do fuzzy match merge with python pandas?,<python><pandas>,2.0,2.0,,
13647222,1,13647445.0,2012-11-30 14:33:41,1,103,"I'm running a simulation which gives me a csv file corresponding in structure to the following pandas data frame:

df = DataFrame({'series': {0: 'A'   1: 'B'   2: 'C'   3: 'A'   4: 'B'   5: 'C'   6: 'A'   7: 'B'   8: 'C'   9: 'A'   10: 'B'   11: 'C'   12: 'A'   13: 'B'   14: 'C'}  'step': {0: '0'   1: '0'   2: '0'   3: '1'   4: '1'   5: '1'   6: '2'   7: '2'   8: '2'   9: '3'   10: '3'   11: '3'   12: '4'   13: '4'   14: '4'}  'value': {0: '0'   1: '0'   2: '5'   3: '1'   4: '0'   5: '4'   6: '2'   7: '1'   8: '3'   9: '3'   10: '2'   11: '2'   12: '4'   13: '4'   14: '1'}})

    step  value series
0      0      0      A
1      0      0      B
2      0      5      C
3      1      1      A
4      1      0      B
5      1      4      C
6      2      2      A
7      2      1      B
8      2      3      C
9      3      3      A
10     3      2      B
11     3      2      C
12     4      4      A
13     4      4      B
14     4      1      C


Given this df  how can I plot the (step  value) pairs for each series?

That is  I would like to get the image



which would be easy to get (just df2plot(""step"") if I could get my DataFrame into the format df2

   A  B  C  step
0  0  0  5     0
1  1  0  4     1
2  2  1  3     2
3  3  2  2     3
4  4  4  1     4


but I don't see how I would do the transformation dfdf2 or obtain the plot from df (which is what I get from my simulation) in any other way

I could change my simulation code to write the corresponding data into in my case 16 separate table columns instead of one series and one value column  but that 16 is the value for most simulations  some use not all of those series and in the future I may need to split those further  so this does not look like the best solution to me

To generate both example DataFrames  I went in the other direction and did

df2 = pandasDataFrame({""step"":xrange(5) ""A"":xrange(5) ""B"":[i*(i+1)/5 for i in xrange(5)] ""C"":xrange(5 0 -1)})
dfA = df2[[""step"" ""A""]]rename(columns={""A"":""value""})
dfA[""series""] = ""A""
dfB = df2[[""step"" ""B""]]rename(columns={""B"":""value""})
dfB[""series""] =""B""
dfC = df2[[""step"" ""C""]]rename(columns={""C"":""value""})
dfC[""series""] =""C""
df = dfAappend(dfB)append(dfC)sort(""step"")
dfindex = xrange(15)
",1274613,,1240268.0,2012-11-30 17:30:14,2012-12-02 13:05:29,"Plotting data lines from pandas dataframe, identified by keys in one column instead by different columns",<python><matplotlib><pandas>,1.0,2.0,,
13457335,1,,2012-11-19 15:50:40,0,58,"After seeing this question about replicating SQL select-statement-like behavior in Pandas  I added this answer showing two ways that could shorten the verbose syntax given in the accepted answer to that question

After playing around with them  my two shorter-syntax methods are significantly slower  and I am hoping someone can explain why

You can assume any functions used below are either from Pandas  IPython  or from the question and answers linked above

import pandas
import numpy as np
N = 100000
df = pandasDataFrame(npround(nprandomrand(N 5)*10))

def pandas_select(dataframe  select_dict):
    inds = dataframeapply(lambda x: reduce(lambda v1 v2: v1 and v2 
                           [elem[0](x[key]  elem[1])
                           for key elem in select_dictiteritems()])  axis=1)
    return dataframe[inds]



%timeit _ = df[(df[1]==3) & (df[2]==2) & (df[4]==5)]
%timeit _ = df[dfapply(lambda x: (x[1]==3) & (x[2]==2) & (x[4]==5)  axis=1)]

import operator
select_dict = {1:(operatoreq 3)  2:(operatoreq 2)  4:(operatoreq 5)}
%timeit _ = pandas_select(df  select_dict)


The output I get is:

In [6]: %timeit _ = df[(df[1]==3) & (df[2]==2) & (df[4]==5)]
100 loops  best of 3: 491 ms per loop

In [7]: %timeit _ = df[dfapply(lambda x: (x[1]==3) & (x[2]==2) & (x[4]==5)  axis=1)]
1 loops  best of 3: 123 s per loop

In [10]: %timeit _ = pandas_select(df  select_dict)
1 loops  best of 3: 16 s per loop


I can buy that the user of reduce  operator functions  and just the function overhead from my pandas_select function could slow it down But it seems excessive Inside of my function  I'm using the same syntax  df[key] logical_op value  but it's much slower

I'm also puzzled why the apply version along axis=1 is so much slower It should literally be just a shortening of the syntax  no?",567620,,,,2012-11-19 16:01:45,Python Pandas: What causes slowdown in different column selection methods?,<python><benchmarking><pandas><apply>,1.0,,,
13464492,1,13471575.0,2012-11-19 23:50:13,1,85,"I am not sure if this is a bug or if it's by design-- perhaps I am missing something and the ohlc aggregator isn't supposed to work with dataframes  Perhaps this behavior is by design because a dataframe with anything other than an index column and a price column could yield strange results?  Other aggregators (mean stdev  etc) work with a dataframe  In any case  I'm trying to get OHLC from this data  and converting to a timeseries doesn't seem to work either 

Here's an example:

import pandas as pd
rng = pddate_range('1/1/2012'  periods=1000  freq='S')

ts = pdSeries(randint(0  500  len(rng))  index=rng)
df = pdDataFrame(randint(0 500  len(rng))  index=rng)

tsresample('5Min'  how='ohlc') # works great
dfresample('5Min'  how='ohlc') # throws a ""NotImplementedError""

newts = pdTimeSeries(df) #am I missing an index command in this line?
# the above line yields this error ""TypeError: Only valid with DatetimeIndex or
  PeriodIndex""


Full NotImplementedError paste:

NotImplementedError                       Traceback (most recent call last)
/home/jeff/ in ()
----> 1 dfresample('5Min'  how='ohlc')

/usr/local/lib/python27/dist-packages/pandas-092dev-py27-linux-x86_64egg/pandas/core/genericpyc in resample(self  rule  how  axis  fill_method  closed  label  convention  kind  loffset  limit  base)
    231                               fill_method=fill_method  convention=convention 
    232                               limit=limit  base=base)
--> 233         return samplerresample(self)
    234 
    235     def first(self  offset):

/usr/local/lib/python27/dist-packages/pandas-092dev-py27-linux-x86_64egg/pandas/tseries/resamplepyc in resample(self  obj)
     66 
     67         if isinstance(axis  DatetimeIndex):
---> 68             rs = self_resample_timestamps(obj)
     69         elif isinstance(axis  PeriodIndex):
     70             offset = to_offset(selffreq)

/usr/local/lib/python27/dist-packages/pandas-092dev-py27-linux-x86_64egg/pandas/tseries/resamplepyc in _resample_timestamps(self  obj)
    189             if len(grouperbinlabels)  191                 result = groupedaggregate(self_agg_method)
    192             else:
    193                 # upsampling shortcut


/usr/local/lib/python27/dist-packages/pandas-092dev-py27-linux-x86_64egg/pandas/core/groupbypyc in aggregate(self  arg  *args  **kwargs)
   1538         """"""
   1539         if isinstance(arg  basestring):
-> 1540             return getattr(self  arg)(*args  **kwargs)
   1541 
   1542         result = {}

/usr/local/lib/python27/dist-packages/pandas-092dev-py27-linux-x86_64egg/pandas/core/groupbypyc in ohlc(self)
    384         For multiple groupings  the result index will be a MultiIndex
    385         """"""
--> 386         return self_cython_agg_general('ohlc')
    387 
    388     def nth(self  n):

/usr/local/lib/python27/dist-packages/pandas-092dev-py27-linux-x86_64egg/pandas/core/groupbypyc in _cython_agg_general(self  how  numeric_only)
   1452 
   1453     def _cython_agg_general(self  how  numeric_only=True):
-> 1454         new_blocks = self_cython_agg_blocks(how  numeric_only=numeric_only)
   1455         return self_wrap_agged_blocks(new_blocks)
   1456 

/usr/local/lib/python27/dist-packages/pandas-092dev-py27-linux-x86_64egg/pandas/core/groupbypyc in _cython_agg_blocks(self  how  numeric_only)
   1490                 values = comensure_float(values)
   1491 
-> 1492             result  _ = selfgrouperaggregate(values  how  axis=agg_axis)
   1493             newb = make_block(result  blockitems  blockref_items)
   1494             new_blocksappend(newb)

/usr/local/lib/python27/dist-packages/pandas-092dev-py27-linux-x86_64egg/pandas/core/groupbypyc in aggregate(self  values  how  axis)
    730                 values = valuesswapaxes(0  axis)
    731             if arity > 1:
--> 732                 raise NotImplementedError
    733             out_shape = (selfngroups ) + valuesshape[1:]
    734 

NotImplementedError: 
",1487345,,1487345.0,2012-11-20 00:00:18,2012-11-20 10:48:15,OHLC aggregator doesn't work with dataframe on pandas?,<data.frame><time-series><pandas>,1.0,4.0,,
13584149,1,13587250.0,2012-11-27 12:21:06,2,53,"I have DataFrames which have different name's but are all indexed by the same time series Now I would like to add the values in them So far I'm using a for loop for this
If I use df1 + df2 I get a DataFrame with the same index but with a column for each name with all NaN values in them
If I use df1add(df2) (with an optional fill_value=0) I get a DataFrame with the values of the first DataFrame  which is the same result as when I use df1combineAdd(df2)

Any hints on how to add the values except for looping over all the indexes and adding the values?",1073420,,,,2012-11-27 15:15:51,Adding DataFrames in pandas,<python><pandas><time-series>,1.0,1.0,,
13654699,1,13655271.0,2012-11-30 23:35:15,3,162,"I have a time-series that is not recognized as a DatetimeIndex despite being indexed by standard YYYY-MM-DD strings with valid dates Coercing them to a valid DatetimeIndex seems to be inelegant enough to make me think I'm doing something wrong

I read in (someone else's lazily formatted) data that contains invalid datetime values and remove these invalid observations

In [1]: df = pdread_csv('datacsv' index_col=0)
In [2]: print df['2008-02-27':'2008-03-02']
Out[2]: 
             count
2008-02-27  20
2008-02-28   0
2008-02-29  27
2008-02-30   0
2008-02-31   0
2008-03-01   0
2008-03-02  17

In [3]: def clean_timestamps(df):
    # remove invalid dates like '2008-02-30' and '2009-04-31'
    to_drop = list()
    for d in dfindex:
        try:
            datetimedate(int(d[0:4]) int(d[5:7]) int(d[8:10]))
        except ValueError:
            to_dropappend(d)
    df2 = dfdrop(to_drop axis=0)
    return df2

In [4]: df2 = clean_timestamps(df)
In [5] :print df2['2008-02-27':'2008-03-02']
Out[5]:
             count
2008-02-27  20
2008-02-28   0
2008-02-29  27
2008-03-01   0
2008-03-02  17


This new index is still only recognized as a 'object' dtype rather than a DatetimeIndex 

In [6]: df2index
Out[6]: Index([2008-01-01  2008-01-02  2008-01-03    2012-11-27  2012-11-28 
   2012-11-29]  dtype=object)


Reindexing produces NaNs because they're different dtypes

In [7]: i = pddate_range(start=min(df2index) end=max(df2index))
In [8]: df3 = df2reindex(index=i columns=['count'])
In [9]: df3['2008-02-27':'2008-03-02']
Out[9]: 
            count
2008-02-27 NaN
2008-02-28 NaN
2008-02-29 NaN
2008-03-01 NaN
2008-03-02 NaN


I create a fresh dataframe with the appropriate index  drop the data to a dictionary  then populate the new dataframe based on the dictionary values (skipping missing values)

In [10]: df3 = pdDataFrame(columns=['count'] index=i)
In [11]: values = dict(df2['count'])
In [12]: for d in i:
    try:
        df3set_value(index=d col='count' value=values[disoformat()[0:10]])
    except KeyError:
        pass
In [13]: print df3['2008-02-27':'2008-03-02']
Out[13]: 

             count
2008-02-27  20
2008-02-28   0
2008-02-29  27
2008-03-01   0
2008-03-02  17

In [14]: df3index
Out[14];

[2008-01-01 00:00:00    2012-11-29 00:00:00]
Length: 1795  Freq: D  Timezone: None


This last part of setting values based on lookups to a dictionary keyed by strings seems especially hacky and makes me think I've missed something important",1574687,,1574687.0,2012-12-01 04:26:42,2012-12-01 04:26:42,Reindexing pandas timeseries from object dtype to datetime dtype,<python><datetime><python-2.7><pandas>,1.0,0.0,,
13454588,1,13456818.0,2012-11-19 13:15:45,0,158,"I'm trying to add a series (composed of a list of [1 2 0 ]) to a candlestick chart I produced with matplotlib  but cannot work out how to include those labels for each specific candle in the graph Basically I'd like to produce a chart like this one:



with the labels with the numbers (my signal series) just over or below each candles
Is there any way I can reach that?

Don't know if it helps  but my series are of the pandas DataFrame kind",1529852,,,,2012-11-19 15:24:43,adding labels to candlestick chart in matplotlib,<pandas><labels><candlestick-chart>,1.0,,,
13460889,1,13462244.0,2012-11-19 19:39:06,0,89,"How to implement the composition pattern? I have a class Container which has an attribute object Contained I would like to  redirect/allow access to all methods of Contained class from Container by simply calling my_containersome_contained_method() Am I doing the right thing in the right way?

I use something like:

class Container:
   def __init__(self):
       selfcontained = Contained()
   def __getattr__(self  item):
       if item in self__dict__: # some overridden
           return self__dict__[item] 
       else:
           return selfcontained__getattr__(item) # redirection


Background:

I am trying to build a class (Indicator) that adds to the functionality of an existing class (pandasDataFrame) Indicator will have all the methods of DataFrame I could use inheritance  but I am following the ""favor composition over inheritance"" advice (see  eg  the answers in: python: inheriting or composition) One reason not to inherit is because the base class is not serializable and I need to serialize

I have found this  but I am not sure if it fits my needs",1579844,,,,2012-11-19 21:27:36,How to redirect all methods of a contained class in Python?,<python><inheritance><pandas><composition>,1.0,4.0,1.0,
13475812,1,13475925.0,2012-11-20 14:54:13,1,123,"I'm trying to calculate daily sums of values using pandas Here's the test file - http://pastebincom/uSDfVkTS

This is the code I came up so far:

import numpy as np
import datetime as dt
import pandas as pd

f = npgenfromtxt('test'  dtype=[('datetime'  '|S16')  ('data'  '",1838601,,,,2012-11-20 15:05:14,Calculate daily sums using python pandas,<python><pandas>,1.0,,1.0,
13659881,1,13659944.0,2012-12-01 13:26:10,2,43,"I'm trying to figure out how to count by number of rows per unique pair of columns (ip  useragent)  eg

d = pdDataFrame({'ip': ['19216801'  '19216801'  '19216801'  '19216802']  'useragent': ['a'  'a'  'b'  'b']})

     ip              useragent
0    19216801     a
1    19216801     a
2    19216801     b
3    19216802     b


To produce:

ip           useragent  
19216801  a           2
19216801  b           1
19216802  b           1


Ideas?",1804520,,,,2012-12-01 13:34:16,Count by unique pair of columns in pandas,<pandas>,1.0,,,
13462802,1,13475127.0,2012-11-19 21:44:44,1,55,Pandas currently allows you to add business days to a given date datetimetoday() + 3*BDay()  I would like to extend the idea of a business day to exclude a given DateIndex of holidays as well as weekends  Is that possible incorporate a DateIndex into an offset?,1618041,,,,2012-11-20 14:15:25,DateOffset in Pandas with a Holiday Calendar,<pandas><holidays>,1.0,,,
13550940,1,,2012-11-25 12:19:50,3,103,"I'm trying to insert long integers in a Pandas Dataframe 

import numpy as np
from pandas import DataFrame

data_scores = [(6311132704823138710  273)  (2685045978526272070  23)  (8921811264899370420  45)  (17019687244989530680L  270)  (9930107427299601010L  273)]
dtype = [('uid'  'u8')  ('score'  'u8')]
data = npzeros((len(data_scores) ) dtype=dtype)
data[:] = data_scores
df_crawls = DataFrame(data)
print df_crawlshead()


But when I look in the dataframe  last values which are long are now negative :


                       uid  score
0  6311132704823138710    273
1  2685045978526272070     23
2  8921811264899370420     45
3 -1427056828720020936    270
4 -8516636646409950606    273


uids are 64 bits unsigned int  so 'u8' should be the correct dtype ? Any ideas ?",253614,,,,2012-11-28 02:43:37,Python pandas insert long integer,<python><numpy><pandas>,2.0,4.0,1.0,
13557547,1,,2012-11-26 01:34:40,1,60,"If I used cmov_mean in scikitstimeseries  what should I use when I ""resample"" in pandas?

When I ""resample"" my daily averages to monthly and then plot both  I notice a big time offset There is a ""convention"" setting to ""start"" or ""end"" but I don't see a ""mid"" setting",1852062,,1240268.0,2012-12-10 22:47:24,2012-12-10 22:47:24,Center a moving average on a Pandas timeseries object,<pandas>,,2.0,,
13557559,1,13567861.0,2012-11-26 01:36:13,1,78,"I can't figure out how to write/read a Series correctlyThe following (and many variations of it) results in the read series being different than the written seriesnote that the series is read into a DataFrame rather than a series

In [55]: s = pdSeries({'a': 1  'b': 2})

In [56]: s
Out[56]: 
a    1
b    2

In [57]: sto_csv('/tmp/scsv')

In [58]: !cat /tmp/scsv
a 1
b 2

In [59]: pdread_csv('/tmp/scsv')
Out[59]: 
   a  1
0  b  2
",993872,,390388.0,2012-11-26 07:20:31,2012-11-26 15:24:59,How to write/read pandas Series to/from csv?,<pandas>,2.0,,,
13675749,1,13708892.0,2012-12-03 00:58:53,2,147,"I have inhomogeneous ~secondly data with a time series index that looks like:

import numpy as np
import pandas as pd

dates = [pddatetime(2012  2  5  17 00 35 327000)  pddatetime(2012  2  5  17 00 37 325000) pddatetime(2012  2  5  17 00 37 776000) pddatetime(2012  2  5  17 00 38 233000) pddatetime(2012  2  5  17 00 40 946000) pddatetime(2012  2  5  17 00 41 327000) pddatetime(2012  2  5  17 00 42 06000) pddatetime(2012  2  5  17 00 44 99000) pddatetime(2012  2  5  17 00 44 99000) pddatetime(2012  2  5  17 00 46 289000) pddatetime(2012  2  5  17 00 49 96000) pddatetime(2012  2  5  17 00 53 240000)]

inhomogeneous_secondish_series = pdSeries(nprandomrandn(len(dates))  name='some_col'  index=pdDatetimeIndex(dates))


In [26]: inhomogeneous_secondish_series
Out[26]: 
2012-02-05 17:00:35327000   -0903398
2012-02-05 17:00:37325000    0535798
2012-02-05 17:00:37776000    0847231
2012-02-05 17:00:38233000   -1280244
2012-02-05 17:00:40946000    1330232
2012-02-05 17:00:41327000    2287555
2012-02-05 17:00:42003072   -1469432
2012-02-05 17:00:44099000   -1174953
2012-02-05 17:00:44099000   -1020135
2012-02-05 17:00:46289000   -0200043
2012-02-05 17:00:49096000   -0665699
2012-02-05 17:00:53240000    0748638
Name: some_col


Which I want to resample to say '5s' Normally I would do:

In [28]: inhomogeneous_secondish_seriesresample('5s')


This yields nicely resampled 5s data anchored to the 0th second; In the result each item in the index will be on a multiple of 5 seconds from the 0th second of a given minute:

2012-02-05 17:00:40   -0200153
2012-02-05 17:00:45   -0009347
2012-02-05 17:00:50   -0432871
2012-02-05 17:00:55    0748638
Freq: 5S


How would I instead have the resampled data anchored around the time of the most recent sample  so the index would look like:


2012-02-05 17:00:38240000  (some correct resample value)
2012-02-05 17:00:43240000  (some correct resample value)
2012-02-05 17:00:48240000  (some correct resample value)
2012-02-05 17:00:53240000  (some correct resample value)
Freq: 5S


I expect the answer probably lies in the loffset paramater for resample() but wondering if there is a more simple way than having to calculate the loffset prior to resampling Would I have to look at the latest sample  figure out it's offset from the nearest normal 5s frequency and feed that into the loffset?",128508,,1452002.0,2012-12-04 11:06:21,2012-12-04 17:50:04,Resample Searies/DataFrame with frequency anchored to specific time,<pandas>,1.0,2.0,,
13575090,1,13581730.0,2012-11-26 23:41:31,1,133,"Suppose I have a nested dictionary 'user_dict' with structure:

Level 1: UserId (Long Integer)

Level 2: Category (String)

Level 3: Assorted Attributes (floats  ints  etc)

For example  an entry of this dictionary would be:

user_dict[12] = {
    ""Category 1"": {""att_1"": 1  
                   ""att_2"": ""whatever""} 
    ""Category 2"": {""att_1"": 23  
                   ""att_2"": ""another""}}


each item in ""user_dict"" has the same structure and ""user_dict"" contains a large number of items which I want to feed to a pandas DataFrame  constructing the series from the attributes In this case a hierarchical index would be useful for the purpose

Specifically  my question is whether there exists a way to to help the DataFrame constructor understand that the series should be built from the values of the ""level 3"" in the dictionary?

If I try something like:

df = pandasDataFrame(users_summary)


The items in ""level 1"" (the user id's) are taken as columns  which is the opposite of what I want to achieve (have user id's as index) 

I know I could construct the series after iterating over the dictionary entries  but if there is a more direct way this would be very useful A similar question would be asking whether it is possible to construct a pandas DataFrame from json objects listed in a file ",1371091,,,,2012-11-27 10:05:17,Construct pandas DataFrame from items in nested dictionary,<python><data.frame><pandas>,1.0,1.0,,
13628860,1,13635082.0,2012-11-29 15:15:15,0,66,"I'm confused about the difference between Pandas Series objects when using reindex_like and related features  For example  consider the following Series objects:

>>> import numpy
>>> import pandas
>>> series = pandasSeries([1  2  3])
>>> x = pandasSeries([True])reindex_like(series)fillna(True)
>>> y = pandasSeries(True  index=seriesindex)
>>> x
0    True
1    True
2    True
>>> y
0    True
1    True
2    True


On the surface x and y appear to be identical in their contents and indexing  However  they must be different in some way because one of them causes an error when using numpylogical_and() and the other does not

>>> numpylogical_and(series  y)
0    True
1    True
2    True
>>> numpylogical_and(series  x)
Traceback (most recent call last):
  File """"  line 1  in 
    numpylogical_and(series  x)
AttributeError: logical_and


What is numpylogical() and complaining about here?  I don't see the difference between the two series  x and y  However  there must be some subtle difference

The Pandas documentation says the Series object is a valid argument to ""most NumPy functions""  Clearly this is true somewhat in this case  Apparently the creation mechanism makes x unusable to this particular numpy function

As a side-note  which of the two creation mechanisms  reindex_like() and the index argument are more efficient and idiomatic for this scenario?  Maybe there is another/better way I haven't considered also",1108031,,,,2012-11-29 21:29:18,AttributeError when using numpy.logical_and with Pandas Series object,<python><numpy><pandas>,1.0,5.0,,
13682044,1,13682381.0,2012-12-03 11:11:56,3,94,"I am looking for an efficient way to remove unwanted parts from strings in a DataFrame column

Data looks like:

    time    result
1    09:00   +52A
2    10:00   +62B
3    11:00   +44a
4    12:00   +30b
5    13:00   -110a


I need to trim these data to:

    time    result
1    09:00   52
2    10:00   62
3    11:00   44
4    12:00   30
5    13:00   110


I tried strlstrip('+-') and strrstrip('aAbBcC')  but got an error:
TypeError: wrapper() takes exactly 1 argument (2 given)

Any pointers would be greatly appreciated!",1872240,,,,2012-12-07 16:31:06,Pandas DataFrame: remove unwanted parts from strings in a column,<python><data.frame><pandas>,2.0,,,
13691485,1,,2012-12-03 20:52:48,0,116,"I am attempting to left merge two dataframes  but I am running into an issue I get only NaN's in columns that are in the right dataframe

This is what I did:

X = read_csv('fileAtxt' sep=' ' header=0);
print ""-----FILE DATA-----""
print X;
X = Xastype(object); # convert every column to string type? does it do it?
print ""-----INTERNALS-----""
pprint(vars(X));

Y = file_to_dataframe('fileBtxt' ' ' 0); 
print ""-----FILE DATA-----""
print Y;
print ""-----INTERNALS-----""
pprint(vars(Y));

Z = merge(X Y how='left');
print Z;
sysexit(); 

Y = file_to_dataframe('tmpchr20threshfrqcount' '\t' 0);
print Ydtypes;

def file_to_dataframe(filename sep header): # list of dict's
    i = 0; k = 0;
    cols = list();
    colNames = list();
    for line in fileinputinput([filename]):
        line = linerstrip('\n');
        lst = linesplit(sep);
        if i == header: #  row number to use as the column names
            for colName in lst:
                colNamesappend(colName);
        elif i > header:
            j = 0;
            record = dict();
            for j in range(0 len(lst)): # iterate over all tokens in the current line
                if j >= len(colNames):
                    colNamesappend('#Auto_Generated_Label_'+ str(k));
                    k += 1;
                record[colNames[j]] = lst[j];
            colsappend(record); # push the record onto stack
        i += 1;
    return DataFramefrom_records(cols);


Here's the output:

-----FILE DATA-----

   Chrom      Gene  Position


0     20    DZANK1  18446022


1     20      TGM6   2380332


2     20  C20orf96    271226


-----INTERNALS-----

{'_data': BlockManager


Items: array([Chrom  Gene  Position]  dtype=object)


Axis 1: array([0  1  2])


ObjectBlock: array([Chrom  Gene  Position]  dtype=object)  3 x 3  dtype object 


 '_item_cache': {}}


-----FILE DATA-----

  Chrom  Position Random


0    20  18446022    ABC


1    20   2380332    XYZ


2    20    271226    PQR


-----INTERNALS-----

{'_data': BlockManager


Items: array([Chrom  Position  Random]  dtype=object)


Axis 1: array([0  1  2])


ObjectBlock: array([Chrom  Position  Random]  dtype=object)  3 x 3  dtype object 


 '_item_cache': {}}



  Chrom      Gene  Position Random

0    20  C20orf96    271226    NaN

1    20      TGM6   2380332    NaN

2    20    DZANK1  18446022    NaN


As you see  there's a column of NaN's there where there should be values from Random column in Y Any ideas on how to debug this?",1867185,,1867185.0,2012-12-07 22:02:22,2012-12-13 02:37:59,Issue with merging two dataframes in Pandas,<python><data><pandas>,1.0,4.0,,
13726573,1,,2012-12-05 15:24:42,1,59,I have a 1 min resolution time series contained in a pandas data frame What is the easiest (and most efficient) way for me to pad these times series in such way that on each date present in the data frame I have 1 min time steps for all 1 min intervals (so the date would have 24 hours worth of 1 min data steps)? If there is no data for a given point in time  it should have an NA instead of a value For example  if I have data for 11-Nov-2012 from 2am to 6pm and data for 16-Nov-2012 from 3pm to 11pm  I want 24 hours of 1 min data points for both dates with NAs attached to the time stamps where there is no data,1879493,,,,2012-12-05 22:50:14,Create a 24 hour 1 min resolution data set in pandas,<python><pandas><time-series>,1.0,1.0,,
13728896,1,13729448.0,2012-12-05 17:24:41,1,85,"Suppose I have two DataFrames a & b where a is larger than b and has all NaNs bindex is a subset of aindex  however b has real values I wish to merge the values from b into a 

In [102]: mset

Out[102]: 

DatetimeIndex: 9446 entries  2012-11-02 07:00:00 to 2012-11-05 15:24:00
Data columns:
open     9207  non-null values
high     9207  non-null values
low      9207  non-null values
close    9207  non-null values
dtypes: float64(4)

In [103]: a
Out[103]: 

DatetimeIndex: 1440 entries  2012-11-14 00:00:00 to 2012-11-14 23:59:00
Freq: T
Data columns:
open     0  non-null values
high     0  non-null values
low      0  non-null values
close    0  non-null values
dtypes: float64(4)


There's an example of what the dataframes look like 

EDIT:

I'd also like to retain the index as well",287950,,287950.0,2012-12-05 17:32:17,2012-12-05 17:56:22,Replacing NaN values with actual values with two pandas DataFrame,<python><pandas>,1.0,,1.0,
13689512,1,13689607.0,2012-12-03 18:35:28,5,94,"I want to use numpydiff on a pandas Series Am I right that this is a bug? Or am I doing it wrong?

In [163]: s = Series(nparange(10))

In [164]: npdiff(s)
Out[164]: 
0   NaN
1     0
2     0
3     0
4     0
5     0
6     0
7     0
8     0
9   NaN

In [165]: npdiff(nparange(10))
Out[165]: array([1  1  1  1  1  1  1  1  1])


I am using pandas 091rc1  numpy 161",1221924,,,,2012-12-03 18:57:49,numpy diff on a pandas Series,<python><numpy><pandas>,1.0,,,
13728208,1,13728364.0,2012-12-05 16:48:06,2,102,"I have problem merging several time series to a common DataFrame The example code I'm using:

import pandas
import datetime
import numpy as np

start = datetimedatetime(2001  1  1)
end = datetimedatetime(2001  1  10)
dates = pandasdate_range(start  end)
serie_1 = pandasSeries(nprandomrandn(10)  index = dates)
start = datetimedatetime(2001  1  2)
end = datetimedatetime(2001  1  11)
dates = pandasdate_range(start  end)
serie_2 = pandasSeries(nprandomrandn(10)  index = dates)
start = datetimedatetime(2001  1  3)
end = datetimedatetime(2001  1  12)
dates = pandasdate_range(start  end)
serie_3 = pandasSeries(nprandomrandn(10)  index = dates)

print 'serie_1'
print serie_1
print 'serie_2'
print serie_2
print 'serie_3'
print serie_3

serie_4 = pandasconcat([serie_1 serie_2]  join='outer'  axis = 1)
print 'serie_4'
print serie_4
serie_5 = pandasconcat([serie_4  serie_3]  join='outer'  axis = 1)
print 'serie_5'
print serie_5


This gives me the error for serie_5 (the second concat): 

Traceback (most recent call last):
  File ""C:\Users\User\Workspaces\Python\Source\TestingPandaspy""  line 29  in 
    serie_5 = pandasconcat([serie_4  serie_3]  join='outer'  axis = 1)
  File ""C:\Python27\lib\site-packages\pandas\tools\mergepy""  line 878  in concat
    verify_integrity=verify_integrity)
  File ""C:\Python27\lib\site-packages\pandas\tools\mergepy""  line 948  in __init__
    selfnew_axes = self_get_new_axes()
  File ""C:\Python27\lib\site-packages\pandas\tools\mergepy""  line 1101  in _get_new_axes
    new_axes[i] = self_get_comb_axis(i)
  File ""C:\Python27\lib\site-packages\pandas\tools\mergepy""  line 1125  in _get_comb_axis
    all_indexes = [x_dataaxes[i] for x in selfobjs]
AttributeError: 'TimeSeries' object has no attribute '_data'


I would like the result to look something like this (with random values in column 2):

                 0         1         2
2001-01-01 -1224602       NaN       NaN
2001-01-02 -1747710 -2618369       NaN
2001-01-03 -0608578 -0030674 -1335857
2001-01-04  1503808 -0050492  1086147
2001-01-05  0593152  0834805 -1310452
2001-01-06 -0156984  0208565 -0972561
2001-01-07  0650264 -0340086  1562101
2001-01-08 -0063765 -0250005 -0508458
2001-01-09 -1092656 -1589261 -0481741
2001-01-10  0640306  0333527 -0111668
2001-01-11       NaN -1159637  0110722
2001-01-12       NaN       NaN -0409387


What is wrong? As I said  probablybasic but I can not figure it out and I'm a beginner",1795245,,,,2012-12-05 20:03:45,Several time series to DataFrame,<python><pandas><time-series>,1.0,,1.0,
13740672,1,13741439.0,2012-12-06 09:36:35,0,64,"I'd like to filter out weekend data and only look at data for weekdays (mon(0)-fri(4))  I'm new to pandas  what's the best way to accomplish this in pandas?

import datetime
from pandas import *

data = read_csv(""datacsv"")
datamy_dt 

Out[52]:
0     2012-10-01 02:00:39
1     2012-10-01 02:00:38
2     2012-10-01 02:01:05
3     2012-10-01 02:01:07
4     2012-10-01 02:02:03
5     2012-10-01 02:02:09
6     2012-10-01 02:02:03
7     2012-10-01 02:02:35
8     2012-10-01 02:02:33
9     2012-10-01 02:03:01
10    2012-10-01 02:08:53
11    2012-10-01 02:09:04
12    2012-10-01 02:09:09
13    2012-10-01 02:10:20
14    2012-10-01 02:10:45



I'd like to do something like:

weekdays_only = data[datamy_dtweekday() ",24718,,,,2012-12-06 10:19:32,in pandas how can I groupby weekday() for a datetime column?,<python><pandas>,1.0,1.0,1.0,
13737992,1,,2012-12-06 06:15:34,3,147,"Given a timeseries  s  with a datetime index I expected to be able to index the timeseries by the date string Am I misunderstanding how this should work?

import pandas as pd
url = 'http://ichartfinanceyahoocom/tablecsvs=SPY&d=12&e=4&f=2012&g=d&a=01&b=01&c=2001&ignore=csv'  
df = pdread_csv(url  index_col='Date'  parse_dates=True)  
s = df['Close']
s['2012-12-04']


Result:

TimeSeriesError                           Traceback (most recent call last)
 in ()
      2 df = pdread_csv(url  index_col='Date'  parse_dates=True)  
      3 s = df['Close']  
----> 4 s['2012-12-04']

    G:\Python27-32\lib\site-packages\pandas\core\seriespyc in __getitem__(self  key)
    468     def __getitem__(self  key):
    469         try:
--> 470             return selfindexget_value(self  key)
    471         except InvalidIndexError:
    472             pass

    G:\Python27-32\lib\site-packages\pandas\tseries\indexpyc in get_value(self  series  key)  

   1030 
   1031             try:
-> 1032                 loc = self_get_string_slice(key)
   1033                 return series[loc]
   1034             except (TypeError  ValueError  KeyError):

G:\Python27-32\lib\site-packages\pandas\tseries\indexpyc in _get_string_slice(self  key)
   1077         asdt  parsed  reso = parse_time_string(key  freq)
   1078         key = asdt
-> 1079         loc = self_partial_date_slice(reso  parsed)
   1080         return loc
   1081 

G:\Python27-32\lib\site-packages\pandas\tseries\indexpyc in _partial_date_slice(self  reso  parsed)
    992     def _partial_date_slice(self  reso  parsed):
    993         if not selfis_monotonic:
--> 994             raise TimeSeriesError('Partial indexing only valid for ordered '
    995                                   'time series')
    996 

TimeSeriesError: Partial indexing only valid for ordered time series 


To be more specific (and perhaps pedantic)  what's the difference between the 2 Timeseries here:

import pandas as pd
url = 'http://ichartfinanceyahoocom/tablecsv?        s=SPY&d=12&e=4&f=2012&g=d&a=01&b=01&c=2001&ignore=csv'
s = pdread_csv(url  index_col='Date'  parse_dates=True)['Close']
rng = date_range(start='2011-01-01'  end='2011-12-31')
ts = Series(randn(len(rng))  index=rng)
print ts__class__
print tsindex[0]__class__
print s1__class__
print s1index[0]__class__
print ts[tsindex[0]]
print s[sindex[0]]
print ts['2011-01-01']
try:
    print s['2012-12-05']
except:
    print ""doesn't work"" 


Result:


-0608673793503
1415
-0608673793503
doesn't work
",1878647,,1878647.0,2012-12-06 09:55:12,2013-01-06 04:13:00,Indexing timeseries by date string,<python><pandas><time-series>,3.0,1.0,,
13753213,1,13753560.0,2012-12-06 21:47:51,1,59,"I'm reading Python for Data Analysis by Wes Mckinney  but I was surprised by this data manipulation You can see all the procedure here but I will try to summarize it here Assume you have something like this: 

In [133]: agg_counts = by_tz_ossize()unstack()fillna(0)
    Out[133]:
    a                 Not Windows   Windows
    tz                  245          276
    Africa/Cairo         0            3
    Africa/Casablanca    0            1
    Africa/Ceuta         0            2
    Africa/Johannesburg  0            1
    Africa/Lusaka        0            1
    America/Anchorage    4            1
    


tz means time zone and Not Windows and Windows are categories extracted from the User Agent in the original data  so we can see that there are 3 Windows users and 0 Non-windows users in Africa/Cairo from the data collected

Then in order to get ""the top overall time zones"" we have:

In [134]: indexer = agg_countssum(1)argsort()
Out[134]:
tz
                                  24
Africa/Cairo                      20
Africa/Casablanca                 21
Africa/Ceuta                      92
Africa/Johannesburg               87
Africa/Lusaka                     53
America/Anchorage                 54
America/Argentina/Buenos_Aires    57
America/Argentina/Cordoba         26
America/Argentina/Mendoza         55
America/Bogota                    62



So at that point  I would have thought that according to the documentation I was summing over columns (in sum(1)) and then sorting according to the result showing arguments (as usual in argsort) First of all  I'm not sure what does it mean ""columns"" in the context of this series because sum(1) is actually summing Not Windows and Windows users keeping that value in the same row as its time zone Furthermore  I can't see a correlation between argsort values and agg_counts For example  Pacific/Auckland has an ""argsort value"" (in In[134]) of 0 and it only has a sum of 11 Windows and Not Windows users Asia/Harbin has an argsort value of 1 and appears with a sum of 3 Windows and Not Windows users

Can someone explain to me what is going on there? Obviously I'm misunderstanding something

Thanks",460147,,460147.0,2012-12-06 21:59:52,2012-12-06 22:46:46,Weird Data manipulation in Pandas,<python><data><pandas>,1.0,,,
13446480,1,13447176.0,2012-11-19 01:20:07,1,125,"I'm trying to remove entries from a data frame which occur less than 100 times 
The data frame data looks like this:

pid   tag
1     23    
1     45
1     62
2     24
2     45
3     34
3     25
3     62


Now I count the number of tag occurrences like this:

bytag = datagroupby('tag')aggregate(npcount_nonzero)


But then I can't figure out how to remove those entries which have low count",1564449,,,,2012-11-25 12:31:33,Python Pandas: remove entries based on the number of occurrences,<python><numpy><python-2.7><pandas>,3.0,,2.0,
13580875,1,13582115.0,2012-11-27 09:19:07,1,178,"Given a data set with something like:

[2  3  4  5  6  7  8  9  10  15  20  25  30  35  40  45  50  55  65  75  85  86  87  88]


The values are always increasing (in fact it's time)  and I want to find out a running average distance between the values I am in effect trying to determine when the data goes from ""1 every second"" to ""1 every 5 seconds"" (or any other value)

I am implementing this in Python  but a solution in any language is most welcome

The output I am looking for the sample input above  would be something like:

[(2  1)  (10  5)  (55  10)  (85  1) ]


where  ""2"" would indicate where the distance between values started out being ""1"" and 
and ""10"" would indicate where the distance shifted to being ""5"" (It would have to be exactly there  if the shift was detected a step later  wouldn't matter)

I am looking for when the average distance between values changes I realize there will be some kind of trade off between stability of the algorithm and sensitivity to changes in the input 

(Is Pandas or NumPy useful for this btw?)",193892,,193892.0,2012-11-27 12:19:17,2012-11-29 21:15:34,Running average / frequency of time series data?,<python><algorithm><numpy><pandas><average>,5.0,9.0,1.0,
13592618,1,13593942.0,2012-11-27 20:38:31,0,61,"I am using multiple threads to access and delete data in my pandas dataframe Because of this  I am wondering is pandas dataframe threadsafe?

Thanks!
Andrew",1449148,,,,2012-11-27 22:01:48,python pandas dataframe thread safe?,<python><thread-safety><pandas>,2.0,,,
13701035,1,13755051.0,2012-12-04 10:39:28,1,47,"When I have a pandasDataFrame df with columns [""A""  ""B""  ""C""  ""D""]  I can filter it using constructions like df[df[""B""] == 2]

How do I do the equivalent of df[df[""B""] == 2]  if B is the name of a level in a MultiIndex instead? (For example  obtained by dfgroupby([""A""  ""B""])mean() or dfsetindex([""A""  ""B""]))",1274613,,,,2012-12-07 00:23:33,boolean indexing on index (instead of dataframe),<python><pandas>,3.0,,,
13751926,1,,2012-12-06 20:25:00,1,193,"I want to use Pandas to work with series in real-time Every second  I need to add the latest observation to an existing series My series are grouped into a DataFrame and stored in an HDF5 file

Here's how I do it at the moment:

>> existing_series = Series([7 13 97]  [0 1 2]) 
>> updated_series = existing_seriesappend( Series([111]  [3]) )


Is this the most efficient way? I've read countless posts but cannot find any that focuses on efficiency with high-frequency data

Edit: I just read about modules shelve and pickle It seems like they would achieve what I'm trying to do  basically save lists on disks Because my lists are large  is there any way not to load the full list into memory but  rather  efficiently append values one at a time?",1883571,,1883571.0,2012-12-07 11:08:47,2012-12-10 00:21:51,Efficiently add single row to Pandas Series or DataFrame,<python><performance><pandas><time-series>,1.0,4.0,,
13757239,1,13761574.0,2012-12-07 05:06:22,-1,87,"I have a pandas DataFrame with 18 columns and about 10000 rows

My first 3 columns have separate values for YEAR  MONTH  and DAY I need to merge these three columns and have the entire date in one column for all the rows

My code so far is:

dfmerge('Year' '/' 'Month')
",781329,,1240268.0,2012-12-07 10:55:12,2012-12-07 10:55:44,Combining rows in DataFrame,<python><numpy><python-3.x><python-2.7><pandas>,1.0,6.0,,
13757490,1,,2012-12-07 05:33:43,0,32,"Possible Duplicate:Combining rows in DataFrame  




I have a DataFrame with 18 columns and about 10000 rows

My first 3 columns have separate values for YEAR  MONTH  and DAY  I need to merge these 3 columns and have the whole date in 1 column for all the rows I am trying to but am unable to do it

My code:

temp = read_table(folder + r'\BBE_12-11-13_0731_edited_2lvm'  sep=r'\t')

cols = ['Year' 

    'Month' 
    'Day' 
    'Hour'
    'Minute' 
    'Seconde' 
    'Depth (m)' 
    'Temperature (Deg C)' 
    'Green (g_l)' 
    'Blue_Green (g_l)' 
    'Diatom (g_l)' 
    'Crypto (g_l)' 
    'Class5 (g_l)' 
    'Class6 (g_l)' 
    'Class7 (g_l)' 
    'Yellow (g_l)' 
    'Total Concentration (incl Yellow) (g_l)' 
    'Total Concentration (g_l)' 
    'Transmission (%)']


df = DataFrame(data = temp)
dfcolumns = cols


I am trying to Merge by using dfmerge('Year' '/' 'Month' '/' 'Day')
But it is not workingPLEASE HELP!!!

Thanks",781329,,317076.0,2012-12-07 05:39:00,2012-12-07 05:39:00,Merge Columns using Python - Pandas DataFrame,<python><python-3.x><python-2.7><data.frame><pandas>,,1.0,,2012-12-07 08:53:22
13762121,1,13764242.0,2012-12-07 11:23:51,0,58,"I all 

I have two dataframes in Pandas:

a:

In [96]: a
Out[96]: 
      count  mean   std  min    max   25%   50%  75%
10m  604656  419  243    0  2592  243  371  55

In [98]: ato_dict()
Out[98]: 
{'25%': {'10m': 2429999828338623} 
 '50%': {'10m': 37100000381469727} 
 '75%': {'10m': 55} 
 'count': {'10m': 6046560} 
 'max': {'10m': 25920000076293945} 
 'mean': {'10m': 41893915969076261} 
 'min': {'10m': 00} 
 'std': {'10m': 24321994530033586}}


and b:

In [97]: b
Out[97]: 
              count  mean   std  min    max   25%   50%   75%
00_900     119842  334  172    0  1437  208  306  437
1800_2700  234074  511  282    0  2592  299  464  682
2700_3600  126376  379  219    0  1955  212  340  513
900_1800   124364  367  183    0  1493  237  340  470

In [99]: bto_dict()
Out[99]: 
{'25%': {'00_900': 20799999237060547 
  '1800_2700': 29900000095367432 
  '2700_3600': 2119999885559082 
  '900_1800': 23681280016899109} 
 '50%': {'00_900': 30579087734222412 
  '1800_2700': 46399998664855957 
  '2700_3600': 34000000953674316 
  '900_1800': 34006340503692627} 
 '75%': {'00_900': 4369999885559082 
  '1800_2700': 68199996948242188 
  '2700_3600': 5130000114440918 
  '900_1800': 46960808038711548} 
 'count': {'00_900': 1198420 
  '1800_2700': 2340740 
  '2700_3600': 1263760 
  '900_1800': 1243640} 
 'max': {'00_900': 14369999885559082 
  '1800_2700': 25920000076293945 
  '2700_3600': 19549999237060547 
  '900_1800': 14930000305175781} 
 'mean': {'00_900': 33417930869221379 
  '1800_2700': 51125810579269269 
  '2700_3600': 37938859684522601 
  '900_1800': 3670476718299061} 
 'min': {'00_900': 00 
  '1800_2700': 00 
  '2700_3600': 00 
  '900_1800': 00} 
 'std': {'00_900': 17153268584149644 
  '1800_2700': 28194581011555386 
  '2700_3600': 21909571297061241 
  '900_1800': 18334834361369423}}


I would like to merge the two dataframe in a new dataframe with a multi-index like:

new_df:

                   count  mean   std  min    max   25%   50%   75%
10m   all          604656  419  243    0  2592  243  371  55
      00_900     119842  334  172    0  1437  208  306  437
      1800_2700  234074  511  282    0  2592  299  464  682
      2700_3600  126376  379  219    0  1955  212  340  513
      900_1800   124364  367  183    0  1493  237  340  470


The element with multi-index ('10m' 'all') is a and the next rows are b

Does anybody has an idea on how to achieve this in Pandas?

Thanks a lot 

Greg

EDIT:

Hello All 

I moved forward and extended the heights I now have an issue because heights and sectors are not sorted from low to high heights  and from low sectors to high sectors

This is what I get:

In [141]: df_stats_windSpeed
Out[142]:
                        count  mean   std  min    max   25%   50%    75%
Height Sector                                                           
10m    All             604656  419  243    0  2592  243  371   550
       [00  900[     119842  334  172    0  1437  208  306   437
       [1800  2700[  234074  511  282    0  2592  299  464   682
       [2700  3600]  126376  379  219    0  1955  212  340   513
       [900  1800[   124364  367  183    0  1493  237  340   470
140m   All             604656  785  363    0  3520  519  770  1020
       [00  900[     116374  669  289    0  2286  449  667   880
       [1800  2700[  243590  895  383    0  3520  629  886  1137
       [2700  3600]  135292  722  340    0  2981  484  698   923
       [900  1800[   109400  739  346    0  2091  462  731  1010
200m   All             604656  847  408    0  3488  538  821  1120
       [00  900[     113475  707  325    0  2456  457  692   945
       [1800  2700[  242157  980  430    0  3488  665  974  1271
       [2700  3600]  143254  775  374    0  3373  508  748  1000
       [900  1800[   105770  793  396    0  2154  475  761  1086
20m    All             604656  482  260    0  2769  299  433   618
       [00  900[     116748  391  181    0  1559  264  365   495
       [1800  2700[  235304  583  299    0  2769  367  532   761
       [2700  3600]  126961  435  234    0  2165  261  393   571
       [900  1800[   125643  422  196    0  1598  286  398   529
40m    All             604656  568  277    0  2939  381  529   710
       [00  900[     120426  480  199    0  1769  346  469   597
       [1800  2700[  238381  665  313    0  2939  448  616   845
       [2700  3600]  128104  536  263    0  2519  355  498   681
       [900  1800[   117745  496  211    0  1679  349  490   622
80m    All             604656  684  312    0  3228  469  666   865
       [00  900[     119330  591  244    0  2074  416  595   758
       [1800  2700[  239146  780  338    0  3228  554  754   974
       [2700  3600]  133220  642  302    0  2794  437  615   811
       [900  1800[   112960  629  271    0  1953  424  638   828


I would like to sort the multi-index so that the height order is: 10  20  40  80  140 and 200m; and the sector: 'All' '[00  900[' '[900  1800[' '[1800  2700[' '[2700  3600]' I tried to reindex like this but it doesn't work:

In [255]: df_statsreindex(index=['10m' '20m' '40m' '80m' '200m' '140m'] level=0)
In [256]: df_statsreindex(index=['All' '[00  900[' '[900  1800[' '[1800  2700[' '[2700  3600]'] level=1)


This is the df dict:

In [257]: df_stats_windSpeedto_dict()
Out[257]: 
{'25%': {('10m'  'All'): 2429999828338623 
  ('10m'  '[00  900['): 20799999237060547 
  ('10m'  '[1800  2700['): 29900000095367432 
  ('10m'  '[2700  3600]'): 2119999885559082 
  ('10m'  '[900  1800['): 23681280016899109 
  ('140m'  'All'): 51884875297546387 
  ('140m'  '[00  900['): 44935483932495117 
  ('140m'  '[1800  2700['): 62855626344680786 
  ('140m'  '[2700  3600]'): 48426017761230469 
  ('140m'  '[900  1800['): 46205065250396729 
  ('200m'  'All'): 53844937086105347 
  ('200m'  '[00  900['): 4572603702545166 
  ('200m'  '[1800  2700['): 66515130996704102 
  ('200m'  '[2700  3600]'): 50821070671081543 
  ('200m'  '[900  1800['): 4749258279800415 
  ('20m'  'All'): 29900000095367432 
  ('20m'  '[00  900['): 26400001049041748 
  ('20m'  '[1800  2700['): 36700000762939453 
  ('20m'  '[2700  3600]'): 26099998950958252 
  ('20m'  '[900  1800['): 28554879426956177 
  ('40m'  'All'): 38135370016098022 
  ('40m'  '[00  900['): 34552559852600098 
  ('40m'  '[1800  2700['): 44779624938964844 
  ('40m'  '[2700  3600]'): 35464469790458679 
  ('40m'  '[900  1800['): 34928045272827148 
  ('80m'  'All'): 46858876943588257 
  ('80m'  '[00  900['): 41649158000946045 
  ('80m'  '[1800  2700['): 55375603437423706 
  ('80m'  '[2700  3600]'): 43738168478012085 
  ('80m'  '[900  1800['): 42378913164138794} 
 '50%': {('10m'  'All'): 37100000381469727 
  ('10m'  '[00  900['): 30579087734222412 
  ('10m'  '[1800  2700['): 46399998664855957 
  ('10m'  '[2700  3600]'): 34000000953674316 
  ('10m'  '[900  1800['): 34006340503692627 
  ('140m'  'All'): 7701094388961792 
  ('140m'  '[00  900['): 66736810207366943 
  ('140m'  '[1800  2700['): 88593416213989258 
  ('140m'  '[2700  3600]'): 69792094230651855 
  ('140m'  '[900  1800['): 73094825744628906 
  ('200m'  'All'): 82149920463562012 
  ('200m'  '[00  900['): 69200782775878906 
  ('200m'  '[1800  2700['): 97363834381103516 
  ('200m'  '[2700  3600]'): 74800474643707275 
  ('200m'  '[900  1800['): 76083860397338867 
  ('20m'  'All'): 43299999237060547 
  ('20m'  '[00  900['): 36500000953674316 
  ('20m'  '[1800  2700['): 53199996948242187 
  ('20m'  '[2700  3600]'): 3929999828338623 
  ('20m'  '[900  1800['): 39796528816223145 
  ('40m'  'All'): 5291872501373291 
  ('40m'  '[00  900['): 4692425012588501 
  ('40m'  '[1800  2700['): 61558408737182617 
  ('40m'  '[2700  3600]'): 49811406135559082 
  ('40m'  '[900  1800['): 48983759880065918 
  ('80m'  'All'): 66626186370849609 
  ('80m'  '[00  900['): 59457294940948486 
  ('80m'  '[1800  2700['): 7544825553894043 
  ('80m'  '[2700  3600]'): 61506271362304687 
  ('80m'  '[900  1800['): 63810868263244629} 
 '75%': {('10m'  'All'): 55 
  ('10m'  '[00  900['): 4369999885559082 
  ('10m'  '[1800  2700['): 68199996948242188 
  ('10m'  '[2700  3600]'): 5130000114440918 
  ('10m'  '[900  1800['): 46960808038711548 
  ('140m'  'All'): 10203519582748413 
  ('140m'  '[00  900['): 87971394062042236 
  ('140m'  '[1800  2700['): 11370761156082153 
  ('140m'  '[2700  3600]'): 92274019718170166 
  ('140m'  '[900  1800['): 10097956657409668 
  ('200m'  'All'): 11203938484191895 
  ('200m'  '[00  900['): 94468526840209961 
  ('200m'  '[1800  2700['): 12706465721130371 
  ('200m'  '[2700  3600]'): 10000725984573364 
  ('200m'  '[900  1800['): 10862814903259277 
  ('20m'  'All'): 6179999828338623 
  ('20m'  '[00  900['): 49499998092651367 
  ('20m'  '[1800  2700['): 76100001335144043 
  ('20m'  '[2700  3600]'): 57100000381469727 
  ('20m'  '[900  1800['): 52929890155792236 
  ('40m'  'All'): 70959796905517578 
  ('40m'  '[00  900['): 59702688455581665 
  ('40m'  '[1800  2700['): 84523344039916992 
  ('40m'  '[2700  3600]'): 68096575736999512 
  ('40m'  '[900  1800['): 62155957221984863 
  ('80m'  'All'): 86509017944335938 
  ('80m'  '[00  900['): 75837295055389404 
  ('80m'  '[1800  2700['): 97384757995605469 
  ('80m'  '[2700  3600]'): 81105818748474121 
  ('80m'  '[900  1800['): 82832918167114258} 
 'count': {('10m'  'All'): 6046560 
  ('10m'  '[00  900['): 1198420 
  ('10m'  '[1800  2700['): 2340740 
  ('10m'  '[2700  3600]'): 1263760 
  ('10m'  '[900  1800['): 1243640 
  ('140m'  'All'): 6046560 
  ('140m'  '[00  900['): 1163740 
  ('140m'  '[1800  2700['): 2435900 
  ('140m'  '[2700  3600]'): 1352920 
  ('140m'  '[900  1800['): 1094000 
  ('200m'  'All'): 6046560 
  ('200m'  '[00  900['): 1134750 
  ('200m'  '[1800  2700['): 2421570 
  ('200m'  '[2700  3600]'): 1432540 
  ('200m'  '[900  1800['): 1057700 
  ('20m'  'All'): 6046560 
  ('20m'  '[00  900['): 1167480 
  ('20m'  '[1800  2700['): 2353040 
  ('20m'  '[2700  3600]'): 1269610 
  ('20m'  '[900  1800['): 1256430 
  ('40m'  'All'): 6046560 
  ('40m'  '[00  900['): 1204260 
  ('40m'  '[1800  2700['): 2383810 
  ('40m'  '[2700  3600]'): 1281040 
  ('40m'  '[900  1800['): 1177450 
  ('80m'  'All'): 6046560 
  ('80m'  '[00  900['): 1193300 
  ('80m'  '[1800  2700['): 2391460 
  ('80m'  '[2700  3600]'): 1332200 
  ('80m'  '[900  1800['): 1129600} 
 'max': {('10m'  'All'): 25920000076293945 
  ('10m'  '[00  900['): 14369999885559082 
  ('10m'  '[1800  2700['): 25920000076293945 
  ('10m'  '[2700  3600]'): 19549999237060547 
  ('10m'  '[900  1800['): 14930000305175781 
  ('140m'  'All'): 35195941925048828 
  ('140m'  '[00  900['): 2286467170715332 
  ('140m'  '[1800  2700['): 35195941925048828 
  ('140m'  '[2700  3600]'): 29814235687255859 
  ('140m'  '[900  1800['): 20905771255493164 
  ('200m'  'All'): 34877243041992188 
  ('200m'  '[00  900['): 24561836242675781 
  ('200m'  '[1800  2700['): 34877243041992188 
  ('200m'  '[2700  3600]'): 33732143402099609 
  ('200m'  '[900  1800['): 21536584854125977 
  ('20m'  'All'): 27689998626708984 
  ('20m'  '[00  900['): 15589999198913574 
  ('20m'  '[1800  2700['): 27689998626708984 
  ('20m'  '[2700  3600]'): 21649999618530273 
  ('20m'  '[900  1800['): 15979999542236328 
  ('40m'  'All'): 29387109756469727 
  ('40m'  '[00  900['): 17693622589111328 
  ('40m'  '[1800  2700['): 29387109756469727 
  ('40m'  '[2700  3600]'): 25192754745483398 
  ('40m'  '[900  1800['): 16793560028076172 
  ('80m'  'All'): 32280239105224609 
  ('80m'  '[00  900['): 20743719100952148 
  ('80m'  '[1800  2700['): 32280239105224609 
  ('80m'  '[2700  3600]'): 27942413330078125 
  ('80m'  '[900  1800['): 19532955169677734} 
 'mean': {('10m'  'All'): 41893915969076261 
  ('10m'  '[00  900['): 33417930869221379 
  ('10m'  '[1800  2700['): 51125810579269269 
  ('10m'  '[2700  3600]'): 37938859684522601 
  ('10m'  '[900  1800['): 3670476718299061 
  ('140m'  'All'): 78465228797278623 
  ('140m'  '[00  900['): 66866495964827086 
  ('140m'  '[1800  2700['): 89531109376609503 
  ('140m'  '[2700  3600]'): 72187838345351443 
  ('140m'  '[900  1800['): 73927146469551381 
  ('200m'  'All'): 84738967491657924 
  ('200m'  '[00  900['): 70707511105622967 
  ('200m'  '[1800  2700['): 97955929565714968 
  ('200m'  '[2700  3600]'): 77549135952858128 
  ('200m'  '[900  1800['): 79270609315399172 
  ('20m'  'All'): 48153452911920738 
  ('20m'  '[00  900['): 39108075360827947 
  ('20m'  '[1800  2700['): 58345712321566516 
  ('20m'  '[2700  3600]'): 43517044317594324 
  ('20m'  '[900  1800['): 42155453833197427 
  ('40m'  'All'): 56803902578298446 
  ('40m'  '[00  900['): 48012498160193742 
  ('40m'  '[1800  2700['): 66519476987395914 
  ('40m'  '[2700  3600]'): 53629782195278182 
  ('40m'  '[900  1800['): 49579161339061493 
  ('80m'  'All'): 68429105603612399 
  ('80m'  '[00  900['): 59149683147210084 
  ('80m'  '[1800  2700['): 78021544717360065 
  ('80m'  '[2700  3600]'): 6424779540308954 
  ('80m'  '[900  1800['): 62855045603079853} 
 'min': {('10m'  'All'): 00 
  ('10m'  '[00  900['): 00 
  ('10m'  '[1800  2700['): 00 
  ('10m'  '[2700  3600]'): 00 
  ('10m'  '[900  1800['): 00 
  ('140m'  'All'): 00 
  ('140m'  '[00  900['): 00 
  ('140m'  '[1800  2700['): 00 
  ('140m'  '[2700  3600]'): 00 
  ('140m'  '[900  1800['): 00 
  ('200m'  'All'): 00 
  ('200m'  '[00  900['): 00 
  ('200m'  '[1800  2700['): 00 
  ('200m'  '[2700  3600]'): 00 
  ('200m'  '[900  1800['): 00 
  ('20m'  'All'): 00 
  ('20m'  '[00  900['): 00 
  ('20m'  '[1800  2700['): 00 
  ('20m'  '[2700  3600]'): 00 
  ('20m'  '[900  1800['): 00 
  ('40m'  'All'): 00 
  ('40m'  '[00  900['): 00 
  ('40m'  '[1800  2700['): 00 
  ('40m'  '[2700  3600]'): 00 
  ('40m'  '[900  1800['): 00 
  ('80m'  'All'): 00 
  ('80m'  '[00  900['): 00 
  ('80m'  '[1800  2700['): 00 
  ('80m'  '[2700  3600]'): 00 
  ('80m'  '[900  1800['): 00} 
 'std': {('10m'  'All'): 24321994530033586 
  ('10m'  '[00  900['): 17153268584149644 
  ('10m'  '[1800  2700['): 28194581011555386 
  ('10m'  '[2700  3600]'): 21909571297061241 
  ('10m'  '[900  1800['): 18334834361369423 
  ('140m'  'All'): 36272652696793761 
  ('140m'  '[00  900['): 28894363480141649 
  ('140m'  '[1800  2700['): 38302160204846252 
  ('140m'  '[2700  3600]'): 34038884427629861 
  ('140m'  '[900  1800['): 3463171121328295 
  ('200m'  'All'): 40834920171291111 
  ('200m'  '[00  900['): 3246180377116834 
  ('200m'  '[1800  2700['): 42979603238677564 
  ('200m'  '[2700  3600]'): 37366849435738714 
  ('200m'  '[900  1800['): 39631501181722597 
  ('20m'  'All'): 25956531035815531 
  ('20m'  '[00  900['): 18115698416394523 
  ('20m'  '[1800  2700['): 29884465540389979 
  ('20m'  '[2700  3600]'): 2342034699432777 
  ('20m'  '[900  1800['): 19553532925384289 
  ('40m'  'All'): 27650269372360587 
  ('40m'  '[00  900['): 19926422334110316 
  ('40m'  '[1800  2700['): 31345356834325013 
  ('40m'  '[2700  3600]'): 26327655213933481 
  ('40m'  '[900  1800['): 21057487187047053 
  ('80m'  'All'): 31164449954856375 
  ('80m'  '[00  900['): 24419473697940042 
  ('80m'  '[1800  2700['): 33838903052504601 
  ('80m'  '[2700  3600]'): 3017648294312663 
  ('80m'  '[900  1800['): 2707882324438323}}


Anybody knows how to reindex this dataframe to have the index levels in sorted order?

Thanks",1613796,,1613796.0,2012-12-14 16:07:57,2012-12-14 16:07:57,merge two dataframe and create a new one with multiindex,<python><data.frame><pandas><multi-index>,3.0,2.0,1.0,
13783721,1,13784026.0,2012-12-09 01:15:12,3,93,"I have a dataset of the following form dropbox download (23kb csv)

The sample rate of the data varies from second to second from 0Hz to over 200Hz in some cases  the highest rate of samples in the data set provided is about 50 samples per second

When samples are taken they are always even spread across the second for example

time                   x
2012-12-06 21:12:40    12875909883327378
2012-12-06 21:12:40     32799224301545976
2012-12-06 21:12:40     98932953779777989
2012-12-06 21:12:43    13207033814856786
2012-12-06 21:12:43    13207033814856786
2012-12-06 21:12:43     6571691352191452
2012-12-06 21:12:44    1171350194748169
2012-12-06 21:12:45     13095622561808861
2012-12-06 21:12:47     61295242676059246
2012-12-06 21:12:48     94774064119961352
2012-12-06 21:12:49     80169378222553533
2012-12-06 21:12:49     80291142695702533
2012-12-06 21:12:49    13655650749231367
2012-12-06 21:12:49    12729790925838365


should be

time                        x
2012-12-06 21:12:40 000ms   12875909883327378
2012-12-06 21:12:40 333ms    32799224301545976
2012-12-06 21:12:40 666ms    98932953779777989
2012-12-06 21:12:43 000ms   13207033814856786
2012-12-06 21:12:43 333ms   13207033814856786
2012-12-06 21:12:43 666ms    6571691352191452
2012-12-06 21:12:44 000ms   1171350194748169
2012-12-06 21:12:45 000ms    13095622561808861
2012-12-06 21:12:47 000ms    61295242676059246
2012-12-06 21:12:48 000ms    94774064119961352
2012-12-06 21:12:49 000ms    80169378222553533
2012-12-06 21:12:49 250ms    80291142695702533
2012-12-06 21:12:49 500ms   13655650749231367
2012-12-06 21:12:49 750ms   12729790925838365


is there an easy way to use the pandas timeseries resampling function or is there some thing built into numpy or scipy that will work?",1638876,,,,2012-12-09 17:05:45,Timeseries Resampling,<python><numpy><pandas><time-series>,1.0,,,
13651117,1,13653490.0,2012-11-30 18:38:41,0,83,"How can I filter which lines of a CSV to be loaded into memory using pandas?  This seems like an option that one should find in read_csv  Am I missing something?

Example: we've a CSV with a timestamp column and we'd like to load just the lines that with a timestamp greater than a given constant",825594,,,,2012-11-30 21:31:28,pandas: filter lines on load in read_csv,<pandas>,3.0,,,
13653030,1,13653249.0,2012-11-30 20:54:35,1,66,"I realize Dataframe takes a map of {'series_name':Series(data  index)}  However  it automatically sorts that map even if the map is an OrderedDict()

Is there a simple way to pass a list of Series(data  index  name=name) such that the order is preserved and the column names are the seriesname?  Is there an easy way if all the indices are the same for all the series? 

I normally do this by just passing a numpy column_stack of seriesvalues and specifying the column names  However  this is ugly and in this particular case the data is strings not floats",1618041,,,,2012-12-13 03:09:14,How do I Pass a List of Series to a Pandas DataFrame?,<python><pandas>,3.0,1.0,,
13686849,1,,2012-12-03 15:51:59,0,41,"I have the following data:

Start Time=2012-04-12   16:13:09    
Finish Time=2012-11-30  13:31:08    
Sample Period=01:00:00      


As a CSV file:

Date(yyyy-mm-dd)    Time(hh:mm:ss)  Celsius (C)
2012-04-12          16:13:09        206
2012-04-12          17:13:09        206
2012-04-12          18:13:09        206
2012-04-12          19:13:09        206
2012-04-12          20:13:09        206
2012-04-12          21:13:09        206
2012-04-12          22:13:09        206
2012-04-12          23:13:09        206


and now I want to read-in data by pandas like this:

df=read_csv('mmf0401txt' skiprows=(0 5) parse_dates=[[0 1]] index_col=0)


Unfortunately  it raises an exception Index (columns [0]) have duplicate values ['2012-04-12']

I don't know why  how can I correct this?",1843099,,1240268.0,2012-12-03 16:01:24,2012-12-03 16:01:24,Index (columns [0]) have duplicate values,<pandas>,,2.0,,
13703720,1,13704307.0,2012-12-04 13:08:29,4,392,"How do I convert a numpydatetime64 object to a datetimedatetime (or Timestamp)?

In the following code  I create a datetime  timestamp and datetime64 objects

import datetime
import numpy as np
import pandas as pd
dt = datetimedatetime(2012  5  1)
# A strange way to extract a Timestamp object  there's surely a better way?
ts = pdDatetimeIndex([dt])[0]
dt64 = npdatetime64(dt)

In [7]: dt
Out[7]: datetimedatetime(2012  5  1  0  0)

In [8]: ts
Out[8]: 

In [9]: dt64
Out[9]: numpydatetime64('2012-05-01T01:00:00000000+0100')


Note: it's easy to get the datetime from the Timestamp:

In [10]: tsto_datetime()
Out[10]: datetimedatetime(2012  5  1  0  0)


But how do we extract the datetime or Timestamp from a numpydatetime64 (dt64)?



Update: a somewhat nasty example in my dataset (perhaps the motivating example) seems to be:

dt64 = numpydatetime64('2002-06-28T01:00:00000000000+0100')


which should be datetimedatetime(2002  6  28  1  0)  and not a long (!) (1025222400000000000L)",1240268,,1240268.0,2012-12-04 17:53:03,2012-12-06 22:40:22,"Converting between datetime, Timestamp and datetime64",<python><datetime><numpy><pandas>,4.0,,1.0,
13784192,1,13786327.0,2012-12-09 02:50:38,1,360,"I'm starting from the pandas Data Frame docs here: http://pandaspydataorg/pandas-docs/stable/dsintrohtml

I'd like to iteratively fill the Data Frame with values in a time series kind of calculation
So basically  I'd like to initialize  data frame with columns A B and timestamp rows  all 0 or all NaN

I'd then add initial values and go over this data calculating the new row from the row before  say row[A][t] = row[A][t-1]+1 or so

I'm currently using the code as below  but I feel it's kind of ugly and there must be a  way to do this with a data frame directly or just a better way in general
Note: I'm using Python 27

import datetime as dt
import pandas as pd
import scipy as s

if __name__ == '__main__':
    base = dtdatetimetoday()date()
    dates = [ base - dttimedelta(days=x) for x in range(0 10) ]
    datessort()

    valdict = {}
    symbols = ['A' 'B'  'C']
    for symb in symbols:
        valdict[symb] = pdSeries( szeros( len(dates))  dates )

    for thedate in dates:
        if thedate > dates[0]:
            for symb in valdict:
                valdict[symb][thedate] = 1+valdict[symb][thedate - dttimedelta(days=1)]

    print valdict
",1707931,,,,2012-12-09 09:40:46,"Creating an empty Pandas DataFrame, then filling it?",<python><data.frame><pandas>,1.0,,,
13690122,1,13693097.0,2012-12-03 19:18:44,1,141,"I have a CSV text file encoded in UTF-16 (so as to preserve Unicode characters when others use Excel) but when doing a read_csv with Pandas 090  I get this cryptic error:

df = pdread_csv('datatxt' encoding='utf-16' sep='\t' header=0)
dfhead()

---------------------------------------------------------------------------
Exception                                 Traceback (most recent call last)
 in ()
----> 1 df = pdread_csv('candidates-spanishtxt' encoding='utf-16' sep='\t' header=0)
  2 dfhead()

/Library/Frameworks/Pythonframework/Versions/27/lib/python27/site-packages/pandas/io/parserspyc in read_csv(filepath_or_buffer  sep  dialect  header  index_col  names  skiprows  na_values  keep_default_na  thousands  comment  parse_dates  keep_date_col  dayfirst  date_parser  nrows  iterator  chunksize  skip_footer  converters  verbose  delimiter  encoding  squeeze  **kwds)
248         kdict['delimiter'] = sep
249 
--> 250     return _read(TextParser  filepath_or_buffer  kdict)
251 
252 @Appender(_read_table_doc)

/Library/Frameworks/Pythonframework/Versions/27/lib/python27/site-packages/pandas/io/parserspyc in _read(cls  filepath_or_buffer  kwds)
198         return parser
199 
--> 200     return parserget_chunk()
201 
202 @Appender(_read_csv_doc)

/Library/Frameworks/Pythonframework/Versions/27/lib/python27/site-packages/pandas/io/parserspyc in get_chunk(self  rows)
853         elif not self_has_complex_date_col:
854             index = self_get_simple_index(alldata  columns)
--> 855             index = self_agg_index(index)
856 
857         elif self_has_complex_date_col:

/Library/Frameworks/Pythonframework/Versions/27/lib/python27/site-packages/pandas/io/parserspyc in _agg_index(self  index  try_parse_dates)
980                 arr  _ = _convert_types(arr  col_na_values)
981                 arraysappend(arr)
--> 982             index = MultiIndexfrom_arrays(arrays  names=selfindex_name)
983         return index
984 

/Library/Frameworks/Pythonframework/Versions/27/lib/python27/site-packages/pandas/core/indexpyc in from_arrays(cls  arrays  sortorder  names)
1570 
1571         return MultiIndex(levels=levels  labels=labels 
-> 1572                           sortorder=sortorder  names=names)
1573 
1574     @classmethod

/Library/Frameworks/Pythonframework/Versions/27/lib/python27/site-packages/pandas/core/indexpyc in __new__(cls  levels  labels  sortorder  names)
1254         assert(len(levels) == len(labels))
1255         if len(levels) == 0:
-> 1256             raise Exception('Must pass non-zero number of levels/labels')
1257 
1258         if len(levels) == 1:

Exception: Must pass non-zero number of levels/labels


Reading the data in line-by-line with csvreader based on this example implies that my data is not incorrectly formatted:

from io import BytesIO
import csv

with open('datatxt' 'rb') as f:
    r = fread()decode('utf-16')encode('utf-8')
    for l in csvreader(BytesIO(r) delimiter='\t'):
        print l

['Country'  'State/City'  'Title'  'Date'  'Catalogue'  'Wikipedia Election Page'  'Wikipedia Individual Page'  'Electoral Institution in Country'  'Twitter'  'CANDIDATE NAME 1'  'CANDIDATE NAME 2']
['Venezuela'  'N/A'  'President'  '10/7/12'  'Hugo Rafael Chavez Frias'  'Hugo Ch\xc3\xa1vez'  'Hugo Ch\xc3\xa1vez'  'Hugo Chavez'  'Hugo Ch\xc3\xa1vez Fr\xc3\xadas'  'Hugo Chavez'  'Hugo Ch\xc3\xa1vez']
['Venezuela'  'N/A'  'President'  '10/7/12'  'Henrique Capriles Radonski'  'Henrique Capriles Radonski'  'Henrique Capriles Radonski'  'Henrique Capriles Radonski'  'Henrique Capriles R'  'Henrique Capriles'  '']


Is there some pre-processing  an addition option in read_csv  or something else that needs to be done before pandasread_csv can read a utf-16 file? Thanks!  ",1574687,,1574687.0,2012-12-03 23:14:01,2012-12-03 23:28:27,Pandas read_csv and UTF-16,<csv><python-2.7><pandas><utf-16>,2.0,2.0,,
13692418,1,,2012-12-03 21:53:13,1,52,"I noticed some strange behavior when using IX on large pandas dataframes

When I called ix on the same dataframe 50 times in a row it ran 10 times faster than when I called ix on 50 different dataframes

Is there caching going on behind the scenes on ix?  I noticed that the bottom loop doubles my memory usage  Why would the memory be increasing?

Is there any way to modify this behavior?

Note that if you use straight up numpy it ran in 74 seconds in both cases with 0 memory increase  which is what led me to believe pandas was caching

Obviously you never want to call ix on each individual element

import pandas as pd
import numpy as np
import datetime as dt
print 'pandas'  pd__version__

li_list = []
for i in range(50):
    li_listappend(pdDataFrame(data=nprandomrandn(50  17000)))

print 'starting'

dt_start = dtdatetimenow()
a = 0
for i in range(50):
    b = li_list[0] #Only access first element
    for j in bcolumns:
        a += bix[i  j]
print (dtdatetimenow()-dt_start)total_seconds()


dt_start = dtdatetimenow()
a = 0
for i in range(50):
    b = li_list[i] #Access all in list
    for j in bcolumns:
        a += bix[i  j]
print (dtdatetimenow()-dt_start)total_seconds()


Output:

pandas 091
starting
3651
22009
",1279469,,,,2012-12-04 01:51:28,Does Pandas Cache values on IX call?,<python><pandas>,2.0,,1.0,
13736988,1,,2012-12-06 04:29:37,0,45,"I try to read-in a file by pandas like this:

df=read_csv('C:\Python27\mmtxt' skiprows=7 index_col=[0 1] names=['Date' 'Time' 'temp'])


then I can get below DataFrame:

Date        Time     celsius    
2012-04-12 16:13:09  206
2012-04-13 00:13:09  206




when I plot the DataFrame the X-axis becomes so messy crowded with time like this""2012-04-12 16:13:09"" How can the X-axis just show the year and month?

plotting code just like this ""dfplot()""

PSmy pandas version is 073 for some reasons I can't upgrade it",1843099,,1843099.0,2012-12-06 04:42:21,2012-12-12 13:49:44,how can I get a clear x-axis in pandas,<python><pandas>,1.0,4.0,,
13757090,1,13758846.0,2012-12-07 04:49:28,0,40,"If I import or create a pandas column that contains no spaces  I can access it as such:

df1 = DataFrame({'key': ['b'  'b'  'a'  'c'  'a'  'a'  'b'] 
                 'data1': range(7)})

df1data1


which would return that series for me  If  however  that column has a space in its name  it isn't accessible via that method:

df2 = DataFrame({'key': ['a' 'b' 'd'] 
                 'data 2': range(3)})

df2data 2      # ",1850663,,,,2012-12-07 07:36:50,Pandas column access w/column names containing spaces,<string><pandas>,1.0,,,
13793805,1,,2012-12-10 01:31:27,-1,92,"Python Script Goal:

Calculating the daily P&L -- for each date in a file; once signal is assigned 
Returns those numbers  as a csv  in a P&L column at the bottom of each date calculated Preserving the original imported csv (Not sure how to best achieve this)
My solution below to 1 IS NOT ideal since the dates are not processed in consecutive order when looping through the index in this way

For example  observe that my code loops through the first  say  11 dates  like so It seems to assign date  precedence rather than year Is there a way to change this?

1/1/2008
1/1/2009 
1/1/2010 
1/10/2007
1/10/2008
1/10/2011
1/10/2012
1/11/2007
1/11/2008
1/11/2010
1/11/2011


Rather than what one would hope for/(expect?)  ie a consecutive list:  

1/2/2007; 1/3/2007; 1/4/2007      12/7/2012;   12/8/12;   12/9/2012 


(Here w/no holidays  & weekdays only)

from pandas import *
from numpy import *
from io import *
from os import *
from sys import *

data = read_csv('__csv')
DF = dataset_index(['date'  'time'])

storeBuy = []
storeSell = []
PL = []

for day in DFindexlevels[0]:
  for time in DFindexlevels[1]:
    if DFsignal[(day time)]=='S':
      storeSellappend(DFprice[(day time)])
    elif DFsignal[(day time)]=='B':
      storeBuyappend(DFprice[(day time)])
    else: 
      pass   
     if len(storeSell)>len(storeBuy):
       for i in range(1 len(storeSell)-len(storeBuy)):   
          storeBuyappend(DFprice[(DFindexlevels[1] == '1620')])
          PLappend([j-i for i  j in zip(storeBuy[:]  storeSell[:])])
      elif len(storeBuy)>len(storeSell): 
         for i in range(1 len(storeSell)-len(storeBuy)): 
           storeSellappend(DFprice[(DFindexlevels[1] == '1620')])
           PLappend([j-i for i  j in zip(storeBuy[:]  storeSell[:])])      
      else:
         pass


Remarks:

The csv file has 62 035 lines Columns labeled: date  time  price  mag 
signal
The script assigns a ""B""  or an ""S"" and stores these signals
in a list 
If the number of ""S""!=""B"" in any given day; then this script appends the
 close price (1620pm) to the ""shorter"" list/array
Then computes PL= Sell-Buy  element wise in the list)
Here the labeled Buys/Sells are indeed differenced; but are not
cumulatively summed  as desired  for each date in the file
Any advice would be greatly appreciated",1374969,,1240268.0,2012-12-10 02:00:07,2012-12-10 02:00:07,Python P&L Calculation with a Multi-Index,<python><pandas><time-series>,1.0,,,2012-12-11 22:45:57
13773777,1,13777063.0,2012-12-08 02:14:19,0,42,"When I have a pandas timestamps like so:

list(uni_index)

Out[95]:

[ 
 


How would I get the differences of these time stamps?

I found a brute for it like so:

npdiff(uni_indexvaluesastype(int))


but it would be nice if it's possible to get back an answer in datetimetimedeltas directly from pandas Maybe it is  I just haven't found it yet?",680232,,,,2012-12-08 11:34:11,how to calculate the differences of a list of pandas timestamps?,<pandas>,1.0,,,
13779308,1,13779725.0,2012-12-08 16:19:39,0,69,"I have some raw data in the following format where record delimiter is ~ and element delimiter is |

date|o|h|l|c|e|f~07-12-2012 09:15|59340000|59455000|59340000|59386500|1749606|1749606~07-12-2012 09:16|59391000|59418000|59363500|59418000|1064557|2814163


Now I want to parse this data into a pandas data frame  but I guess the format that pandas data frame understands is key - column  So basically i am able to parse this data into three rows using split('~')

date|o|h|l|c|e|f
07-12-2012 09:15|59340000|59455000|59340000|59386500|1749606|1749606
07-12-2012 09:16|59391000|59418000|59363500|59418000|1064557|2814163


But is there a api using which I can split it on a basis of column so something like as shown below or is there a api in pandas which I can use directly to feed my data 

date - 07-12-2012 09:15 07-12-2012 09:16
o - 59340000 59391000 
h  etc
l
c
e
f 
",825707,,,,2012-12-12 15:54:22,How to parse delimited data in python ( pandas ) to create DataFrame?,<python><pandas>,2.0,,,
13793321,1,,2012-12-10 00:12:52,1,67,"I have two DataFrames:

df1 = ['Date_Time' 
    'Temp_1' 
    'Latitude' 
    'N_S' 
    'Longitude' 
    'E_W']

df2 = ['Date_Time' 
    'Year' 
    'Month' 
    'Day' 
    'Hour' 
    'Minute' 
    'Seconds']


As You can see both DataFrames have Date_Time as a common column I want to Join these two DataFrames by matching Date_Time

My current code is: dfjoin(df2  on='Date_Time')  but this is giving an error",781329,,1240268.0,2012-12-10 00:32:26,2012-12-10 01:36:13,Joining Table/DataFrames with common Column in Python,<python><pandas>,1.0,1.0,,
13824840,1,13825263.0,2012-12-11 16:57:10,0,82,"I am unable to create a dataframe which has escaped quotes when using read_csv
 (Note: R's readcsv works as expected)

My code:

import pandas as pd
pdread_csv('datacsv')
#error!
CParserError: Error tokenizing data C error: Expected 2 fields in line 4  saw 3


datacsv

SEARCH_TERM ACTUAL_URL
""bra tv bord"" ""http://wwwikeacom/se/sv/catalog/categories/departments/living_room/10475/?se%7cps%7cnonbranded%7cvardagsrum%7cgoogle%7ctv_bord""
""tv p hjul"" ""http://wwwikeacom/se/sv/catalog/categories/departments/living_room/10475/?se%7cps%7cnonbranded%7cvardagsrum%7cgoogle%7ctv_bord""
""SLAGBORD  \""Bergslagen\""  IKEA:s 1700-tals serie"" ""http://wwwikeacom/se/sv/catalog/categories/departments/living_room/10475/?se%7cps%7cnonbranded%7cvardagsrum%7cgoogle%7ctv_bord""


How can I read this csv and avoid this error?

My guess is that pandas is using some regular expressions which cannot handle the ambiguity and trips on the third row  or more specifically: \""Bergslagen\""",387251,,1240268.0,2012-12-11 20:21:25,2012-12-11 20:46:39,Escaped quotes in pandas read_csv,<pandas>,1.0,2.0,1.0,
13838405,1,,2012-12-12 11:09:42,-1,75,"I have python pandas dataframe  in which a column contains month name

How can I  do a custom sort using a dictionary  for example:

custom_dict = {'March':0  'April':1  'Dec':3}  
",1645853,,1240268.0,2012-12-12 11:54:02,2012-12-12 11:54:02,Custom sorting in pandas dataframe,<python><pandas>,2.0,1.0,,
13769600,1,13770992.0,2012-12-07 19:17:18,0,46,"When using pandas statsmomentsrolling_mean(array window) function I noticed that putting an extra argument changes the output  and is only padded with nans in the beginning not the end

In[1]: import pandas as pd

In[2]: pdstatsmomentsrolling_mean(nparange(12) 6)
Out[2]: 
array([ nan   nan   nan   nan   nan   25   35   45   55   65   75 
        85])


I expected there to be 6 nans: 3 at the beginning and 3 at the end
What am I missing here?

/M ",1394513,,1240268.0,2012-12-07 22:13:23,2012-12-07 22:13:23,Behaviour of rolling_mean,<pandas>,1.0,,1.0,2012-12-08 14:13:44
13832938,1,13842337.0,2012-12-12 04:44:03,1,54,"I read-in a file and plot it with pandas DataFrame The index is DatetimeIndex  and then I use ginput(1) method to get one point  however the coordinate which I get is wrong

The code is as follows:

import pandas as pd
from matplotlibdates import num2date  date2num
ts = pddate_range('2012-04-12 16:13:09'  '2012-04-14 00:13:09'  freq='H')
df = pdDataFrame(index=ts)
df[0] = 206


I then plot and click on the graph using ginput:

dfplot()
t = pylabginput(n=1) #click somewhere near 13-APR-2012


However  the first item appears to be a float

In [8]: x = t[0][0] # ~ 37063167741935479

In [9]: num2date(x)
Out[9]: datetimedatetime(1015  10  3  16  15  29  32253  tzinfo=)
# this is way out!


The docs suggest that it should be using these floats (from datetonum):

In [10]: dt = pdto_datetime('13-4-2012'  dayfirst=True)

In [11]: date2num(dt)
Out[11]: 7346060


What is this float  and how can I convert it to a datetime?

Note: If I remove one of the rows from the dataframe this works correctly:

df1 = dfdrop(ts[1]  axis=0)

",1843099,,1240268.0,2012-12-12 10:59:16,2012-12-12 14:54:01,Ginput giving wrong date for datetimeindex,<python><matplotlib><pandas>,1.0,2.0,,
13852008,1,13855833.0,2012-12-13 02:31:37,0,44,"I have a cluster of nodes where each one produces about 200 statistics on the the performance of CPU/network/disk etc I have so far looped though various node's log files and  parsed them into a data frame object per node and put into a dict keyed by node id:

(Here the first column is the index label of the DataFrame)

    { 'node00': 
            
                               core 0    core 1    core 2   core 3   group 0
    Avg IPC (w/ idle)           009      012     006      006      008
    Avg CPI (w/ idle)          1117      803    1562     1697     1295
    Avg IPC (w/o idle)          048      078     064      063      063
    Avg CPI (w/o idle)          210      128     156      159      163
    User IPC (w/o idle)         070      102     085      084      085
    
     
     'node01':
            
    Avg IPC (w/ idle)           005      012     006      006      008
    Avg CPI (w/ idle)           917      803    1562     1697     1295
    Avg IPC (w/o idle)          048      078     064      063      063
    Avg CPI (w/o idle)          210      128     156      159      163
    User IPC (w/o idle)         070      102     085      084      085

    }


I plan to write a general purpose function that would take the name of the statistic as the argument and then plot bar graphs of the particular statistic across all nodes in the cluster Bars of different cores can be stacked or side-by-side But the x-axis will points need to be the nodes for easy comparison

Any suggestions? I am new to Pandas/matplotlib so any hint would be great",814907,,814907.0,2012-12-13 02:49:56,2012-12-13 08:42:53,"pandas dict of dataframe values, need graphing suggestion",<matplotlib><pandas>,1.0,1.0,,
13785932,1,13788301.0,2012-12-09 08:42:47,2,90,"I have a pandasDatetimeIndex  eg:

pddate_range('2012-1-1 02:03:04000' periods=3 freq='1ms')
>>> [2012-01-01 02:03:04    2012-01-01 02:03:04002000]


I would like to round the dates (Timestamps) to the nearest second How do I do that? The expected result is similar to:

[2012-01-01 02:03:04000000    2012-01-01 02:03:04000000]


Is it possible to accomplish this by rounding a Numpy datetime64[ns] to seconds without changing the dtype [ns]?

nparray(['2012-01-02 00:00:00001'] dtype='datetime64[ns]')
",1579844,,1579844.0,2012-12-09 14:43:26,2012-12-09 15:05:37,How to round a Pandas `DatetimeIndex`?,<date><datetime><numpy><pandas><dateformat>,2.0,3.0,,
13807758,1,13808212.0,2012-12-10 19:28:39,1,53,I am reading a file into a Pandas DataFrame that may have invalid (ie NaN) rows This is sequential data  so I have row_id+1 refer to row_id When I use framedropna()  I get the desired structure  but the index labels stay as they were originally assigned How can the index labels get reassigned 0 to N-1 where N is the number of rows after dropna()?,394430,,,,2012-12-10 20:02:32,How to delete a row in a Pandas DataFrame and relabel the index?,<index><pandas><delete-row>,1.0,,,
13828891,1,13829026.0,2012-12-11 21:34:19,1,65,"Objective: 

To create an Index that accommodates a pre-existing set of price data from a csv file  I can build an index using list comprehensions If it's done in that way  the construction would give me a filtered list of length 86 772--when run over 1/3/2007-8/30/2012 for 42 times (ie 10 minute intervals) However  my data of prices coming from the csv is length: 62 034 Observe that the difference in length is due to data cleaning issues

That said  I am not sure how to overcome the apparent mismatch between the real data and this pre-built (list comp) dataframe 

Attempt: 

Am I using the first two lines incorrectly? 

data=pdread_csv('___csv'  parse_dates={'datetime':[0 1]})set_index('datetime')

dt_index = pdDatetimeIndex([datetimecombine(idate itime) for i in dataindex])

ts = pdSeries(datapricesvalues  dt_index)


Questions: 

As I understand it  I should use 'combine' since I want the index construction to be completely informed by my csv file And  'combine' returns a new datetime object whose date components are equal to the given date objects  and whose time components are equal to the given time objects
When I parse_dates  is it lumping the time and date together and considering it to be a 'date'?
Is there a better way to achieve the stated objective?
Traceback Error:


  AttributeError: 'unicode' object has no attribute 'date'
",1374969,,,,2012-12-11 23:58:06,Data Cleaning Consequence on a Pre-Built Index,<python><pandas><time-series>,1.0,1.0,,
13854476,1,13854901.0,2012-12-13 06:46:04,0,48,"Another pandas question

Reading Wes Mckinney's excellent book about Data Analysis and Pandas  I encountered the following thing that I thought should work:

Suppose I have some info about tips

In [119]:

tipshead()
Out[119]:
total_bill  tip      sex     smoker    day   time    size  tip_pct
0    1699   101    Female  False   Sun     Dinner  2   0059447
1    1034   166    Male    False   Sun     Dinner  3   0160542
2    2101   350    Male    False   Sun     Dinner  3   0166587
3    2368   331    Male    False   Sun     Dinner  2   0139780
4    2459   361    Female  False   Sun     Dinner  4   0146808


and I want to know the five largest tips in relation to the total bill  that is  tip_pct for smokers and non-smokers separately So this works:

def top(df  n=5  column='tip_pct'): 
    return dfsort_index(by=column)[-n:]

In [101]:

tipsgroupby('smoker')apply(top)
Out[101]:
           total_bill   tip sex smoker  day time    size    tip_pct
smoker                                  
False   88   2471   585    Male    False   Thur    Lunch   2   0236746
185  2069   500    Male    False   Sun     Dinner  5   0241663
51   1029   260    Female  False   Sun     Dinner  2   0252672
149  751    200    Male    False   Thur    Lunch   2   0266312
232  1161   339    Male    False   Sat     Dinner  2   0291990

True    109  1431   400    Female  True    Sat     Dinner  2   0279525
183  2317   650    Male    True    Sun     Dinner  4   0280535
67   307    100    Female  True    Sat     Dinner  1   0325733
178  960    400    Female  True    Sun     Dinner  2   0416667
172  725    515    Male    True    Sun     Dinner  2   0710345


Good enough  but then I wanted to use pandas' transform to do the same like this:

def top_all(df):
    return dfsort_index(by='tip_pct')

tipsgroupby('smoker')transform(top_all)


but instead I get this:

TypeError: Transform function invalid for data types


Why? I know that transform requires to return an array of the same dimensions that it accepts as input  so I thought I'd be complying with that requirement just sorting both slices (smokers and non-smokers) of the original DataFrame without changing their respective dimensions Can anyone explain why it failed? ",460147,,460147.0,2012-12-13 06:52:09,2012-12-13 07:19:15,pandas' transform doesn't work sorting groupby output,<python><aggregate><pandas>,1.0,,,
13851535,1,13851602.0,2012-12-13 01:28:24,0,117,"I have a pandas DataFrame and I want to delete rows from it where the length of the string in a particular column is greater than 2 I know I can use dfdropna() to get rid of rows that contain any NaN  but I'm not seeing how to remove rows based on a conditional expression 

The answer for this question seems very close to what I want -- it seems like I should be able to do something like this:

df[(len(df['column name']) ",1080717,,,,2012-12-13 02:46:39,How to delete rows from a pandas DataFrame based on a conditional expression,<python><pandas>,1.0,,,
13865176,1,13865838.0,2012-12-13 17:25:08,2,121,"So Python  with the pandas module seems like a great option to matlab and R This is why I've very recently switched to this There are resources out there  and I've searched the forum but couldn't find anything similar If you have links to some tutorials or other useful material out there  please post them

Wes McKinney has a great and elaborate tutorial on pandas
http://wwwyoutubecom/watch?v=w26x-z-BdWQ&list=FLJ5xKwlfj7wg8S_A5SgR6Wg&feature=mh_lolz

At 1:10 he shows an example of how to index the rows in a dataframe by dates rather than integers
I would like to do something similar

The difference is that I have 3 variables  Y1  Y2  Y3  each with a column of timestamps  X1  X2  X3

TestFiletxt:  
X1  Y1  X2  Y2  X3  Y3
27/11/2012  11436  29/11/2012  20631  4/12/2012   10209  
28/11/2012  11468  30/11/2012  20185  5/12/2012   9973  
29/11/2012  11414  3/12/2012   19962  6/12/2012   9736  
30/11/2012  11355  4/12/2012   19562  7/12/2012   9509  
3/12/2012   11309  5/12/2012   18908  10/12/2012  9259  
4/12/2012   11118  6/12/2012   18288  11/12/2012  8109  
5/12/2012   10873  7/12/2012   17973  
6/12/2012   10582  10/12/2012  17788  
7/12/2012   10264  11/12/2012  17554  
10/12/2012  9886  
11/12/2012  9164  


Where I want to do 4 things:

Associate data in Yi by its date in Xi for i = 1 2 3
Index rows by dates
Remove all data that is older than 4/12/2012 which is the first date of Y3
Be able to access all date by date and column only 
Here is a test file which describes how the data is read and how it prints
You can see that X1 is correctly parsed to the pandas date format  but not X2 or X3 which is what I attempted to do by specifying
index_col=[0 2 4]
and
parse_dates = True

TestFilepy:
import pandas as pd

df = pdread_csv('TestFiletxt' sep='\t'  index_col=[0 2 4]  parse_dates = True)

print 'pandas version: '  pd__version__
print df


Gives output:

pandas version:  0100b1
X1         X2         X3              Y1      Y2      Y3                   
2012-11-27 29/11/2012 4/12/2012   11436  20631  10209
2012-11-28 30/11/2012 5/12/2012   11468  20185   9973
2012-11-29 3/12/2012  6/12/2012   11414  19962   9736
2012-11-30 4/12/2012  7/12/2012   11355  19562   9509
2012-03-12 5/12/2012  10/12/2012  11309  18908   9259
2012-04-12 6/12/2012  11/12/2012  11118  18288   8109
2012-05-12 7/12/2012  None        10873  17973     NaN
2012-06-12 10/12/2012 None        10582  17788     NaN
2012-07-12 11/12/2012 None        10264  17554     NaN
2012-10-12 None       None         9886     NaN     NaN
2012-11-12 None       None         9164     NaN     NaN


Wanted output:

                Y1      Y2       Y3                 
2012-04-12  11118  19562   10209
2012-05-12  10873  18908    9973
2012-06-12  10582  18288    9736
2012-07-12  10264  17973    9509
2012-10-12   9886  17788    9259
2012-11-12   9164  17554    8109


If you have any idea of how to do this  your help is much appreciated:)",1897966,,1897966.0,2012-12-13 20:53:26,2012-12-13 20:53:26,Pandas dataframe indexing by date,<python><pandas><time-series>,2.0,,1.0,
13767950,1,13768161.0,2012-12-07 17:21:07,1,27,"I am trying to add a column of smaller len into a DataFrame where indexes of smaller item are a subset of a larger item So if RIMM has data for every single day  but GOOG is missing some day I want to add RIMM to the matrix with header GOOG

             GOOG
03/12/2012    1
29/11/2012    1
26/11/2012    1

             RIMM    
03/12/2012    1       
30/11/2012    1
29/11/2012    1       
28/11/2012    1
27/11/2012    1
26/11/2012    1       


So it looks something like this

         RIMM    GOOG
03/12/2012    1       1
30/11/2012    1      NaN
29/11/2012    1       1
28/11/2012    1      NaN
27/11/2012    1      NaN
26/11/2012    1       1


I am new to this data type  so any suggestions/tips are welcome",1575088,,,,2012-12-07 17:35:02,missing data in Dataframe,<python><numpy><pandas>,1.0,1.0,,
13794560,1,13798506.0,2012-12-10 03:21:45,2,61,"The following lines give AttributeError: 'module' object has no attribute 'bdate_range'

I think this might have something to do with a circular reference; but  I don't know where 

import pandas as pd

times = pdbdate_range(start=pddatetime(2012 11 14 0 0 0) 
                       end=pddatetime(2012 11 17 0 0 0) 
                       freq='10T')


This is the traceback:

AttributeError              Traceback (most recent call last)
 in ()
       4 
       5 
 ----> 6 times = pdbdate_range(start=pddatetime(2012 11 14 0 0 0) end=pddatetime(2012 11 17 0 0 0) 
 freq='10T')
       7 filtered_times = [x for x in times if xtime() >= time(9 30) and xtime() ",1374969,,1240268.0,2012-12-10 09:41:36,2012-12-10 09:52:31,bdate_range AttributeError,<python><pandas>,1.0,3.0,,
13842088,1,13842286.0,2012-12-12 14:40:45,2,106,"I've created a pandas DataFrame

df=DataFrame(index=['A' 'B' 'C']  columns=['x' 'y'])


and got this


    x    y
A  NaN  NaN
B  NaN  NaN
C  NaN  NaN



Then I want to assign value to particular cell  for example for row 'C' and column 'x'
I've expected to get such result:


    x    y
A  NaN  NaN
B  NaN  NaN
C  10  NaN


with this code:

dfxs('C')['x']=10


but contents of df haven't changed It's again only Nan's in dataframe 

Any suggestions?",1675248,,,,2012-12-12 14:56:28,Set value for particular cell in pandas DataFrame,<python><pandas>,1.0,0.0,,
13871150,1,,2012-12-14 00:54:24,1,60,"The idea behind this question is  that when I'm working with full datetime tags and data from different days  I sometimes want to compare how the hourly behavior compares
But because the days are different  I can not directly plot two 1-hour data sets on top of each other

My naive idea would be that I need to remove the day from the datetime index on both sets and then plot them on top of each other What's the best way to do that?

Or  alternatively  what's the better approach to my problem?",680232,,,,2012-12-14 05:19:58,How to remove day from datetime index in pandas?,<python><pandas>,1.0,,,
13887013,1,,2012-12-14 22:14:41,-2,49,"Having created a scatter plot with pandas I don't know how to create the regresion line that would be the least squared from the points
looking for examples in http://matplotliborg i haven't found any similar graph

Thanks you a lot in advance !!",1905285,,,,2012-12-15 01:39:41,plot least squared line in a scatter plot,<plot><pandas><scatter>,1.0,,,
13899914,1,13900044.0,2012-12-16 09:07:52,3,44,"I have a DataFrame as follows How can I select rows whose second index is in ['two' 'three']?

index = MultiIndex(levels=[['foo'  'bar'  'baz'  'qux'] 
                               ['one'  'two'  'three']] 
                       labels=[[0  0  0  1  1  2  2  3  3  3] 
                               [0  1  2  0  1  1  2  0  1  2]])
hdf = DataFrame(nprandomrandn(10  3)  index=index 
            columns=['A'  'B'  'C'])

In [3]: hdf
Out[3]: 
                  A         B         C
foo one   -1274689  0946294 -0149131
    two   -0015483  1630099  0085461
    three  1396752 -0272583 -0760000
bar one   -1151217  1269658  2457231
    two   -1657258 -1271384 -2429598
baz two    1124609  0138720 -1994984
    three  0124298 -0127099 -0409736
qux one    0535038  1139026  0414842
    two    0287724  0461041 -0268918
    three -0259649  0226574 -0558334
",1907561,,1240268.0,2012-12-16 09:43:01,2013-01-16 04:04:05,Hiearchical indexing based on sub level,<pandas>,2.0,,,
13854140,1,13898325.0,2012-12-13 06:22:08,0,90,"I'm using Pandas (091) to write a physics code I have two dataframes:

Levels:

class 'pandascoreframeDataFrame'>
Int64Index: 37331 entries  0 to 37330
Data columns:
atomic_number    37331  non-null values
ion_number       37331  non-null values
level_number     37331  non-null values
energy           37331  non-null values
g                37331  non-null values
metastable       37331  non-null values


Lines:


Int64Index: 314338 entries  0 to 314337
Data columns:
id                    314338  non-null values
wavelength            314338  non-null values
atomic_number         314338  non-null values
ion_number            314338  non-null values
f_ul                  314338  non-null values
f_lu                  314338  non-null values
level_number_lower    314338  non-null values
level_number_upper    314338  non-null values
dtypes: float64(3)  int64(7)


There's a couple of things I need to do: 
I need to join levels with lines (atom  ion  level): at first on atom  ion  level_number_upper and then atom  ion  level_number_lower Is there a way to precompute the join - memory is not an issue  but speed is 

I also need to group levels (on atom  ion) and do an operation on levels I did this already (incredibly fast)  but then had trouble joining the resulting series with the lines dataframe

How do I do this?

Cheers
   Wolfgang

update v1:

To show what I want to join merge here a code snippet

def calc_group_func(group):
    return npsum(group['g']*npexp(-group['energy'])
grouped_data = levelsgroup_by('atomic_number'  'ion_number')
grouped_dataapply(calc_group_func)


and then I want to join/merge grouped data with lines on atomic_number and ion_number",288558,,288558.0,2012-12-13 13:18:18,2012-12-16 03:21:12,Pandas join grouped and normal dataframe,<python><pandas>,1.0,4.0,,
13890673,1,13891083.0,2012-12-15 08:35:45,3,111,"I have a pandas dataframe like:

In [61]: df = DataFrame(nprandomrand(3 4)  index=['art' 'mcf' 'mesa'] 
                        columns=['pol1' 'pol2' 'pol3' 'pol4'])

In [62]: df
Out[62]: 
          pol1      pol2      pol3      pol4
art   0661592  0479202  0700451  0345085
mcf   0235517  0665981  0778774  0610344
mesa  0838396  0035648  0424047  0866920


and I want to generate a row with the average for the policies across benchmarks and then plot it

Currently  the way I do this is:

df = dfT
df['average'] = dfapply(average  axis=1)
df = dfT
dfplot(kind='bar')


Is there an elegant way to avoid the double transposition?

I tried:

dfappend(DataFrame(dfapply(average))T)
dfplot(kind='bar')


This will append the correct values but does not update the index properly and the graph is messed up

A clarification The result of the code with the double transposition is this:  
This is what I want To show both the benchmarks and the average of the policies  not just the average I was just curious if I can do it better 

Note that the legend is usually messed up For a fix: 

ax = dfplot(kind='bar')
axlegend(patches  list(dfcolumns)  loc='best')
",1186611,,1301710.0,2012-12-15 22:51:58,2012-12-15 22:51:58,pandas: generate and plot average,<python><matplotlib><plot><pandas>,1.0,3.0,1.0,
13906077,1,,2012-12-16 22:24:04,2,88,"I have a CSV file which contains a date in the format 2011 1 10 

The format in my dataframe  which I am loading that date into  should be 2011-01-10

I am using the following code to load the date out of the file:

read_csv(""testcsv""  parse_date[[0 2]])


How do I convert it to read in as 2011-01-10?",1908555,,1908555.0,2012-12-17 02:46:17,2012-12-17 02:46:17,reading date field from csv file into dataframe,<python><date><data.frame><pandas>,1.0,,,
13886019,1,13888672.0,2012-12-14 20:47:09,1,41,"after plotting a figure I get a figure legend as below:

DataFrame1plot(legend=False)
patch labels=axget_legend_handels_labels()
DateFrame1legend(loc='best')
pltshow()


How can I delete the 'Temp' in (Temp 2005)  let become just 2005?

the DataFrame1 has three keys:Month Year Temp",1843099,,,,2012-12-15 03:27:15,reconstruction figure legend in pandas,<python><pandas>,2.0,1.0,,
13888468,1,13888546.0,2012-12-15 01:29:46,3,78,"I know that I can get the unique values of a DataFrame by resetting the index but is there a way to avoid this step and get the unique values directly?

Given I have:

        C
 A B     
 0 one  3
 1 one  2
 2 two  1


I can do:

df = dfreset_index()
uniq_b = dfBunique()
df = dfset_index(['A' 'B'])


Is there a way built in pandas to do this?",8590,,,,2012-12-15 02:21:36,Get unique values from index column in MultiIndex,<pandas>,1.0,2.0,1.0,
13893227,1,13893632.0,2012-12-15 14:51:27,6,774,"I have two pandas dataframes one called 'orders' and another one called 'daily_prices'
daily_prices is as follows:

              AAPL    GOOG     IBM    XOM
2011-01-10  33944  61421  14278  7157
2011-01-13  34264  61669  14392  7308
2011-01-26  34082  61650  15574  7589
2011-02-02  34129  61200  15793  7946
2011-02-10  35142  61644  15932  7968
2011-03-03  35640  60956  15873  8219
2011-05-03  34514  53389  16784  8200
2011-06-03  34042  52308  16097  7819
2011-06-10  32303  50951  15914  7684
2011-08-01  39326  60677  17628  7667
2011-12-20  39246  63037  18414  7997


orders is as follows:

           direction  size ticker  prices
2011-01-10       Buy  1500   AAPL  33944
2011-01-13      Sell  1500   AAPL  34264
2011-01-13       Buy  4000    IBM  14392
2011-01-26       Buy  1000   GOOG  61650
2011-02-02      Sell  4000    XOM   7946
2011-02-10       Buy  4000    XOM   7968
2011-03-03      Sell  1000   GOOG  60956
2011-03-03      Sell  2200    IBM  15873
2011-06-03      Sell  3300    IBM  16097
2011-05-03       Buy  1500    IBM  16784
2011-06-10       Buy  1200   AAPL  32303
2011-08-01       Buy    55   GOOG  60677
2011-08-01      Sell    55   GOOG  60677
2011-12-20      Sell  1200   AAPL  39246


index of both dataframes is datetimedate
'prices' column in the 'orders' dataframe was added by using a list comprehension to loop through all the orders and look up the specific ticker for the specific date in the 'daily_prices' data frame and then adding that list as a column to the 'orders' dataframe I would like to do this using an array operation rather than something that loops can it be done? i tried to use:

daily_pricesix[dates tickers] 

but this returns a matrix of cartesian product of the two lists i want it to return a column vector of only the  price of a specified ticker for a specified date",1167915,,,,2012-12-15 15:47:22,Vectorized look-up of values in Pandas dataframe,<python><pandas><vectorization>,1.0,,3.0,
13926089,1,13977244.0,2012-12-18 04:08:09,7,169,"How can I retrieve specific columns from a pandas HDFStore?  I regularly work with very large data sets that are too big to manipulate in memory  I would like to read in a csv file iteratively  append each chunk into HDFStore object  and then work with subsets of the data  I have read in a simple csv file and loaded it into an HDFStore with the following code:    

tmp = pdHDFStore('testh5')
chunker = pdread_csv('carscsv'  iterator=True  chunksize=10  names=['make' 'model' 'drop'])
tmpappend('df'  pdconcat([chunk for chunk in chunker]  ignore_index=True))


And the output:

In [97]: tmp
Out[97]:

File path: testh5
/df     frame_table (typ->appendable nrows->1930 indexers->[index])


My Question is how do I access specific columns from tmp['df']?  The documenation makes mention of a select() method and some Term objects  The examples provided are applied to Panel data; however  and I'm too much of a novice to extend it to the simpler data frame case  My guess is that I have to create an index of the columns somehow  Thanks!",919872,,,,2012-12-22 01:27:49,Selecting columns from pandas.HDFStore table,<python><pandas><hdfs>,2.0,0.0,3.0,
13834690,1,,2012-12-12 07:16:56,0,36,"Possible Duplicate:pip install fails with /usr/bin/clang: No such file or directory  




I'm getting this error when trying to install Pandas using either easy_install or pip (OSX Mountain Lion)  


  unable to execute clang: No such file or directory  


Does anyone know how to fix this issue?",464277,,,,2012-12-12 09:43:57,Python Pandas installation error,<python><installation><pandas>,1.0,1.0,1.0,2012-12-17 07:08:29
13924801,1,13925665.0,2012-12-18 01:01:44,1,36,"In the last statement of this routine I get a TypeError

data = {'state': ['Ohio'  'Ohio'  'Ohio'  'Nevada'  'Missouri'] 
        'year': [2000  2001  2002  2001  2002] 
        'items': [5  12  6  45  0]}
frame = DataFrame(data)

def summary_pivot(df  row=['state'] column=['year'] value=['items'] func=len):
    return dfpivot_table(value  rows=row cols=column 
                   margins=True  aggfunc=func  fill_value=0)

test = summary_pivot(frame)

In [545]: test
Out[545]: 
          items                 
year       2000  2001  2002  All
state                           
Missouri      0     0     1    1
Nevada        0     1     0    1
Ohio          1     1     1    3
All           1     2     2    5


price = DataFrame(index=['Missouri'  'Ohio']  columns = ['price']  data = [200  250])

In [546]: price
Out[546]: 
          price
Missouri    200
Ohio        250

test * price


TypeError: can only call with other hierarchical index objects

How can I get past this error  so I can multiply correctly the number of items in each state by the corresponding price?",1479269,,1479269.0,2012-12-18 01:06:50,2012-12-18 03:04:50,ignoring hierarchical index during matrix operations,<python><pandas>,1.0,,,
13937022,1,13937141.0,2012-12-18 16:11:22,2,50,"Q is similar to this:
use a list of values to select rows from a pandas dataframe

I want to dataframe if either value in two columns are in a list
Return both columns (combine results of #1 and #4 

import numpy as np
from pandas import *


d = {'one' : [1  2  3  4]  'two' : [5  6  7  8] 'three' : [9  16  17  18]}

df = DataFrame(d)
print df

checkList = [1 7]

print df[dfone == 1 ]#1
print df[dfone == 7 ]#2
print df[dftwo == 1 ]#3
print df[dftwo == 7 ]#4

#print df[dfone == 1 or dftwo ==7]
print df[dfoneisin(checkList)]
",428862,,,,2012-12-18 16:34:54,Using pandas to select rows using two different columns from dataframe?,<python><pandas>,1.0,,,
13937097,1,13937497.0,2012-12-18 16:16:16,2,70,"Possible Duplicate:Python challenging string encoding  




I am trying to put a list of tickers into a SQL query  I am new to Python

cus is a list set up as ['x'  'y'    'a']

test = psqlframe_query(
    ""select * from dataBase where cus IN (%r)""  con = db) % cus


I tried this and it failed  

My next attempt was to try and remove the brackets and then paste the list in  How do I remove the brackets so that I get:

cus2 = 'x'  'y'    'a'


I was going to do:

str = ""select * from dataBase where cus IN (%r)"" % cus2
test2 = psqlframe_query(str  con = db)


Or is there another way I should be attempting this?

Thank you",1911092,,89391.0,2012-12-18 16:17:18,2012-12-18 18:21:09,Remove Brackets from Python List for an SQL query,<python><pandas>,1.0,1.0,,2012-12-19 00:19:52
13938704,1,13938831.0,2012-12-18 17:54:06,2,89,"I'm a newbie to pandas dataframe  and I wanted to apply a function to each column so that it computes for each element x  x/max of column 

I referenced this question  but am having trouble accessing the maximum of each column Thanks in advance
Pandas DataFrame: apply function to all columns

Input:

      A  B  C  D
   0  8  3  5  8
   1  9  4  0  4
   2  5  4  3  8
   3  4  8  5  1


Output:

      A     B     C    D
   0  8/9  3/8  5/5  8/8
   1  9/9  4/8  0/5  4/8
   2  5/9  4/8  3/5  8/8
   3  4/9  8/8  5/5  1/8
",1490464,,,,2012-12-18 18:03:13,Apply function on Pandas dataframe,<python><pandas>,1.0,2.0,,
13824973,1,,2012-12-11 17:04:36,0,107,"I know it is possible to use dfxs(lbl) to access the rows of the DataFrame with all indices equal lbl  however  I have the following issue:

I want to be able to iterate through a time series T (simple sequential list of datetimes) as follows:

Where dfA is a dataframe indexed by all T  and dfB is a dataframe with multiple (some repeated) indices from T  but not all T

for t in T:
    for r in dfBxs(t)iterrows():
        # do something with r on values in dfA @ t
    # do something else with values in dfA @ t


The problem I am having is:

If t is not in df  a KeyError is raised
If there is only one entry for t in df  a Series object results
If there are more than one t in df  a DataFrame object results
As you can see  this would make for rather ugly code for something that should be fairly straight-forward  I am sure there must be a more pandasic way of doing this  but it is not obvious to me

Update:

Dumps of T and dfB as follows:

Note: I have changed T in the original code to a DatetimeIndex  but this should not change the original premise

In [26]: T
Out[26]: 

[2000-03-15 00:00:00    2012-12-26 00:00:00]
Length: 3191  Freq: None  Timezone: None

In [27]: orders #equivalent to dfB
Out[27]: 

DatetimeIndex: 15322 entries  2000-03-15 00:00:00 to 2012-12-27 00:00:00
Data columns:
Symbol    15322  non-null values
Type      15322  non-null values
Number    15322  non-null values
dtypes: int64(1)  object(2)


This is enough data to get the original code snippet working  if pandas behaved as I expected/hoped it should

Additionally  to show that dfB contains multiple indices of the same values at places:

In [30]: ordersxs(t) #equivalent to dfBxs(t)
Out[30]: 
             Symbol Type  Number
Date                            
2012-12-26  0596HK  Buy    1000
2012-12-26  0387HK  Buy    1000
2012-12-26  0342HK  Buy    1000
2012-12-26  0343HK  Buy    1000
2012-12-26  0491HK  Buy    1000
",1742878,,1742878.0,2013-01-03 12:50:40,2013-01-03 12:50:40,How to retrieve rows from Pandas DataFrame with same index labels in consistant way?,<python><pandas>,1.0,2.0,,
13872533,1,13876784.0,2012-12-14 04:13:35,1,82,"I have a temperature file with many years temperature records  in a format as below:

2012-04-12 16:13:09 206
2012-04-12 17:13:09 209
2012-04-12 18:13:09 206
2007-05-12 19:13:09 54
2007-05-12 20:13:09 206
2007-05-12 20:13:09 206
2005-08-11 11:13:09 206
2005-08-11 11:13:09 175
2005-08-13 07:13:09 206
2006-04-13 01:13:09 206


Every year has different numbers  time of the records  so the pandas datetimeindices are all different

I want to plot the different year's data in the same figure for comparing  The X-axis is Jan to Dec  the Y-axis is temperature How should I go about doing this? ",1843099,,1240268.0,2012-12-14 10:26:01,2012-12-14 10:26:01,Plot different DataFrames in the same figure,<python><matplotlib><pandas>,2.0,,1.0,
13940753,1,13941149.0,2012-12-18 20:13:53,2,196,"I have two pandas DataFrames - weight has a simple Index on a Land Use columns concentration has a MultiIndex on Land Use and Parameter

import pandas
import StringIO

conc_string = StringIOStringIO(""""""\
Land Use Parameter 1E 1N 1S 2
Airfield BOD5 (mg/l) 0418 0118 0226 1063
Airfield Ortho P (mg/l) 0002 0001 0001 0002
Airfield TSS (mg/l) 1773 1147 0862 0183
Airfield Zn (mg/l) 0001 0001 495E-05 0001
""Commercial"" BOD5 (mg/l) 0036 00419  0315
""Commercial"" Cu (mg/l) 437E-05 734E-05  000039
""Commercial"" O&G (mg/l) 00385 0127  0263
Open Space TSS (mg/l) 0371 301 1209 0147
Open Space Zn (mg/l) 00127 00069 00132 0007
""Parking Lot"" BOD5 (mg/l) 0924 00668 2603 319
""Parking Lot"" O&G (mg/l) 102 0149 1347 188
""Rooftops"" BOD5 (mg/l) 0135 100 00562 0310"""""")

weight_string = StringIOStringIO(""""""\
Land Use 1E 1N 1S 2
Airfield 0511 00227 00616 0394
Commercial 00005 01704 0 01065
Open Space 00008 0005 00002 00004
""Parking Lot"" 033 0514 0252 0171
Rooftops 0081 0028 850E-05 0003"""""")

concentration = pandasread_csv(conc_string  index_col=[0 1])
weight = pandasread_csv(weight_string  index_col=0)


In this case  the columns (1E  1N  1S  and 2) are drainage basins

What I would like to do is divide all of the concentrations independent of Parameter by the weights where the basin (column names) and Land Use

I'm not having much luck here concentration / weight certainly does't work I'm not having much luck stacking the dataframes and joining either

wstk = pandasDataFrame(weightstack())
wstkindexnames = ['Land Use'  'Basin']
wstkrename(columns={0:'weight'}  inplace=True)

cstk = pandasDataFrame(concentrationstack())
cstkindexnames = ['Land Use'  'Parameter'  'Basin']
cstkrename(columns={0:'concentration'}  inplace=True)

wstkjoin(cstk  on=['Land Use'  'Basin']) # fails 
cstkjoin(wstk  on=['Land Use'  'Basin']) # fails 


The last two lines don't raise an error when I leave off the on kwarg  but return NaN results for the joined column They also fail if I drop the index on both stacked DataFrames (eg  do wstkreset_index(inplace=True) before the join)

Any suggestions?

Thanks",1552748,,,,2012-12-18 20:41:08,"Aligning DataFrames with same columns, different index levels",<python><pandas>,1.0,,,
13786209,1,13787487.0,2012-12-09 09:23:54,3,202,"I'd like to plot what Excel calls an ""Exponential Trend/Regression"" on a stock chart When I run the code below in the IPython notebook it simply says ""The kernel has died  would you like to restart it?"" Any ideas on how to fix it? Also  this is just attempting to do a linear regression and I'm not quite sure how to do a regression on exponential data

import datetime
import matplotlibpyplot as plt
import statsmodelsapi as sm
from pandasiodata import DataReader

sp500 = DataReader(""AGG""  ""yahoo""  start=datetimedatetime(2000  1  1)) # returns a DataFrame
sp500[""regression""] = smOLS(sp500[""Adj Close""]  sp500index)fit()fittedvalues()
top = pltsubplot2grid((3 1)  (0  0)  rowspan=2)
topplot(sp500index  sp500[""Adj Close""]  'b-'  sp500index  sp500[""regression""]  'r-')
bottom = pltsubplot2grid((3 1)  (2 0))
bottombar(sp500index  sp500Volume)
pltgcf()set_size_inches(18 8)
",64421,,1301710.0,2012-12-09 11:14:35,2012-12-10 14:54:44,Regression on stock data using pandas and matplotlib,<python><matplotlib><pandas><statsmodels>,2.0,2.0,1.0,
13867294,1,,2012-12-13 19:47:51,2,138,"I have to clean a input data file in python Due to typo error  the datafield may have strings instead of numbers I would like to identify all fields which are a string and fill these with NaN using pandas Also  I would like to log the index of those fields

One of the crudest way is to loop through each and every field and checking whether it is a number or not  but this consumes lot of time if the data is big 

My csv file contains data similar to the following table:

Country  Count  Sales
USA         1   65000
UK          3    4000
IND         8       g
SPA         3    9000
NTH         5   80000



Assume that i have 60 000 such rows in the data

Ideally I would like to identify that row IND has an invalid value under SALES column Any suggestions on how to do this efficiently?",1645853,,1240268.0,2012-12-13 20:37:17,2012-12-14 11:14:32,cleaning big data using python,<python><pandas>,5.0,2.0,,
13878959,1,13879549.0,2012-12-14 12:44:18,0,106,"I'd like to build a running sum over a pandas dataframe I have something like:

10/10/2012:  50   0
10/11/2012: -10  90
10/12/2012: 100  -5


And I would like to get:

10/10/2012:  50   0
10/11/2012:  40  90
10/12/2012: 140  85


So every cell should be the sum of itself and all previous cells  how should I do this without using a loop",55070,,1240268.0,2012-12-14 13:19:15,2012-12-14 13:25:21,Running sum in pandas (without loop),<python><pandas>,2.0,4.0,,
13921647,1,13921674.0,2012-12-17 20:27:52,2,63,"New to Python

In R  you can get the dimension of a matrix using dim()   What is the corresponding function in Python Pandas for their data frame?

Thanks",1911092,,,,2012-12-17 20:29:56,Python - Dimension of Data Frame,<python><pandas>,1.0,1.0,,
13950053,1,13958624.0,2012-12-19 10:16:57,2,61,"What is the standard way of plotting a timeseries (dates) of quiver or barbs? I often have timeseries in a Pandas DataFrame and plot them like this:

pltplot(dfindexto_pydatetime()  dfparameter)


This works very well  the x-axis can be treated as genuine dates which is very convenient for formatting or setting the xlim() with Datetime object etc

Using this with quiver or barbs in the same way result in:

TypeError: float() argument must be a string or a number


This can be overcome with something like:

axbarbs(dfindexvaluesastype('d')  npones(size) * 65  dfUvalues  dfVvalues  length=8  pivot='middle')
axset_xticklabels(dfindexto_pydatetime())


Which works  but would mean that everywhere i have to convert the dates to floats and then manually override the labels Is there a better way?

Here is some sample code resembling my case:

import matplotlibpyplot as plt
import numpy as np
import pandas as pd

size = 10

wspd = nprandomrandint(0 40 size=size)
wdir = nplinspace(0 360 * nppi/180  num=size)
U = -wspd*npsin(wdir)
V = -wspd*npcos(wdir)

df = pdDataFrame(npvstack([U V])T  index=pddate_range('2012-1-1'  periods=size  freq='M')  columns=['U'  'V'])

fig  ax = pltsubplots(1 1  figsize=(15 4))

axplot(dfindexvaluesastype('d')  dfV * 01 + 4  color='k')
axquiver(dfindexvaluesastype('d')  npones(size) * 35  dfUvalues  dfVvalues  pivot='mid')
axbarbs(dfindexvaluesastype('d')  npones(size) * 65  dfUvalues  dfVvalues  length=8  pivot='middle')

axset_xticklabels(dfindexto_pydatetime())


",1755432,,,,2012-12-20 10:17:25,Quiver or Barb with a date axis,<python><matplotlib><pandas>,2.0,,,
13924972,1,13925150.0,2012-12-18 01:24:53,2,58,"I have a dataframe of historical stock trades  The frame has columns like ['ticker'  'date'  'cusip'  'profit'  'security_type']  Initially:

trades['cusip'] = npnan
trades['security_type'] = npnan


I have historical config files that I can load into frames that have columns like ['ticker'   'cusip'  'date'  'name'  'security_type'  'primary_exchange']

I would like to UPDATE the trades frame with the cusip and security_type from config  but only where the ticker and date match

I thought I could do something like:

pdmerge(trades  config  on=['ticker'  'date']  how='left')


But that doesn't update the columns  it just adds the config columns to trades

The following works  but I think there has to be a better way  If not  I will probably do it outside of pandas

for date in trades['date']unique():
    config = get_config_file_as_df(date)
    ## config['date'] == date
    for ticker in trades['ticker'][trades['date'] == date]:
        trades['cusip'][ 
                           (trades['ticker'] == ticker)
                         & (trades['date']   == date)
                       ] \
            = config['cusip'][config['ticker'] == ticker]values[0]

        trades['security_type'][ 
                           (trades['ticker'] == ticker)
                         & (trades['date']   == date)
                       ] \
            = config['security_type'][config['ticker'] == ticker]values[0]
",314880,,,,2012-12-18 02:06:33,Updating pandas DataFrame by key,<python><pandas>,1.0,,1.0,
13927267,1,13929347.0,2012-12-18 06:08:06,1,37,"I have the following problem - the output of the calculation gives me two atomic_number fields:

In [7]: partition_functions
Out[7]: 
atomic_number  ion_number
14             0             11291802
               1              5866805
               2              1004422
               3              2000202
26             0             59650557
               1             66895978
               2             28186253
               3              6569105

In [8]: def group_func(group):
    return group[1:]/group[:-1]values
   : 

In [9]: partition_functionsgroupby(level='atomic_number')apply(group_func)
Out[9]: 
atomic_number  atomic_number  ion_number
14             14             1             0519563
                              2             0171204
                              3             1991397
26             26             1             1121464
                              2             0421345
                              3             0233061


I tried a couple of things - including making a new Series and nothing worked

Thanks in advance for the help",288558,,,,2012-12-18 08:53:51,apply on group replicating complete MultiIndex,<pandas>,1.0,1.0,,
13929884,1,13930121.0,2012-12-18 09:29:07,2,93,"Im new to Python and Pandas but have a CSV file with multiple columns that I have read in to a dataframe  I would like to plot a scatter plot of x=Index and y='data' Where the index is Index of the dataframe and is a date
Thanks heaps
Jason",1911866,,,,2012-12-18 09:43:16,Pandas scatter plot,<python><pandas>,1.0,,,
13941472,1,13941980.0,2012-12-18 21:03:21,3,78,"Trying to use the awfully useful pandas to deal with data as time series  I am now stumbling over the fact that there do not seem to exist libraries that can directly interpolate (with a spline or similar) over data that has DateTime as an x-axis? I always seem to be forced to convert first to some floating point number  like seconds since 1980 or something like that

I was trying the following things so far  sorry for the weird formatting  I have this stuff only in the ipython notebook  and I can't copy cells from there:

from scipyinterpolate import InterpolatedUnivariateSpline as IUS
type(bb2temp): pandascoreseriesTimeSeries
s = IUS(bb2tempindexto_pydatetime()  bb2temp  k=1)
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
 in ()
----> 1 s = IUS(bb2tempindexto_pydatetime()  bb2temp  k=1)

/Library/Frameworks/EPD64framework/Versions/73/lib/python27/site-packages/scipy/interpolate/fitpack2py in __init__(self  x  y  w  bbox  k)
    335         #_data == x y w xb xe k s n t c fp fpint nrdata ier
    336         self_data = dfitpackfpcurf0(x y k w=w 
--> 337                                       xb=bbox[0] xe=bbox[1] s=0)
    338         self_reset_class()
    339 

TypeError: float() argument must be a string or a number


By using bb2tempindexvalues (that look like these:

array([1970-01-15 184:00:35884999  1970-01-15 184:00:58668999 
       1970-01-15 184:01:22989999  1970-01-15 184:01:45774000 
       1970-01-15 184:02:10095000  1970-01-15 184:02:32878999 
       1970-01-15 184:02:57200000  1970-01-15 184:03:19984000 


) as x-argument  interestingly  the Spline class does create an interpolator  but it still breaks when trying to interpolate/extrapolate to a larger DateTimeIndex (which is my final goal here) Here is how that looks:

all_times = divcaltimedindexlevels[2] # part of a MultiIndex

all_times

[2009-07-20 00:00:00045000    2009-07-20 00:30:00018000]
Length: 14063  Freq: None  Timezone: None

s(all_timesvalues) # applying the above generated interpolator
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
 in ()
----> 1 s(tallvalues)

/Library/Frameworks/EPD64framework/Versions/73/lib/python27/site-packages/scipy/interpolate/fitpack2py in __call__(self  x  nu)
    219 #            return dfitpacksplev(*(self_eval_args+(x )))
    220 #        return dfitpacksplder(nu=nu *(self_eval_args+(x )))
--> 221         return fitpacksplev(x  self_eval_args  der=nu)
    222 
    223     def get_knots(self):

/Library/Frameworks/EPD64framework/Versions/73/lib/python27/site-packages/scipy/interpolate/fitpackpy in splev(x  tck  der  ext)
    546 
    547         x = myasarray(x)
--> 548         y  ier =_fitpack_spl_(x  der  t  c  k  ext)
    549         if ier == 10:
    550             raise ValueError(""Invalid input data"")

TypeError: array cannot be safely cast to required type


I tried to use s(all_times) and s(all_timesto_pydatetime()) as well  with the same TypeError: array cannot be safely cast to required type

Am I  sadly  correct? Did everybody get used to convert times to floating points so much  that nobody thought it's a good idea that these interpolations should work automatically? (I would finally have found a super-useful project to contribute) Or would you like to prove me wrong and earn some SO points? ;) 

Edit: Warning: Check your pandas data for NaNs before you hand it to the interpolation routines They will not complain about anything but just silently fail",680232,,680232.0,2012-12-21 01:25:19,2012-12-21 01:25:19,Python splines or other interpolations that work with time on x-axis?,<python><scipy><pandas>,1.0,,,
13953276,1,13953332.0,2012-12-19 13:17:29,2,78,"I have the following input file:

2012 10 3 AAPL BUY 200
2012 12 5 AAPL SELL 200


How can I read this in into a pandas dataframe wth following columns:

index: default int range # 0
column1: datetime(2012 10 3 16) # 2012-10-03 16:00
column2: string # AAPL
column3: string # BUY
column4: integer # 200


Example:

0 2012-10-03 16:00 AAPL BUY  200
1 2012-12-05 16:00 AAPL SELL 200


Tried (pandas 07):

In[2]: pandasioparsersread_csv(""inputcsv""  parse_dates=[[0 1 2]]  header=None)
Out[2]: 
    X1  X2  X3   X4   X5  X6
0  2012   10    3  AAPL   BUY  200
1  2012   12    5  AAPL  SELL  200
",1915862,,1915862.0,2012-12-20 08:05:53,2012-12-20 08:05:53,python pandas date read_table,<python><date><pandas>,1.0,,,
13910891,1,,2012-12-17 08:49:06,1,30,"I just follow this tutorial : http://pandaspydataorg/pandas-docs/dev/r_interfacehtml
when I type : 

import pandasrpycommon as com


it gives below error msg:

Traceback (most recent call last):
  File """"  line 1  in 
  File ""D:\Python\Lib\site-packages\pandas\rpy\commonpy""  line 11  in 
    from rpy2robjectspackages import importr
ImportError: No module named packages


How to fix it ?

Pandas 081 + Python 26 + R 2122 + rpy2 208 + window-xp",1072888,,,,2012-12-17 08:49:06,Error when calling R from Pandas,<pandas><rpy2>,,2.0,,
13930367,1,13931877.0,2012-12-18 09:56:32,2,77,"I would like to fill gaps in a column in my DataFrame using a cubic spline If I were to export to a list then I could use the numpy's interp1d function and apply this to the missing values

Is there a way to use this function inside pandas?",1911866,,1240268.0,2012-12-18 11:29:24,2012-12-18 11:46:25,Interpolating time series in Pandas using Cubic spline,<python><pandas>,1.0,1.0,,
13956564,1,13965098.0,2012-12-19 16:17:29,1,76,"Im doing some log analysis and examining the length of a queue every few minutes I know when the files entered the queue(a simple filesystem directory) and when they left With that  I can plot the length of the queue at given intervals So far so good  though the code is a bit procedural:

ts = pddate_range(start='2012-12-05 10:15:00'  end='2012-12-05 15:45'  freq='5t')
tmpdf = dfcopy()
for d in ts:
    tmpdf[d] = (tmpdfdate_in  d)
queue_length = tmpdf[list(ts)]apply(func=npsum)


But  I want to compare the real length with the length at a given consumption rate(eg 1 per second  etc) I cant just subtract a constant because the queue cant go beyond zero

I have done it  but at a very procedural way I have tried to use pandas window functions with little success  because cant access the result thats already been calculated for the previous element This was the first thing I tried which is deadly wrong:

imagenes_min = 60 * imagenes_sec
def roll(window_vals):
    return max(00  window_vals[-1] + window_vals[-2] - imagenes_min)

pdrolling_apply(arg=imagenes_introducidas  func=roll   window = 2  min_periods=2)


The real code is like this  which I think its too verbose and slow:

imagenes_sec = 105
imagenes_min = imagenes_sec * 60 *5
imagenes_introducidas = df3aetresample(rule='5t' how='count')
imagenes_introducidashead()

def accum_minus(serie  rate):
    acc = 0
    retval = npzeros(len(serie))
    for i a in enumerate(serievalues):
       acc = max(0  a + acc - rate)
       retval[i] = acc
    return Series(data=retval  index=serieindex)

est_1 = accum_minus(imagenes_introducidas  imagenes_min)
comparativa = DataFrame(data = { 'real': queue_length  'est_1_sec': est_1 })
comparativaplot()




This seems an easy task but I dont know how to do it properly May be pandas isnt the tool but some numpy or scipy magic

UPDATE: df3 is like this(some columns ommited):

                               aet             date_out
date_in                                               
2012-12-05 10:08:59318600  Z2XG17  2012-12-05 10:09:37172300
2012-12-05 10:08:59451300  Z2XG17  2012-12-05 10:09:38048800
2012-12-05 10:08:59587400  Z2XG17  2012-12-05 10:09:39044100


UPDATE 2: This seems faster  still not very elegant

imagenes_sec = 105
imagenes_min = imagenes_sec * 60 *5
imagenes_introducidas = df3aetresample(rule='5t' how='count')

def add_or_zero(x  y):
    return max(00  x + y - imagenes_min)

v_add_or_zero = npfrompyfunc(add_or_zero  2 1)
xx = v_add_or_zeroaccumulate(imagenes_introducidasvalues  dtype=npobject)

dd = DataFrame(data = {'est_1_sec' : xx  'real': queue_length}  index=imagenes_introducidasindex)
ddplot()
",150300,,150300.0,2012-12-19 20:48:49,2012-12-20 04:01:03,pandas window functions: accessing to intermediate results,<python><numpy><scipy><pandas>,1.0,3.0,1.0,
13914077,1,13986115.0,2012-12-17 12:21:08,3,81,"I have a large time-series set of data at 30 minute intervals and trying to do a sliding window on this set of data but separately for each point of the day using pandas

I'm no statistician and not great at thinking or coding for this sort of work but here is my clumsy attempt at doing what I want I'm really looking for help improving it as I know there will be a better way of doing this  possibly using MultiIndexes and some proper iteration? But I have struggled to do this across the 'time-axes'

def sliding_window(run data type='mean'):
    data = dataasfreq('30T')
    for x in date_range(runSTART  runEND  freq='1d'):
        if int(datetimestrftime(x  ""%w"")) == 0 or int(datetimestrftime(x  ""%w"")) == 6:
            points = dataselect(weekends)truncate(x - relativedelta(days=runWINDOW) x + relativedelta(days=runWINDOW))groupby(lambda date: minutes(date  x))mean()
        else:
            points = dataselect(weekdays)truncate(x - relativedelta(days=runWINDOW) x + relativedelta(days=runWINDOW))groupby(lambda date: minutes(date  x))mean()
        for point in pointsindex:
            data[datetime(xyear xmonth xday pointhour pointminute)] = points[point]
    return data


runSTART  runEND and runWINDOW are two points within data and 45 (days) I've been staring at this code a lot so I'm not sure what (if any) of it make sense to anyone else  please ask so that I can clarify anything else

SOLVED: (Solution courtesy of crewbum)

The modified function which as expected goes stupidly fast:

def sliding_window(run data am='mean' days='weekdays'):
    data = dataasfreq('30T')
    data = DataFrame({'Day': [ddate() for d in dataindex]  'Time': [dtime() for d in dataindex]  'Weekend': [weekday_string(d) for d in dataindex]  'data': data})
    pivot = datapivot_table(values='data'  rows='Day'  cols=['Weekend'  'Time'])
    pivot = pivot[days]
    if am == 'median':
        mean = rolling_median(pivot  runWINDOW*2  min_periods=1)
    mean = rolling_mean(pivot  runWINDOW*2  min_periods=1)
    return DataFrame({'mean': unpivot(mean)  'amax': nptile(pivotmax()values  pivotshape[0])  'amin': nptile(pivotmin()values  pivotshape[0])}  index=dataindex)


The unpivot function:

def unpivot(frame):
    N  K = frameshape
    return Series(framevaluesravel('C')  index=[datetimecombine(d[0]  d[1]) for d in zip(npasarray(frameindex)repeat(K)  nptile(npasarray(frameix[0]index)  N))])


The center=True on sliding_mean appears to be broken at the moment  will file it in github if I get the chance",1410646,,1410646.0,2012-12-21 17:46:12,2012-12-21 17:46:12,Sliding window average across time axes,<python><pandas>,1.0,5.0,2.0,
13958129,1,13961138.0,2012-12-19 17:47:40,3,64,"I am having lots of issues working with DataFrames with date indexes

from pandas import DataFrame  date_range
# Create a dataframe with dates as your index
data = [1  2  3  4  5  6  7  8  9  10]
idx = date_range('1/1/2012'  periods=10  freq='MS')
df = DataFrame(data  index=idx  columns=['Revenue'])
df['State'] = ['NY'  'NY'  'NY'  'NY'  'FL'  'FL'  'GA'  'GA'  'FL'  'FL'] 

In [6]: df
Out[6]: 
       Revenue   State
2012-01-01   1      NY
2012-02-01   2      NY
2012-03-01   3      NY
2012-04-01   4      NY
2012-05-01   5      FL
2012-06-01   6      FL
2012-07-01   7      GA
2012-08-01   8      GA
2012-09-01   9      FL
2012-10-01   10     FL


I am trying to add an additional column named 'Mean' with the group averages:

I tried this  but it does not work:

df2 = df
df2['Mean'] = dfgroupby(['State'])['Revenue']apply(lambda x: mean(x))

In [9]: df2head(10)
Out[9]:
       Revenue    State    Mean
2012-01-01   1       NY     NaN
2012-02-01   2       NY     NaN
2012-03-01   3       NY     NaN
2012-04-01   4       NY     NaN
2012-05-01   5       FL     NaN
2012-06-01   6       FL     NaN
2012-07-01   7       GA     NaN
2012-08-01   8       GA     NaN
2012-09-01   9       FL     NaN
2012-10-01   10      FL     NaN


But I am trying to get:

       Revenue    State    Mean
2012-01-01   1       NY     25
2012-02-01   2       NY     25
2012-03-01   3       NY     25
2012-04-01   4       NY     25
2012-05-01   5       FL     75
2012-06-01   6       FL     75
2012-07-01   7       GA     75
2012-08-01   8       GA     75
2012-09-01   9       FL     75
2012-10-01   10      FL     75


How can I get this DataFrame?",1821873,,1240268.0,2012-12-19 21:15:17,2012-12-26 01:57:31,How to apply function to date indexed DataFrame,<index><group-by><pandas>,2.0,,1.0,
13918355,1,13923312.0,2012-12-17 16:49:25,3,67,"Suppose I have two tables A and B 

Table A has a multi-level index (a  b) and one column (ts)
b determines univocally ts

A = pdDataFrame(
     [('a'  'x'  4)  
      ('a'  'y'  6)  
      ('a'  'z'  5)  
      ('b'  'x'  4)  
      ('b'  'z'  5)  
      ('c'  'y'  6)]  
     columns=['a'  'b'  'ts'])set_index(['a'  'b'])
AA = Areset_index()


Table B is another one-column (ts) table with non-unique index (a)
The ts's are sorted ""inside"" each group  ie  Bix[x] is sorted for each x
Moreover  there is always a value in Bix[x] that is greater than or equal to
the values in A

B = pdDataFrame(
    dict(a=list('aaaaabbcccccc')  
         ts=[1  2  4  5  7  7  8  1  2  4  5  8  9]))set_index('a')


The semantics in this is that B contains observations of occurrences of an event of type indicated by the index 

I would like to find from B the timestamp of the first occurrence of each event type after the timestamp specified in A for each value of b In other words  I would like to get a table with the same shape of A  that instead of ts contains the ""minimum value occurring after ts"" as specified by table B

So  my goal would be:

C: 
('a'  'x') 4
('a'  'y') 7
('a'  'z') 5
('b'  'x') 7
('b'  'z') 7
('c'  'y') 8


I have some working code  but is terribly slow

C = AAapply(lambda row: (
    row[0]  
    row[1]  
    Bix[row[0]]irow(npsearchsorted(Bts[row[0]]  row[2])))  axis=1)set_index(['a'  'b'])


Profiling shows the culprit is obviously Bix[row[0]]irow(npsearchsorted(Bts[row[0]]  row[2]))) However  standard solutions using merge/join would take too much RAM in the long run

Consider that now I have 1000 a's  assume constant the average number of b's per a (probably 100-200)  and consider that the number of observations per a is probably in the order of 300 In production I will have 1000 more a's

1 000 000 x 200 x 300 = 60 000 000 000 rows

may be a bit too much to keep in RAM  especially considering that the data I need is perfectly described by a C like the one I discussed above

How would I improve the performance?",163400,,1406409.0,2012-12-17 17:01:28,2012-12-18 07:37:43,Non standard interaction among two tables to avoid very large merge,<python><join><merge><pandas><binary-search>,2.0,2.0,,
13952977,1,,2012-12-19 12:58:01,3,37,"Is it possible to use a Series as index for a DataFrame of the same length?
An example:

I have a DataFrame ""df"" and a Series ""c""; the length of ""c"" is the same as the indexes of ""df"" The following code:

df1= DataFrame(df  index=c)


returns me a DataFrame with the right indexes but empty columns",1915817,,,,2012-12-19 13:10:10,Series as indexes for DataFrame,<pandas>,1.0,0.0,,
13983498,1,13983540.0,2012-12-21 02:26:29,2,70,"This is more of a hack that almost works

#!/usr/bin/env python

from pandas import *
import matplotlibpyplot as plt
from numpy import zeros

# Create original dataframe
df = DataFrame(nprandomrand(5 4)  index=['art' 'mcf' 'mesa' 'perl' 'gcc'] 
                        columns=['pol1' 'pol2' 'pol3' 'pol4'])
# Estimate average
average = dfmean()
averagename = 'average'

# Append dummy row with zeros and then average
row = DataFrame([dict({p:00 for p in dfcolumns})  ])

df = dfappend(row)
df = dfappend(average)

print df

dfplot(kind='bar')
pltshow()


and gives:

             pol1      pol2      pol3      pol4
art      0247309  0139797  0673009  0265708
mcf      0951582  0319486  0447658  0259821
mesa     0888686  0177007  0845190  0946728
perl     0902977  0863369  0194451  0698102
gcc      0836407  0700306  0739659  0265613
0        0000000  0000000  0000000  0000000
average  0765392  0439993  0579993  0487194


and

It gives the visual separation between benchmarks and average 
Is there a way to get rid of the 0 at the x-axis??

It turns out that DataFrame does not allow me to have muptiple dummy rows this way
My solution was to change 

row = pdDataFrame([dict({p:00 for p in dfcolumns})  ])  


into 

row = pdSeries([dict({p:00 for p in dfcolumns})  ]) 
rowname = """"


Series can be named with empty string",1186611,,1186611.0,2012-12-21 05:23:08,2012-12-21 05:23:08,Visually separating bar chart clusters in pandas,<python><matplotlib><plot><pandas>,1.0,,2.0,
13976491,1,13977632.0,2012-12-20 16:29:56,1,46,"Is it possible to split a time series on it's gaps For example  suppose we had the following:

rng2011 = pddate_range('1/1/2011'  periods=72  freq='H')
rng2012 = pddate_range('1/1/2012'  periods=72  freq='H')
Y = rng2011union(rng2012)


Is it possible to look for gaps of a year or more  and split the data frame on them?

I imagine this would go something like:

Ygroupby(Ymap(lambda x: xyear))


Except that this splits on the year date  and I'm interested in specifying an interval gap rather than the year attribute of the row

The application is I've got trip logs from a gps  but no delineation of when one trip ended and another began I'd like to split on gaps of ten minutes or longer",870178,,1252759.0,2012-12-20 16:57:41,2012-12-20 17:42:15,Split a series on time gaps in pandas?,<python><pandas>,1.0,,,
13983876,1,13984485.0,2012-12-21 03:24:49,5,80,"The Objective

I have some financial trading data for multiple products in CSV format that I would like to analyse using pandas The trades happen at non-regular intervals and are timestamped to 1sec accuracy  which results in some trades occurring ""at the same time""  ie with identical timestamps

The objective for the moment is to produce plots of the cummulative traded quantity for each product

Current Progress

The trading data has been read into a DataFrame using read_csv()  index on parsed datetime 


DatetimeIndex: 447 entries  2012-12-07 17:16:46 to 2012-12-10 16:28:29
Data columns:
Account Name    447  non-null values
Exchange        447  non-null values
Instrument      447  non-null values
Fill ID         447  non-null values
Side            447  non-null values
Quantity        447  non-null values
Price           447  non-null values
dtypes: float64(1)  int64(1)  object(5)


A little work is done to add a ""QuantitySigned"" column

I've done a ""groupby"" so that I can access the data by instrument

grouped = tradesgroupby('Instrument'  sort=True)
for name  group in grouped:
        groupQuantitySignedcumsum()plot(label=name)
pltlegend()


The Question

The above works  but I would like to have the TimeSeries (one per instrument) in one DataFrame  ie a column per instrument  so that I can use DataFrameplot() The problem is that no two TimeSeries have the exactly the same index  ie I would need to merge all the TimeSeries' indexes

I know that this is supposed to work  given the trivial example below:

index=pddate_range('2012-12-21'  periods=5)
s1 = Series(randn(3)  index=index[:3])
s2 = Series(randn(3)  index=index[2:])
df = DataFrame(index=index)
df['s1'] = s1
df['s2'] = s2


However  an exception is thrown when attempting to aggregate the TimeSeries into a DataFrame and I believe it has to do with the duplicate index elements:

grouped = tradesgroupby('Instrument'  sort=True)
df = DataFrame(index=tradesindex)
for name  group in grouped:
        df[name] = groupQuantitySignedcumsum()
dfplot()

Exception: Reindexing only valid with uniquely valued Index objects


Am I going about this ""correctly""? Are there any suggestions on how to go about this in a better way?

Runnable Example

Here is a runnable example that throws the exception:

import pandas as pd
from pandas import Series
from pandas import DataFrame

index = pdtseriesindexDatetimeIndex(['2012-12-22'  '2012-12-23'  '2012-12-23'])

s1 = Series(randn(2)  index[:2]) # No duplicate index elements
df1 = DataFrame(s1  index=index) # This works

s2 = Series(randn(2)  index[-2:]) # Duplicate index elements
df2 = DataFrame(s2  index=index) # This throws


The Solution

Thanks to @crewbum for the solution

grouped = tradesgroupby('Instrument'  sort=True)
dflist = list()
for name  group in grouped:
    dflistappend(DataFrame({name : groupQuantitySignedcumsum()}))
results = pdconcat(dflist)
results = resultssort()ffill()fillna(0)
resultsplot()


Note: I forward fill first before then setting remaining NaNs to zero As @crewbum pointed out  ffill() and bfill() are new to 0100

I'm using:

pandas 0100
numpy 161
Python 273
",652585,,652585.0,2012-12-21 05:43:28,2012-12-21 05:43:28,Adding hetrogenous TimeSeries to a DataFrame,<python><data.frame><pandas><time-series>,1.0,3.0,1.0,
13965036,1,,2012-12-20 03:51:52,1,51,"When I merge two CSV files  of the format (date  someValue)  I see some duplicate records

If I reduce the records to half the problem goes away However  if I double the size of both the files it worsens Appreciate any help!

My code:

i = pdDataFramefrom_csv('icsv')
i = ireset_index()
e = pdDataFramefrom_csv('ecsv')
e = ereset_index()

total_df = pdmerge(i  e  right_index=False  left_index=False 
                    right_on=['date']  left_on=['date']  how='left')
total_df = total_dfsort(column='date')


(Note: the dupulicate records for 11/15  11/16  12/17  12/18)

In [7]: total_df
Out[7]:
                  date  Cost  netCost
25 2012-11-15 00:00:00     1        2
26 2012-11-15 00:00:00     1        2
31 2012-11-16 00:00:00     1        2
32 2012-11-16 00:00:00     1        2
37 2012-11-17 00:00:00     1        2
2  2012-11-18 00:00:00     1        2
5  2012-11-19 00:00:00     1        2
8  2012-11-20 00:00:00     1        2
11 2012-11-21 00:00:00     1        2
14 2012-11-22 00:00:00     1        2
17 2012-11-23 00:00:00     1        2
20 2012-11-24 00:00:00     1        2
23 2012-11-25 00:00:00     1        2
29 2012-11-26 00:00:00     1        2
35 2012-11-27 00:00:00     1        2
0  2012-11-28 00:00:00     1        2
3  2012-11-29 00:00:00     1        2
6  2012-11-30 00:00:00     1        2
9  2012-12-01 00:00:00     1        2
12 2012-12-02 00:00:00     1        2
15 2012-12-03 00:00:00     1        2
18 2012-12-04 00:00:00     1        2
21 2012-12-05 00:00:00     1        2
24 2012-12-06 00:00:00     1        2
30 2012-12-07 00:00:00     1        2
36 2012-12-08 00:00:00     1        2
1  2012-12-09 00:00:00     2        2
4  2012-12-10 00:00:00     2        2
7  2012-12-11 00:00:00     2        2
10 2012-12-12 00:00:00     2        2
13 2012-12-13 00:00:00     1        2
16 2012-12-14 00:00:00     2        2
19 2012-12-15 00:00:00     2        2
22 2012-12-16 00:00:00     2        2
27 2012-12-17 00:00:00     1        2
28 2012-12-17 00:00:00     1        2
33 2012-12-18 00:00:00     1        2
34 2012-12-18 00:00:00     1        2


icsv

date Cost
2012-11-15 00:00:00 1
2012-11-16 00:00:00 1
2012-11-17 00:00:00 1
2012-11-18 00:00:00 1
2012-11-19 00:00:00 1
2012-11-20 00:00:00 1
2012-11-21 00:00:00 1
2012-11-22 00:00:00 1
2012-11-23 00:00:00 1
2012-11-24 00:00:00 1
2012-11-25 00:00:00 1
2012-11-26 00:00:00 1
2012-11-27 00:00:00 1
2012-11-28 00:00:00 1
2012-11-29 00:00:00 1
2012-11-30 00:00:00 1
2012-12-01 00:00:00 1
2012-12-02 00:00:00 1
2012-12-03 00:00:00 1
2012-12-04 00:00:00 1
2012-12-05 00:00:00 1
2012-12-06 00:00:00 1
2012-12-07 00:00:00 1
2012-12-08 00:00:00 1
2012-12-09 00:00:00 2
2012-12-10 00:00:00 2
2012-12-11 00:00:00 2
2012-12-12 00:00:00 2
2012-12-13 00:00:00 1
2012-12-14 00:00:00 2
2012-12-15 00:00:00 2
2012-12-16 00:00:00 2
2012-12-17 00:00:00 1
2012-12-18 00:00:00 1


ecsv

date netCost
2012-11-15 00:00:00 2
2012-11-16 00:00:00 2
2012-11-17 00:00:00 2
2012-11-18 00:00:00 2
2012-11-19 00:00:00 2
2012-11-20 00:00:00 2
2012-11-21 00:00:00 2
2012-11-22 00:00:00 2
2012-11-23 00:00:00 2
2012-11-24 00:00:00 2
2012-11-25 00:00:00 2
2012-11-26 00:00:00 2
2012-11-27 00:00:00 2
2012-11-28 00:00:00 2
2012-11-29 00:00:00 2
2012-11-30 00:00:00 2
2012-12-01 00:00:00 2
2012-12-02 00:00:00 2
2012-12-03 00:00:00 2
2012-12-04 00:00:00 2
2012-12-05 00:00:00 2
2012-12-06 00:00:00 2
2012-12-07 00:00:00 2
2012-12-08 00:00:00 2
2012-12-09 00:00:00 2
2012-12-10 00:00:00 2
2012-12-11 00:00:00 2
2012-12-12 00:00:00 2
2012-12-13 00:00:00 2
2012-12-14 00:00:00 2
2012-12-15 00:00:00 2
2012-12-16 00:00:00 2
2012-12-17 00:00:00 2
2012-12-18 00:00:00 2
",1917577,,1240268.0,2012-12-20 14:28:29,2012-12-20 23:13:55,Dataframe merge creates duplicate records in pandas (0.7.3),<merge><duplicates><pandas>,1.0,4.0,,
13993524,1,13994123.0,2012-12-21 16:15:15,4,91,"I've started using Pandas for some large Datasets and mostly it works really well There are some questions I have regarding the indices though

I have a MultiIndex with three levels - let's say a  b  c How do I slice along index a - I just want the values where a = 5   7  10  13 Doing dfix[[5  7  10  13]] does not work as pointed out in the documentation
I need to have different indices on a DF - can I create these multiple indices and not associate them to a dataframe and use them to give me back the raw ndarray index?
Can I slice a MultiIndex on its own not in a series or Dataframe?
Thanks in advance",288558,,,,2013-01-08 04:11:08,Pandas slicing along multiindex and separate indices,<python><pandas>,2.0,2.0,1.0,
13962133,1,,2012-12-19 22:11:18,2,47,"Just installed pandas 0100 and the following line to create an interpolated version of an existing columns fails:

prep_bcgps['lati'] = prep_bcgps['lat']apply(pdsSeriesinterpolate)

TypeError: unbound method interpolate() must be called with Series instance as first argument (got float64 instance instead)


Can somebody direct me to the new syntax?

Thanks 

Luc",1708646,,1240268.0,2012-12-19 22:19:17,2012-12-19 22:19:17,Pandas interpolate changed in version 0.10?,<python><pandas><interpolate>,1.0,1.0,,
13982250,1,13984352.0,2012-12-20 23:41:03,2,66,"I suppose this is fairly easy but I tried for a while to get an answer without much success I want to produce a stacked bar plot for two categories but I have such information in two separate date frames:

This is the code:

first_babies = live[livebirthord == 1] # first dataframe
others = live[livebirthord != 1] # second dataframe

fig = figure()
ax1 = figadd_subplot(1 1 1)

first_babiesgroupby(by=['prglength'])size()plot(
                     kind='bar'  ax=ax1  label='first babies') # first plot
othersgroupby(by=['prglength'])size()plot(kind='bar'  ax=ax1  color='r' 
               label='others') #second plot
ax1legend(loc='best')
ax1set_xlabel('weeks')
ax1set_ylabel('frequency')
ax1set_title('Histogram')




But I want something like this or as I said  a stacked bar plot in order to better distinguish between categories:



I can't use stacked=True because it doesn't work using two different plots and I can't create a new dataframe because first_babies and othersdon't have the same number of elements

Thanks",460147,,460147.0,2012-12-21 00:29:11,2012-12-21 05:34:44,Stacked bar plots from two different sources in Pandas,<python><matplotlib><pandas>,1.0,,2.0,
13999850,1,14000420.0,2012-12-22 03:40:35,5,93,"The default output format of to_csv() is:

12/14/2012  12:00:00 AM


I cannot figure out how to output only the date part with specific format:

20121214


or date and time in two separate columns in the csv file:

20121214   084530


The documentation is too brief to give me any clue as to how to do these Can anyone help?",1642513,,,,2012-12-22 06:10:38,How to specify date format when using pandas.to_csv?,<python><pandas>,1.0,,1.0,
14016247,1,14016590.0,2012-12-24 01:53:49,2,71,"I have a pandas DataFrame like this:

                    a         b
2011-01-01 00:00:00 1883381  -0416629
2011-01-01 01:00:00 0149948  -1782170
2011-01-01 02:00:00 -0407604 0314168
2011-01-01 03:00:00 1452354  NaN
2011-01-01 04:00:00 -1224869 -0947457
2011-01-01 05:00:00 0498326  0070416
2011-01-01 06:00:00 0401665  NaN
2011-01-01 07:00:00 -0019766 0533641
2011-01-01 08:00:00 -1101303 -1408561
2011-01-01 09:00:00 1671795  -0764629


Is there an efficient way to find the ""integer"" index of rows with NaNs? In this case the desired output should be [3  6]",1642513,,,,2012-12-25 18:41:23,Python - find integer index of rows with NaN in pandas,<python><pandas>,2.0,1.0,1.0,
14020365,1,14020738.0,2012-12-24 10:52:47,3,69,"The following link has a very similar problem solved using python dictionaries Python: merging dictionaries with lists in lists as values and counting them

I would like to know if the following problem can be solved using python pandas library I tried using merge and join but I am not sure how to go about getting the desired result 

The problem is as follows:

From 2 csv files  I read in a dictionary

dict1 = {'M1': {'H': '1'  'J' : '2'}  'M2': {'H': '1'  'J' : '2'}  'M3': {'H': '1'  'J' : '2'}}
dict2 = {'M1': {'H': '4'  'J' : '6'}  'M2': {'H': '2'  'J' : '5'}  'M4': {'H': '9'  'J' : '8'}}


Required Output Table: 

List of all Keys in both the dictionaries with their sum of sub-dictionary [{H J}] values for the matching keys between two dictionaries

Example: M1 is present both in dict1 and dict2  so final output for M1 should be 

final_M1['H'] = 1 (from dict1['M1']) + 4 (from dict2['M1']) = 5

Similarly for M3  M3 is present only in dict1  so nothing has to be done and that values have to be retained

Sample Output:

---------------------
M    |  H  |   J
---------------------
M1   |  5  |   8
---------------------
M2   |  3  |   7
---------------------
M3   |  1  |   2
---------------------
M4   |  9  |   8


To get the unique set of two dictionaries 

keys = set(dict1keys())union(dict2keys())


Similar to the logic used in the link above  The solution using python dictionary looks like this:

for k in keys:
print ""Key:""  k
d1val = dict1get(k  {})
d2val = dict2get(k  {})
if (len(d1val) == 0):
    print ""d2val H:""  d2val['H']

if (len(d2val) == 0):
    print ""d1val H:""  d1val['H']

if (len(d1val) != 0 and len(d2val) != 0):
    print ""Test""
    print ""d1val H:""  d1val['H']
    print ""d2val H:""  d2val['H']
    print ""d1val H + d2val H = ""  int(d1val['H']) + int(d2val['H'])
print ""***********""


How to implement the same logic in python pandas? I also would like to if using pandas library for such operation will be efficient considering if the input data set is of the range of 10 000 rows per file ",1652054,,,,2012-12-26 11:36:52,Merging CSV data and counting cell values in python pandas,<python><pandas>,1.0,,2.0,
13978682,1,13979130.0,2012-12-20 18:57:18,1,45,"The following situation often arises from my data analysis Say I have two vectors of data  x and y  from some observations x has more data points and thus contains some values that are not observed in y Now I want to make them into categorical variables

x=['a' 'b' 'c' 'd' 'e']  #data points
y =['a' 'c' 'e']         #data of the same nature as x but with fewer data points  

fx = pandasCategoricalfrom_array(x)
fy = pandasCategoricalfrom_array(y)

print fxindex
print fyindex

Categorical: 
array([a  b  c  d  e]  dtype=object)
Levels (5): Index([a  b  c  d  e]  dtype=object) Categorical: 
array([a  c  e]  dtype=object)
Levels (3): Index([a  c  e]  dtype=object)


I see that now they have different levels and labels mean different things (1 means b in fx but c in fy) 

This obviously make it hard for code that work with both fx and fy as they expect fxlabels and fylabels have the same encoding/meaning

But I do not see how to 'normalize' fx and fy so that they have the same levels and fxlables and fylables have the same coding fylabels = fxlables clearly does not work As the following demonstrates that it changes the meanings of the labels [a c e] becomes [a b c]

fylevels = fxlevels
print fy

Categorical: 
array([a  b  c]  dtype=object)
Levels (5): Index([a  b  c  d  e]  dtype=object)


Does anyone have any ideas?

Another related scenario is that I have an existing  known index  and want to factor the data into this index For example  I know that every data point has to take one of the five values [a  b  c  d  e] and I already have an index Index([a  b  c  d  e]  dtype=object) and I want to factorize vector y=['a' 'c' 'e'] into a Categoricial variable with Index([a  b  c  d  e]  dtype=object) as its levels I am not sure how it can be done either and would like someone who knows to give some clues

PS It is possible but cumbersome to do such things in R 

Thanks 
Tom",1919717,,,,2012-12-21 04:58:38,Change Categorical Variable levels to What I provide/Combine levels two categorical variables,<pandas>,1.0,,,
13980516,1,13981054.0,2012-12-20 21:11:31,-1,45,"I have a DataFrame object with a datetime as index:

In [210]:
f
fplot(legend=True)

Out[210]:
                         user_time  sys_time  wait_io_time
date_time                                             
2012-11-01 08:59:27          3         1             0
2012-11-01 08:59:32          0         0             0
2012-11-01 08:59:37         20         2             1
2012-11-01 08:59:42          0         0             0
2012-11-01 08:59:47          0         0             0


fplot() causes this error:

-> 1367     plot_objgenerate()
--> 674         self_make_plot()
-> 1000             self_make_ts_plot(data  **selfkwds)
---> 81     left  right = _get_xlim(axget_lines())
--> 220         left = min(x[0]ordinal  left)
AttributeError: 'numpyint64' object has no attribute 'ordinal'


I do see one line graph  displaying values from the user_time data  so suspecting that the data values from sys_time may be causing the issue Following suggestion from https://githubcom/pydata/pandas/issues/1737  I installed a newer version of matplotlib (111) but no luck  and plotting fails on the data from the bug url as well",814907,,,,2012-12-20 21:47:35,Issues in printing time series graph from pandas dataframe,<matplotlib><pandas>,1.0,2.0,,
14036397,1,14039589.0,2012-12-26 05:37:27,2,66,"I know that it is possible to offset with the periods argument  but how would one go about return-izing daily price data that is spread throughout a month (trading days  for example)?

Example data is:

In [1]: dfAAPL
2009-01-02 16:00:00    9036
2009-01-05 16:00:00    9418
2009-01-06 16:00:00    9262
2009-01-07 16:00:00    9062
2009-01-08 16:00:00    9230
2009-01-09 16:00:00    9019
2009-01-12 16:00:00    8828
2009-01-13 16:00:00    8734
2009-01-14 16:00:00    8497
2009-01-15 16:00:00    8302
2009-01-16 16:00:00    8198
2009-01-20 16:00:00    7787
2009-01-21 16:00:00    8248
2009-01-22 16:00:00    8798
2009-01-23 16:00:00    8798

2009-12-10 16:00:00    19559
2009-12-11 16:00:00    19384
2009-12-14 16:00:00    19614
2009-12-15 16:00:00    19334
2009-12-16 16:00:00    19420
2009-12-17 16:00:00    19104
2009-12-18 16:00:00    19459
2009-12-21 16:00:00    19738
2009-12-22 16:00:00    19950
2009-12-23 16:00:00    20124
2009-12-24 16:00:00    20815
2009-12-28 16:00:00    21071
2009-12-29 16:00:00    20821
2009-12-30 16:00:00    21074
2009-12-31 16:00:00    20983
Name: AAPL  Length: 252


As you can see  simply offsetting by 30 would not produce correct results  as there are gaps in the timestamp data  not every month is 30 days  etc  I know there must be an easy way to do this using pandas",1742878,,1742878.0,2013-01-06 05:35:24,2013-01-06 05:35:24,How to get DataFrame.pct_change to calculate monthly change on daily price data?,<python><pandas>,1.0,1.0,1.0,
13968520,1,13983684.0,2012-12-20 09:05:55,1,71,"I am using pandas and matplotlib to generate bar-graphs with lots of bars

I know how to cycle through a list of selected colors (How to give a pandas/matplotlib bar graph custom colors)
The question is what colors to select so that my graph prints nicely on a paper (it is for a research paper) What I am most interested in is sufficient contrast between the columns and a  selection of colors that looks pleasant I would like to have multiple colors instead of gray-scale or single-hue colorschemes 

Are there any predetermined schemes to select from that people use? ",1186611,,,,2012-12-21 02:55:08,Color selection for matplotlib that prints well,<printing><matplotlib><pandas>,2.0,3.0,,
13977719,1,13979142.0,2012-12-20 17:49:41,2,62,"I have a csv file which contains date and time stamps as two of the columns I am using pandas read_csv to read the contents into a dataframe My ultimate goal is to plot time series graphs from the data 

!head vmstatcsv
wait_proc sleep_proc swapped_memory free_memory buffered_memory cached_memory swapped_in swapped_out received_block sent_block interrups context_switches user_time sys_time idle_time wait_io_time stolen_time date time
0 0 10896 3776872 380028 10284052 0 0 6 16 7716 4755 3 1 96 0 0 2012-11-01 08:59:27
0 0 10896 3776500 380028 10284208 0 0 0 40 7471 4620 0 0 99 0 0 2012-11-01 08:59:32
0 0 10896 3749840 380028 10286864 0 0 339 19 7479 4704 20 2 77 1 0 2012-11-01 08:59:37
0 0 10896 3747536 380028 10286964 0 0 17 118 7488 4638 0 0 99 0 0 2012-11-01 08:59:42
0 0 10896 3747452 380028 10287148 0 0 0 24 7489 4676 0 0 99 0 0 2012-11-01 08:59:47


df = read_csv(""vmstatcsv""  parse_dates=[['date' 'time']])
f = DataFrame(df  columns=[ 'date_time'   'user_time'  'sys_time'  'wait_io_time'])

In [3]: f
Out[3]:
date_time               user_time  sys_time     wait_io_time
0  2012-11-01 08:59:27          3         1             0
1  2012-11-01 08:59:32          0         0             0
2  2012-11-01 08:59:37         20         2             1
3  2012-11-01 08:59:42          0         0             0
4  2012-11-01 08:59:47          0         0             0


So far  we could read the data correctly and date_time is combined in the DataFrame There are issues if I try to used the date_time from df as index Specifying index = dfdate_time gives all NaN values:

dindex = f['date_time']
print dindex
g = DataFrame(f  columns=[ 'user_time'  'sys_time'  'wait_io_time']  index=dindex)

In [7]: g
Out[7]:
0    2012-11-01 08:59:27
1    2012-11-01 08:59:32
2    2012-11-01 08:59:37
3    2012-11-01 08:59:42
4    2012-11-01 08:59:47
Name: date_time  ",814907,,1240268.0,2012-12-20 20:08:19,2012-12-20 20:08:19,pandas: read_csv combined date-time columns as index into a dataframe,<pandas><data-analysis>,2.0,,,
14035817,1,,2012-12-26 03:52:31,2,58,"DataFrameix() does not seem to slice the DataFrame that I want when negative indexing is used 

I have a DataFrame object and want to slice the last 2 rows

    In [90]: df = pdDataFrame(nprandomrandn(10  4))

    In [91]: df
    Out[91]: 
            0         1         2         3
    0  1985922  0664665 -2800102  1695480
    1  0580509  0782473  1032970  1559917
    2  0584387  1798743  0095950  0071999
    3  1956221  0075530 -0391008  1692585
    4 -0644979 -1959265  0749394 -0437995
    5 -1204964  0653912 -1426602  2409855
    6  1178886  2177259 -0165106  1145952
    7  1410595 -0761426 -1280866  0609122
    8  0110534 -0234781 -0819976  0252080
    9  1798894  0553394 -1358335  1278704


One way to do it:

    In [92]: df[-2:]
    Out[92]: 
              0         1         2         3
    8  0110534 -0234781 -0819976  0252080
    9  1798894  0553394 -1358335  1278704


Anther way to do it:

    In [93]: dfix[len(df)-2:  :]
    Out[93]: 
              0         1         2         3
    8  0110534 -0234781 -0819976  0252080
    9  1798894  0553394 -1358335  1278704


Now I want to use negative indexing  but having problem:

    In [94]: dfix[-2:  :]
    Out[94]: 
              0         1         2         3
    0  1985922  0664665 -2800102  1695480
    1  0580509  0782473  1032970  1559917
    2  0584387  1798743  0095950  0071999
    3  1956221  0075530 -0391008  1692585
    4 -0644979 -1959265  0749394 -0437995
    5 -1204964  0653912 -1426602  2409855
    6  1178886  2177259 -0165106  1145952
    7  1410595 -0761426 -1280866  0609122
    8  0110534 -0234781 -0819976  0252080
    9  1798894  0553394 -1358335  1278704


How do I use negative indexing with DataFrameix() correctly? Thanks",1928972,,,,2012-12-27 00:12:26,slicing pandas DataFrame with negative index with ix() method,<indexing><data.frame><pandas><slicing>,2.0,,,
13984461,1,13984636.0,2012-12-21 04:53:12,1,87,"I am using pandas to convert intraday data  stored in data_m  to daily data For some reason resample added rows for days that were not present in the intraday data For example  1/8/2000 is not in the intraday data  yet the daily data contains a row for that date with NaN as the value DatetimeIndex has more entries than the actual data Am I doing anything wrong?

data_mresample('D'  how = mean)head()
Out[13]: 
           x
2000-01-04 8803879581
2000-01-05 8765036649
2000-01-06 8893156250
2000-01-07 8780037433
2000-01-08 NaN

data_mresample('D'  how = mean)
Out[14]: 

DatetimeIndex: 4729 entries  2000-01-04 00:00:00 to 2012-12-14 00:00:00
Freq: D
Data columns:
x    3241  non-null values
dtypes: float64(1)
",1642513,,1642513.0,2012-12-21 05:03:49,2012-12-21 05:16:00,Python pandas resample added dates not present in the original data,<python><pandas>,2.0,,,
13996253,1,,2012-12-21 19:44:15,0,35,"I have been trying to make a set of GPS-related classes and functions  and decided to use Pandas  although I still use Numpy arrays  because Numpy's feature set is obviously much more complete

Something that bothered me a lot is the constant doubt as to ""should I use a numpy array or a pandas series/dataframe to represent this entity?"" It is very common (or at least I do it a lot) to pass data from one class/method/function to another  and the data I work with (mostly tracklogs) is very well represented with Pandas dataframe

The question(s) I have  then  are the following:

1) If classes should not reveal implementation details  how should I pass the array/series data without making it obvious (and thus creating a dependency) that it is a Numpy ndarray  or a Pandas dataframe  or whatever?

2) If I choose an ""implementation neutral"" format to pass the data (json  nested lists  etc)  doesn't it spoil the very reason to use those libraries in the first place?

3) [Optional question] If  from time to time  I end up needing Numpy functionality  wouldn't it be better to make my custom library ""numpy-only"" instead of intermixing Pandas and Numpy  sometimes converting back and forth between data formats?

Thanks for any insights!",401828,,401828.0,2012-12-21 19:59:19,2012-12-21 19:59:19,How to enforce encapsulation while using a mixed combination of similar libraries?,<oop><numpy><pandas><encapsulation><ooad>,,1.0,,2012-12-23 05:08:03
13996302,1,13998600.0,2012-12-21 19:49:00,2,98,"I have a time series object grouped of the type  groupedsum() gives the desired result but I cannot get rolling_sum to work with the groupby object Is there any way to apply rolling functions to groupby objects? For example:

x = range(0  6)
id = ['a'  'a'  'a'  'b'  'b'  'b']
df = DataFrame(zip(id  x)  columns = ['id'  'x'])
dfgroupby('id')sum()
id    x
a    3
b   12


However  I would like to have something like:

  id  x
0  a  0
1  a  1
2  a  3
3  b  3
4  b  7
5  b  12
",1642513,,1642513.0,2012-12-21 20:27:57,2012-12-21 23:41:42,Python - rolling functions for GroupBy object,<python><pandas>,2.0,4.0,1.0,
14043553,1,,2012-12-26 16:50:47,0,54,"I'm setting up my first analysis in Python and Pandas (newbie in both)  and have a few questions/issues on how to set this up properly  

Essentially  I am trying to look at user behavior in a time-series  but I have more users than days  so I am attempting to look monthly  I've built the DataFrame this way: 

df2 = pdDataFrame({'ID':range(100)})
df2['Day1'] = randomsample(xrange(1000)  100)
df2['Day2'] = randomsample(xrange(1000)  100)
df2['Day3'] = randomsample(xrange(1000)  100)


I've tried to add an index to the 'ID' column several ways  but 1) am not sure I need it and 2) none of my methods will take  Here is what I have tried:

df2 = pdDataFrame({'ID':range(100)}  index_col='ID')
df2 = pdDataFrame({'ID':range(100)}  index_col=0)
df2index(0)
df2index('ID')
df2reindex(index='ID')
df2reindex(index=0)


The end output of what I am trying to get to create a new dataframe which will show whether the Day2 values is 95% less than Day 1  whether Day 3 is 95% less than Day 2 - onward (imagine I had a DataFrame of 100 columns)  The output I would look might look like this:

ID   Day2   Day3
1    NaN    1
2    NaN    NaN
3    NaN    NaN
4    1      NaN


I believe the appropriate way to determine this is run a for loop with something like this:

for i in df2:
  if (Day2-Day1)/Day1 ",854739,,,,2012-12-29 07:21:22,Referencing/Calculating Previous columns in a DataFrame in Pandas,<python><pandas>,2.0,,,
14025879,1,14060360.0,2012-12-24 21:38:40,10,360,"Another pandas question:

I have this table with hierarchical indexing:

In [51]:
from pandas import DataFrame
f = DataFrame({'a': ['1' '2' '3']  'b': ['2' '3' '4']})
fcolumns = [['level1 item1'  'level1 item2'] [''  'level2 item2']  ['level3 item1'  'level3 item2']]
f
Out[51]:
    level1 item1    level1 item2
                    level2 item2
    level3 item1    level3 item2
0         1              2
1         2              3
2         3              4


It happens that selecting level1 item1 produces the following error:

In [58]: f['level1 item1']
AssertionError: Index length did not match values


However  this seems to be somewhat related to the number of levels When I reduce the number of levels to only two  there is no such error:

from pandas import DataFrame
f = DataFrame({'a': ['1' '2' '3']  'b': ['2' '3' '4']})
fcolumns = [['level1 item1'  'level1 item2'] [''  'level1 item2']]
f
Out[59]:
     level1 item1   level1 item2
                    level1 item2
0          1              2
1          2              3
2          3              4


Instead  the previous DataFrame gives the expected series:

In [63]:
f['level1 item1']
Out[63]:
0    1
1    2
2    3
Name: level1 item1


Filling the gap below level1 item1 with a dummy character ""fixes"" this issue but it's not a good solution

How can I fix this issue without resorting to fill those columns with dummy names?

Thanks a lot!

Original example:



This table was produced using the following indexes: 

index = [nparray(['Size and accumulated size of adjusted gross income'  'All returns'  'All returns'  'All returns'  'All returns'  'All returns'  'Taxable returns'  'Taxable returns'  'Taxable returns'  'Taxable returns'  'Taxable returns'  'Taxable returns'  'Taxable returns'  'Taxable returns'  'Taxable returns'  'Taxable returns'  'Taxable returns'  'Taxable returns'  'Taxable returns'  'Taxable returns'  'Taxable returns']) 
nparray([''  'Number of returns'  'Percent of total'  'Adjusted gross income less deficit'  'Adjusted gross income less deficit'  'Adjusted gross income less deficit'  'Number of returns'  'Percent of total'  'Adjusted gross income less deficit'  'Adjusted gross income less deficit'  'Taxable income'  'Taxable income'  'Taxable income'  'Income tax after credits'  'Income tax after credits'  'Income tax after credits'  'Total income tax'  'Total income tax'  'Total income tax'  'Total income tax'  'Total income tax']) 
nparray([''  ''  ''  ''  ''  ''  ''  '' ''  ''  'Number of returns'  'Amount'  'Percent of total'  'Number of returns'  'Amount'  'Percent of total'  'Amount'  'Percent of'  'Percent of'  'Percent of'  'Average total income tax (dollars)']) 
nparray([''  ''  ''  'Amount'  'Percent of total'  'Average (dollars)'  'Average (dollars)'  'Average (dollars)'  'Amount'  'Percent of total'  'Percent of total'  'Percent of total'  'Percent of total'  'Percent of total'  'Percent of total'  'Percent of total'  'Percent of total'  'Total'  'Taxable income'  'Adjusted gross income less deficit'  'Adjusted gross income less deficit'])]

dfcolumns = index


That's an almost perfect copy of some data in a CSV file but you can see that below ""Number of returns""  ""Percent of total"" and ""Adjusted gross income less deficit"" there  is a gap That gap produces this error when I try to select Number of returns:

In [68]: df['Taxable returns']['Number of returns']
AssertionError: Index length did not match values


I don't understand this error So a good explanation would be highly appreciated In any case  when I fill that gap using this index (notice the first elements in the third numpy array): 

index = [nparray(['Size and accumulated size of adjusted gross income'  'All returns'  'All returns'  'All returns'  'All returns'  'All returns'  'Taxable returns'  'Taxable returns'  'Taxable returns'  'Taxable returns'  'Taxable returns'  'Taxable returns'  'Taxable returns'  'Taxable returns'  'Taxable returns'  'Taxable returns'  'Taxable returns'  'Taxable returns'  'Taxable returns'  'Taxable returns'  'Taxable returns']) 
nparray([''  'Number of returns'  'Percent of total'  'Adjusted gross income less deficit'  'Adjusted gross income less deficit'  'Adjusted gross income less deficit'  'Number of returns'  'Percent of total'  'Adjusted gross income less deficit'  'Adjusted gross income less deficit'  'Taxable income'  'Taxable income'  'Taxable income'  'Income tax after credits'  'Income tax after credits'  'Income tax after credits'  'Total income tax'  'Total income tax'  'Total income tax'  'Total income tax'  'Total income tax']) 
nparray(['1'  '2'  '3'  '4'  '5'  '6'  '7'  '8' '9'  '10'  'Number of returns'  'Amount'  'Percent of total'  'Number of returns'  'Amount'  'Percent of total'  'Amount'  'Percent of'  'Percent of'  'Percent of'  'Average total income tax (dollars)']) 
nparray([''  ''  ''  'Amount'  'Percent of total'  'Average (dollars)'  'Average (dollars)'  'Average (dollars)'  'Amount'  'Percent of total'  'Percent of total'  'Percent of total'  'Percent of total'  'Percent of total'  'Percent of total'  'Percent of total'  'Percent of total'  'Total'  'Taxable income'  'Adjusted gross income less deficit'  'Adjusted gross income less deficit'])]

dfcolumns = index


I get proper results:

In [71]: df['Taxable returns']['Number of returns']
Out[71]:
7
Average (dollars)
0    90 660 104
1    3 495

",460147,,460147.0,2012-12-26 04:07:28,2012-12-27 19:46:05,Assertion Error in columns in DataFrame with hierarchical indexing,<python><pandas><hierarchical>,1.0,2.0,1.0,
14035148,1,,2012-12-26 01:20:26,1,69,"Merry Christmas  Im still very new to Python and Pandas so help is appreciated
I am trying to read in a netCDF file  which I can do and then import that into a Pandas Dataframe  The netcDF file is 2D so I just want to 'dump it in'  I have tried the DataFrame method but it doesn't recognize the object  Presumably I need to convert the netCDF object to a 2D numpy array?  Again thanks for any ideas on the best way to do this
Best wishes
Jason",1911866,,,,2013-01-24 01:18:17,Import netCDF file to Pandas dataframe,<python><data.frame><pandas><netcdf>,2.0,,,
14057007,1,,2012-12-27 15:24:13,1,53,Sorry just getting into Pandas  this seems like it should be a very straight forward question How can I use the isin('X') to remove rows that are in the list X? In R I would write !which(a %in% b),1927088,,1301710.0,2012-12-28 15:29:56,2012-12-28 15:29:56,Remove rows not .isin('X'),<python><filtering><pandas>,2.0,,,
14059094,1,,2012-12-27 18:02:41,3,90,"I'm trying to multiply two existing columns in a pandas Dataframe (orders_df) - Prices (stock close price) and Amount (stock quantities) and add the calculation to a new column called 'Value' For some reason when I run this code  all the rows under the 'Value' column are positive numbers  while some of the rows should be negative Under the Action column in the DataFrame there are seven rows with the 'Sell' string and seven with the 'Buy' string

for i in orders_dfAction:
 if i  == 'Sell':
  orders_df['Value'] = orders_dfPrices*orders_dfAmount
 elif i == 'Buy':
  orders_df['Value'] = -orders_dfPrices*orders_dfAmount)


Please let me know what i'm doing wrong !",1889418,,1301710.0,2012-12-28 14:36:58,2013-01-10 10:20:30,I want to multiply two columns in a pandas DataFrame and add the result into a new column,<python><python-2.7><pandas>,3.0,,,
14075337,1,14087161.0,2012-12-28 20:25:27,2,128,"I have a csv file with date  time  price  mag  signal 
62035 rows; there are 42 times of day associated to each unique date in the file

For each date  when there is an 'S' in the signal column append the corresponding price at the time the 'S' occurred Below is the attempt

from pandas import *
from numpy import *
from io import *
from os import *
from sys import *

DF1 = read_csv('___csv')
idf=DF1set_index(['date' 'time' 'price'] inplace=True)
sStore=[]
for i in idfindex[i][0]:
  sStoreappend([idfindex[j][2] for j in idf[j][1] if idf['signal']=='S'])
sStorehead()    




Traceback (most recent call last)
 in ()
  1 sStore=[]
  2 
----> 3 for time in idfindex[i][0]:
  4 
  5     sStoreappend([idfindex[j][2] for j in idf[j][1] if idf['signal']=='S'])

 NameError: name 'i' is not defined


I do not understand why the i index is not permitted here Thanks

I also think it's strange that : 

idfindexlevels[0] will show the dates ""not parsed"" as it is in the file but out of order Despite that  parse_date=True as an argument in set_index

I bring this up since I was thinking of side swiping the problem with something like: 

for i in idfindexlevels[0]:

   sStoreappend([idfindex[j][2] for j in idfindexlevels[1] if idf['signal']=='S'])

 sStorehead() 


My edit 12/30/2012 based on DSM's comment below:

I would like to use your idea to get the P&L  as I commented below Where if S!=B  for any given date  we difference using the closing time  1620 

v=[df[""signal""]==""S""]
t=[df[""time""]==""1620""]
u=[df[""signal""]!=""S""]

df[""price""][[v and (u and t)]]


That is  ""give me the price at 1620; (even when it doesn't give a ""sell signal""  S) so that I can diff with the ""extra B's""--for the special case where B>S This ignores the symmetric concern (where S>B) but for now I want to understand this logical issue 

On traceback  this expression gives: 

ValueError: boolean index array should have 1 dimension


Note that in order to invoke df[""time'] I do not set_index here Trying the union operator | gives: 

TypeError: unsupported operand type(s) for |: 'list' and 'list'


Looking at Max Fellows's approach 

@Max Fellows 

The point is to close out the positions at the end of the day; so we need to capture to price at the close to ""unload"" all those B  S which were accumulated; but didn't net each other out
If I say: 

filterFunc1 = lambda row: row[""signal""] == ""S"" and ([row[""signal""] != ""S""][row[""price""]==""1620""])
filterFunc2 =lambda row: ([row[""price""]==""1620""][row[""signal""] != ""S""])

filterFunc=filterFunc1 and filterFunc2

filteredData = itertoolsifilter(filterFunc  reader)


On traceback:

IndexError: list index out of range
",1374969,,1374969.0,2012-12-30 19:42:43,2012-12-30 19:42:43,List Comprehension Loop,<python><pandas>,4.0,3.0,,
14085517,1,14086002.0,2012-12-29 20:13:46,2,83,"I have a problem I did this:

In [405]: pippo=ass_t1pivot_table(['Rotazioni a volume' 'Distribuzione Ponderata'] rows=['SEGM1' 'DESC'])sort()

In [406]: pippo
Out[406]: 
                      Distribuzione Ponderata  Rotazioni a volume
SEGM1 DESC                                                       
AD     ACCADINAROLO                    74040       140249693409
      ZYMIL AMALAT Z                   90085       321529053570
FUN   SPECIALMALAT S                   88650       120711182177
NORM   STD INNAROLO                    49790       162259216710
       STD PNAROLO                    52125      1252174695695
       STD PLNAROLO                    54230       213257829615
      BONTA' MALAT B                   79280       520454366419
      DA STD RILGARD                   35290       554927497875
      OVANE VTMANTO                   15040       466232639628
      WEIGHT MALAT W                   79170       118628572692


My goal is to have each 'SEGM1' sorted by 'Distribuzione Ponderata' Egin 'NORM' subset the first row should be ""BONTA' MALAT B"" with the higher level of 'Distribuzione Ponderata'
I was able to achieve partially the result using groupby method but without being able to set multiple columns
Someone can help me please? 
Thanks a lot!

M",1937003,,1427416.0,2012-12-29 20:15:50,2013-01-08 14:34:55,pandas sorting pivot_table or grouping dataframe?,<python><table><sorting><pivot><pandas>,1.0,,0.0,
14087579,1,,2012-12-30 01:43:16,1,72,"I am trying to install pandas so that I can use it in iPython notebook I was able to install iPython notebook but can't get pandas to work I run sudo easy_install pandas and get this message: 

$ sudo easy_install pandas 

Searching for pandas
Reading http://pypipythonorg/simple/pandas/
Reading http://pandaspydataorg
Reading http://pandassourceforgenet
Best match: pandas 0100
Downloading http://pypipythonorg/packages/source/p/pandas/pandas-0100zip#md5=7a9e05448f86ce3e8b73370d87765926
Processing pandas-0100zip
Running pandas-0100/setuppy -q bdist_egg --dist-dir /tmp/easy_install-OpjlmZ/pandas-0100/egg-dist-tmp-baBKW6
warning: no files found matching 'setupeggpy'
no previously-included directories found matching 'doc/build'
warning: no previously-included files matching '*so' found anywhere in distribution
warning: no previously-included files matching '*pyd' found anywhere in distribution
warning: no previously-included files matching '*pyc' found anywhere in distribution
warning: no previously-included files matching 'git*' found anywhere in distribution
warning: no previously-included files matching 'DS_Store' found anywhere in distribution
warning: no previously-included files matching '*png' found anywhere in distribution
In file included from pandas/indexc:259:
pandas/src/numpy_helperh: In function infer_type:
pandas/src/numpy_helperh:32: error: PyDatetimeArrType_Type undeclared (first use in this function)
pandas/src/numpy_helperh:32: error: (Each undeclared identifier is reported only once
pandas/src/numpy_helperh:32: error: for each function it appears in)
pandas/src/numpy_helperh: In function get_datetime64_value:
pandas/src/numpy_helperh:53: error: PyDatetimeScalarObject undeclared (first use in this function)
pandas/src/numpy_helperh:53: error: expected expression before ) token
pandas/src/numpy_helperh: In function is_datetime64_object:
pandas/src/numpy_helperh:85: error: PyDatetimeArrType_Type undeclared (first use in this function)
In file included from pandas/indexc:266:
pandas/src/datetime/np_datetimeh: At top level:
pandas/src/datetime/np_datetimeh:107: error: expected declaration specifiers or  before NPY_CASTING
In file included from pandas/indexc:267:
pandas/src/datetime/np_datetime_stringsh:46: error: expected declaration specifiers or  before NPY_CASTING
pandas/src/datetime/np_datetime_stringsh:84: error: expected declaration specifiers or  before NPY_CASTING
pandas/indexc: In function __pyx_pf_6pandas_5index_get_value_at:
pandas/indexc:1899: error: NPY_DATETIME undeclared (first use in this function)
pandas/indexc: In function __pyx_f_6pandas_5index_11IndexEngine_get_value:
pandas/indexc:2395: error: NPY_DATETIME undeclared (first use in this function)
pandas/indexc: In function __pyx_f_6pandas_5index_convert_scalar:
pandas/indexc:8514: error: NPY_DATETIME undeclared (first use in this function)
pandas/indexc: In function __pyx_f_8datetime__cstring_to_dts:
pandas/indexc:9151: error: NPY_UNSAFE_CASTING undeclared (first use in this function)
pandas/indexc:9151: warning: passing argument 5 of parse_iso_8601_datetime from incompatible pointer type
pandas/indexc:9151: warning: passing argument 6 of parse_iso_8601_datetime from incompatible pointer type
pandas/indexc:9151: warning: passing argument 7 of parse_iso_8601_datetime from incompatible pointer type
pandas/indexc:9151: error: too many arguments to function parse_iso_8601_datetime
error: Setup script exited with error: command 'gcc' failed with exit status 1
",1367204,,,,2012-12-30 01:43:16,Just got a macbook Air 10.8 but I can't install pandas,<python><bash><terminal><root><pandas>,,3.0,,2012-12-30 10:33:10
14012235,1,,2012-12-23 15:34:17,2,78,"I have a 3125MB csv file containing EURUSD 1min OHLC data from 27/7/2003 to date  but the dates are all adjusted for daylight saving  meaning I get duplicates and gaps

Seeing as it's such a big file the default date parser was way too slow  so I did this:

tizo = dateutiltztzfile('/usr/share/zoneinfo/GB')
def date_parse_1min(s):
    return datetime(int(s[6:10])  
                    int(s[3:5])  
                    int(s[0:2])  
                    int(s[11:13]) 
                    int(s[14:16]) tzinfo=tizo)

df = read_csv(""EURUSD_1m_clean_w_headercsv"" index_col=0 parse_dates=True  date_parser=date_parse_1min)

#verify that it's got the tz right:
dfindex
Exception AttributeError: ""'NoneType' object has no attribute 'toordinal'"" in 'pandastslib_localize_tso' ignored
Exception AttributeError: ""'NoneType' object has no attribute 'toordinal'"" in 'pandastslib_localize_tso' ignored

[2003-07-26 23:00:00    2012-12-15 23:59:00]
Length: 4938660  Freq: None  Timezone: tzfile('/usr/share/zoneinfo/GB')


No idea why there are attribute errors there

dfindexget_duplicates()

[2003-10-26 01:00:00    2012-10-28 01:59:00]
Length: 600  Freq: None  Timezone: None
df1 = dftz_convert('GMT')
df1indexget_duplicates()

[2003-10-26 01:00:00    2012-10-28 01:59:00]
Length: 600  Freq: None  Timezone: None


How can I get pandas to remove the daylight saving offset? Obviously I could work out the right integer indexes that need changing and do it like that  but there must be a better way",1019856,,,,2012-12-24 03:03:07,Pandas read_csv and remove daylight saving,<python><pandas><dateutil>,1.0,2.0,,
14025549,1,14025598.0,2012-12-24 20:43:59,7,65,"How can I change every element in a DataFrame with hierarchical indexing? For example  maybe I want to convert strings into floats:

from pandas import DataFrame
f = DataFrame({'a': ['1 000' '2 000' '3 000']  'b': ['2 000' '3 000' '4 000']})
fcolumns = [['level1'  'level1'] ['item1'  'item2']]
f
Out[152]:
        level1
     item1   item2
0    1 000   2 000
1    2 000   3 000
2    3 000   4 000


I tried this:

def clean(group):
    group = groupmap(lambda x: xreplace(' '  ''))
    return group
fapply(clean)
Out[153]:
(level1  item1) (level1  item2)
0    1000    2000
1    2000    3000
2    3000    4000


As you can see  it changes the hierarchical indexing quite a bit How can I avoid this? Or maybe there is a better way

Thanks",460147,,,,2012-12-25 00:08:06,Changing data in a dataframe with hierarchical indexing,<python><pandas><hierarchical>,1.0,,,
14075855,1,,2012-12-28 21:11:54,1,55,"In my data I have stock volumes for order sequence and times  I need to go through each part of the order and find when it ends  by grabbing the next part of the chains time

I am just starting in python and I would do this by subsetting each stock into its own pool  then adding then do another loop to find the time of the next order for that sequence Ultimately  in R/Matlab you could go X$time[1:end-1] ",1927088,,487339.0,2012-12-28 22:23:36,2012-12-29 00:00:31,groupby functions to get subsequent value,<group-by><pandas>,1.0,0.0,,
14092798,1,14092840.0,2012-12-30 17:19:57,1,65,"This is a pretty bizarre bug:

The following code:

a = 'string1'
b = 'string2'

test_dict = {'col1':{a:type(a)  b:type(b)} 'col2':{a:type(a)  b:type(b)}}
pdDataFrame(test_dict)


In a normal ipython console yields the following as expected:

                col1          col2
string1  
string2  


However in the ipython notebook  the cells where the type should be displayed is empty:

",1438637,,,,2012-12-31 06:09:54,Ipython Notebook not printing type() in dataframe,<python><pandas><ipython><ipython-notebook>,2.0,,,
14084234,1,,2012-12-29 17:37:37,1,123,"I have a DataFrame with the following structure:


DatetimeIndex: 3333 entries  2000-01-03 00:00:00+00:00 to 2012-11-21 00:00:00+00:00
Data columns:
open          3333  non-null values
high          3333  non-null values
low           3333  non-null values
close         3333  non-null values
volume        3333  non-null values
amount        3333  non-null values
pct_change    3332  non-null values
dtypes: float64(7)


The pct_change column contains percent change data

Given a filtered DatetimeIndex from the DataFrame above:


[2000-03-01 00:00:00    2012-11-01 00:00:00]
Length: 195  Freq: None  Timezone: UTC


I want to filter starting each date entry and return the first row where pct_change column is below 0015 

I came up with this solution but it is very slow:

stops = []
#dates = DatetimeIndex
for d in dates:
    #check if pct_change is below -0015 starting from date of signal return date of first match
    match = df[df[""pct_change""] ",439693,,439693.0,2012-12-29 17:43:17,2013-01-02 19:58:16,pandas efficient way to get first filtered row for each DatetimeIndex entry,<python><numpy><pandas><time-series>,2.0,,,
14093069,1,,2012-12-30 17:51:16,0,44,"I'm trying to assign an index that will begin at X and end at Y  

For example:

startday -01012012 
endday - 31022012 


Another DataFrame mimics 14 stock orders during that period of time with the following columns:
Date  Ticker  Stock Amount  Price (closing price)  Value (orders_dfAmount * orders_dfPrice)

I'm looking to calculate the end of day value of the portfolio (cash + stock value)  starting cash equals 1 000 000

I'm not sure how to assign the index for the DataFrame  (for the first DataFrame) for the entire period So far I was able to calculate on the portfolio value for the 14 days of stock orders

cashappend(1000000)
cash = pdSeries(index=timestamps)
day = startday
for i in orders_dfDate:
    if i == day:
        cash = cash + orders_dfValue
",1889418,,1240268.0,2012-12-30 18:05:40,2012-12-30 18:05:40,"How to set an index range from a ""startday"" to an ""endday"" and cycle through the DataFrame",<python><pandas>,,3.0,,
14112020,1,14112100.0,2013-01-01 16:19:09,2,40,"I have run into an issue when comparing two DatetimeIndex's with different lengths in an assert like the following:

In [1]: idx1 = pddate_range('2010-01-01' '2010-12-31' freq='D')

In [2]: idx2 = pddate_range('2010-01-01' '2010-11-01' freq='D')

In [3]: assert (idx1 == idx2)all()


I get the error:

---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
 in ()
----> 1 assert (idx1 == idx2)all()

/opt/local/Library/Frameworks/Pythonframework/Versions/27/lib/python27/site-packages/pandas-0101dev_dcd9df7-py27-macosx-108-x86_64egg/pandas/tseries/indexpyc in wrapper(self  other)
     75         result = func(other)
     76 
---> 77         return resultview(npndarray)
     78 
     79     return wrapper

AttributeError: 'NotImplementedType' object has no attribute 'view'


Which is fine if this is not implemented  yet  but is there some pandas way of doing this?

Note:
I have used the following with success:

In [3]: assert list(idx1) == list(idx2)


So  the following also works:

In [3]: assert list(dfindex) == list(testindex)


But I would like to know if there is a more pandas-ish way of doing this",1742878,,1240268.0,2013-01-02 01:24:11,2013-01-02 01:24:11,Ensure two Pandas DatetimeIndexes are the same?,<python><pandas>,1.0,0.0,1.0,
13988111,1,13993480.0,2012-12-21 10:10:00,4,138,"On my debian squeeze system  I ran into a python problem that can be distilled to the following:

import numpy
import datetime
from matplotlib import pyplot
x = [datetimedatetimeutcfromtimestamp(i) for i in numpyarange(100000 200000 3600)]
y = range(len(x))

# See matplotlib handle a series of datetimes just fine
pyplotplot(x  y)
# []

import pandas

# Now we try exactly what we did before
pyplotplot(x  y)
Traceback (most recent call last):
  File """"  line 1  in 
  File ""/usr/lib/pymodules/python26/matplotlib/pyplotpy""  line 2141  in plot
    ret = axplot(*args  **kwargs)
  File ""/usr/lib/pymodules/python26/matplotlib/axespy""  line 3432  in plot
    for line in self_get_lines(*args  **kwargs):
  File ""/usr/lib/pymodules/python26/matplotlib/axespy""  line 311  in _grab_next_args
    for seg in self_plot_args(remaining  kwargs):
  File ""/usr/lib/pymodules/python26/matplotlib/axespy""  line 288  in _plot_args
    x  y = self_xy_from_xy(x  y)
  File ""/usr/lib/pymodules/python26/matplotlib/axespy""  line 204  in _xy_from_xy
    bx = selfaxesxaxisupdate_units(x)
  File ""/usr/lib/pymodules/python26/matplotlib/axispy""  line 982  in update_units
    self_update_axisinfo()
  File ""/usr/lib/pymodules/python26/matplotlib/axispy""  line 994  in _update_axisinfo
    info = selfconverteraxisinfo(selfunits  self)
  File ""/usr/local/lib/python26/dist-packages/pandas/tseries/converterpy""  line 184  in axisinfo
    majfmt = PandasAutoDateFormatter(majloc  tz=tz)
  File ""/usr/local/lib/python26/dist-packages/pandas/tseries/converterpy""  line 195  in __init__
    datesAutoDateFormatter__init__(self  locator  tz  defaultfmt)
TypeError: __init__() takes at most 3 arguments (4 given)


I'm not interested in the cause of the particular error shown  it's obvious enough that pandas expected a different version of matplotlib -- that's a fair risk of getting one package from the standard debian repository and the other through pip  and I already 'solved' that part of the problem by allowing pip to upgrade matplotlib

The real issue is -- and now comes a threefold question: how can it be that just importing pandas broke matplotlib's ability to handle datetime objects  when just two lines earlier pandas was clearly not even involved in that same operation? Does pandas upon import silently alter other modules in the top level namespace to force them to make use of pandas methods? And is this acceptable behavour for a python module? Because I need to be able to rely on it that importing  say  a random number module  won't silently change  say  the pickle module to apply a random salt to everything it writes

Update with further information

python is 266 (current debian stable from package 266-3+squeeze7)

matplotlib version was debian's 0993-1 (current debian stable from package python-matplotlib)

pandas version was 090 (installed with 'pip install pandas'  a while ago -- not today)

Platform is an i386 running debian Squeeze

Steps to replicate

(obvious) Bootstrap a clean debian squeeze i386 installation and chroot into it
apt-get update
apt-get install python python-matplotlib
apt-get install python-pip build-essential python-dev
pip install --upgrade numpy
pip install pandas
Now start an interactive python session

import numpy
import datetime
# Next two lines added to original example to avoid hassle with DISPLAY in chroot
import matplotlib
matplotlibuse('agg')
from matplotlib import pyplot

x = [datetimedatetimeutcfromtimestamp(i) for i in numpyarange(100000 200000 3600)]
y = range(len(x))

pyplotplot(x  y)

import pandas

pyplotplot(x  y)
",1921146,,1921146.0,2012-12-21 12:43:32,2012-12-21 16:19:59,Importing pandas in python changes how matplotlib handles datetime objects?,<python><datetime><matplotlib><pandas>,1.0,7.0,,
14002158,1,14002174.0,2012-12-22 10:49:03,2,39,I'd like to set the time zone of the values of a column in a Pandas DataFrame I am reading the DataFrame with pandasread_csv(),1579844,,,,2012-12-22 11:43:29,How to set time zone of values in a Pandas DataFrame?,<numpy><timezone><pandas>,1.0,,,
14004545,1,,2012-12-22 16:34:15,0,40,"I have a pandas Series with values of type datetime64[ns] The dates are in EST timezone  and I would like to convert them to UTC timezone

Eg 

 s=pdSeries(pddate_range('2012-1-1 1:30' periods=3 freq='min'))


How to convert s to UTC?

(Note that I don't actually use date_range() so using its tz parameter is not an option)",1579844,,1579844.0,2012-12-22 17:02:08,2012-12-23 16:11:07,How to convert the time zone of the values of a Pandas Series,<datetime><numpy><timezone><pandas>,1.0,,,
14024287,1,,2012-12-24 17:48:49,2,54,"I am using the groupby and sum to quickly aggregate accros two data sets 

A which contains:

sequence shares
1        100
2        200
3        50
1        200


B which contains:

sequence shares
1        100
2        200
2        50
3        50


I am using A=Agroupby(['sequence'])sum() and B=Bgroupby(['sequence'])sum() to sum the shares across each sequence I then want to concatenate these sets again and sum the shares across sequences However  I try using C = concat([A B]) and now find that I only have the column shares as an index and cannot group by sequence Cgroup(['sequence'])sum() gives me an error KeyError: u'no item named sequence'

What I would like to get out is C:

sequence shares
1        400
2        450
3        100


Can anyone explain what is going on here? I could concatenate before groupby()sum() but I really want to knock these data sets down to smaller sizes before concatenating them",1927088,,487339.0,2012-12-24 18:35:25,2013-01-15 14:09:05,Pandas groupby on concatenated groupby objects,<python><pandas>,1.0,,,
14075326,1,14076064.0,2012-12-28 20:23:34,0,34,"When I execute the following statement:

DataFrame(randn(3 1) index=[date(2012 10 1) date(2012 9 1) date(2012 8 1)] columns=['test'])plot() 


I get the following exception:

File ""/usr/local/lib/python27/dist-packages/pandas-0100-py27-linux-x86_64egg/pandas/tseries/converterpy""  line 317  in call
    (estimate  dmin  dmax  selfMAXTICKS * 2))
RuntimeError: MillisecondLocator estimated to generate 5270400 ticks from 2012-08-01 00:00:00+00:00 to 2012-10-01 00:00:00+00:00: exceeds LocatorMAXTICKS* 2 (2000)

Any workaround available for this bug ?",1915862,,,,2012-12-28 21:40:55,pandas RuntimeError in tseries/convertor when plotting,<python><plot><pandas>,1.0,,,
14077355,1,14077469.0,2012-12-29 00:06:16,0,36,"I'm having an issue changing a pandas DataFrame index to a datetime from an integer I want to do it so that I can call reindex and fill in the dates between those listed in the table Note that I have to use pandas 073 at the moment because I'm also using qstk  and qstk relies on pandas 073

First  here's my layout:

(Pdb) df
    AAPL  GOOG   IBM   XOM                 date
1      0     0  4000     0  2011-01-13 16:00:00
2      0  1000  4000     0  2011-01-26 16:00:00
3      0  1000  4000     0  2011-02-02 16:00:00
4      0  1000  4000  4000  2011-02-10 16:00:00
6      0     0  1800  4000  2011-03-03 16:00:00
7      0     0  3300  4000  2011-06-03 16:00:00
8      0     0     0  4000  2011-05-03 16:00:00
9   1200     0     0  4000  2011-06-10 16:00:00
11  1200     0     0  4000  2011-08-01 16:00:00
12     0     0     0  4000  2011-12-20 16:00:00

(Pdb) type(df['date'])


(Pdb) df2 = DataFrame(index=df['date'])
(Pdb) df2
Empty DataFrame
Columns: array([]  dtype=object)
Index: array([2011-01-13 16:00:00  2011-01-26 16:00:00  2011-02-02 16:00:00 
       2011-02-10 16:00:00  2011-03-03 16:00:00  2011-06-03 16:00:00 
       2011-05-03 16:00:00  2011-06-10 16:00:00  2011-08-01 16:00:00 
       2011-12-20 16:00:00]  dtype=object)

(Pdb) df2merge(df left_index=True right_on='date')
    AAPL  GOOG   IBM   XOM                 date
1      0     0  4000     0  2011-01-13 16:00:00
2      0  1000  4000     0  2011-01-26 16:00:00
3      0  1000  4000     0  2011-02-02 16:00:00
4      0  1000  4000  4000  2011-02-10 16:00:00
6      0     0  1800  4000  2011-03-03 16:00:00
8      0     0     0  4000  2011-05-03 16:00:00
7      0     0  3300  4000  2011-06-03 16:00:00
9   1200     0     0  4000  2011-06-10 16:00:00
11  1200     0     0  4000  2011-08-01 16:00:00
12     0     0     0  4000  2011-12-20 16:00:00


I have tried multiple things to get a datetime index:

1) Using the reindex() method with a list of datetime values This creates a datetime index  but then fills in NaNs for the data in the DataFrame I'm guessing that this is because the original values are tied to the integer index and reindexing to datetime tries to fill the new indices with default values (NaNs if no fill method is indicated) Thusly:

(Pdb) dfreindex(index=df['date'])
                     AAPL  GOOG  IBM  XOM date
date                                          
2011-01-13 16:00:00   NaN   NaN  NaN  NaN  NaN
2011-01-26 16:00:00   NaN   NaN  NaN  NaN  NaN
2011-02-02 16:00:00   NaN   NaN  NaN  NaN  NaN
2011-02-10 16:00:00   NaN   NaN  NaN  NaN  NaN
2011-03-03 16:00:00   NaN   NaN  NaN  NaN  NaN
2011-06-03 16:00:00   NaN   NaN  NaN  NaN  NaN
2011-05-03 16:00:00   NaN   NaN  NaN  NaN  NaN
2011-06-10 16:00:00   NaN   NaN  NaN  NaN  NaN
2011-08-01 16:00:00   NaN   NaN  NaN  NaN  NaN
2011-12-20 16:00:00   NaN   NaN  NaN  NaN  NaN


2) Using DataFramemerge with my original df and a second dataframe  df2  that is basically just a datetime index with nothing else So I end up doing something like:

(pdb) df2merge(df left_index=True right_on='date')
    AAPL  GOOG   IBM   XOM                 date
1      0     0  4000     0  2011-01-13 16:00:00
2      0  1000  4000     0  2011-01-26 16:00:00
3      0  1000  4000     0  2011-02-02 16:00:00
4      0  1000  4000  4000  2011-02-10 16:00:00
6      0     0  1800  4000  2011-03-03 16:00:00
8      0     0     0  4000  2011-05-03 16:00:00
7      0     0  3300  4000  2011-06-03 16:00:00
9   1200     0     0  4000  2011-06-10 16:00:00
11  1200     0     0  4000  2011-08-01 16:00:00


(and vice-versa) But I always end up with this kind of thing  with integer indices

3) Starting with an empty DataFrame with a datetime index (created from the 'date' field of df) and a bunch of empty columns Then I attempt to assign each column by setting the columns with the same
names to be equal to the columns from df:

(Pdb) df2['GOOG']=0
(Pdb) df2
                     GOOG
date                     
2011-01-13 16:00:00     0
2011-01-26 16:00:00     0
2011-02-02 16:00:00     0
2011-02-10 16:00:00     0
2011-03-03 16:00:00     0
2011-06-03 16:00:00     0
2011-05-03 16:00:00     0
2011-06-10 16:00:00     0
2011-08-01 16:00:00     0
2011-12-20 16:00:00     0
(Pdb) df2['GOOG'] = df['GOOG']
(Pdb) df2
                     GOOG
date                     
2011-01-13 16:00:00   NaN
2011-01-26 16:00:00   NaN
2011-02-02 16:00:00   NaN
2011-02-10 16:00:00   NaN
2011-03-03 16:00:00   NaN
2011-06-03 16:00:00   NaN
2011-05-03 16:00:00   NaN
2011-06-10 16:00:00   NaN
2011-08-01 16:00:00   NaN
2011-12-20 16:00:00   NaN


So  how in pandas 073 do I get df to be re-created with an datetime index instead of the integer index? What am I missing? ",1410836,,,,2012-12-29 11:47:38,Having an issue changing index from integer to date in pandas,<python><pandas>,1.0,2.0,,
14125428,1,14125503.0,2013-01-02 16:20:20,0,65,"I have a pandas groupby object called grouped I can get groupedmean() and other simple functions to work  but I cannot get groupedquantile() to work I get the following error when attempting to run groupedquantile():

ValueError: ('invalid literal for float(): groupA'  u'occurred at index groups')


I am grouping by text labels  so I am not sure why the function tries to convert it to a float It should be computing the quantile using the floats within each group Can someone help to point out what I am doing wrong?",1642513,,,,2013-01-02 16:25:28,How to apply quantile to pandas groupby object?,<python><pandas>,1.0,1.0,,
14081357,1,,2012-12-29 11:50:42,0,33,"I am trying to apply a function to a column in a pandas DataFrame  using sth like:

df = pdDataFrame(list(source_data)  columns = ['a' 'b'])
df['a'] = df['a']apply(lambda x: func(x))


Where func is a function defined earlier in the code

It works on my development environment (OS X mountain lion + python 273 + pandas 0101); however  it reports the following error when run on my server (ubuntu 804 server edition + python 273 + pandas 0100)

AttributeError: 'unicode' object has no attribute 'apply'


Has anyone come across this problem before? Any pointer would be greatly appreciated!",1872240,,,,2012-12-29 11:50:42,pandas df.apply() works on Mac OS X but not on Ubuntu,<python><osx><ubuntu><pandas>,,2.0,,2012-12-29 15:44:45
14092339,1,14093125.0,2012-12-30 16:24:49,1,45,"having following timeseries:

In [65]: p
Out[65]: 
Date
2008-06-02    12520
2008-06-03    12447
2008-06-04    12440
2008-06-05    12689
2008-06-06    12284
2008-06-09    12314
2008-06-10    12253
2008-06-11    12073
2008-06-12    12119
Name: SPY


how can I slice on a specfic date +/- 2 neighbouring (business) days  so ie if d = '2008-06-06':

 -2   2008-06-04    12440
 -1   2008-06-05    12689
  0   2008-06-06    12284
  1   2008-06-09    12314
  2   2008-06-10    12253
",1915862,,1915862.0,2012-12-30 16:37:43,2012-12-30 17:57:46,slice pandas timeseries on date +/- 2 business days,<python><pandas><time-series><slice>,1.0,,,
14102195,1,14102891.0,2012-12-31 13:28:27,3,83,"I am using a pandas/python dataframe  I am trying to do a lag subtraction 

I am currently using:

newCol = dfcol - dfcolshift()


This leads to a NaN in the first spot:

NaN
45
63
23



First question:  Is this the best way to do a subtraction like this?

Second:  If I want to add a column (same number of rows) to this new column  Is there a way that I can make all the NaN's 0's for the calculation?

Ex:

col_1 = 
Nan
45
63
23

col_2 = 
10
10
10
10

new_col = 
10
55
73
33


and NOT

NaN
55
73
33


Thank you",1911092,,891306.0,2012-12-31 13:43:37,2012-12-31 14:41:17,Pandas column addition/subtraction,<python><pandas>,1.0,,,
14106439,1,,2012-12-31 22:10:55,0,101,"I have tried installing from 

source (python setuppy install into the extracted tar ball dir)
using pip
using easy_install but nothing seems to workI have downloaded and upgraded xcode  installed command-line tools
I cloned the github repository for pandas 

cd /pandas
python setuppy install 
running install
running bdist_egg
running egg_info
writing requirements to pandasegg-info/requirestxt
writing pandasegg-info/PKG-INFO
writing top-level names to pandasegg-info/top_leveltxt
writing dependency_links to pandasegg-info/dependency_linkstxt
reading manifest file 'pandasegg-info/SOURCEStxt'
reading manifest template 'MANIFESTin'
warning: no files found matching 'setupeggpy'
no previously-included directories found matching 'doc/build'
warning: no previously-included files matching '*so' found anywhere in distribution
warning: no previously-included files matching '*pyd' found anywhere in distribution
warning: no previously-included files matching '*pyc' found anywhere in distribution
warning: no previously-included files matching 'DS_Store' found anywhere in distribution
writing manifest file 'pandasegg-info/SOURCEStxt'
installing library code to build/bdistmacosx-106-intel/egg
running install_lib
running build_py
copying pandas/versionpy -> build/libmacosx-106-intel-27/pandas
running build_ext
**gcc-42 not found  using clang instead**
building 'pandasindex' extension
clang -fno-strict-aliasing -fno-common -dynamic -arch i386 -arch x86_64 -g -O2 -DNDEBUG -g -O3 -I/Library/Frameworks/Pythonframework/Versions/27/lib/python27/site-packages/numpy/core/include -Ipandas/src/klib -Ipandas/src -I/Library/Frameworks/Pythonframework/Versions/27/include/python27 -c pandas/indexc -o build/tempmacosx-106-intel-27/pandas/indexo
In file included from pandas/indexc:260:
In file included from pandas/src/klib/khash_pythonh:3:
pandas/src/klib/khashh:573:1: warning: expression result unused [-Wunused-value]
KHASH_MAP_INIT_STR(str  size_t)
^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
pandas/src/klib/khashh:565:2: note: expanded from macro 'KHASH_MAP_INIT_STR'
    KHASH_INIT(name  kh_cstr_t  khval_t  1  kh_str_hash_func  kh_str_hash_equal)
    ^


---more output like thatand in the end

Installed /Library/Frameworks/Pythonframework/Versions/27/lib/python27/site-      packages/pandas-0101dev_c934e02-py27-macosx-106-intelegg
Processing dependencies for pandas==0101dev-c934e02
Searching for pytz
Reading http://pypipythonorg/simple/pytz/
Reading http://pytzsourceforgenet
Reading http://sourceforgenet/project/showfilesphp?group_id=79122
Reading http://wwwstuartbishopnet/Software/pytz
Reading http://sourceforgenet/projects/pytz/
Best match: pytz 2012h
Downloading http://pypipythonorg/packages/27/p/pytz/pytz-2012h-py27egg#md5=4258fcfc023e9ff0057405d935fc6e1d
Processing pytz-2012h-py27egg
creating /Library/Frameworks/Pythonframework/Versions/27/lib/python27/site-packages/pytz-2012h-py27egg
Extracting pytz-2012h-py27egg to /Library/Frameworks/Pythonframework/Versions/27/lib/python27/site-packages
Adding pytz 2012h to easy-installpth file

Installed /Library/Frameworks/Pythonframework/Versions/27/lib/python27/site-packages/pytz-2012h-py27egg
-----
Installed /Library/Frameworks/Pythonframework/Versions/27/lib/python27/site-packages/six-120-py27egg
Searching for numpy==162
Best match: numpy 162
Adding numpy 162 to easy-installpth file

Using /Library/Frameworks/Pythonframework/Versions/27/lib/python27/site-packages
Finished processing dependencies for pandas==0101dev-c934e02

ipython

Python 273 (v273:70274d53c1dd  Apr  9 2012  20:52:43) 
Type ""copyright""  ""credits"" or ""license"" for more information

IPython 014dev -- An enhanced Interactive Python
?         -> Introduction and overview of IPython's features
%quickref -> Quick reference
help      -> Python's own help system
object?   -> Details about 'object'  use 'object??' for extra details

In [1]: import pandas


seems to work without issues

when i use easy_install to install pandas  the on-screen output seems to suggest that it worked but on loading  python is not able to find the library

sudo easy_install pandas
Searching for pandas
Best match: pandas 0101dev-c934e02
Processing pandas-0101dev_c934e02-py27-macosx-108-intelegg
pandas 0101dev-c934e02 is already the active version in easy-installpth

Using /usr/local/lib/python27/site-packages/pandas-0101dev_c934e02-py27-macosx-108-intelegg
Processing dependencies for pandas
Finished processing dependencies for pandas

dekumar-mn:ipython dekumar$ python
Python 273 (v273:70274d53c1dd  Apr  9 2012  20:52:43) 
[GCC 421 (Apple Inc build 5666) (dot 3)] on darwin
Type ""help""  ""copyright""  ""credits"" or ""license"" for more information

>>> import pandas as pd
Traceback (most recent call last):
File """"  line 1  in 
ImportError: No module named pandas
",435587,,198633.0,2012-12-31 23:12:31,2012-12-31 23:24:56,python pandas installation issues,<python><pandas><easy-install><brew>,1.0,10.0,,
14110721,1,14110955.0,2013-01-01 13:05:42,2,74,"I have a df :

>>> df
                   sales     cash
STK_ID RPT_Date                  
000568 20120930   80093   57488
000596 20120930   32585   26177
000799 20120930   14784    8157


And want to change first row's index value from ('000568' '20120930') to ('000999' '20121231')  Final result will be :

>>> df
                   sales     cash
STK_ID RPT_Date                  
000999 20121231   80093   57488
000596 20120930   32585   26177
000799 20120930   14784    8157


How to do ?",1072888,,,,2013-01-01 13:58:15,How to change Pandas dataframe index value?,<python><pandas>,1.0,,1.0,
14129979,1,14131840.0,2013-01-02 22:07:28,2,66,"I have a large correlation matrix in a pandas python DataFrame: df (342  342)

How do I take the mean  sd  etc of all of the numbers in the upper triangle not including the 1's along the diagonal?

Thank you",1911092,,,,2013-01-03 01:52:10,Mean of a correlation matrix - pandas data fram,<python><pandas>,2.0,1.0,,
14132216,1,14137316.0,2013-01-03 02:22:42,1,93,"I am trying to compute a daily P&L  with 10 min prices in a csv (there are 42 times for each date)---where number of buys and number of sells in a day could be unequal If they are unequal  the program should use the closing price for that unique date df[""price""][t] to subtract (from/by) depending on whether it's a buy or sell

import pandas as pd

df=pdread_csv(""filecsv""  names=""date time price mag signal""split())

s=df[""signal""]==""S""
b=df[""signal""]==""B""
ns=df[""signal""]!=""S""
nb=df[""signal""]!=""B""
t=df[""time""]==""1620""

a1=df[""price""][buy|(nb & t)]
b1=df[""date""][buy|(nb & t)]

h=df[""price""][s|(ns & t)]
g=df[""date""][s|(ns & t)]


c1=zip(b1 a1)
c=zip(g h)


c1  c are lists containing number of buys and sells  alongside its respective date The problem here is c1 & c are strings--once they're zipped; hence cannot be subtracted Is it possible to make a1  h floating point numbers so that I can difference them?

I want to match dates in c  c1 to subtract the prices at the Sells-Buys: S_i-B_i  for all i on a given day  then sum all and return that one value  for every date I'd like to difference the prices at h-a1  only when the dates match 

Some sample data:

date    time    price   mag signal 

1/3/2007    930 14228
1/3/2007    940 14232  0
1/3/2007    950 14228  0
1/3/2007    1000    14205  0
1/3/2007    1010    14228  0
1/3/2007    1020    14262  1  S 





1/3/2007    1230    14242  -1  B

1/3/2007    1240    14248  0
1/3/2007    1250    14258  1   S

1/3/2007    1300    1426    0
1/3/2007    1310    1425    0
1/3/2007    1320    14235  -1  B

1/3/2007    1330    14218  0
1/3/2007    1340    14215  0
1/3/2007    1350    14205  0
1/3/2007    1400    1421    0
1/3/2007    1410    14172  -1  B

1/3/2007    1420    14128  -1  B

1/3/2007    1430    14148  0
1/3/2007    1440    14135  0
1/3/2007    1450    1410    0
1/3/2007    1500    14072  -1  B

1/3/2007    1510    14102  1   S

1/3/2007    1520    14095  -1  B

1/3/2007    1530    14105  1   S

1/3/2007    1540    14125  0


1/3/2007    1610    14155  1   S

1/3/2007    1620    1414    -1  B

1/4/2007    930 14122  0
1/4/2007    940 1411    0
1/4/2007    950 1413    0
1/4/2007    1000    14122  0
1/4/2007    1010    14072  -1  B

The result of the zip  say  c1 should look something like this:

[('1/3/2007'  '14242') 
('1/3/2007'  '14235') 
('1/3/2007'  '14172') 
('1/3/2007'  '14128') 
('1/3/2007'  '14072') 
('1/3/2007'  '14095') 
('1/3/2007'  '1414') 

 etc - all dates in between

 ('8/30/2012' '1324')]


Thanks very much",1374969,,,,2013-01-03 10:46:35,"zip(,) string to float?",<python><pandas><time-series>,1.0,1.0,,2013-01-05 09:28:25
14023037,1,14024968.0,2012-12-24 15:20:23,1,55,"How can I reshape this dataframe with Pandas

id | col1 | col2 | col3     | value  
-----------------------------------
1  | A1   | B1   | before   | 20     
2  | A1   | B1   | after    | 13
3  | A1   | B2   | before   | 11
4  | A1   | B2   | after    | 21
5  | A2   | B1   | before   | 18 
6  | A2   | B1   | after    | 22


 into the following format?

col1 | col2 | before  | after
-------------------------------
A1   | B1   | 20      | 13
A1   | B2   | 11      | 21
A1   | B1   | 18      | 22


EDIT: A1 in the last line of the second table is supposed to be A2

As the data is paired (eg ""before"" and ""after"") I need the columns to be aligned without 'NAs' 

dfpivot(index='col1'  columns='col3'  values='value')


does not work because col1 does not result in an unique index I could create an additional column which would result in being unique Is that the only way to go?",446462,,446462.0,2012-12-24 20:01:06,2012-12-24 20:01:06,Reshape a Dataframe with Pandas,<pivot><pandas><reshape>,2.0,,,
14043958,1,,2012-12-26 17:28:37,3,66,"I'm looking for a good way to store and use conditional probabilities in python

I'm thinking of using a pandas dataframe If the conditional probabilities of some X are P(X=A|P1=1  P2=1) = 02  P(X=B|P1=2  P2=1) = 09 etc  I would use the dataframe

         A    B
P1 P2          
1  1   02  08
   2   05  05
2  1   09  01
   2   09  01


and given the marginal probabilities of P1 and P2 as Series

1    04
2    06
Name: P1

1    07
2    03
Name: P2


I would like to obtain the Series of marginal probabilities of X  ie the series

A    0602
B    0398
Name: X


I can get what I want by

X = sum(
    sum(
        Xxs(i  level=""P1"")*P1[i]
        for i in P1index
        )xs(j)*P2[j]
    for j in P2index
    )
Xname=""X""


but this is not easily generalizable to more dependencies  the asymmetry between the first xs with level and the second one without looks weird and as usual when working with pandas I'm very sure that there is a better solution using it's tricks and methods

Is pandas a good tool for this  should I represent my data in another way  and what is the best way to do this calculation  which is essentially an indexed tensor product  in pandas?",1274613,,1301710.0,2012-12-27 17:55:03,2012-12-28 08:24:23,Probability tensor multiplication using pandas.DataFrame,<python><statistics><pandas><probability><matrix-multiplication>,1.0,,,
14167487,1,14169558.0,2013-01-05 01:15:49,2,71,"I have an irregularly indexed time series of data with seconds resolution like:

import pandas as pd
idx = ['2012-01-01 12:43:35'  '2012-03-12 15:46:43'  
       '2012-09-26 18:35:11'  '2012-11-11 2:34:59']
status = [1  0  1  0]
df = pdDataFrame(status  index=idx  columns = ['status'])
df = dfreindex(pdto_datetime(dfindex))

In [62]: df
Out[62]: 
                     status
2012-01-01 12:43:35       1
2012-03-12 15:46:43       0
2012-09-26 18:35:11       1
2012-11-11 02:34:59       0


and I am interested in the fraction of the year when the status is 1 The way I currently do it is that I reindex df with every second in the year and use forward filling like:

full_idx = pddate_range(start = '1/1/2012'  end = '12/31/2012'  freq='s')
df1 = dfreindex(full_idx  method='ffill')


which returns a DataFrame that contains every second for the year which I can then calculate the mean for  to see the percentage of time in the 1 status like:

In [66]: df1
Out[66]: 

DatetimeIndex: 31536001 entries  2012-01-01 00:00:00 to 2012-12-31 00:00:00
Freq: S
Data columns:
status    31490186  non-null values
dtypes: float64(1)


In [67]: df1statusmean()
Out[67]: 031953371123308066


The problem is that I have to do this for a lot of data  and reindexing it for every second in the year is most expensive operation by far 

What are better ways to do this?",717636,,,,2013-01-06 23:33:16,Calculate time in certain state for time series data,<python><pandas>,1.0,,,
14046006,1,14046244.0,2012-12-26 20:40:06,3,62,"Basic question on a pandas dataframe  I have a 1x1 dataframe with a datapoint and there are no column headers (currently)  df[0 0] does not work because I think it is expecting a column name  In addition  df0 doesn't work nor df[0 '']  dfix[0 0] does work  

In general  am I required to have a column name?  Is it a best practice to use column names with pandas dataframes?  If my sql query does not have column headers  is it best to add them at that point?  

Thanks for the help",1911092,,919872.0,2012-12-26 21:17:20,2012-12-26 21:17:20,No Column Names in pandas python,<python><pandas>,1.0,,1.0,
14060111,1,14060382.0,2012-12-27 19:24:25,2,60,"I am doing a basic pdread_table of a txt file  The first column is a list of cusips  The cusip ""65248E10"" is being read as a number 65248E10 = 652480000000000 (E10 as scientific notation)

I have been going through the pandas but I can't figure out how to require it to stay as a character  http://pandaspydataorg/pandas-docs/dev/generated/pandasioparsersread_tablehtml#pandasioparsersread_table

Also  even if I put header = 0  it seems to be putting the first row as the headers and then row 0 is the second row and so on  If my text file has no column names  how can I get that to default to NULL (or 1  2  3  etc)

Thanks for the help  I am new to pandas/python",1911092,,1844389.0,2012-12-27 19:29:22,2012-12-27 19:47:41,read_table pandas python numeric error,<python><pandas>,2.0,,,
14089039,1,14089046.0,2012-12-30 07:25:32,0,38,"I have been unsuccessful in reproducing the hierarchical indexing example on page 148 of Wes McKinney's ""Python for Data Analysis"" book I am currently using python 27 on Mac OS X 1082 (I have the same problem on ubuntu precise as well with pandas 07-010)

Any and all help is appreciated 

frame = pdDataFrame(nparange(12)reshape((4  3))  index = list('aabb')  
                  columns = [['Ohio'  'Ohio'  'Colorado']['Green'  'Red'  'Green']])  


Error message:

TypeError                                 Traceback (most recent call last)
 in ()
      1 frame = pdDataFrame(nparange(12)reshape((4  3))  index = list('aabb')  
----> 2                   columns = [['Ohio'  'Ohio'  'Colorado']['Green'  'Red'  'Green']])  

TypeError: list indices must be integers  not tuple


FYI I am using version 09 of the pandas library:

pd__version__
'090rc1'
",435587,,1240268.0,2012-12-30 10:44:34,2012-12-30 10:44:34,hierarchical indexing not working in python pandas,<python><pandas>,1.0,2.0,,
14144867,1,14148511.0,2013-01-03 18:24:00,4,126,"Can the pandas data analysis module run on Google App Engine?

My first inclination is no: the web page states ""critical code paths compiled to C"" So since this is not a purely python package  you cannot simply copy a directory or ZIP file into your app engine project

Is it possible to ""disable"" the C extensions and have the module run in pure python (albeit slower)?",359001,,,,2013-01-03 23:04:30,Can Pandas run on Google App Engine for Python?,<python><google-app-engine><pandas>,1.0,2.0,,
14178913,1,14179036.0,2013-01-06 03:12:22,1,51,"I have a dataframe filled with True and False values  and I'd like to get a dataframe from it with the True replaced with 1 and the False replaced with npNaN I've tried using dataframereplace  but it gave a dataframe filled with all True Is there a way to do it without using for loops and if's?  

Example  this is the dataframe I have  with T for True and F for False (not strings 'T' and 'F'; sorry  could not figure out how to format a nicely spaced table in the wiki):

2008-01-02 16:00:00  T T F
2008-01-03 16:00:00  T T T
2008-01-04 16:00:00  T T F
2008-01-07 16:00:00  T T T
2008-01-08 16:00:00  T T F    

This is what I would like to change it to:   

2008-01-02 16:00:00  1 1 npNaN
2008-01-03 16:00:00  1 1 1
2008-01-04 16:00:00  1 1 npNaN
2008-01-07 16:00:00  1 1 1
2008-01-08 16:00:00  1 1 npNaN    

These are the lines I tried to replace the True and False  and got a dataframe filled with all True values:

dfreplace(to_replace=True  value=1  inplace=True  method=None)   
dfreplace(to_replace=False  value=npNAN  inplace=True  method=None)  


When tried separately  the first line alone does not change anything; the second line converts all the values to True",1392722,,,,2013-01-06 16:43:29,how to convert dataframe of booleans to dataframe of 1 and np.NaN?,<pandas>,2.0,,,
14095463,1,14095663.0,2012-12-30 22:43:48,1,53,"I'd like to have access to the special methods provided by the Datetimeindex class such as month  day  etc However I can't seem to make a series in a dataframe be a Datetimeindex without making it the dataframe's index Take the following example:

dates

Out[119]:
         Dates
0     1/1/2012
1     1/2/2012
2     1/3/2012
3     1/4/2012
4     1/5/2012
5     1/6/2012
6     1/7/2012
7     1/8/2012
8     1/9/2012
9    1/10/2012
10  12/31/2012

date_series = pdDatetimeIndex(datesDates)
date_seriesmonth
Out[115]: array([ 1   1   1   1   1   1   1   1   1   1  12])

datesDates = pdDatetimeIndex(datesDates)
datesDatesmonth
AttributeError: 'Series' object has no attribute 'month'


I also tried converting the series to timestamps using pdto_datetime but it still doesn't work

I know I can work around this  but it seems like this functionality should exist?",1438637,,,,2012-12-30 23:13:23,How to make a Datetimeindex not be the index in a dataframe,<python><pandas>,2.0,,,
14162723,1,14163209.0,2013-01-04 18:26:06,2,63,"I am trying to write a Pandas dataframe (or can use a numpy array) to a mysql database using MysqlDB  MysqlDB doesn't seem understand 'nan' and my database throws out an error saying nan is not in the field list I need to find a way to convert the 'nan' into a NoneType

Any ideas? ",365781,,,,2013-01-05 12:22:20,Replacing Pandas or Numpy Nan with a None to use with MysqlDB,<numpy><pandas><mysql-python>,2.0,,,
14174241,1,,2013-01-05 17:13:06,1,56,"I am new to python and pandas 
I have a datetime indexed dataframe I want to select rows for which the time is > 08:00:00
I tried using pdDataFrameselect function It is failing because the index has duplicate entries 

Am I trying it correctly?

Is there a way around it?

Is it a bad practice to have data indexed with duplicate entries?

>>> dfhead(10)
                            A
time                         
1900-01-01 00:01:01456170  0
1900-01-01 00:01:01969600  0
1900-01-01 00:01:04305494  0
1900-01-01 00:01:13860365  0
1900-01-01 00:01:19666371  0
1900-01-01 00:01:24920744  0
1900-01-01 00:01:24931466  0
1900-01-01 00:02:07522741  0
1900-01-01 00:02:13857793  0
1900-01-01 00:02:34817765 -7
>>> timeindexvalid = lambda x : xto_datetime() > datetime(1900  1  1  8)
>>> dfselect(timeindexvalid)
Traceback (most recent call last):

    raise Exception('Reindexing only valid with uniquely valued Index '
Exception: Reindexing only valid with uniquely valued Index objects
",1945648,,,,2013-01-05 23:51:48,Selecting from a dataframe with duplicate index,<python><pandas>,1.0,,0.0,
14180615,1,14184415.0,2013-01-06 08:44:13,1,67,"I am having a real strange behaviour when trying to reindex a dataframe in pandas My version of Pandas is 0100 and I use Python 27
Basically  when I load a dataframe:

eurusd = pdDataFrameload('EUR_USD_30Mindf')drop_duplicates()dropna()

eurusd


DatetimeIndex: 119710 entries  2003-02-02 17:30:00 to 2012-12-28 17:00:00
Data columns:
open     119710  non-null values
high     119710  non-null values
low      119710  non-null values
close    119710  non-null values
dtypes: float64(4)


and then I try to reindex inside a larger date range:

newindex  = pdDateRange(datetimedatetime(2002 1 1)  datetimedatetime(2012 12 31)  offset=pddatetoolsMinute(30))

newindex


[2002-01-01 00:00:00    2012-12-31 00:00:00]
Length: 192817  Freq: 30T  Timezone: None


I get strange behaviour when trying to reindex the dataframe If I reindex one larger part of the dataset I get this error:

eurusd[29558:29560]reindex(index=newindex)

Exception: Reindexing only valid with uniquely valued Index objects


But  if I do the same for two subsets of the data above  I don't get the error:

Here's the first subset  with no problems 

eurusd[29558:29559]reindex(index=newindex)


DatetimeIndex: 192817 entries  2002-01-01 00:00:00 to 2012-12-31 00:00:00
Freq: 30T
Data columns:
open     1  non-null values
high     1  non-null values
low      1  non-null values
close    1  non-null values
dtypes: float64(4)


and here's the second subset  still no problems 

eurusd[29559:29560]reindex(index=newindex)


DatetimeIndex: 192817 entries  2002-01-01 00:00:00 to 2012-12-31 00:00:00
Freq: 30T
Data columns:
open     1  non-null values
high     1  non-null values
low      1  non-null values
close    1  non-null values
dtypes: float64(4)


I am really going crazy about this  and cannot understand the reasons of this It seems like the dataframe is 'clean' from duplicates  and duplicated indexes I can provide the pickle file for the dataframe if you want",1529852,,,,2013-01-06 16:55:19,problems with reindexing dataframes: Reindexing only valid with uniquely valued Index objects,<data.frame><pandas><reindex>,1.0,7.0,,
14124710,1,14125116.0,2013-01-02 15:37:07,0,66,"New to pandas python

I have a dataframe (df) with two columns of cusips
I want to turn those columns into a list of the unique entries of the two columns

My first attempt was to do the following:

cusips = pdconcat(df['long']  df['short'])  

This returned the error: The truth value of an array with more than one element is ambiguous  Use aany() or aall()

I have read a few postings  but I am still having trouble with why this comes up  What am I missing here?

Also  what's the most efficient way to select the unique entries in a column or a dataframe?  Can I call it in one function?  Does the function differ if I want to create a list or a new  one-coulmn dataframe?

Thank you",1911092,,,,2013-01-02 16:22:29,Two Columns of a pandas dataframe - Concat in Python,<python><pandas>,2.0,,,
14181732,1,,2013-01-06 11:42:34,0,96,"I am having trouble with installing Python's Pandas Library on my new MacBookPro running Mountain Lion

I tried installing Pandas via easy_install  However  there is obviously an issue with easy_install and gcc (see error message I get below) It seems that easy_install cannot find the gcc-compiler that it apparently requires (gcc-40) I did everything exactly as I did on my previous MacBook running Snow Leopard where everything worked fine I'm more of an applied person and not so much into programming itself  so I'd appreciate any help :-)

Here's a list of information and things I tried but didn't solve the problem:

XCode 45 is installed  I also installed the Command Line Tools from within Xcode (also re-installed Xcode) 
Starting a gcc compiler in the terminal also works fine

Stefans-MacBook-Pro:/ Stefan$ which gcc
/usr/bin/gcc
Stefans-MacBook-Pro:/ Stefan$ gcc
i686-apple-darwin11-llvm-gcc-42: no input files
Is there a problem with the path settings so that when gcc is called  it is not appropriately redirected to the compiler? As I am not that experienced I don't want to start creating links without having double-checked with you
I suspect the problem to be more easy_install&gcc-specific than Pandas-specific as I get the same error when trying to install Cython via easy_install
I'm a bit confused as under Snow Leopard everything worked absolutely fine

Thanks a lot!

Here's the full error message:

Stefans-MacBook-Pro:~ Stefan$ sudo easy_install pandas   
Searching for pandas
Processing pandas-0100zip
Writing /tmp/easy_install-zr8Lfg/pandas-0100/setupcfg
Running pandas-0100/setuppy -q bdist_egg --dist-dir /tmp/easy_install-zr8Lfg/pandas-0100/egg-dist-tmp-N1xJeV
warning: no files found matching 'setupeggpy'
no previously-included directories found matching 'doc/build'
warning: no previously-included files matching '*so' found anywhere in distribution
warning: no previously-included files matching '*pyd' found anywhere in distribution
warning: no previously-included files matching '*pyc' found anywhere in distribution
warning: no previously-included files matching 'git*' found anywhere in distribution
warning: no previously-included files matching 'DS_Store' found anywhere in distribution
warning: no previously-included files matching '*png' found anywhere in distribution
gcc-40: error: unrecognized command line option '-arch'
error: Setup script exited with error: command 'gcc-40' failed with exit status 1
",1952633,,,,2013-01-24 04:11:57,Installing Python's Pandas under Mountain Lion via easy_install,<python><gcc><pandas><easy-install>,2.0,19.0,,
14184115,1,,2013-01-06 16:25:19,0,45,"I'm attempting to use a pandas dataframe that contains different pieces of options data to calculate implied volatility  For the implied volatility I'm using mibian Here's the code:

optionsDatato_dict():

    {'callclose': {0: Decimal('4') 
      1: Decimal('262') 
      2: Decimal('264') 
      3: Decimal('17') 
      4: Decimal('135')} 
     'daystoexpiration': {0: 43L  1: 43L  2: 43L  3: 43L  4: 43L} 
     'expiration': {0: datetimedate(2013  2  16) 
      1: datetimedate(2013  2  16) 
      2: datetimedate(2013  2  16) 
      3: datetimedate(2013  2  16) 
      4: datetimedate(2013  2  16)} 
     'impvol': {0: nan  1: nan  2: nan  3: nan  4: nan} 
     'putclose': {0: Decimal('054') 
      1: Decimal('065') 
      2: Decimal('076') 
      3: Decimal('108') 
      4: Decimal('156')} 
     'strike': {0: Decimal('39') 
      1: Decimal('40') 
      2: Decimal('41') 
      3: Decimal('42') 
      4: Decimal('43')} 
     'symbol': {0: 'A'  1: 'A'  2: 'A'  3: 'A'  4: 'A'} 
     'underlyingclose': {0: Decimal('4286') 
      1: Decimal('4286') 
      2: Decimal('4286') 
      3: Decimal('4286') 
      4: Decimal('4286')}}

optionsData = optionsDataT

def calcvol(info):
    print infoname
    print 'Starting procedure'
    tempmb = mbBS([info['underlyingclose']  
            info['strike']  
            25  
            info['daystoexpiration']] 
           callPrice=float(info['callclose'])  
           putPrice=info['putclose'])
    print 'mb created'
    impvol = tempmbimpliedVolatility
    print 'implied vol calculated'
    info['impvol'] = impvol
    print 'impvol set'
    del impvol  tempmb
    print 'vars deleted'
    return info

a = optionsDataapply(calcvol)


When I run through all of that  it sets the impvol on the first element in optionsData  but seems to subsequently give me this error:

---------------------------------------------------------------------------
ZeroDivisionError                         Traceback (most recent call last)
 in ()
----> 1 a = optionsDataapply(calcvol)

C:\Python27\lib\site-packages\pandas-0100-py27-win32egg\pandas\core\framepyc in apply(self  func  axis  broadcast  raw  args  **kwds)
   4079                     return self_apply_raw(f  axis)
   4080                 else:
-> 4081                     return self_apply_standard(f  axis)
   4082             else:
   4083                 return self_apply_broadcast(f  axis)

C:\Python27\lib\site-packages\pandas-0100-py27-win32egg\pandas\core\framepyc in _apply_standard(self  func  axis  ignore_failures)
   4154                     # no k defined yet
   4155                     pass
-> 4156                 raise e
   4157 
   4158         if len(results) > 0 and _is_sequence(results[0]):

ZeroDivisionError: ('float division by zero'  u'occurred at index 1')

0
Starting procedure
mb created
implied vol calculated
impvol set
vars deleted
0
Starting procedure
mb created
implied vol calculated
impvol set
vars deleted
1
Starting procedure


I've got to be missing something simple  I've tried wrapping each of the values in float() as I pass them to mibian  and still get the same issue  I'd very much appreciate any guidance

Also  if you know of a more efficient way to calculate implied volatility using a dataframe  I'm all ears",1850663,,,,2013-01-06 17:52:44,Pandas DataFrame.apply() and mibian,<python><data.frame><pandas>,1.0,1.0,,
14079766,1,14079919.0,2012-12-29 07:42:36,0,31,"I'm trying to understand what is the best way to go about doing this:

Basically I will have a bunch of Android/iOS users who will be logging sensor data on their phone  except each user will have its own timestamp depending on when people start the app etc So say I have two users:

User 1: (5 45)  (6 34)  (8 32)
User 2: (5 35)  (7 32)  (9 32)


The format is (time t  and some arbitrary value)

What would be the best way to synchronize the two datasets? Do I have to write an algorithm which is then going to go back and say on User 2  insert the following entry (6 35) So eventually the new data looks like:

User 1: (5 45)  (6 34)  (7 34)  (8 32)  (9 32)
User 2: (5 35)  (6 35)  (7 32)  (8 32)  (9 32)


This could be very data intensive though  because I am expecting to have about 300 users  and each will have about 36 000 data entries Any advice would be appreciated - also something I could do on the app side that could help that would be appreciated

I believe I am going to have to do something like this when I have all my data - but since this is an actively developing project I thought I'd get some advice first",529690,,1136195.0,2012-12-29 07:46:56,2012-12-29 08:35:45,"Synchronize Dataset - Multiple users, multiple timestamps",<android><ios><dataset><timestamp><pandas>,1.0,2.0,,
14189695,1,14189912.0,2013-01-07 03:59:30,4,51,"Is there a shorter way of dropping a column MultiIndex level (in my case  basic_amt) except transposing it twice?

In [704]: test
Out[704]: 
           basic_amt               
Faculty          NSW  QLD  VIC  All
All                1    1    2    4
Full Time          0    1    0    1
Part Time          1    0    2    3

In [705]: testreset_index(level=0  drop=True)
Out[705]: 
         basic_amt               
Faculty        NSW  QLD  VIC  All
0                1    1    2    4
1                0    1    0    1
2                1    0    2    3

In [711]: testtranspose()reset_index(level=0  drop=True)transpose()
Out[711]: 
Faculty    NSW  QLD  VIC  All
All          1    1    2    4
Full Time    0    1    0    1
Part Time    1    0    2    3
",1479269,,,,2013-01-10 22:57:29,reset_index for DataFrame columns,<python><pandas>,1.0,,1.0,
14192741,1,14193170.0,2013-01-07 09:03:57,5,81,"Summary:
This doesn't work:

df[dfkey==1]['D'] = 1


but this does:

dfD[dfkey==1] = 1


Why?

Reproduction:

In [1]: import pandas as pd

In [2]: from numpyrandom import randn

In [4]: df = pdDataFrame(randn(6 3) columns=list('ABC'))

In [5]: df
Out[5]: 
          A         B         C
0  1438161 -0210454 -1983704
1 -0283780 -0371773  0017580
2  0552564 -0610548  0257276
3  1931332  0649179 -1349062
4  1656010 -1373263  1333079
5  0944862 -0657849  1526811

In [6]: df['D']=00

In [7]: df['key']=3*[1]+3*[2]

In [8]: df
Out[8]: 
          A         B         C  D  key
0  1438161 -0210454 -1983704  0    1
1 -0283780 -0371773  0017580  0    1
2  0552564 -0610548  0257276  0    1
3  1931332  0649179 -1349062  0    2
4  1656010 -1373263  1333079  0    2
5  0944862 -0657849  1526811  0    2


This doesn't work:

In [9]: df[dfkey==1]['D'] = 1

In [10]: df
Out[10]: 
          A         B         C  D  key
0  1438161 -0210454 -1983704  0    1
1 -0283780 -0371773  0017580  0    1
2  0552564 -0610548  0257276  0    1
3  1931332  0649179 -1349062  0    2
4  1656010 -1373263  1333079  0    2
5  0944862 -0657849  1526811  0    2


but this does:

In [11]: dfD[dfkey==1] = 34

In [12]: df
Out[12]: 
          A         B         C    D  key
0  1438161 -0210454 -1983704  34    1
1 -0283780 -0371773  0017580  34    1
2  0552564 -0610548  0257276  34    1
3  1931332  0649179 -1349062  00    2
4  1656010 -1373263  1333079  00    2
5  0944862 -0657849  1526811  00    2


Link to notebook

My question is: Why does only the 2nd way work? I can't seem to see a difference in selection/indexing logic?

Version is 0100",680232,,,,2013-01-07 10:32:39,Understanding pandas dataframe indexing,<python><pandas>,2.0,2.0,2.0,
14181519,1,14181629.0,2013-01-06 11:10:06,2,56,"This question is somewhat related to Visually separating bar chart clusters in pandas

I am reading and plotting the tmpcsv file:

pol1 pol2 pol3
perim 054 064 040
mst 008 012 012
treeadd 025 034 035
health 014 017 017
bisort 048 056 056
em3d 014 017 017

g721d 141 258 258
mesa 116 18 18
epic 182 243 243
jpege 118 168 168

gzip 115 143 145
vpr 019 024 024
gcc 082 111 115
mcf 005 005 005
crafty 067 117 117


with:

#!/usr/bin/env python

from pandas import *
import matplotlibpyplot as plt
from numpy import zeros

# Create original dataframe
df = read_csv('tmpcsv' sep='\s')

print df

dfplot(kind='bar')
pltshow()


and I get:

         pol1  pol2  pol3
perim    054  064  040
mst      008  012  012
treeadd  025  034  035
health   014  017  017
bisort   048  056  056
em3d     014  017  017
nan       NaN   NaN   NaN
g721d    141  258  258
mesa     116  180  180
epic     182  243  243
jpege    118  168  168
nan       NaN   NaN   NaN
gzip     115  143  145
vpr      019  024  024
gcc      082  111  115
mcf      005  005  005
crafty   067  117  117


and:



Note the separation of the clusters with the empty lines This is the effect I want 
Is there a way to replace the 'nan' label with """" in the x-axis?

I tried:
    dfrename(index={'nan': """"})

However there is an assertion failing

assert(new_axisis_unique)


Probably because there are multiple 'nan' indexing the df Ideas?

-Thanks",1186611,,,,2013-01-06 11:28:28,Modify nan index in pandas,<python><plot><pandas>,1.0,,,
14199168,1,14283678.0,2013-01-07 15:42:28,7,459,"I'm trying to merge a series of dataframes in pandas I have a list of dfs  dfs and a list of their corresponding labels labels and I want to merge all the dfs into 1 df in such that the common labels from a df get the suffix from its label in the labels list ie:

def mymerge(dfs  labels):
  labels_dict = dict([(d  l) for d  l in zip(dfs  labels)])
  merged_df = reduce(lambda x  y:
                     pandasmerge(x  y  
                                  suffixes=[labels_dict[x]  labels_dict[y]]) 
                     dfs)
  return merged_df


When I try this  I get the error:

pandastoolsmergeMergeError: Combinatorial explosion! (boom)


I'm trying to make a series of merges that at each merge grows at most by number of columns N  where N is the number of columns in the ""next"" df in the list The final DF should have as many columns as all the df columns added together  so it grow additively and not be combinatorial

The behavior I'm looking for is: Join dfs on the column names that are specified (eg specified by on=) or that the dfs are indexed by Unionize the non-common column names (as in outer join) If a column appears in multiple dfs  optionally overwrite it Looking more at the docs  it sounds like update might be the best way to do this Though when I try join='outer' it raises an exception signaling that it's not implemented

EDIT: 

Here is my attempt at an implementation of this  which does not handle suffixes but illustrates the kind of merge I'm looking for:

def my_merge(dfs_list  on):
    """""" list of dfs  columns to merge on """"""
    my_df = dfs_list[0]
    for right_df in dfs_list[1:]:
        # Only put the columns from the right df
        # that are not in the existing combined df (ie new)
        # or which are part of the columns to join on
        new_noncommon_cols = [c for c in right_df \
                              if (c not in my_dfcolumns) or \
                                 (c in on)]
        my_df = pandasmerge(my_df 
                             right_df[new_noncommon_cols] 
                             left_index=True 
                             right_index=True 
                             how=""outer"" 
                             on=on)
    return my_df


This assumes that the merging happens on the indices of each of the dfs New columns are added in an outer-join style  but columns that are common (and not part of the index) are used in the join via the on= keyword

Example:

df1 = pandasDataFrame([{""employee"": ""bob"" 
                         ""gender"": ""male"" 
                         ""bob_id1"": ""a""} 
                        {""employee"": ""john"" 
                         ""gender"": ""male"" 
                         ""john_id1"": ""x""}])
df1 = df1set_index(""employee"")
df2 = pandasDataFrame([{""employee"": ""mary"" 
                         ""gender"": ""female"" 
                         ""mary_id1"": ""c""} 
                        {""employee"": ""bob"" 
                         ""gender"": ""male"" 
                         ""bob_id2"": ""b""}])
df2 = df2set_index(""employee"")
df3 = pandasDataFrame([{""employee"": ""mary"" 
                         ""gender"": ""female"" 
                         ""mary_id2"": ""d""}])
df3 = df3set_index(""employee"")
merged = my_merge([df1  df2  df3]  on=[""gender""])
print ""MERGED: ""
print merged


The twist on this would be one where you arbitrarily tag a suffix to each df based on a set of labels for columns that are common  but that is less important Is the above merge operation something that can be done more elegantly in pandas or that already exists as a builtin?",248237,,1655939.0,2013-01-15 14:48:18,2013-01-15 14:48:18,Combinatorial explosion while merging dataframes in pandas,<python><numpy><scipy><pandas>,2.0,11.0,,
14105774,1,14106334.0,2012-12-31 20:22:16,0,48,"In [22]: ts
Out[22]:

[NaT    2012-12-31 00:00:00]
Length: 11  Freq: None  Timezone: None

In [23]: tsyear
Out[23]: array([  -1  2012  2012  2012  2012  2012  2012  2012  2012  2012  2012])


This happens when using apply as well

tsapply(lambda x: pdTimestamp(x)year)

0       -1
1     2012
2     2012
3     2012
4     2012
5     2012
6     2012
7     2012
8     2012
9     2012
10    2012
Name: Dates


is it a bug that NaTyear == -1?",1438637,,1438637.0,2012-12-31 21:25:39,2012-12-31 22:04:06,Pandas NaT's to -1?,<python><timestamp><pandas>,2.0,12.0,,
14125172,1,,2013-01-02 16:04:22,1,51,"I have a DataFrame indexed by dates like this:


DatetimeIndex: 1853141 entries  2012-03-01 00:00:00 to 2012-06-16 23:59:55
Data columns:
Open Bid ESM2    1853141  non-null values
Open Ask ESM2    1853141  non-null values
dtypes: float64(2)


The index has a period of 5 sec with some ""holes""  so i have created a TimeSeries with the same date range and period without holes:


[2012-03-01 00:00:00    2012-06-16 23:59:55]
Length: 1866240  Freq: 5S  Timezone: None


Now i want to use this time series as index for the DataFrame above with the holes listed as NaN How can I do this",1915817,,,,2013-01-02 18:47:59,"How to index a DataFrame with ""holes"" with a TimeSeries without holes",<python><pandas><time-series>,1.0,,,
14149156,1,14149319.0,2013-01-03 23:50:49,2,47,"I have two dataframes df1 is multi-indexed:

                value
first second    
a     x         0471780
      y         0774908
      z         0563634
b     x         -0353756
      y         0368062
      z         -1721840


and df2:

      value
first   
a     10
b     20


How can I merge the two data frames with only one of the multi-indexes  in this case the 'first' index? The desired output would be:

                value1      value2
first second    
a     x         0471780    10
      y         0774908    10
      z         0563634    10
b     x         -0353756   20
      y         0368062    20
      z         -1721840   20
",1642513,,1305086.0,2013-01-03 23:52:10,2013-01-08 04:17:51,Merge multi-indexed with single-indexed data frames in pandas,<python><pandas>,2.0,,,
14178194,1,14179954.0,2013-01-06 00:58:33,3,104,"I want to plot multiple lines from a pandas dataframe and setting different options for each line I would like to do something like

testdataframe=pdDataFrame(nparange(12)reshape(4 3))
testdataframeplot(style=['s-' 'o-' '^-'] color=['b' 'r' 'y'] linewidth=[2 1 1])


This will raise some error messages:

linewidth is not callable with a list
In style I can't use 's' and 'o' or any other alphabetical symbol  when defining colors in a list
Also there is some more stuff which seems weird to me

when I add another plot command to the above code testdataframe[0]plot() it will plot this line in the same plot  if I add the command testdataframe[[0 1]]plot() it will create a new plot
If i would call testdataframe[0]plot(style=['s-' 'o-' '^-'] color=['b' 'r' 'y']) it is fine with a list in style  but not with a list in color
Hope somebody can help  thanks",1952043,,1867980.0,2013-01-06 02:30:38,2013-01-06 21:37:22,"Python pandas, Plotting options for multiple lines",<python><plot><pandas>,1.0,,2.0,
14199718,1,14200289.0,2013-01-07 16:13:45,1,47,"I have customer records with id  timestamp and status 

ID  TS  STATUS
1 10 GOOD
1 20 GOOD
1 25 BAD
1 30 BAD
1 50 BAD
1 600 GOOD
2 40 GOOD
  


I am trying to calculate how much time is spent in consecutive BAD statuses (lets imagine order above is correct) per customer So for customer id=1  30-25 50-30 600-50 in total 575 seconds was spent in BAD status 

What is the method of doing this in Pandas? If I calculate diff() on TS  that would give me differences  but how can I tie that 1) to the customer 2) certain status ""blocks"" for that customer? 

Sample data:

df = pandasDataFrame({'ID':[1 1 1 1 1 1 2] 
                       'TS':[10 20 25 30 50 600 40] 
                       'Status':['G' 'G' 'B' 'B' 'B' 'G' 'G']
                       } 
                      columns=['ID' 'TS' 'Status'])


Thanks ",423805,,243434.0,2013-01-08 02:10:12,2013-01-08 02:47:17,Taking Differences of Records When Status Changes - Pandas,<python><pandas>,2.0,,,
14206217,1,,2013-01-08 00:01:52,0,47,"I'd like to use a DataFrame to manage data from many trials of an experiment I'm controlling with Python code Ideally I will have one master dataframe with a row for each trial that lives in the main function namespace  and then a separate dict (or dataframe) returned from the function that I call to execute the important bits of code for each trial

What is the best way to do a running update of the master dataframe with this returned set of data? So far I've come up with:

df = dfappend(df_trial  ignore_index=True)


or

df = pdconcat([df  df_trial])


But neither seem ideal (and both take a relatively long time according to %timeit) Is there a more Pandonic way?",1533576,,1533576.0,2013-01-08 01:50:10,2013-01-08 03:44:25,Running update of Pandas dataframe,<python><pandas>,1.0,,,
14224068,1,14224444.0,2013-01-08 21:09:25,0,52,"I am using pandasDataFrame in a multi-threaded code (actually a custom subclass of DataFrame called Sound) I have noticed that I have a memory leak  since the memory usage of my program augments gradually over 10mn  to finally reach ~100% of my computer memory and crash

I used objgraph to try tracking this leak  and found out that the count of instances of MyDataFrame is going up all the time while it shouldn't : every thread in its run method creates an instance  makes some calculations  saves the result in a file and exits  so no references should be kept

Using objgraph I found that all the data frames in memory have a similar reference graph :



I have no idea if that's normal or not  it looks like this is what is keeping my objects in memory Any idea  advice  insight ?",312598,,,,2013-01-08 23:01:20,Memory leak using pandas dataframe,<python><pandas>,1.0,2.0,,
14224172,1,14224489.0,2013-01-08 21:16:05,1,50,"As part of a unit test  I need to test two DataFrames for equality  The order of the columns in the DataFrames is not important to me  However  it seems to matter to Pandas:

import pandas
df1 = pandasDataFrame(index = [1 2 3 4])
df2 = pandasDataFrame(index = [1 2 3 4])
df1['A'] = [1 2 3 4]
df1['B'] = [2 3 4 5]
df2['B'] = [2 3 4 5]
df2['A'] = [1 2 3 4]
df1 == df2


Results in:

Exception: Can only compare identically-labeled DataFrame objects


I believe the expression df1 == df2 should evaluate to a DataFrame containing all True values  Obviously it's debatable what the correct functionality of == should be in this context  My question is: Is there a Pandas method that does what I want?  That is  is there a way to do equality comparison that ignores column order?",1572508,,,,2013-01-08 21:58:47,Equality in Pandas DataFrames - Column Order Matters?,<python><pandas>,1.0,2.0,,
14241993,1,,2013-01-09 16:40:32,1,54,"Does anyone have a link to a description of all the possible inputs into psqlframe_query() for pandas?

I have a situation where I have a number of parameters psqlexecute(string  con  params)  There doesn't seem to be a similar option in psqlframe_query  Any recommendations?

Thanks",1911092,,,,2013-01-24 22:26:34,psql.frame_query,<python><pandas>,1.0,5.0,,
14246817,1,14247036.0,2013-01-09 21:27:11,4,86,"Dataframe:
  one two
a  1  x
b  1  y
c  2  y
d  2  z
e  3  z

grp = DataFramegroupby('one')
grpagg(lambda x: ???) #or equivalent function


Desired output from grpagg:

one two
1   x|y
2   y|z
3   z


My agg function before integrating dataframes was u""|""join(sorted(set(x)))  Ideally I want to have any number of columns in the group and agg returns the u""|""join(sorted(set()) for each column item like two above  I also tried npcharjoin()  

Love Pandas and it has taken me from a 800 line complicated program to a 400 line walk in the park that zooms  Thank you :)",1649635,,,,2013-01-09 22:06:38,python pandas custom agg function,<python><numpy><pandas>,1.0,,,
14200990,1,,2013-01-07 17:29:10,0,49,"I run Python 273 (default  Apr 10 2012  23:31:26) [MSC v1500 32 bit (Intel)] on win32 (WinPython distribution)

>>>pandas__version__
'0100'


I having the following issue: when I create a Pandas DataFrame with strictly more than 100 lines in a Serie  it crashes Python with no error message

This runs fine:

>>>from pandas import DataFrame  Series  date_range
>>>DataFrame({'bar': Series(randn(100)  index= date_range('1990-1-1'  periods=100  freq='D'))})
>>>len(100)
100


This crashes Python immediately with no message

>>>DataFrame({'bar': Series(randn(101)  index= date_range('1990-1-1'  periods=101  freq='D'))})


However  sometimes  I just restart the computer and it runs fine again with numbers several orders of magnitude above 100 But this issue comes back often

[EDIT] copy/paste mistake edit

[EDIT2] I used the exact same WinPython setup which is running fine on another PC with the same code  it keeps crashing What's weird is: it use to work fine two weeks ago",437863,,437863.0,2013-01-10 13:30:58,2013-01-10 13:30:58,Pandas Dataframe creation crashes Python when series lenght is above 100,<python-2.7><pandas>,,7.0,,
14218728,1,14218930.0,2013-01-08 15:45:21,1,53,"I feel like there has to be a quick solution to my problem  I hacked out a poorly implemented solution using multiple list comprehensions which is not ideal whatsoever Maybe someone could help out here 

I have a set of values which are strings (eg 32B  15M  11T) where naturally the last character denotes million  billion  trillion Within the set there are also NaN/'none' values which should remain untouched I wish to convert these to floats or ints  so in the given example (3200000000  1500000  1100000000000)

TIA",287950,,,,2013-01-08 15:55:00,Converting string of numbers and letters to int/float in pandas dataframe,<python><pandas>,1.0,1.0,,
14248346,1,,2013-01-09 23:28:33,1,59,"I'm constructing a dictionary using a dictionary comprehension which has read_csv embedded within it This constructs the dictionary fine  but when I then push it into a DataFrame all of my data goes to null and the dates get very wacky as well Here's sample code and output:

In [129]: a= {xsplit("""")[0] : read_csv(x  parse_dates=True  index_col=[0])[""Settle""] for x in t[:2]}

In [130]: a
Out[130]: 
{'SPH2010': Date
2010-03-19    117295
2010-03-18    116610
2010-03-17    116570
2010-03-16    115950
2010-03-15    115030
2010-03-12    115130
2010-03-11    115060
2010-03-10    114570
2010-03-09    114050
2010-03-08    113710
2010-03-05    113650
2010-03-04    112230
2010-03-03    111860
2010-03-02    111740
2010-03-01    111460

2008-04-10    13704
2008-04-09    13677
2008-04-08    13787
2008-04-07    13784
2008-04-04    13778
2008-04-03    13799
2008-04-02    13777
2008-04-01    13766
2008-03-31    13291
2008-03-28    13240
2008-03-27    13347
2008-03-26    13407
2008-03-25    13570
2008-03-24    13573
2008-03-20    13298
Name: Settle  Length: 495 
 'SPM2011': Date
2011-06-17    12794
2011-06-16    12690
2011-06-15    12654
2011-06-14    12899
2011-06-13    12716
2011-06-10    12692
2011-06-09    12874
2011-06-08    12770
2011-06-07    12848
2011-06-06    12850
2011-06-03    12963
2011-06-02    13124
2011-06-01    13121
2011-05-31    13439
2011-05-27    13299

2009-07-10    8566
2009-07-09    8612
2009-07-08    8560
2009-07-07    8617
2009-07-06    8779
2009-07-02    8758
2009-07-01    9026
2009-06-30    9003
2009-06-29    9080
2009-06-26    9011
2009-06-25    9038
2009-06-24    8852
2009-06-23    8776
2009-06-22    8760
2009-06-19    9034
Name: Settle  Length: 497}

In [131]: DataFrame(a)
Out[131]: 

DatetimeIndex: 806 entries  2189-09-10 03:33:28879144 to 1924-01-20 06:06:06621835
Data columns:
SPH2010    0  non-null values
SPM2011    0  non-null values
dtypes: float64(2)


Thanks!

EDIT: 

I've also tried doing this with concat and I get the same results",287950,,287950.0,2013-01-10 00:00:32,2013-01-10 00:14:22,Loading pandas DataFrame from dict of series possible glitch?,<python><pandas>,1.0,4.0,,
14216572,1,,2013-01-08 14:00:48,-6,102,"import numpy as np
import pandas as pd
from pandas import DataFrame
import qstkutilqsdateutil as du
import datetime as dt
import qstkutilDataAccess as da
import qstkutiltsutil as tsu
import sys

# python marketsimpy 1000000 orderscsv valuescsv
#cash = int(sysargv[1])
#in_fn = sysargv[2]
#out_fn = sysargv[3]
cash=1000000


orders =pdread_csv(""orderscsv""  parse_dates=[[0 1 2]]  header=None index_col=0)
names=['sym' 'type' 'quan']
del orders['X6']
ordersindexname='date'
orderscolumns=names
#the time adding is amazing!
timeofday=dttimedelta(hours=16)
ordersindex=ordersindex+timeofday
startday =ordersindex[0]
endday =ordersindex[-1] 
print startday
print endday

symbols=list(set(orders['sym']))
print symbols


timestamps = dugetNYSEdays(startday endday timeofday)
dataobj = daDataAccess('Yahoo')
close = dataobjget_data(timestamps  symbols  ""close"")

#empty DataFrame for updating
values=DataFrame(npzeros(len(close)) index=timestamps columns=['total'])

#portfolio things 
#portfolio={symbol:0 for symbol in symbols}
#index=['quantity']
#portfolio=DataFrame(npzeros(4) index=index columns=symbols)
portfolio=DataFrame({}  columns = symbols index=index)
portfolio=portfoliofillna(0)
for time in timestamps:
    valuesix[time]['total']=npsum(portfolioix['quantity']values*closeix[time]values)+cash
    for stime in ordersindex:
        if time==stime and ordersix[stime]['type']=='Buy':
            portfolio[ordersix[stime]['sym'][0]][0]+=ordersix[stime]['quan']
            summ=npsum(portfolioix['quantity']values*closeix[stime]values)
            cash-=portfolio[ordersix[stime]['sym'][0]][0]*closeix[stime][ordersix[stime]['sym'][0]]
            valuesix[time]['total']=cash+summ
        elif time==stime and ordersix[stime]['type']=='Sell':
            portfolio[ordersix[stime]['sym'][0]][0]-=ordersix[stime]['quan']
            summ=npsum(portfolioix['quantity']values*closeix[stime]values)
            cash+=portfolio[ordersix[stime]['sym'][0]][0]*closeix[stime][ordersix[stime]['sym'][0]]
            valuesix[time]['total']=cash+summ


Basically you read in orders from a csv file  I changed it into a DataFrame  values are also a Dataframe updating the total value everyday The last thing what I want to do is to read in order  updating my portfolio balance and cash balance  then adding them back to the values
The loop couldn't work",1958327,,1240268.0,2013-01-08 14:49:56,2013-01-08 14:49:56,Python basic simulation of buy sell,<python><pandas>,1.0,4.0,,2013-01-08 14:48:40
14237749,1,,2013-01-09 14:44:49,1,54,"I have following input transcsv file:

Date Currenncy Symbol Type Units UnitPrice Cost Tax
2012-03-14 USD AAPL BUY 1000
2012-05-12 USD SBUX SELL 500


The fields UnitPrice  Cost and Tax are optional If they are not specified I expect NaN in the DataFrame cell

I read the csv file with:

t = pandasread_csv('transcsv'  parse_dates=True  index_col=0)


and got the following result:

           Currenncy Symbol  Type  Units   UnitPrice       Cost       Tax
Date                                                                     
2012-03-14       USD   AAPL   BUY   1000  2012-05-12  012-05-12  12-05-12
2012-02-05       USD   SBUX  SELL    500         NaN        NaN       NaN


Why are there no NaN in the first row and is the Date repeated?
Any workaround to get NaN for the unspecified fields? ",1915862,,,,2013-01-09 15:27:48,pandas.read_csv() strange behavior for empty (default) values,<python><csv><pandas>,2.0,2.0,1.0,
14260251,1,,2013-01-10 14:33:24,1,37,"I use a lot of tuples in my data files as record keys When I load a table from text the tuples are strings in the dataframe

I convert to tuple now  using:

df['KEY'] = df['KEY']map(lambda x: eval(x))


Is it possible to get the string evaluation automatically?

Luc",1708646,,846892.0,2013-01-10 14:33:49,2013-01-11 16:39:05,Pandas load_table string to tuple conversion,<python><tuples><pandas>,1.0,2.0,,
14217581,1,14218626.0,2013-01-08 14:51:01,2,111,"I've problem exporting a dataframe to CSV file 

Data types are String and Float64 values like this:

In [19]: segmenti_t0
Out[19]:
SEGM1  SEGM2
AD     PS         83
       SCREMATO     06
CRE    STD          12
FUN    INTERO       00
       PS         20
       SCREMATO     00
NORM   INTERO      131
       PS        695
       SCREMATO     52
Name: Quota volume_t0


I try to export this dataframe with this command:

IN [20]: segmenti_t0to_csv('C:Users\MarioRossi\provacsv'  sep="";"")


When I try to open it with Excel or I try to import it in excel from the csv file with formatting parameters I obtain really strange formatting for float values like 695000 or 520000000 or date times formatting too like this:

NORM    INTERO  1301
NORM    PS    690500
NORM    SCREMATO    502


Consider that I m using European format ("" "" as decimal as I use to import the original raw data from csv files)

Please help me: I developed a software (with GUI and so on) and I cant deliver it for that reason!

Thanks",1937003,,449449.0,2013-01-08 14:52:38,2013-01-08 16:00:45,python pandas csv exporting,<python><csv><pandas><exporting>,1.0,4.0,,
14235487,1,14235744.0,2013-01-09 12:45:02,1,31,"Say I have two series: a and b 

a = Series(None  index=['a' 'b' 'c'])
b = Series('lol'  index=['j' 'k' 'l'])


I would like to store b as one of the elements of a 

a['a'] = b


but I get

ValueError: setting an array element with a sequence

Is it possible to store a pandas series inside a pandas series? How can I do it? Thanks",182172,,,,2013-01-09 13:13:22,How to do a series of series in pandas,<python><pandas>,1.0,4.0,,
14247586,1,,2013-01-09 22:22:05,3,58,"I have a dataframe with ~300K rows and ~40 columns
I want to find out if any rows contain null values - and put these 'null'-rows into a separate dataframe so that I could explore them easily

I can create a mask explicitly:

mask=False
for col in dfcolumns: mask = mask | df[col]isnull()
dfnulls = df[mask]


Or I can do something like:

dfix[dfindex[(dfT == npnan)sum() > 1]]


Is there a more elegant way of doing it (locating rows with nulls in them)?",1442475,,1442475.0,2013-01-10 16:06:45,2013-01-10 16:06:45,Python Pandas How to select rows with one or more nulls from a DataFrame without listing columns explicitly?,<python><null><pandas>,1.0,,0.0,
14268179,1,,2013-01-10 22:10:00,1,32,"Something weird is happening in applying a function on a pandaSeries

In [508]: id = lambda x : x
In [509]: tt = lambda x : type(x)
In [510]: timeSeries
Out[510]: 
0   1900-01-01 20:11:49075690
1   1900-01-01 20:11:49082546
2   1900-01-01 20:11:52535287
3   1900-01-01 20:11:52535372
4   1900-01-01 20:11:52535528
Name: timeSeries

In [511]: timeSeriesmap(id)
Out[511]: 
0    1969-12-06 172:11:49075690
1    1969-12-06 172:11:49082546
2    1969-12-06 172:11:52535287
3    1969-12-06 172:11:52535372
4    1969-12-06 172:11:52535528
Name: timeSeries


Here the timestamps have changed by applying the id function
The hour in the new time is incorrect
More: the type is also changed

In [513]: type(timeSeries[0])
Out[513]: pandaslibTimestamp

In [512]: timeSeriesmap(tt)
Out[512]: 
0    
1    
2    
3    
4    
Name: timeSeries


I think type change is fine as long as the timestamp they map to is preserved

In [515]: npdatetime64(timeSeries[0])
Out[515]: 1900-01-01 20:11:49075690


Can someone please explain what is happening here?
I want to do a map with a customized function where I'm expecting pandaslibTimestamp type
or either if there is a way to convert the npdatetime64 to a valid time 

Edit: Following works to cast the npdatetime64 back to Timestamp Still the above is weird

In [528]: pdlibTimestamp(time2[0])
Out[528]: 
In [529]: time2[0]
Out[529]: 1969-12-06 172:11:49075690


My guess is it is about internal formatting difference between npdatetime64 vs pandaslibTimestamp??",1945648,,1240268.0,2013-01-10 22:20:49,2013-01-10 22:20:49,Weirdness in Pandas.Series.map function for a TimeStamp series,<python-2.7><pandas>,,1.0,,
14248706,1,14248948.0,2013-01-10 00:05:42,2,47,"Given:

ser = Series(['one'  'two'  'three'  'two'  'two'])


How do I plot a basic histogram of these values?

Here is an ASCII version of what I'd want to see in matplotlib:

     X
 X   X   X
-------------
one two three


I'm tired of seeing:

TypeError: cannot concatenate 'str' and 'float' objects
",26002,,,,2013-01-10 00:43:33,How can I plot a histogram in pandas using nominal values?,<pandas><histogram>,1.0,,,
14263560,1,14283423.0,2013-01-10 17:19:25,0,29,"The HTML file that comes out of Dataframeto_html() does not create hyperlinks when the string content of one of its columns matches an URI

Is there a way to generate hyperlinks in html docs from a DataFrame?",680232,,,,2013-01-11 17:24:09,dataframe.to_html does not create hyperlinks,<python><pandas>,1.0,1.0,,
14268794,1,,2013-01-10 22:56:37,2,86,"I have a large dataset (see example format below) and I need to do the follow thinks:

identify the repeated values that appear on columns 1 2 5 - if the all repeated then I need to remove redundant rows and average the value in column 8 (this is successful with the code I will post - 
after step one  I want to round the values on columns 1 2 to whole number (no decimals)
I want to reintroduce columns 3  4  6 and 7 -
columns 3  6  and 7 need to have a specific value I will dictate (eg 3 should be all 0  6 all 1  and column 7 all 1) (similar to input file)
column 4 needs to increase by one  based on number of different values on column 4) (similar to input file
here is a sample input file: data (name of the file)

56499115   737127789  0   1   1530    1   1   160225
56499115   737127789  0   1   8250    1   1   144405
56499115   737127789  0   2   1530    1   1   148637
56499115   737127789  0   2   8250    1   1   148918
56499117   737127789  0   3   1530    1   1   160002
56499117   737127789  0   3   8250    1   1   154333
56499104   737127676  0   4   1530    1   1   1473
56499104   737127676  0   4   8250    1   1   156138
56499104   737127676  0   5   1530    1   1   162453
56499104   737127676  0   5   8250    1   1   156138


and here is the code I have up to know (currently I supplement in calc)

import os
import numpy as np
import pandas as pd
datadirectory = '/media/data'
oschdir = 'datadirectory'
df = pdread_csv('/media/data/datadat')
sorted_data = dfgroupby([""X1"" ""X2"" ""X5""])[""X8""]mean()reset_index()
tuple_data = [tuple(x) for x in sorted_datavalues]
datas = npasarray(tuple_data)
npsavetxt('sorted_data_roundeddat'  datas  fmt='%s'  delimiter='\t')


but his gives me only the 4 columns  and no rounded data",1804537,,748858.0,2013-01-10 23:02:41,2013-01-11 16:25:33,"refine, average, round data python",<python><numpy><pandas><average>,3.0,,,
14276661,1,14280694.0,2013-01-11 10:56:57,0,129,"I often deal with ascii tables containing few columns (normally less than 10) and up to tens of millions of lines They look like

176792 -230523 0430772 32016 1 1 2 
177042 -187729 0430562 32016 1 1 1
177047 -154957 0431853 31136 1 1 1

177403 -0657246 0432905 31152 1 1 1


I have a number of python codes that read  manipulate and save files I have always used numpyloadtxt and numpysavetxt to do it But numpyloadtxt takes at least 5-6Gb RAM to read 1Gb ascii file

Yesterday I discovered Pandas  that solved almost all my problems: pandasread_table together with numpysavetxt improved the execution speed (of 2) of my scripts by a factor 3 or 4  while being very memory efficient 

All good until the point when I try to read in a file that contains a few commented lines at the beginning The doc string (v=0101dev_f73128e) tells me that line commenting is not supported  and that will probably come I think that this would be great: I really like the exclusion of line comments in numpyloadtxt
Is there any idea on how this will become available? Would be also nice to have the possibility to skip those lines (the doc states that they will be returned as empy)

Not knowing how many comment lines I have in my files (I process thousands of them coming from different people)  as now I open the file  count the number of lines starting with a comment at the beginning of the file:

def n_comments(fn  comment):
    with open(fname  'r') as f:
        n_lines = 0
        pattern = recompile(""^\s*{0}""format(comment))
        for l in f:
            if patternsearch(l) is None:
                break
            else:
                n_lines += 1
    return n_lines


and then 

pandasread_table(fname  skiprows=n_comments(fname  '#')  header=None  sep='\s')


Is there any better way (maybe within pandas) to do it?

Finally  before posting  I looked a bit at the code in pandasioparserspy to understand how pandasread_table works under the hood  but I got lost Can anyone point me to the places that implement the reading of the files?

Thanks

EDIT2: I thought to get some improvement getting rid of some of the if in @ThorstenKranz second implementation of FileWrapper  but did get almost no improvements

class FileWrapper(file):
    def __init__(self  comment_literal  *args):
        super(FileWrapper  self)__init__(*args)
        self_comment_literal = comment_literal
        self_next = self_next_comment

    def next(self):
        return self_next()

    def _next_comment(self):
        while True:
            line = super(FileWrapper  self)next()
            if not linestrip()[0] == self_comment_literal:
                self_next = self_next_no_comment
                return line
    def _next_no_comment(self):
        return super(FileWrapper  self)next()
",1860757,,1860757.0,2013-01-16 10:54:44,2013-01-16 10:54:44,Python pandas: read file skipping commented,<python><io><pandas>,2.0,1.0,2.0,
14198103,1,14198133.0,2013-01-07 14:41:13,1,39,"I have a series of hourly prices Each price is valid throughout the whole 1-hour period What is the best way to represent these prices in Pandas that would enable me to index them in arbitrary higher frequencies (such as minutes or seconds) and do arithmetic with them?

Data specifics

Sample prices might be:

>>> prices = Series(randn(5)  pddate_range('2013-01-01 12:00'  periods = 5  freq='H'))
>>> prices
2013-01-01 12:00:00   -1001692
2013-01-01 13:00:00   -1408082
2013-01-01 14:00:00   -0329637
2013-01-01 15:00:00    1005882
2013-01-01 16:00:00    1202557
Freq: H


Now  what representation to use if I want the value at 13:37:42(I expect it to be the same as at 13:00)?

>>> prices['2013-01-01 13:37:42']

KeyError: 


Resampling

I know I could resample the prices and fill in the details (ffill  right?)  but that doesn't seem like such a nice solution  because I have to assume the frequency I'm going to be indexing it at and it reduces readability with too many unnecessary data points

Time spans

At first glance a PeriodIndex seems to work

>>> price_periods = pricesto_period()
>>> price_periods['2013-01-01 13:37:42']
-1408082


But a time-spanned series doesn't offer some of the other functionality I expect from a Series Say that I have another series amounts that says how many items I bought in a certain moment If I wanted to calculate the prices I would want to multiply the two series'

>>> amounts = Series([1 2 2]  pdDatetimeIndex(['2013-01-01 13:37'  '2013-01-01 13:57'  '2013-01-01 14:05']))
>>> amounts*price_periods


but that yields an exception and sometimes even freezes my IPy Notebook Indexing doesn't help either

>>> ts_periods[amountsindex]


Are PeriodIndex structures still a work in progress or these features aren't going to be added? Is there maybe some other structure I should have used (or should use for now  before PeriodIndex matures)? I'm using Pandas version 090dev-1e68fd9",544059,,,,2013-01-07 14:56:38,What representation should I use in Pandas for data valid throughout an interval?,<python><pandas><time-series>,1.0,2.0,,
14284266,1,14321532.0,2013-01-11 18:18:42,2,184,"I have a pandas time series data frame df

date is the index  Three columns  cusip  ticker  factor

I want to decile the data per date  About 100 factors per dateEach date will be deciled 1 to 10

As a first attempt  I tried to decile the whole data frame regardless of date  I used:

factor = pdcut(dffactor  10)  #This gave an error:

adj = (mx - mn) * 0001 # 01% of the range



  SybaseError: ('Layer: 2  Origin: 4\ncs_calc: cslib user api layer: common library error: The conversion/operation resulted in overflow')


The dataframe has 1mm rows  Is it a size issue?  An nan issue?

Three questions

What is wrong with the current function?
How do I get the count of number of nan's in a column?
Any recommendations on deciling per date?
Thank you for the help  New to pandas python

SAMPLE DATA:

df:             cusip      ticker    factor
date
2012-01-05       XXXXX       ABC       426
2012-01-05       YYYYY       BCD       -125
(100 more stocks on this date)  
2012-01-06       XXXXX       ABC       325
2012-01-06       YYYYY       BCD       -155
(100 more stocks on this date)


OUTPUT for what I would like:

#column with the deciles  lined up with the df
decile
10
2

10
3



I can then append this to my dataframe to have a new column  Each date is deciled and each data point then has their corresponding decile on that date  Thanks

Stack Trace:

Traceback (most recent call last): File """"  line 1  in  File ""/misc/apps/linux/python-261/lib/python26/site-packages/pandas-0100-py26-linux-x86_64egg/pandas/core/groupbypy""  line 1817  in transform res = wrapper(group)

File ""/misc/apps/linux/python-261/lib/python26/site-packages/pandas-0100-py26-linux-x86_64egg/pandas/core/groupbypy""  line 1807  in  wrapper = lambda x: func(x  *args  **kwargs) File """"  line 1  in  File ""/misc/apps/linux/python-261/lib/python26/site-packages/pandas-0100-py26-linux-x86_64egg/pandas/tools/tilepy""  line 138  in qcut bins = algosquantile(x  quantiles)

File ""/misc/apps/linux/python-261/lib/python26/site-packages/pandas-0100-py26-linux-x86_64egg/pandas/core/algorithmspy""  line 272  in quantile return algosarrmap_float64(q  _get_score) File ""generatedpyx""  line 1841  in pandasalgosarrmap_float64 (pandas/algosc:71156) File ""/misc/apps/linux/python-261/lib/python26/site-packages/pandas-0100-py26-linux-x86_64egg/pandas/core/algorithmspy""  line 257  in _get_score idx % 1)

File ""/misc/apps/linux/python-261/lib/python26/site-packages/pandas-0100-py26-linux-x86_64egg/pandas/core/algorithmspy""  line 279  in _interpolate return a + (b - a) * fraction File ""build/bdistlinux-x86_64/egg/Sybasepy""  line 246  in _cslib_cb SybaseError: ('Layer: 2  Origin: 4\ncs_calc: cslib user api layer: common library error: The conversion/operation resulted in overflow'  )
",1911092,,919872.0,2013-01-14 16:14:20,2013-01-14 16:14:20,Trouble with pandas cut,<python><pandas>,1.0,4.0,1.0,
14298401,1,,2013-01-12 22:03:47,3,70,"I have two dataframes  the first is of the form (note that the dates are datetime objects):

df = DataFrame('key': [0 1 2 3 4 5] 
               'date': [date0 date1  date2  date3  date4  date5] 
               'value': [0 10 20 30 40 50])


And a second which is of the form:

df2 = DataFrame('key': [0 1 2 3 4 5] 
                'valid_from': [date0  date0  date0  date3  date3  date3] 
                'valid_to': [date2  date2  date2  date5  date5  date5] 
                'value': [0  100  200  300  400  500])


And I'm trying to efficiently join where the keys match and the date is between the valid_from and valid_to  What I've come up with is the following:

def map_keys(df2  key  date):
    value = df2[df2['key'] == key & 
        df2['valid_from'] = date]['value']values[0]
    return value

keys = df['key']values
dates = df['date']values
keys_dates = zip(keys  dates)

values = []
for key_date in keys_dates:
    value = map_keys(df2  key_date[0]  key_date[1])
    valuesappend(value)

df['joined_value'] = values


While this seems to do the job it doesn't feel like a particularly elegant solution  I was wondering if anybody had a better idea for a join such as this

Thanks for you help - it is much appreciated",1889456,,,,2013-01-13 23:25:31,SQL like joins in pandas,<python><pandas>,1.0,,1.0,
14298433,1,14298661.0,2013-01-12 22:08:44,2,54,"I have a dataframe with numerical columns For each column I would like calculate quantile information and assign each row to one of them I tried to use the qcut() method to return a list of bins but instead ended up calculating the bins individually  What I thought might exist but I couldn't find it would be a method like dfto_quintile(num of quantiles)  This is what I came up with but I am wondering if there is a more succint/pandas way of doing this

import pandas as pd

#create a dataframe
df = pdDataFrame(randn(10  4)  columns=['A'  'B'  'C'  'D'])

def quintile(df  column):
    """"""
    calculate quintiles and assign each sample/column to a quintile 
    """"""
    #calculate the quintiles using pandas quantile() here
    quintiles = [df[column]quantile(value) for value in [00 02 04 06 08]]
    quintilesreverse() #reversing makes the next loop simpler

    #function to check membership in quintile to be used with pandas apply
    def check_quintile(x  quintiles=quintiles):
        for num level in enumerate(quintiles):
            #print number  level  level[1]
            if  x >= level:
                print x  num
                return num+1

    df[column] = df[column]apply(check_quintile)

quintile(df 'A')


thanks 
zach cp

EDIT: After seeing DSMs answer the function can be written much simpler (below) Man  thats sweet

def quantile(column  quantile=5):
    q = qcut(column  quantile)
    return len(qlevels)- qlabels
dfapply(quantile)
#or
df['A']apply(quantile)
",983191,,983191.0,2013-01-12 23:29:55,2013-01-12 23:29:55,Convert data to the quantile bin,<python><pandas>,1.0,,1.0,
14298447,1,14298829.0,2013-01-12 22:10:47,2,62,"I have an intraday 30-second interval time series data in a CSV file with the following format:

20120105  080000    1
20120105  080030    2
20120105  080100    3
20120105  080130    4
20120105  080200    5


How can I read it into a pandas data frame with these two different indexing schemes:

1  Combine date and time into a single datetime index

2  Use date as the primary index and time as the secondary index in a multiindex dataframe

What are the pros and cons of these two schemes? Is one generally more preferable than the other? In my case  I would like to look at time-of-the-day analysis but am not entirely sure which scheme will be more convenient for my purpose Thanks in advance",1642513,,,,2013-01-13 01:47:40,Combine date column and time column into index in pandas data frame,<python><pandas>,1.0,1.0,1.0,
14265539,1,14267566.0,2013-01-10 19:13:55,2,54,"I have a rather big dataset (2678271  52) and a 5-dimensional index which consumes 65% of the machine's memory
When I call

dfsortlevel(k)


I receive the following error:



MemoryError                               Traceback (most recent call last)
 in ()
----> 1 df = dfsortlevel(4)

/usr/local/lib/python27/dist-packages/pandas-091-py27-linux-x86_64egg/pandas/core/framepyc in sortlevel(self  level  axis  ascending)
   2978             raise Exception('can only sort by level with a hierarchical index')
   2979 
-> 2980         new_axis  indexer = the_axissortlevel(level  ascending=ascending)
   2981 
   2982         if self_datais_mixed_dtype():

/usr/local/lib/python27/dist-packages/pandas-091-py27-linux-x86_64egg/pandas/core/indexpyc in sortlevel(self  level  ascending)
   1856         indexer = _indexer_from_factorized((primary ) + tuple(labels) 
   1857                                            (primshp ) + tuple(shape) 
-> 1858                                            compress=False)
   1859         if not ascending:
   1860             indexer = indexer[::-1]

/usr/local/lib/python27/dist-packages/pandas-091-py27-linux-x86_64egg/pandas/core/groupbypyc in _indexer_from_factorized(labels  shape  compress)
   2124         max_group = npprod(shape)
   2125 
-> 2126     indexer  _ = libgroupsort_indexer(comp_idsastype(npint64)  max_group)
   2127 
   2128     return indexer

/usr/local/lib/python27/dist-packages/pandas-091-py27-linux-x86_64egg/pandas/libso in pandaslibgroupsort_indexer (pandas/src/tseriesc:55052)()

MemoryError: 



Is there a hard-coded condition which throws this error? Or is it possible that even though the data only uses 65% of the memory (according to htop) the operation eats up the remaining memory?",942591,,,,2013-01-10 21:27:27,When does pandas (pandas.pydata.org) throw a memory error on df.sortlevel(k)?,<python><pandas>,1.0,3.0,,
14281644,1,14294757.0,2013-01-11 15:46:01,0,33,"Rows used to be able to work as maps for string interpolation  like this:

from pandas import *

speeds = read_csv('resultscsv')
row = speedsix[3]
print(""%(my_column_name)s"" % row)


that worked a few months ago  but doesn't seem to work in the latest Pandas  How can I turn a row into a map  or otherwise do simple & concise custom printing of rows?",3917,,,,2013-01-12 15:24:18,How do I turn a row into a map?,<pandas>,1.0,2.0,,
14300137,1,14306902.0,2013-01-13 02:38:30,3,107,"what is the best way to make a series of scatter plots using matplotlib from a pandas dataframe in Python? For example if I have a dataframe df that has some columns of interest  I find myself typically converting everything to arrays:

import matplotlibpylab as plt
# df is a DataFrame: fetch col1 and col2 
# and drop na rows if any of the columns are NA
mydata = df[[""col1""  ""col2""]]dropna(how=""any"")
# Now plot with matplotlib
vals = mydatavalues
pltscatter(vals[:  0]  vals[:  1])


The problem with converting everything to array before plotting is that it forces you to break out of dataframes Consider these two use cases where having the full dataframe is essential to plotting:

For example  what if you wanted to now look at all the values of col3 for the corresponding values that you plotted in the call to scatter  and color each point (or size) it by that value? You'd have to go back  pull out the non-na values of col1 col2 and check what their corresponding values

Is there a way to plot while preserving the dataframe? For example:

mydata = dfdropna(how=""any""  subset=[""col1""  ""col2""])
# plot a scatter of col1 by col2  with sizes according to col3
scatter(mydata([""col1""  ""col2""])  s=mydata[""col3""])

Similarly  imagine that you wanted to filter or color each point differently depending on the values of some of its columns Eg what if you wanted to automatically plot the labels of the points that meet a certain cutoff on col1  col2 alongside them (where the labels are stored in another column of the df)  or color these points differently  like people do with dataframes in R  For example:

mydata = dfdropna(how=""any""  subset=[""col1""  ""col2""]) 
myscatter = scatter(mydata[[""col1""  ""col2""]]  s=1)
# Plot in red  with smaller size  all the points that 
# have a col2 value greater than 05
myscatterreplot(mydata[""col2""] > 05  color=""red""  s=05)

How can this be done? thanks

EDIT Reply to crewbum:

Thanks! You say that the best way is to plot each condition (like subset_a  subset_b) separately What if you have many conditions  eg you want to split up the scatters into 4 types of points or even more  plotting each in different shape/color How can you elegantly apply condition a  b  c  etc and make sure you then plot ""the rest"" (things not in any of these conditions) as the last step? 

Similarly in your example where you plot col1 col2 differently based on col3  what if there are NA values that break the association between col1 col2 col3? For example if you want to plot all col2 values based on their col3 values  but some rows have an NA value in either col1 or col3  forcing you to use dropna first So you would do:

mydata = dfdropna(how=""any""  subset=[""col1""  ""col2""  ""col3"")


then you can plot using mydata like you show -- plotting the scatter between col1 col2 using the values of col3 But mydata will be missing some points that have values for col1 col2 but are NA for col3  and those still have to be plotted so how would you basically plot ""the rest"" of the data  ie the points that are not in the filtered set mydata? Thanks again",248237,,243434.0,2013-01-16 16:39:54,2013-01-16 16:39:54,making matplotlib scatter plots from dataframes in Python's pandas,<python><matplotlib><plot><data.frame><pandas>,1.0,1.0,3.0,
14197088,1,,2013-01-07 13:35:40,2,75,"I have a data frame with 4 columns

A    B     C      D
e    2     =     ",781329,,597607.0,2013-01-07 13:50:32,2013-01-07 23:19:17,Spilt value from a column in a DataFrame using Python,<python><numpy><python-2.7><data.frame><pandas>,2.0,2.0,,
14225676,1,14225838.0,2013-01-08 23:12:25,1,72,"How can I export a list of DataFrames into one Excel spreadsheet?
The docs for to_excel state:


  Notes
  If passing an existing ExcelWriter object  then the sheet will be added
  to the existing workbook  This can be used to save different
  DataFrames to one workbook
  
  writer = ExcelWriter('outputxlsx')df1to_excel(writer  'sheet1')df2to_excel(writer  'sheet2')writersave()


Following this  I thought I could write a function which saves a list of DataFrames to one spreadsheet as follows:

from openpyxlwriterexcel import ExcelWriter
def save_xls(list_dfs  xls_path):
    writer = ExcelWriter(xls_path)
    for n  df in enumerate(list_dfs):
        dfto_excel(writer 'sheet%s' % n)
    writersave()


However (with a list of two small DataFrames  each of which can save to_excel individually)  an exception is raised (Edit: traceback removed):

AttributeError: 'str' object has no attribute 'worksheets'


Presumably I am not calling ExcelWriter correctly  how should I be in order to do this?",1240268,,1240268.0,2013-01-08 23:42:42,2013-01-08 23:42:42,Save list of DataFrames to multisheet Excel spreadsheet,<python><pandas><openpyxl>,1.0,1.0,1.0,
14300768,1,14307460.0,2013-01-13 04:50:56,2,71,"I'm looking for a way to do something like the various rolling_* functions of pandas  but I want the window of the rolling computation to be defined by a range of values (say  a range of values of a column of the DataFrame)  not by the number of rows in the window

As an example  suppose I have this data:

>>> print d
   RollBasis  ToRoll
0          1       1
1          1       4
2          1      -5
3          2       2
4          3      -4
5          5      -2
6          8       0
7         10     -13
8         12      -2
9         13      -5


If I do something like rolling_sum(d  5)  I get a rolling sum in which each window contains 5 rows  But what I want is a rolling sum in which each window contains a certain range of values of RollBasis  That is  I'd like to be able to do something like droll_by(sum  'RollBasis'  5)  and get a result where the first window contains all rows whose RollBasis is between 1 and 5  then the second window contains all rows whose RollBasis is between 2 and 6  then the third window contains all rows whose RollBasis is between 3 and 7  etc  The windows will not have equal numbers of rows  but the range of RollBasis values selected in each window will be the same  So the output should be like:

>>> droll_by(sum  'RollBasis'  5)
    1    -4    # sum of elements with 1 ",1427416,,1427416.0,2013-01-13 05:02:41,2013-01-13 21:57:16,pandas rolling computation with window based on values instead of counts,<python><pandas>,2.0,,1.0,
14295531,1,,2013-01-12 16:51:06,2,34,"What's the simplest way of merging back changes to a pandas dataframe after filtering via fancy indexing? 

For example  define a dataframe with two columns x and y  and select all the rows where x is an even integer  and then set the corresponding values in y to 0  

d = pdDataFrame({'x':range(10)  'y':range(11 21)})
d[dx % 2 == 0]['y'] = 0


The ""fancy indexing"" boolean query makes a copy of the dataframe  so the changes are never propagated back to the original dataframe Is there a better of performing this operation? 

My current solution is to define a temporary dataframe w  based on the fancy boolean indexing  set the corresponding values in 'y' to 0 in w  and then merge w back to d using the index There must be a more efficient (and hopefully more direct) way of doing this:

w = d[dx % 2 == 0]
wy = 0
",190894,,,,2013-01-12 17:02:22,pandas fancy indexing and merging back,<python><pandas>,1.0,,1.0,
14301913,1,14306921.0,2013-01-13 08:36:21,0,49,"If I have the following Dataframe

>>> df = pdDataFrame({'Name': ['Bob'] * 3 + ['Alice'] * 3  \
'Destination': ['Athens'  'Rome'] * 3  'Length': nprandomrandint(1  6  6)}) 
>>> df    
  Destination  Length   Name
0      Athens       3    Bob
1        Rome       5    Bob
2      Athens       2    Bob
3        Rome       1  Alice
4      Athens       3  Alice
5        Rome       5  Alice


I can goup by name and destination

>>> grouped = dfgroupby(['Name'  'Destination'])
>>> for nm  gp in grouped:
>>>     print nm
>>>     print gp
('Alice'  'Athens')
  Destination  Length   Name
4      Athens       3  Alice
('Alice'  'Rome')
  Destination  Length   Name
3        Rome       1  Alice
5        Rome       5  Alice
('Bob'  'Athens')
  Destination  Length Name
0      Athens       3  Bob
2      Athens       2  Bob
('Bob'  'Rome')
  Destination  Length Name
1        Rome       5  Bob


but I would like a new multi-indexed dataframe out of it that looks something like

                Length
Alice   Athens       3
        Rome         1
        Rome         5
Bob     Athens       3
        Athens       2
        Rome         5


It seems there should be a way to do something like Dataframe(grouped) to get my multi-indexed Dataframe  but instead I get a PandasError (""DataFrame constructor not properly called!"")

What is the easiest way to get this? Also  anyone know if there will ever be an option to pass a groupby object to the constructor  or if I'm just doing it wrong?

Thanks",386279,,243434.0,2013-01-13 18:51:41,2013-01-13 18:51:41,Convert pandas group by object to multi-indexed Dataframe,<python><group-by><data.frame><pandas><multi-index>,1.0,,,
14334898,1,14447383.0,2013-01-15 09:45:25,1,23,"I run pandas OLS on a data set If I run it with less than 20 time series in the x-value  everything works fine
Is there a maximum of dependants pandasols can handle?
This is what I'm doing  except that I have the data on file instead of fetching it with the DataReader:

from pandas import Series  DataFrame  ols
from pandasiodata import DataReader
from DataContainer import DataContainer
import random

window = 21
basic = DataReader(""BHI""  ""yahoo"")
print len(basic)
dependance = 15

sp100 = [
            ""AAPL""  ""ABT""  ""ACN""  ""AEP""  ""ALL""  ""AMGN""  ""AMZN""  ""APC"" 
            ""AXP""  ""BA""  ""BAC""  ""BAX""  ""BK""  ""BMY""  ""BRKB""  ""CAT""  ""C""  ""CL"" 
            ""CMCSA""  ""COF""  ""COP""  ""COST""  ""CPB""  ""CSCO""  ""CVS""  ""CVX""  ""DD""  ""DELL"" 
            ""DIS""  ""DOW""  ""DVN""  ""EBAY""  ""EMC""  ""EXC""  ""F""  ""FCX""  ""FDX""  ""GD""  ""GE"" 
            ""GILD""  ""GOOG""  ""GS""  ""HAL""  ""HD""  ""HNZ""  ""HON""  ""HPQ""  ""IBM""  ""INTC"" 
            ""JNJ""  ""JPM_1""  ""KFT""  ""KO""  ""LLY""  ""LMT""  ""LOW""  ""MA""  ""MCD""  ""MDT""  ""MET"" 
            ""MMM""  ""MO""  ""MON""  ""MRK""  ""MS""  ""MSFT""  ""NKE""  ""NOV""  ""NSC""  ""NWSA"" 
            ""NYX""  ""ORCL""  ""OXY""  ""PEP""  ""PFE""  ""PG""  ""PM""  ""QCOM""  ""RF""  ""RTN"" 
            ""SBUX""  ""SLB""  ""SLE""  ""SO""  ""SPG""  ""T""  ""TGT""  ""TWX""  ""TXN""  ""UNH""  ""UPS"" 
            ""USB""  ""UTX""  ""VZ""  ""WAG""  ""WFC""  ""WMB""  ""WMT""  ""XOM""
        ]

keys = randomsample(sp100  dependance)

data = {key: DataReader(key  ""yahoo"") for key in keys}
vals = {key: DataFrame(data=Series(data[key]  name=key)  index=basicindex) for key in data}
model = ols(y=basic  x=vals  window=window)


The error occurs as soon as dependance >= 20  but never for dependance ",1073420,,,,2013-01-21 21:18:36,Maximum number of dependencies in pandas ols?,<python><pandas><linear-regression>,1.0,0.0,,
14288864,1,14289730.0,2013-01-12 00:29:30,2,52,"I have the following DataFrame:

in  year   ni  d  m   x    y        q
1   2012   1   2  0  NaN  NaN       3
6   2012   2   1  1    9    9       1
5   2012   3   1  1   17   17       1
3   2012   4   0  3   37   37       0
5   2012   5   1  0  NaN  NaN       3
2   2012   6   3  1   15   15       3


When I use dfreindex(index=[1 2 3 4 5 6]) - basically column 'ni' (the index I want to use) - then this will change the order of my dataframe  which I try to avoid I know I can do it with rename  but the data has 5 0000 rows and it's quite weary writing such a dictionary

So is there a way to remain the order but change the index or is there a trick to do a quicker rename or simply adapt ni as the index?",1563867,,,,2013-01-12 02:53:53,Reindex without changing order,<python><pandas>,1.0,1.0,,
14327664,1,14327941.0,2013-01-14 22:08:51,1,35,"I have lots of missing values when calculating rollng_mean with:

import datetime as dt
import pandas as pd
import pandasiodata as web

stocklist = ['MSFT'  'BELGBR']

# read historical prices for last 11 years
def get_px(stock  start):
    return webget_data_yahoo(stock  start)['Adj Close']

today = dtdatetoday()
start = str(dtdate(todayyear-11  todaymonth  todayday))

px = pdDataFrame({n: get_px(n  start) for n in stocklist})
pxffill()
sma200 = pdrolling_mean(px  200)


got following result:

In [14]: px
Out[14]: 

DatetimeIndex: 2836 entries  2002-01-14 00:00:00 to 2013-01-11 00:00:00
Data columns:
BELGBR    2270  non-null values
MSFT       2769  non-null values
dtypes: float64(2)

In [15]: sma200
Out[15]: 

DatetimeIndex: 2836 entries  2002-01-14 00:00:00 to 2013-01-11 00:00:00
Data columns:
BELGBR    689  non-null values
MSFT       400  non-null values
dtypes: float64(2)


Any idea why most of the sma200 rolling_mean values are missing and how to get the complete list ? ",1915862,,,,2013-01-14 22:29:12,missing values using pandas.rolling_mean,<python><pandas>,2.0,,,
14331891,1,14335258.0,2013-01-15 05:55:04,0,38,"I have a dataframe df which has duplicate columns: (I need duplicate columns dataframe   which will be pass as a parameter to matplotlib to plot  so the columns name and content might be same or different) 

>>> df
                                         PE     RT    Ttl_mkv      PE
STK_ID    RPT_Date                                  
11_STK79  20130115  41932  2744   3629155  41932
21_STK58  20130115  14223  0048  30302324  14223
22_STK229 20130115  22436  0350  15968313  22436
23_STK34  20130115 -63252  0663   4168189 -63252


I can get the second column by : df[dfcolumns[1]]    

>>> df[dfcolumns[1]]
STK_ID     RPT_Date
11_STK79   20130115    2744
21_STK58   20130115    0048
22_STK229  20130115    0350
23_STK34   20130115    0663


but if I want to get the first column by df[dfcolumns[0]]   it will give :

>>> df[dfcolumns[0]]
                                   PE      PE
STK_ID    RPT_Date                
11_STK79  20130115  41932  41932
21_STK58  20130115  14223  14223
22_STK229 20130115  22436  22436
23_STK34  20130115 -63252 -63252


Which have two columns That will make my application down for the application just wants the first column but Pandas give 1st & 4th column!  Is it a bug or it is designed as this on purpose ? How to bypass this  issue ? 

My pandas version is 081 ",1072888,,,,2013-01-15 13:00:31,Duplicate column Pandas dataframe slice issue,<pandas>,1.0,,,
14340398,1,,2013-01-15 14:57:30,-1,32,"A dataframe df with duplicate columns :

>>> df
                                       PE     PB      PE
STK_ID    RPT_Date                       
11_STK79  20130115  42178  3095  42178
21_STK58  20130115  14259  1792  14259
22_STK229 20130115  22634  1797  22634
23_STK34  20130115 -63571  0937 -63571
24_STK79  20130115  55467  3182  55467


When I try to 'dfreset_index()'  it gives below error :

>>> dfreset_index()
Traceback (most recent call last):
  File """"  line 1  in 
  File ""D:\Python\Lib\site-packages\pandas\core\framepy""  line 583  in __repr__
    selfto_string(buf=buf)
  File ""D:\Python\Lib\site-packages\pandas\core\framepy""  line 1228  in to_string
    formatterto_string(force_unicode=force_unicode)
  File ""D:\Python\Lib\site-packages\pandas\core\formatpy""  line 207  in to_string
    str_columns = self_get_formatted_column_labels()
  File ""D:\Python\Lib\site-packages\pandas\core\formatpy""  line 389  in _get_formatted_column_labels
    dtypes = selfframedtypes
  File ""D:\Python\Lib\site-packages\pandas\core\framepy""  line 1699  in __getattr__
    (type(self)__name__  name))
AttributeError: 'DataFrame' object has no attribute 'dtypes'


Is it a bug or how to reset_index for dataframe with duplicate columns ?

Pandas 081

Code for toy example:

In [1]: from pandas import *
In [2]: import io
In [3]: text = """"""STK_ID    RPT_Date  PE     PB      PE
   : 11_STK79  20130115  42178  3095  42178
   : 21_STK58  20130115  14259  1792  14259
   : 22_STK229 20130115  22634  1797  22634
   : 23_STK34  20130115 -63571  0937 -63571
   : 24_STK79  20130115  55467  3182  55467
   : """"""
In [4]: df = read_csv(ioBytesIO(text)  sep=""\s+""  header=0  index_col=[0 1])

In [5]: df
Out[5]:
                        PE     PB    PE1
STK_ID    RPT_Date
11_STK79  20130115  42178  3095  42178
21_STK58  20130115  14259  1792  14259
22_STK229 20130115  22634  1797  22634
23_STK34  20130115 -63571  0937 -63571
24_STK79  20130115  55467  3182  55467
",1072888,,919872.0,2013-01-15 15:13:16,2013-01-15 15:13:16,"Pandas dataframe with duplicate columns can't ""reset_index()""",<pandas>,,3.0,,
14235984,1,14236239.0,2013-01-09 13:12:41,1,56,"With this code  it is possible to export every data frame in a new worksheet iterating data frames list:

def save_xls(list_dfs  xls_path):
    writer = ExcelWriter(xls_path)
    for n  df in enumerate(list_dfs):
        dfto_excel(writer 'sheet_dati%s' % n)
    writersave()
save_xls(list_dfs  xls_path)


But its possible to export two or more data frames in a single worksheet?",1937003,,1240268.0,2013-01-09 13:58:01,2013-01-09 16:07:19,Export many small DataFrames to a single Excel worksheet,<python><excel><export><pandas>,2.0,0.0,,
14256839,1,14257036.0,2013-01-10 11:24:43,2,70,"Say I have the following file testtxt:

Aaa Bbb
Foo 0
Bar 1
Baz NULL


(The separator is actually a tab character  which I can't seem to input here)
And I try to read it using pandas (0100):

In [523]: pdread_table(""testtxt"")
Out[523]:
   Aaa  Bbb
0  Foo  NaN
1  Bar    1
2  Baz  NaN


Note that the zero value in the first column has suddenly turned into NaN! I was expecting a DataFrame like this:

   Aaa   Bbb
0  Foo     0
1  Bar     1
2  Baz   NaN


What do I need to change to obtain the latter? I suppose I could use pdread_table(""testtxt""  na_filter=False) and subsequently replace 'NULL' values with NaN and change the column dtype Is there a more straightforward solution?",183054,,183054.0,2013-01-10 11:41:41,2013-01-10 11:41:41,Python pandas read_table converts zero to NaN,<python><pandas>,2.0,3.0,,
14297959,1,,2013-01-12 21:07:30,0,24,"I am trying to load a csv file with OHLC data in the following format

In [49]: !head '500008csv'
03 Jan 2000 1285 1311 1274 1311 976500    
04 Jan 2000 1354 1360 1256 1333 2493000    
05 Jan 2000 1268 1334 1237 1268 1680000    
06 Jan 2000 1260 1330 1227 1234 2800500    
07 Jan 2000 1253 1270 1182 1257 2763000    
10 Jan 2000 1358 1358 1358 1358 13500    
11 Jan 2000 1466 1466 1340 1347 1694220    
12 Jan 2000 1366 1399 1320 1354 519164    
13 Jan 2000 1367 1387 1354 1380 278400    
14 Jan 2000 1384 1399 1330 1350 718814    


I tried the following which loads the data

df = read_csv('500008csv'  parse_dates=[0 1 2]  usecols=range(6)  
                            header=None  index_col=0)


But now I want to name the columns to be named So  I tried 

df = read_csv('500008csv'  parse_dates=[0 1 2]  usecols=range(6) 
                            header=None  index_col=0  names='d o h l c v'split())


but this fails saying 

IndexError: list index out of range


Can someone point out what I am doing wrong?",1910424,,487339.0,2013-01-12 21:40:39,2013-01-12 22:09:51,usecols with parse_dates and names,<python><pandas>,1.0,1.0,,
14341805,1,14342919.0,2013-01-15 16:06:47,1,51,"I have two pandas dataframes:  dfLeft and dfRight with the date as the index

dfLeft:

            cusip    factorL
date  
2012-01-03    XXXX      45
2012-01-03    YYYY      62

2012-01-04    XXXX      47
2012-01-04    YYYY      61



dfRight:

            idc__id    factorR
date  
2012-01-03    XXXX      50
2012-01-03    YYYY      60

2012-01-04    XXXX      51
2012-01-04    YYYY      62


Both have a shape close to (121900 3)

I tried the following merge:

test = pdmerge(dfLeft  dfRight  left_index=True  right_index=True  left_on='cusip'  right_on='idc__id'  how = 'inner')


This gave test a shape of (60643500  6)

Any recommendations on what is going wrong here?  I want it to merge based on both date and cusip/idc_id  Note:  for this example the cusips are lined up  but in reality that may not be so

Thanks

Expected Output
test:

             cusip    factorL    factorR
date  
2012-01-03    XXXX      45          50
2012-01-03    YYYY      62          60

2012-01-04    XXXX      47          51
2012-01-04    YYYY      61          62
",1911092,,1911092.0,2013-01-15 16:42:37,2013-01-15 17:06:12,Pandas Merge (pd.merge) How to set the index and join,<python><pandas>,2.0,,,
14341584,1,14345393.0,2013-01-15 15:56:14,1,61,"I have just discovered pandas and am impressed by its capabilities 
I am having difficulties  understanding how to work with DataFrame with MultiIndex 

I have two questions : 

(1) Exporting the DataFrame 

Here my problem: 
This dataset 

import pandas as pd
import StringIO
d1 = StringIOStringIO(
     """"""Gender Employed Region Degree
     m yes east ba
     m yes north ba
     f yes south ba
     f no east ba
     f no east bsc
     m no north bsc
     m yes south ma
     f yes west phd
     m no west phd
     m yes west phd """"""
   )

df = pdread_csv(d1)

# Frequencies tables
tab1 = pdcrosstab(dfGender  dfRegion)
tab2 = pdcrosstab(dfGender  [dfRegion  dfDegree])
tab3 = pdcrosstab([dfGender  dfEmployed]  [dfRegion  dfDegree])

# Now we export the datasets 
tab1to_excel('H:/test_tab1xlsx')  # OK 
tab2to_excel('H:/test_tab2xlsx') # fails 
tab3to_excel('H:/test_tab3xlsx') # fails 


One work-around I could think of is to change the columns (The way R does) 

def NewColums(DFwithMultiIndex):
       NewCol = []
       for item in DFwithMultiIndexcolumns:
               NewColappend('-'join(item))
       return NewCol 

# New Columns 
tab2columns = NewColums(tab2)
tab3columns = NewColums(tab3)

# New export  
tab2to_excel('H:/test_tab2xlsx')  # OK
tab3to_excel('H:/test_tab3xlsx')  # OK


My question is : Is there a more efficient way to do this in Pandas that I missed in the documentation ?

2) Selecting columns 

This new structure does not allow to select colums on a given variable (the advantage of hierarchical indexing in first place) How can I select columns containing a given string (eg '-ba') ? 

PS: I have seen this question which is related but have not understood the reply proposed",1043144,,1240268.0,2013-01-15 17:37:31,2013-01-15 19:40:56,Exporting Pandas DataFrame with MultiIndex,<python><pandas>,1.0,2.0,,
14345739,1,14345875.0,2013-01-15 19:54:38,1,70,"I have a similar problem to the one posted here: 

Pandas DataFrame: remove unwanted parts from strings in a column

I need to remove newline characters from within a string in a DataFrame Basically  I've accessed an api using python's json module and that's all ok Creating the DataFrame works amazingly  too However  when I want to finally output the end result into a csv  I get a bit stuck  because there are newlines that are creating false 'new rows' in the csv file

So basically I'm trying to turn this: 

'this is a paragraph

And this is another paragraph'

into this:

'this is a paragraph And this is another paragraph'

I don't care about preserving any kind of '\n' or any special symbols for the paragraph break So it can be stripped right out

I've tried a few variations:

misc['product_desc'] = misc['product_desc']strip('\n')

AttributeError: 'Series' object has no attribute 'strip'


here's another

misc['product_desc'] = misc['product_desc']strstrip('\n')

TypeError: wrapper() takes exactly 1 argument (2 given)

misc['product_desc'] = misc['product_desc']map(lambda x: xstrip('\n'))
misc['product_desc'] = misc['product_desc']map(lambda x: xstrip('\n\t'))


There is no error message  but the newline characters don't go away  either Same thing with this:

misc = miscreplace('\n'  '')


The write to csv line is this:

misc_idto_csv('C:\Users\jlalonde\Desktop\misc_w_idcsv'  sep=' '  na_rep=''  index=False  encoding='utf-8')


Version of Pandas is 091

Thanks! :)",1819380,,919872.0,2013-01-15 20:51:04,2013-01-15 20:51:04,Replacing part of string in python pandas dataframe,<python><csv><pandas>,1.0,,,
14304506,1,14304621.0,2013-01-13 14:37:13,0,31,"I am trying to replicate one example out of Wes McKinney's book on Pandas  the code is here (it assumes all names datafiles are under names folder)

# -*- coding: utf-8 -*-
import numpy as np
import pandas as pd

years = range(1880  2011)
pieces = []
columns = ['name'  'sex'  'births']
for year in years: 
    path = 'names/yob%dtxt' % year
    frame = pdread_csv(path  names=columns)
    frame['year'] = year
    piecesappend(frame)

names = pdconcat(pieces  ignore_index=True)
names

def get_tops(group):    
    return groupsort_index(by='births'  ascending=False)[:1000]

grouped = namesgroupby(['year' 'sex'])
groupedapply(get_tops)


I am using Pandas 010 and Python 27 The error I am seeing is this:

Traceback (most recent call last):
  File ""namespy""  line 21  in 
    groupedapply(get_tops)
  File ""/usr/local/lib/python27/dist-packages/pandas-0100-py27-linux-i686egg/pandas/core/groupbypy""  line 321  in apply
    return self_python_apply_general(f)
  File ""/usr/local/lib/python27/dist-packages/pandas-0100-py27-linux-i686egg/pandas/core/groupbypy""  line 324  in _python_apply_general
    keys  values  mutated = selfgrouperapply(f  selfobj  selfaxis)
  File ""/usr/local/lib/python27/dist-packages/pandas-0100-py27-linux-i686egg/pandas/core/groupbypy""  line 585  in apply
    values  mutated = splitterfast_apply(f  group_keys)
  File ""/usr/local/lib/python27/dist-packages/pandas-0100-py27-linux-i686egg/pandas/core/groupbypy""  line 2127  in fast_apply
    results  mutated = libapply_frame_axis0(sdata  f  names  starts  ends)
  File ""reducepyx""  line 421  in pandaslibapply_frame_axis0 (pandas/libc:24934)
  File ""/usr/local/lib/python27/dist-packages/pandas-0100-py27-linux-i686egg/pandas/core/framepy""  line 2028  in __setattr__
    self[name] = value
  File ""/usr/local/lib/python27/dist-packages/pandas-0100-py27-linux-i686egg/pandas/core/framepy""  line 2043  in __setitem__
    self_set_item(key  value)
  File ""/usr/local/lib/python27/dist-packages/pandas-0100-py27-linux-i686egg/pandas/core/framepy""  line 2078  in _set_item
    value = self_sanitize_column(key  value)
  File ""/usr/local/lib/python27/dist-packages/pandas-0100-py27-linux-i686egg/pandas/core/framepy""  line 2112  in _sanitize_column
    raise AssertionError('Length of values does not match '
AssertionError: Length of values does not match length of index


Any ideas? ",423805,,,,2013-01-13 14:53:02,Pandas Group Example Errors,<pandas>,1.0,1.0,,
14308512,1,14308712.0,2013-01-13 21:43:15,2,46,"I have a timeseries of 10+ years and want to abbreviate the xtick labels to 2-digit years
How can I do that ?

import matplotlibpyplot as plt
import datetime as dt
import pandas as pd
import pandasiodata as web

stocklist = ['MSFT']

# read historical prices for last 11 years
def get_px(stock  start):
    return webget_data_yahoo(stock  start)['Adj Close']

today = dtdatetoday()
start = str(dtdate(todayyear-11  todaymonth  todayday))
px = pdDataFrame({n: get_px(n  start) for n in stocklist})
pltplot(pxindex  px[stocklist[0]])
pltshow()
",1915862,,,,2013-01-14 06:14:26,How to abbreviate xtick labels years to 2 digits in a matplotlib plot,<python><matplotlib><pandas>,1.0,1.0,,
14358567,1,14360423.0,2013-01-16 12:31:42,0,49,"I have a pandasDataFrame with measurements taken at consecutive points in time Along with each measurement the system under observation had a distinct state at each point in time Hence  the DataFrame also contains a column with the state of the system at each measurement State changes are much slower than the measurement interval As a result  the column indicating the states might look like this (index: state):

1:  3
2:  3
3:  3
4:  3
5:  4
6:  4
7:  4
8:  4
9:  1
10: 1
11: 1
12: 1
13: 1


Is there an easy way to retrieve the indices of each segment of consecutively equal states That means I would like to get something like this:

[[1 2 3 4]  [5 6 7 8]  [9 10 11 12 13]]


The result might also be in something different than plain lists

The only solution I could think of so far is manually iterating over the rows  finding segment change points and reconstructing the indices from these change points  but I have the hope that there is an easier solution",283649,,,,2013-01-16 14:40:01,Finding consecutive segments in a pandas data frame,<python><pandas>,2.0,,,
14262433,1,14268804.0,2013-01-10 16:20:32,4,335,"I have tried to puzzle out an answer to this question for many months while learning pandas  I use SAS for my day-to-day work and it is great for it's out-of-core support  However  SAS is horrible as a piece of software for numerous other reasons

One day I hope to replace my use of SAS with python and pandas  but I currently lack an out-of-core workflow for large datasets  I'm not talking about ""big data"" that requires a distributed network  but rather files too large to fit in memory but small enough to fit on a hard-drive

My first thought is to use HDFStore to hold large datasets on disk and pull only the pieces I need into dataframes for analysis  Others have mentioned MongoDB as an easier to use alternative  My question is this:

What are some best-practice workflows for accomplishing the following:

Loading flat files into a permanent  on-disk database structure
Querying that database to retrieve data to feed into a pandas data structure
Updating the database after manipulating pieces in pandas
Real-world examples would be much appreciated  especially from anyone who uses pandas on ""large data""

Edit -- an example of how I would like this to work:

Iteratively import a large flat-file and store it in a permanent  on-disk database structure  These files are typically too large to fit in memory
In order to use Pandas  I would like to read subsets of this data (usually just a few columns at a time) that can fit in memory
I would create new columns by performing various operations on the selected columns
I would then have to append these new columns into the database structure
I am trying to find a best-practice way of performing these steps Reading links about pandas and pytables it seems that appending a new column could be a problem

Edit -- Responding to Jeff's questions specifically:

I am building consumer credit risk models The kinds of data include phone  SSN and address characteristics; property values; derogatory information like criminal records  bankruptcies  etc The datasets I use every day have nearly 1 000 to 2 000 fields on average of mixed data types: continuous  nominal and ordinal variables of both numeric and character data  I rarely append rows  but I do perform many operations that create new columns
Typical operations involve combining several columns using conditional logic into a new  compound column For example  if var1 > 2 then newvar = 'A' elif var2 = 4 then newvar = 'B'  The result of these operations is a new column for every record in my dataset
Finally  I would like to append these new columns into the on-disk data structure  I would repeat step 2  exploring the data with crosstabs and descriptive statistics trying to find interesting  intuitive relationships to model
A typical project file is usually about 1GB  Files are organized into such a manner where a row consists of a record of consumer data  Each row has the same number of columns for every record  This will always be the case
It's pretty rare that I would subset by rows when creating a new column  However  it's pretty common for me to subset on rows when creating reports or generating descriptive statistics  For example  I might want to create a simple frequency for a specific line of business  say Retail credit cards  To do this  I would select only those recrods where the line of business = retail in addition to whichever columns I want to report on  When creating new columns  however  I would pull all rows of data and only the columns I need for the operations
The modeling process requires that I analyze every column  look for interesting relationships with some outcome variable  and create new compound columns that describe those relationships  The columns that I explore are usually done in small sets  For example  I will focus on a set of say 20 columns just dealing with property values and observe how they relate to defaulting on a loan  Once those are explored and new columns are created  I then move on to another group of columns  say college education  and repeat the process  What I'm doing is creating candidate variables that explain the relationship between my data and some outcome  At the very end of this process  I apply some learning techniques that create an equation out of those compound columns
It is rare that I would ever add rows to the dataset  I will nearly always be creating new columns (variables or features in statistics/machine learning parlance)

Thanks",919872,,919872.0,2013-01-11 15:31:46,2013-01-11 22:11:52,"""Large data"" work flows using pandas",<python><mongodb><pandas><hdf5>,2.0,,7.0,
14286807,1,,2013-01-11 21:19:06,2,88,"I'm working on a way to transform sequence/genotype data from a csv format to a genepop format

I have two dataframes: df1 is empty  df1index (rows = samples) consists of almost the same as df2index  except I inserted ""POP"" in several places (to specify the different populations) df2 holds the data  with Loci as columns

I want to insert the values from df2 into df1  keeping empty rows where df1index = 'POP'

I tried join  combine  combine_first and concat  but they all seem to take the rows that exist in both df's 

Is there a way to do this?",1851740,,985906.0,2013-01-11 21:27:55,2013-01-12 03:06:57,How do I join two dataframes (pandas) with different indices?,<python><pandas>,1.0,2.0,2.0,
14323299,1,14323422.0,2013-01-14 17:16:21,1,42,"I have a df:

date          cusip   value
2012-12-20     XXXX     423
2012-12-20     YYYY     634
2012-12-20     ZZZZ     812
2012-12-21     XXXX     578
2012-12-21     YYYY     662
2012-12-21     ZZZZ     909


I want to subset where I select only the cusips that exist in a list:

cusList = ('XXXX'  'ZZZZ')

The sub_df would be:

date          cusip   value
2012-12-20     XXXX     423
2012-12-20     ZZZZ     812
2012-12-21     XXXX     578
2012-12-21     ZZZZ     909


Any recommendations?  Thanks",1911092,,846892.0,2013-01-14 17:18:44,2013-01-14 17:24:02,Subset a DF in Pandas Python,<python><pandas>,1.0,2.0,,
14360261,1,14360369.0,2013-01-16 14:07:34,2,46,"Let's say I have two very long series - big and small

index = pddate_range(start='1952'  periods=10**6  freq='s')
big = pdSeries(npones(len(index))*97  index)
small = pdSeries(npones(len(index))*2  index)


What I would like to achieve is create a new series which combines big and small  alternating between their values  using borders to determine when to switch to the other one (eg there is a border every 5 sec)

borders = pddate_range(start='1952'  periods=len(index)/50  freq='5s')


Is there an efficient matrix-based operation combo that can be used to achieve this? I tried looking at various join  merge etc operators in the docs  but couldn't find anything offering similar logic

I could achieve this using a for-loop  but that lasts over a minute even for a series of len() 105

alternating = pdSeries()
for i in range(1  100  2):
    b0 = borders[i-1]
    b1 = borders[i]
    b2 = borders[i+1]
    sec = pdoffsetsSecond(1)
    alternating = alternatingappend(small[b0:b1-sec])append(big[b1:b2-sec])


Sample output of alternatinghead(24)

1952-01-16 00:00:00     2
1952-01-16 00:00:01     2
1952-01-16 00:00:02     2
1952-01-16 00:00:03     2
1952-01-16 00:00:04     2
1952-01-16 00:00:05    97
1952-01-16 00:00:06    97
1952-01-16 00:00:07    97
1952-01-16 00:00:08    97
1952-01-16 00:00:09    97
1952-01-16 00:00:10     2
1952-01-16 00:00:11     2
1952-01-16 00:00:12     2
1952-01-16 00:00:13     2
1952-01-16 00:00:14     2
1952-01-16 00:00:15    97
1952-01-16 00:00:16    97
1952-01-16 00:00:17    97
1952-01-16 00:00:18    97
1952-01-16 00:00:19    97
1952-01-16 00:00:20     2
1952-01-16 00:00:21     2
1952-01-16 00:00:22     2
1952-01-16 00:00:23     2
",544059,,,,2013-01-16 14:12:59,How can I efficiently combine two time series to alternate?,<python><pandas><time-series>,1.0,,,
14356577,1,14359050.0,2013-01-16 10:46:55,1,55,"Following the example here

http://wwwrandalolsoncom/2013/01/14/filling-in-pythons-gaps-in-statistics-packages-with-rmagic/

I tried the same on a different data set found here  in an IPython notebook

https://githubcom/burakbayramli/kod/blob/master/delltest/delltgz

from pandas import *
orders = read_csv(""dellcsv"" sep="" "")
%load_ext rmagic
%R -i orders print(summary(orders))


I get

     Length Class  Mode
[1 ] 25     -none- list
[2 ] 25     -none- list
[3 ] 25     -none- list



However the same in R

data ",423805,,938949.0,2013-01-16 18:22:18,2013-01-16 18:22:18,"RMagic, IPython and Summary Information",<pandas><ipython><rpy2><ipython-notebook>,1.0,4.0,,
14361634,1,14365475.0,2013-01-16 15:16:23,0,37,"I would like to use pandas OLS function to fit a trendline to my data Series Does anyone knows how to use the datetime index from the pandas Series as predictor in the OLS?

For example  let say that I have a simple time series:

>>> ts
2001-12-31    19828763
2002-12-31    20112191
2003-12-31    19509116
2004-12-31    19913656
2005-12-31    19701649
2006-12-31    20022819
2007-12-31    20103024
2008-12-31    20132712
2009-12-31    19850609
2010-12-31    19290640
2011-12-31    19936210
2012-12-31    19664813
Freq: A-DEC


I would like to do an OLS on it using the index as predictor:

model = pdols(y=ts x=tsindex intercept=True)


But as x is a list of datetime index  the function returns an error Anyone has an idea?

I could use linregress from scipystats but I wonder if it is possible with Pandas

Thanks 
Greg",1613796,,1613796.0,2013-01-16 17:39:06,2013-01-16 18:46:11,OLS with pandas: datetime index as predictor,<python><datetime><pandas><linear-regression><series>,1.0,,,
14386117,1,14386296.0,2013-01-17 18:56:14,1,50,"I am researching/backtesting a trading system

I have a Pandas dataframe containing OHLC data and have added several calculated columns which identify price patterns that I will use as signals to initiate positions

I would now like to add a further column that will keep track of the current net position I have tried using dfapply()  but passing the dataframe itself as the argument instead of the row object  as with the latter I seem to be unable to look back at previous rows to determine whether they resulted in any price patterns:

open_campaigns = []
Campaign = namedtuple('Campaign'  'open position stop')

def calc_position(df):
  # sum of current positions + any new positions

  if entered_long(df):
    open_campaignsadd(
        Campaign(
            calc_long_open(dfHighshift(1))  
            calc_position_size(df)  
            calc_long_isl(df)
        )
    )

  return sum(campaignposition for campaign in open_campaigns)

def entered_long(df):
  return buy_pattern(df) & (dfHigh > dfHighshift(1))

df[""Position""] = dfapply(lambda row: calc_position(df)  axis=1)


However  this returns the following error:

ValueError: ('The truth value of an array with more than one element is ambiguous Use aany() or aall()'  u'occurred at index 1997-07-16 08:00:00')


Rolling window functions would seem to be the natural fit  but as I understand it  they only act on a single time series or column  so wouldn't work either as I need to access the values of multiple columns at multiple timepoints

How should I in fact be doing this?",1583083,,,,2013-01-17 19:08:00,How to look back at previous rows from within Pandas dataframe function call?,<python><data.frame><pandas><algorithmic-trading><quantitative-finance>,1.0,,,
14270163,1,,2013-01-11 01:18:06,1,56,"I am working on a large project that does SPC analysis and have 1000's of different unrelated dataframe objects Does anyone know of a module for storing objects in memory?  I could use a python dictionary but would like it more elaborate and functional mechanisms like locking  thread safe  who has it and a waiting list etc? I was thinking of creating something that behaves like my local public library system  The way it checks in and out books to one owner etc

Thank you for your help",1968530,,243434.0,2013-01-11 03:29:53,2013-01-26 19:24:43,Pandas storing 1000's of dataframe objects,<python><object><data><storage><pandas>,2.0,1.0,,
14281871,1,14290688.0,2013-01-11 15:58:31,1,38,"I was experimenting with the kagglecom Titanic data set (data on every person on the Titanic) and came up with a gender breakdown like this:

gender = dfsexvalue_counts()
gender

male   577
female 314 


I would like to find out the percentage of each gender on the Titanic

My approach is slightly less than ideal:

from __future__ import division
pcts = gender / gendersum()
pcts

male      0647587
female    0352413


Is there a better (more idiomatic) way?

Thanks!",26002,,,,2013-01-12 05:52:47,"Given a pandas Series that represents frequencies of a value, how can I turn those frequencies into percentages?",<pandas>,1.0,1.0,1.0,
14287903,1,14296342.0,2013-01-11 22:46:25,2,52,"When I run this in one cell  both plot lines are blue  I could have sworn I saw Wes do a demo of baby names with two plots where the two plot lines came out in different colors without having to specify a color:

pdSeries(randn(100))cumsum()plot()
pdSeries(randn(100))cumsum()plot()


Yes  it was at 2:06:44 of this YouTube video: http://wwwyoutubecom/watch?v=w26x-z-BdWQ",26002,,,,2013-01-13 01:27:41,How do you make plotting two pandas Series in the same ipython notebook cell use different colors automatically?,<pandas><ipython-notebook>,2.0,1.0,,
14301004,1,14306366.0,2013-01-13 05:40:50,0,55,"I have a data frame that consists of a time series data with 15-second intervals:

date_time             value    
2012-12-28 11:11:00   1032
2012-12-28 11:11:15   1031
2012-12-28 11:11:30   1034
2012-12-28 11:11:45   1035
2012-12-28 11:12:00   1033


The data spans many years I would like to group by both year and time to look at the distribution of time-of-day effect over many years For example  I may want to compute the mean and standard deviation of every 15-second interval across days  and look at how the means and standard deviations change from 2010  2011  2012  etc I naively tried datagroupby(lambda x: [xyear  xtime]) but it didn't work How can I do such grouping?",1642513,,,,2013-01-13 18:11:16,Group by multiple time units in pandas data frame,<python><pandas>,1.0,,,
14380371,1,14383654.0,2013-01-17 13:40:24,2,92,"Is there an easy way to export a data frame (or even a part of it) to LaTeX?  

I searched in google and was only able to find solutions using asciitables",1898534,,1240268.0,2013-01-17 18:07:44,2013-01-18 17:41:35,Export a LaTeX table from pandas DataFrame,<python><latex><data.frame><pandas>,2.0,0.0,1.0,
14386897,1,,2013-01-17 19:46:05,3,64,"Why am I getting inconsistent group size ""counts""?
I am using Pandas 100  with a 6 million row dataset being reduced to a 400k groupby:

In [16]: dfgroupby('Z ID')size()[470009:470010]
Out[16]:
Z ID
994555          6

In [14]: df[df['Z ID'] == 994555]groupby('Z ID')size() 
Out[14]:
Z ID
994555          9


When I reviewed the raw data there are 9 items

EDIT: Raw Data
The full data set is 6 million records  Pandas size() works great on the little guy

            FilterDate           Z ID   AR Code AA Code
48349    12/1/20072/28/2009    994555  377     202
151060   2/1/20084/30/2009     994555  377     202
204179   3/1/20085/31/2009     994555  377     202
244504   4/1/20086/30/2009     994555  377     202
302728   5/1/20087/31/2009     994555  377     202
365780   6/1/20088/31/2009     994555  377     202
431555   7/1/20089/30/2009     994555  377     202
499234   8/1/200810/31/2009    994555  377     202
786937   12/1/20082/28/2010    994555  377     202
",1649635,,1649635.0,2013-01-17 22:12:56,2013-01-17 22:12:56,"Pandas groupby size ""count"" intermittent under-count",<pandas>,,4.0,,
14363640,1,,2013-01-16 16:57:30,3,53,"In short  I have a Python Pandas data frame that is read in from an Excel file using 'read_table'  I would like to keep a handful of the series from the data  and purge the rest  I know that I can just delete what I don't want one-by-one using 'del data['SeriesName']'  but what I'd rather do is specify what to keep instead of specifying what to delete

If the simplest answer is to copy the existing data frame into a new data frame that only contains the series I want  and then delete the existing frame in its entirety  I would satisfied with that solution  but if that is indeed the best way  can someone walk me through it?

TIA  I'm a newb to Pandas  :)",344160,,344160.0,2013-01-16 17:20:35,2013-01-16 17:20:35,Python Pandas - Deleting multiple series from a data frame in one command,<python><pandas>,2.0,,,
14416660,1,14417036.0,2013-01-19 17:04:02,2,83,"I'm dealing with a balance sheet which I've parsed into pandas using:

    table = xls_fileparse('Consolidated_Balance_Sheet')
    tableix[:  1]

    0         None
    1         None
    2      $ 3 029
    3         1989
    5         None
    6     $ 34 479


I'm trying to identify the rows with unicode and strip the $ sign and comma  converting to float

    for row in tableix[:  1]:
        if isinstance(row  unicode):
            print type(row)  row
            num = float(rowlstrip('$')replace(' ' ''))
            print num
            row = num
            print type(row)  row


This produces the following output:

     $ 3 029
    30290
     30290
     $ 34 479
    344790
     344790


However  the value is unchanged when I check the table

    tableix[2  1]
    u'$ 3 029'


How can I correctly change the value to a float?

EDIT: Thanks for the two responses  I can reproduce those with no problem However when I use the apply function to my case I get an 'unhashable type' error

In [167]: thead = tablehead()
In [168]: thead

Out[168]:
         Consolidated Balance Sheet (USD $)  Sep 30  2012  Dec 31  2011
    0    In Millions  unless otherwise specified     None    None
    1    Current assets                              None    None
    2    Cash and cash equivalents                   $ 3 029 $ 2 219
    3    Marketable securities - current             1989    1461
    4    Accounts receivable - net                   4409    3867

In [170]: def no_comma_or_dollar(num):
              if isinstance(num  unicode):
                  return float(numlstrip('$')replace(' ' ''))
              else:
                  return num

          thead[:  1] = thead[:  1]apply(no_comma_or_dollar)


Produces the following:

 TypeError: unhashable type


I can't get my head around why as I'm not changing the keys  just the values Is there another way to change the values in the dataframe?

EDIT2:

In [171]: theadto_dict()
Out[171]: {u'Consolidated Balance Sheet (USD $)': {0: u'In Millions  unless otherwise specified' 
  1: u'Current assets' 
  2: u'Cash and cash equivalents' 
  3: u'Marketable securities - current' 
  4: u'Accounts receivable - net'} 
 u'Dec 31  2011': {0: None  1: None  2: u'$ 2 219'  3: 14610  4: 38670} 
 u'Sep 30  2012': {0: None  1: None  2: u'$ 3 029'  3: 19890  4: 44090}}
",1993029,,1993029.0,2013-01-19 21:16:11,2013-01-20 02:10:00,pandas dataframe row change type,<python><parsing><pandas><floating-accuracy>,2.0,3.0,1.0,
14438295,1,,2013-01-21 12:08:58,0,47,"I'm currently trying to build a fairly simple script that will compare two DataFrames from a CSV and perform an inner merge to remove duplicates Now I noticed that one of my CSVs looks like this:

Row [0]: One column
Row [1:]: 2+ columns


Now  when I try to import it via pandascsv_read() I get the following error

Traceback (most recent call last):
 File """"  line 1  in 
File ""/Library/Frameworks/Pythonframework/Versions/73/lib/python27/site-packages/pandas-0100-py27-macosx-105-i386egg/pandas/io/parserspy""  line 391  in parser_f
return _read(filepath_or_buffer  kwds)
File ""/Library/Frameworks/Pythonframework/Versions/73/lib/python27/site-packages/pandas-0100-py27-macosx-105-i386egg/pandas/io/parserspy""  line 207  in _read
return parserread()
File ""/Library/Frameworks/Pythonframework/Versions/73/lib/python27/site-packages/pandas-0100-py27-macosx-105-i386egg/pandas/io/parserspy""  line 624  in read
ret = self_engineread(nrows)
File ""/Library/Frameworks/Pythonframework/Versions/73/lib/python27/site-packages/pandas-0100-py27-macosx-105-i386egg/pandas/io/parserspy""  line 945  in read
data = self_readerread(nrows)
File ""parserpyx""  line 634  in pandas_parserTextReaderread (pandas/src/parserc:5795)
File ""parserpyx""  line 656  in pandas_parserTextReader_read_low_memory (pandas/src/parserc:6015)
File ""parserpyx""  line 734  in pandas_parserTextReader_read_rows (pandas/src/parserc:6892)
File ""parserpyx""  line 791  in pandas_parserTextReader_convert_column_data (pandas/src/parserc:7596)
File ""parserpyx""  line 1015  in pandas_parserTextReader_get_column_name (pandas/src/parserc:10425)


I assume this has to do with the first row  as when I delete it the problem is gone How can I ignore such errors and just fill the rest with empty values?

Best 
Oliver",1563867,,733291.0,2013-01-21 12:20:54,2013-01-21 12:20:54,Ignore First Row in DataFrame CSVread,<python><pandas>,,3.0,,
14349055,1,,2013-01-15 23:45:53,22,518,"Is there a way to make matplotlib behave identically to R  or almost like R  in terms of plotting defaults? For example R treats its axes pretty differently from matplotlib The following histogram


has ""floating axes"" with outward ticks  such that there are no inner ticks (unlike matplotlib) and the axes do not cross ""near"" the origin Also  the histogram can ""spillover"" to values that are not marked by the tick - eg the x-axis ends at 3 but the histograms extends slightly beyond it How can this be achieved automatically for all histograms in matplotlib?

Related question: scatter plots and line plots have different default axes settings in R  for example:


There no inner ticks again and the ticks face outward Also  the ticks start slightly after the origin point (where the y and x axes cross at the bottom left of the axes) and the ticks end slightly before the axes end This way the labels of the lowest x-axis tick and lowest y-axis tick can't really cross  because there's a space between them and this gives the plots a very elegant clean look  Note that there's also considerably more space between the axes ticklabels and the ticks themselves

Also  by default there are no ticks on the non-labeled x or y axes  meaning the y-axis on the left that is parallel to the labeled y-axis on the right has no ticks  and same for the x-axis  again removing clutter from the plots

Is there a way to make matplotlib look like this? And in general to look by default as much as default R plots?  I like matplotlib a lot but I think the R defaults / out-of-the-box plotting behavior really have gotten things right and its default settings rarely lead to overlapping tick labels  clutter or squished data  so I  would like the defaults to be as much like that as possible",248237,,,,2013-01-19 19:17:24,making matplotlib graphs look like R by default?,<python><r><matplotlib><plot><pandas>,3.0,4.0,8.0,
14430263,1,14430418.0,2013-01-20 22:52:33,2,26,"In my project I build a class with pandas DataFrame as a core The values in dataframe depends upon some specification and I initialise it with some letter representing the data I want to work with I put all my functions to create dataframe inside an __init__ as I understand this functions are one off only and no needs for them after the initialisation Also I don't want to have access to this functions after my class is in use in later code (I am not sure if this is ""pythonic"" way to do so)

After building basic class with __str__ and plotData() methods I would like to apply some filters and build a new class where additional column is the filter I would like to do that in __init__ but keep everything what already was done In another words I don't want to re-write the whole __init__ only want to add new column to the basic dataframe

In similar fashion I would like to add an additional plot in the plotData() function 

My original code has already quite a few of lines but the principles are very similar to code listed below 

import pandas as pd
import pylab as pl
class myClass(object):
    def __init__(self  frameType = 'All'):
        def method1():
            myFrame = pdDataFrame({'c1':[1 2 3] 'c2':[4 5 6] 'c3':[7 8 9]})
            return myFrame
        def method2():
            myFrame = pdDataFrame({'c1':[1 2 3] 'c2':[4 5 6] 'c3':[7 8 9]})
            return myFrame
        def makingChiose(self):
            if selfframeType == 'All':
                variable = method1() + method2() 
            elif selfframeType == 'a':
                variable = method1()
            elif selfframeType == 'b':
                variable = method2()
            else:
                variable =  pdDataFrame({'c1':[0 0 0] 'c2':[0 0 0] 'c3':[0 0 0]})
            #print 'FROM __init__ : %s' % variable
            return variable           
        selfframeType = frameType      
        selfcObject = makingChiose(self) # object created by the class
    def __str__(self):
        return str(selfcObject)
    def plotData(self):
        selffig1 = plplot(selfcObject['c1'] selfcObject['c2'])
        selffig2 = plplot(selfcObject['c1'] selfcObject['c3'])
        plshow()

class myClassAv(myClass):
    def addingCol(self):
        print 'CURRENT cObject \n%s' % selfcObject # the object is visible 
        selfcObject['avarage'] = (selfcObject['c1']+selfcObject['c2']+selfcObject['c3'])/3
        print 'THIS WORKS IN GENERAL\n%s' % str((selfcObject['c1']+selfcObject['c2']+selfcObject['c3'])/3) # creating new column works
    def plotData(self):
        # Function to add new plot to already existing plots
        selffig3 = plplot(selfcObject['c1'] selfcObject['avarage'])
if __name__ == '__main__':
    myObject1 = myClass()
    print 'myObject1 =\n%s' % myObject1
    myObject1plotData()
    myObject2 = myClass('a')
    print 'myObject2 =\n%s' % myObject2
    myObject3 = myClass('b')
    print 'myObject3 =\n%s' % myObject3
    myObject4 = myClass('c')
    print 'myObject4 =\n%s' % myObject4

    myObject5 = myClassAv('a')addingCol()
    print 'myObject5 =\n%s' % myObject5
    myObject5plotData()


Most of the code works  at least in the initialisation but I have an error when I try to create new dataframe with additional column When I put as the new __init__ I create a completely new initialisation and I loose all what was already done I created a new function but I would prefer have the additional column after I call a new class not a function inside the new class The output from the code looks like this:

myObject1 =
    c1   c2   c3
0  11  44  77
1  22  55  88
2  33  66  99
myObject2 =
   c1  c2  c3
0   1   4   7
1   2   5   8
2   3   6   9
myObject3 =
    c1   c2   c3
0  01  04  07
1  02  05  08
2  03  06  09
myObject4 =
   c1  c2  c3
0   0   0   0
1   0   0   0
2   0   0   0
CURRENT cObject 
   c1  c2  c3
0   1   4   7
1   2   5   8
2   3   6   9
THIS WORKS IN GENERAL
0    4
1    5
2    6
myObject5 =
None
Traceback (most recent call last):
  File ""C:\Users\src\tryspy""  line 57  in 
    myObject5plotData()
AttributeError: 'NoneType' object has no attribute 'plotData'


The question is: Can I 'partially' override the superclass's method to have what was previously inside this method with some new functionality? I would like to initialise myClassAv() to dataframe with four columns instead of three like myClass() and I'd like to have myClassAv()plotData() to plot a third line but keep two from base class

I don't know how to interpret an error and why myObject5 is None  but I suspect it is something with inheritance

Also if you have suggestion that I should do all my idea in different way I will appreciate to hear them  ",1661173,,190597.0,2013-01-20 23:13:32,2013-01-20 23:25:42,creating subclass form class returning pandas dataFrame,<python><inheritance><pandas>,1.0,,,
14447925,1,,2013-01-21 21:54:04,1,44,"I'm attempting to write the results of a regression back to MySQL  but am having problems  iterating through the fitted values and getting the NaNs to write as null values  Originally  I did the iteration this way:

for i in dataframe:
    cur = cnxcursor()
    query = (""UPDATE Regression_DataInput SET FITTEDVALUES=""+(dataframe['yhat']__str__())+"" where timecount=""+(datafrane['timecount']__str__())+"";"")
    curexecute(query)
    cnxcommit()
    curclose()


which SQL thew back to me by saying: 

 ""mysqlconnectorerrorsProgrammingError: 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'NaN'


So  I've been trying to filter out the NaNs by only asking Python to commit when yhat does not equal NaN:

for i in dataframe:
    if cleandf['yhat']>(-1000):
        cur = cnxcursor()
        query = (""UPDATE Regression_DataInput SET FITTEDVALUES=""+(dataframe['yhat']__str__())+"" where timecount=""+(datafrane['timecount']__str__())+"";"")
        curexecute(query)
        cnxcommit()
       curclose()


But then I get this:  

ValueError: The truth value of an array with more than one element is ambiguous Use aany() or aall()


So  I try to get around it with this in my above syntax:

if cleandf['yhat'][i]>(-1000):


but then get this:

ValueError: Can only tuple-index with a MultiIndex


And then tried adding itterows() to both as in:

 for i in dataframeiterrows():
        if cleandf['yhat'][i]>(-1000):


but get the same problems as above

I'm not sure what I'm doing wrong here  but assume it's something with iterating in Pandas DataFrames    But  even if I got the iteration right  I would want to write Nulls into SQL where the NaN appeared

So  how do you think I should do this?  ",1784454,,,,2013-01-21 21:54:04,Iterating and Writing Panda Dataframe NaNs back to MySQL,<python><mysql><iteration><pandas>,,1.0,,
14362632,1,,2013-01-16 16:05:26,0,36,"When I invoke rename on a pivot table  I lose the axis labels:

In [5]: df = pdDataFrame(dict(x=[0 0 1 0 1]  y=[1 0 1 1 0]  z=[0 0 1 0 1]))
In [6]: pt = pdpivot_table(df  'z'  cols='x'  rows='y')
In [7]: print pt
x  0  1
y      
0  0  1
1  0  1
In [8]: labels = {0:'False'  1:'True'}
In [9]: print ptrename(index=labels  columns=labels) # discards ""x"" and ""y""
       False  True
False      0     1
True       0     1


Is there a way to do this without losing the axis labels?",1332492,,,,2013-01-16 16:05:26,Renaming a pandas pivot table without losing axis labels,<python><pandas><pivot-table>,,2.0,,
14432059,1,14443106.0,2013-01-21 03:21:21,1,29,Can we load a panda data frame in NET space using iron python? If not I am thinking of converting panda df into a csv file and then reading in net space,899811,,,,2013-01-21 16:34:02,can we load panda dataframe in .net ironpython?,<ironpython><pandas>,1.0,,,
14439060,1,,2013-01-21 12:52:29,1,57,"Is it possible to read a table with missing data -- using the columns-based labels? I have the following table:

                      Band                             Band                %  of
  Band   Peak         for                          %   for  Area           Total
   No   Pos   Delta Sep  Separ  Height  FWHM Gauss Area Ratio   Area   Area 
    4   21401   467   3     270    5737  169   90    3   067   10836  3076
    2   21204   270   1     270    1391  210   90    1   067    3254   924
    3   21131   197   0             8580  170   90    0          16255  4614
    1   20934   000   0             2193  200   90    0           4882  1386


I want to read it in a column-based way I believe pandas can do it I'd skip first two rows -- but how do I make pandas read the remaining data in a column-based way?

Any other solution to parse such tables (with missing data) would also be ok",788700,,,,2013-01-28 15:11:45,Read a table with missing data,<python><pandas>,2.0,2.0,,
14449367,1,14449538.0,2013-01-22 00:00:38,2,24,"This is likely very simple but I can't figure out what's wrong 
I am having trouble listing elements of a DataFrame Sometimes the elements of a DataFrame are listed and sometimes it's simply a description of the number and types of the data columns 
I know that the number of rows is a factor but even when I have only a few rows  I only get the description back 
For example:
If I have a DataFrame called 'allpledges'  it gives me a description

In [5]:

allpledges

Out[5]:


Int64Index: 305384 entries  0 to 305383
Data columns:
Pledge#       305384  non-null values
Source        305384  non-null values
Date          305384  non-null values
Break         305384  non-null values
Progcode      237002  non-null values


Which is understandable because it's too many rows to display 
But when I try to view a few  it still gives me the same thing

In [13]:

allpledges[:5]

Out[13]:


Int64Index: 5 entries  0 to 4
Data columns:
Pledge#       5  non-null values
Source        5  non-null values
Date          5  non-null values
Break         5  non-null values
Progcode      0  non-null values


When what I wanted was the top five rows listed out I have seen this done in tutorials  but can't figure out what I am doing wrong here ",442158,,,,2013-01-22 00:28:27,Listing elements vs dataframe description When happens when?,<python><pandas>,1.0,,1.0,
14451185,1,14451264.0,2013-01-22 03:37:04,3,63,"I've got a data frame and want to filter or bin by a range of values and then get the counts of values in each bin 

Currently  I'm doing this:

x = 5
y = 17
z = 33
filter_values = [x  y  z]
filtered_a = df[dffiltercol  x]
filtered_b = filtered_b[filtered_b  y]
c_count = filtered_cfiltercolcount()


But is there a more concise way to accomplish the same thing?",24718,,,,2013-01-22 12:05:44,Better binning in pandas,<python><pandas><binning>,1.0,,1.0,
14371476,1,,2013-01-17 02:53:45,0,45,"I have a CSV with epoch GMT timestamp at irregular intervals paired with a value  I tried reading in from the CSV but all the times are shifted to my local timezone  How do I have it just read in as-is (in GMT)?  Then I would like the resample to one minute intervals  HOWEVER  I would like to be able to have it skip gaps which are larger than a user specified value  If this is not possible  is there way to resample to to one minute  but in the gaps  put in an arbitrary value like 00?

 Data:
 Time Data
 1354979750250 02343
 1354979755250 23433
 1354979710250 12343

 def date_utc(s):
     return parse(s  tzinfos=tzutc)

 x = read_csv(""time2csv""  date_parser=date_utc  converters={'Time': lambda x:datetimefromtimestamp(int(x)/1000)})set_index('Time')
",1647854,,,,2013-01-17 03:07:21,Pandas: Read Timestamp from CSV in GMT then resample,<python><csv><timezone><timestamp><pandas>,1.0,,,
14431646,1,14432914.0,2013-01-21 02:19:42,1,65,"I have a list of stockmarket data pulled from Yahoo in a pandas DataFrame (see format below) The date is serving as the index in the DataFrame I want to write the data (including the index) out to a SQLite database 

             AAPL     GE
Date
2009-01-02  8995  1476
2009-01-05  9375  1438
2009-01-06  9220  1458
2009-01-07  9021  1393
2009-01-08  9188  1395


Based on my reading of the write_frame code for Pandas  it does not currently support writing the index I've attempted to use to_records instead  but ran into the issue with Numpy 162 and datetimes Now I'm trying to write tuples using itertuples  but SQLite throws an error that the data type isn't supported (see code and result below) I'm relatively new to Python  Pandas and Numpy  so it is entirely possible I'm missing something obvious I think I'm running into a problem trying to write a datetime to SQLite  but I think I might be overcomplicating this 

I think I may be able to fix the issue by upgrading to Numpy 17 or the development version of Pandas  which has a fix posted on GitHub I'd prefer to develop using release versions of software - I'm new to this and I don't want stability issues confusing matters further 

Is there a way to accomplish this using Python 272  Pandas 0100  and Numpy 162? Perhaps cleaning the datetimes somehow? I'm in a bit over my head  any help would be appreciated 

Code:

import numpy as np
import pandas as pd
from pandas import DataFrame  Series
import sqlite3 as db

# download data from yahoo
all_data = {}

for ticker in ['AAPL'  'GE']:
    all_data[ticker] = pdiodataget_data_yahoo(ticker  '1/1/2009' '12/31/2012')

# create a data frame
price = DataFrame({tic: data['Adj Close'] for tic  data in all_dataiteritems()})

# get output ready for database export
output = priceitertuples()
data = tuple(output)

# connect to a test DB with one three-column table titled ""Demo""
con = dbconnect('c:/Python27/testdb')
wildcards = ' 'join(['?'] * 3)
insert_sql = 'INSERT INTO Demo VALUES (%s)' % wildcards
conexecutemany(insert_sql  data)


Result:

---------------------------------------------------------------------------
InterfaceError                            Traceback (most recent call last)
 in ()
----> 1 conexecutemany(insert_sql  data)

InterfaceError: Error binding parameter 0 - probably unsupported type
",1995577,,1240268.0,2013-01-21 05:09:10,2013-01-21 23:35:54,How to write Pandas dataframe to sqlite with Index,<python><sqlite3><pandas>,1.0,1.0,,
14455188,1,,2013-01-22 09:23:37,0,63,"Please check my code below I am trying to locate a memory leak in below program

run_before and run_after are supporting functions to calculate number of dictionaries before and after a function call

I call processDate for each date  I check the dictionary count after each call The total number of dictionaries keep increasing by a number of 10 000 approx  Only local dictionaries are created There is a global disctionary which updates on each call  but it doesn't add up new dictionaries  at least not in the count of 10 000 I am not getting any idea on where this dictionary count is increasing Is it internal to pandas dataframe slicing? 

EDIT: Adding complete class on request It has dependencies on other classes I am pretty sure that other classes dont have this memory leak because the leak started to happen only when I introduced the class below

import math logging time
import numpy as np
import alphaCalculator regConfig pdb pandas time
import regressor
from basics import *
from matplotlib import pyplot as plt
import gc
from collections import defaultdict
from gc import get_objects


class AlphaAnalyser(alphaCalculatorAlphaCalculator):
    def __init__(self regInfo dateFrom=None dateTo=None):
        alphaCalculatorAlphaCalculator__init__(self regInfo dateFrom dateTo)

        selfstockList=regInfostockListReg # list of stock to calc the stat on

        selfthresholdList=[-1 0 01 03 05 07] # alpha threshold it's a pct of the spread
        selflagList=[100 600 1800 5000] # in sec

        #initialise result dictionnaries
        selffields=([""nbPaperTrades"" ""alpha"" ""obj"" ""obj2"" ""objalpha"" ""objalpha2""])
        for l in selflagList:
            selffields+=[""realised_""+str(l)]

        selfresults=pandasDataFrame(columns=[""date"" ""ric"" ""threshold""]+selffields)
        selfresultsset_index([""date"" ""ric"" ""threshold""] drop=False)
        selfbefore=defaultdict(int)
        selfafter=defaultdict(int)

    def resetInfo(self regInfo):
        selfregInfo=regInfo
        selfstockList=regInfostockListReg

    def getDtAlpha(self stock date):
        selftickDatasetDate(date)
        if stock not in selfindexrefDataix[datedate()isoformat()]ric:
            #loggingwarning(""[%s]: %s doesn't exist on day %s"" % (__name__  stock  datedate()isoformat()))
            return()
        return(selfcomputeAlpha(product=stock date=date))

    def run_before(self):
        selfbefore=defaultdict(int)
        for i in get_objects():selfbefore[type(i)]+=1

    def run_after(self):
        selfafter=defaultdict(int)
        gccollect()
        for i in get_objects():selfafter[type(i)]+=1
        print(""Objects which are not garbage collected: ->"")
        print(""Dict count diff ("" + str(selfafter[type({})]) + ""+"" + str(selfbefore[type({})]) + ""): ""+str(selfafter[type({})] - selfbefore[type({})]))

    def processDate(self date):
        dt1=0
        dt2=0
        if selfregInforegType==regConfigRegTypeINDEX_REG or selfstockList==[]:
            stockList = selfindexrefDataix[datedate()isoformat()]rictolist()
        else:
            stockList=selfstockList
        selftickDatasetDate(date)
        result=[]
        for stock in stockList:
            if stock not in selfindexrefDataix[datedate()isoformat()]ric:
                #loggingwarning(""[%s]: %s doesn't exist on day %s"" % (__name__  stock  datedate()isoformat()))
                continue
            print stock

            alphaData=selfcomputeAlpha(stock date) ## no dictionaries are created in this function
            if not(selfisValid):
                continue
            objData=selfcomputeObj(stock date) ## no dictionaries are created in this function
            if not(selfisValid):
                continue
            nanfilter=npisnan(alphaData[""VALUES""])
            nanfilter+=npisnan(objData[""VALUES""])
            nanfilter=~nanfilter
            alphaData=alphaData[""VALUES""][nanfilter]
            objData=objData[""VALUES""][nanfilter]
            spread=selfindexrefDataix[datedate()isoformat() stock][""spread""]
            paperTrades={}
            dfs=[]
            for i in range(len(selfthresholdList)):
                t=selfthresholdList[i]
                if t=t*spread)
                else:
                    paperTrades[t]=(abs(alphaData)>=t*spread)*(abs(alphaData)0:
            selfresults=selfresultsappend(selfrowResults)
",899811,,899811.0,2013-01-22 10:04:31,2013-01-22 10:04:31,why is there a memory leak in below python + pandas code?,<python><pandas>,,2.0,0.0,
14355151,1,14370190.0,2013-01-16 09:35:36,0,158,"I'm trying to build a ETL toolkit with pandas  hdf5

My plan was  

extracting a table from mysql to a DataFrame;  
put this DataFrame into a HDFStore;
But when i was doing the step 2  i found putting a dataframe into a *h5 file costs too much time

the size of table in source mysql server: 498MB
52 columns
924 624 records

the size of *h5 file after putting the dataframe inside : 513MB
the 'put' operation costs 849345677137 seconds

My questions are:
Is this time costs normal?
Is there any way to make it faster?

Update 1

thanks Jeff

my codes are pretty simple:

extract_store = HDFStore('extract_storeh5')
extract_store['df_staff'] = df_staff
and when i trying 'ptdump -av fileh5'  i got an error  but i still could load the dataframe object from this h5 file:

  tablesexceptionsHDF5ExtError: HDF5 error back trace
  
  File ""///src/H5Fc""  line 1512  in H5Fopen
      unable to open file   File ""///src/H5Fc""  line 1307  in H5F_open
      unable to read superblock   File ""///src/H5Fsuperc""  line 305  in H5F_super_read
      unable to find file signature   File ""///src/H5Fsuperc""  line 153  in H5F_locate_signature
      unable to find a valid file signature  
  
  End of HDF5 error back trace  
  
  Unable to open/create file 'extract_storeh5'  


some other infos:pandas version: '0100'
os: ubuntu server 1004 x86_64
cpu: 8 * Intel(R) Xeon(R) CPU X5670  @ 293GHz
MemTotal: 51634016 kB

I will update the pandas to 0101-dev and try again

Update 2

I had updated pandas to '0101dev-6e2b6ea'
but the time costs wasn't decreased  it costs 88415 s seconds this time
the output of 'ptdump -av fileh5 ' is :

    / (RootGroup) ''  
      /_v_attrs (AttributeSet)  4 attributes:  
       [CLASS := 'GROUP'   
        PYTABLES_FORMAT_VERSION := '20'   
        TITLE := ''   
        VERSION := '10']  
    /df_bugs (Group) ''  
      /df_bugs_v_attrs (AttributeSet)  12 attributes:  
       [CLASS := 'GROUP'   
        TITLE := ''   
        VERSION := '10'   
        axis0_variety := 'regular'   
        axis1_variety := 'regular'   
        block0_items_variety := 'regular'   
        block1_items_variety := 'regular'   
        block2_items_variety := 'regular'   
        nblocks := 3   
        ndim := 2   
        pandas_type := 'frame'   
        pandas_version := '0101']  
    /df_bugs/axis0 (Array(52 )) ''  
      atom := StringAtom(itemsize=19  shape=()  dflt='')  
      maindim := 0  
      flavor := 'numpy'  
      byteorder := 'irrelevant'  
      chunkshape := None  
      /df_bugs/axis0_v_attrs (AttributeSet)  7 attributes:  
       [CLASS := 'ARRAY'   
        FLAVOR := 'numpy'   
        TITLE := ''   
        VERSION := '23'   
        kind := 'string'   
        name := None   
        transposed := True]  
    /df_bugs/axis1 (Array(924624 )) ''  
      atom := Int64Atom(shape=()  dflt=0)  
      maindim := 0  
      flavor := 'numpy'  
      byteorder := 'little'  
      chunkshape := None  
      /df_bugs/axis1_v_attrs (AttributeSet)  7 attributes:  
       [CLASS := 'ARRAY'   
        FLAVOR := 'numpy'   
        TITLE := ''   
        VERSION := '23'   
        kind := 'integer'   
        name := None   
        transposed := True]  
    /df_bugs/block0_items (Array(5 )) ''  
      atom := StringAtom(itemsize=12  shape=()  dflt='')  
      maindim := 0   
      flavor := 'numpy'  
      byteorder := 'irrelevant'  
      chunkshape := None  
      /df_bugs/block0_items_v_attrs (AttributeSet)  7 attributes:  
       [CLASS := 'ARRAY'   
        FLAVOR := 'numpy'   
        TITLE := ''   
        VERSION := '23'   
        kind := 'string'   
        name := None   
        transposed := True]  
    /df_bugs/block0_values (Array(924624  5)) ''  
      atom := Float64Atom(shape=()  dflt=00)  
      maindim := 0  
      flavor := 'numpy'  
      byteorder := 'little'  
      chunkshape := None  
      /df_bugs/block0_values_v_attrs (AttributeSet)  5 attributes:  
       [CLASS := 'ARRAY'   
        FLAVOR := 'numpy'   
        TITLE := ''   
        VERSION := '23'   
        transposed := True]  
    /df_bugs/block1_items (Array(19 )) ''  
      atom := StringAtom(itemsize=19  shape=()  dflt='')  
      maindim := 0  
      flavor := 'numpy'  
      byteorder := 'irrelevant'  
      chunkshape := None  
      /df_bugs/block1_items_v_attrs (AttributeSet)  7 attributes:  
       [CLASS := 'ARRAY'   
        FLAVOR := 'numpy'   
        TITLE := ''   
        VERSION := '23'   
        kind := 'string'   
        name := None   
        transposed := True]  
    /df_bugs/block1_values (Array(924624  19)) ''  
      atom := Int64Atom(shape=()  dflt=0)  
      maindim := 0  
      flavor := 'numpy'  
      byteorder := 'little'  
      chunkshape := None  
      /df_bugs/block1_values_v_attrs (AttributeSet)  5 attributes:  
       [CLASS := 'ARRAY'   
        FLAVOR := 'numpy'   
        TITLE := ''    
        VERSION := '23'   
        transposed := True]  
    /df_bugs/block2_items (Array(28 )) ''  
      atom := StringAtom(itemsize=18  shape=()  dflt='')  
      maindim := 0  
      flavor := 'numpy'  
      byteorder := 'irrelevant'  
      chunkshape := None  
      /df_bugs/block2_items_v_attrs (AttributeSet)  7 attributes:  
       [CLASS := 'ARRAY'   
        FLAVOR := 'numpy'   
        TITLE := ''   
        VERSION := '23' 
        kind := 'string'   
        name := None   
        transposed := True]  
    /df_bugs/block2_values (VLArray(1 )) ''  
      atom = ObjectAtom()  
      byteorder = 'irrelevant'  
      nrows = 1  
      flavor = 'numpy'  
      /df_bugs/block2_values_v_attrs (AttributeSet)  5 attributes:  
       [CLASS := 'VLARRAY'   
        PSEUDOATOM := 'object'   
        TITLE := ''    
        VERSION := '13'   
        transposed := True]  


and I had tried your code below (putting the dataframe into hdfstore with the param 'table' is True)   but got an error instead  it seemed like python's datatime type was not supported :

  Exception: cannot find the correct atom type -> [dtype->object] object
  of type 'datetimedatetime' has no len()


Update 3

thanks jeff 
Sorry for the delay

tablesversion : '240'
yes  the 884 seconds is only the put operation costs without the pull operation from mysql
a row of dataframe (dfix[0]):

bug_id                                   1
assigned_to                            185
bug_file_loc                          None
bug_severity                      critical
bug_status                          closed
creation_ts            1998-05-06 21:27:00
delta_ts               2012-05-09 14:41:41
short_desc                    Two cursors
host_op_sys                        Unknown
guest_op_sys                       Unknown
priority                                P3
rep_platform                          IA32
reporter                                56
product_id                               7
category_id                            983
component_id                         12925
resolution                           fixed
target_milestone                       ws1
qa_contact                             412
status_whiteboard                         
votes                                    0
keywords                                SR
lastdiffed             2012-05-09 14:41:41
everconfirmed                            1
reporter_accessible                      1
cclist_accessible                        1
estimated_time                        000
remaining_time                        000
deadline                              None
alias                                 None
found_in_product_id                      0
found_in_version_id                      0
found_in_phase_id                        0
cf_type                             Defect
cf_reported_by                 Development
cf_attempted                           NaN
cf_failed                              NaN
cf_public_summary                         
cf_doc_impact                            0
cf_security                              0
cf_build                               NaN
cf_branch                                 
cf_change                              NaN
cf_test_id                             NaN
cf_regression                      Unknown
cf_reviewer                              0
cf_on_hold                               0
cf_public_severity                     ---
cf_i18n_impact                           0
cf_eta                                None
cf_bug_source                          ---
cf_viss                               None
Name: 0  Length: 52


the picture of dataframe( just type 'df' in ipython notebook):


Int64Index: 924624 entries  0 to 924623
Data columns:
bug_id                 924624  non-null values
assigned_to            924624  non-null values
bug_file_loc           427318  non-null values
bug_severity           924624  non-null values
bug_status             924624  non-null values
creation_ts            924624  non-null values
delta_ts               924624  non-null values
short_desc             924624  non-null values
host_op_sys            924624  non-null values
guest_op_sys           924624  non-null values
priority               924624  non-null values
rep_platform           924624  non-null values
reporter               924624  non-null values
product_id             924624  non-null values
category_id            924624  non-null values
component_id           924624  non-null values
resolution             924624  non-null values
target_milestone       924624  non-null values
qa_contact             924624  non-null values
status_whiteboard      924624  non-null values
votes                  924624  non-null values
keywords               924624  non-null values
lastdiffed             924509  non-null values
everconfirmed          924624  non-null values
reporter_accessible    924624  non-null values
cclist_accessible      924624  non-null values
estimated_time         924624  non-null values
remaining_time         924624  non-null values
deadline               0  non-null values
alias                  0  non-null values
found_in_product_id    924624  non-null values
found_in_version_id    924624  non-null values
found_in_phase_id      924624  non-null values
cf_type                924624  non-null values
cf_reported_by         924624  non-null values
cf_attempted           89622  non-null values
cf_failed              89587  non-null values
cf_public_summary      510799  non-null values
cf_doc_impact          924624  non-null values
cf_security            924624  non-null values
cf_build               327460  non-null values
cf_branch              614929  non-null values
cf_change              300612  non-null values
cf_test_id             12610  non-null values
cf_regression          924624  non-null values
cf_reviewer            924624  non-null values
cf_on_hold             924624  non-null values
cf_public_severity     924624  non-null values
cf_i18n_impact         924624  non-null values
cf_eta                 3910  non-null values
cf_bug_source          924624  non-null values
cf_viss                725  non-null values
dtypes: float64(5)  int64(19)  object(28)


after 'convert_objects()':

dtypes: datetime64[ns](2)  float64(5)  int64(19)  object(26)


and putting the converted dataframe into hdfstore costs: 74950 s :)
it seems that reducing the number of 'object' dtypes is the key to decrease time costs

and putting the converted dataframe into hdfstore with the param 'table' is true still returns that error

/usr/local/lib/python26/dist-packages/pandas-0101dev_6e2b6ea-py26-linux-x86_64egg/pandas/io/pytablespyc in create_axes(self  axes  obj  validate  nan_rep  data_columns  min_itemsize  **kwargs)
   2203                 raise
   2204             except (Exception)  detail:
-> 2205                 raise Exception(""cannot find the correct atom type -> [dtype->%s] %s"" % (bdtypename  str(detail)))
   2206             j += 1
   2207 
Exception: cannot find the correct atom type -> [dtype->object] object of type 'datetimedatetime' has no len()


I'm trying to put the dataframe without datetime columns
Update 4

There are 4 columns in mysql whose type is datetime:
creation_ts
delta_ts
lastdiffed
deadline

After calling the convert_objects():

creation_ts:

Timestamp: 1998-05-06 21:27:00


delta_ts:

Timestamp: 2012-05-09 14:41:41


lastdiffed

datetimedatetime(2012  5  9  14  41  41)


deadline is always None  no matter before or after calling 'convert_objects'

None


putting the dataframe without column 'lastdiff' costs 69175 s
when putting the dataframe without column 'lastdiff' and setting param 'table' equal to  True  I got an new error  :

/usr/local/lib/python26/dist-packages/pandas-0101dev_6e2b6ea-py26-linux-x86_64egg/pandas/io/pytablespyc in create_axes(self  axes  obj  validate  nan_rep  data_columns  min_itemsize  **kwargs)
   2203                 raise
   2204             except (Exception)  detail:
-> 2205                 raise Exception(""cannot find the correct atom type -> [dtype->%s] %s"" % (bdtypename  str(detail)))
   2206             j += 1
   2207 

Exception: cannot find the correct atom type -> [dtype->object] object of type 'Decimal' has no len()


the type of columns 'estimated_time'  'remaining_time'  'cf_viss' is 'decimal' in mysql
Update 5

I had transformed these 'decimal' type columns to 'float' type  by the code below:

no_diffed_converted_df_bugsestimated_time = no_diffed_converted_df_bugsestimated_timemap(float)


and now  the time costs is 37284 s
but the 'table' version putting still raised an error:

/usr/local/lib/python26/dist-packages/pandas-0101dev_6e2b6ea-py26-linux-x86_64egg/pandas/io/pytablespyc in create_axes(self  axes  obj  validate  nan_rep  data_columns  min_itemsize  **kwargs)
   2203                 raise
   2204             except (Exception)  detail:
-> 2205                 raise Exception(""cannot find the correct atom type -> [dtype->%s] %s"" % (bdtypename  str(detail)))
   2206             j += 1
   2207 

Exception: cannot find the correct atom type -> [dtype->object] object of type 'datetimedate' has no len()
",807695,,807695.0,2013-01-18 09:25:36,2013-01-24 04:46:52,how to make pandas HDFStore 'put' operation faster,<pandas>,2.0,14.0,1.0,
14405544,1,,2013-01-18 18:45:20,3,84,"i have looked for an answer to this question as it seems pretty simple  but have not been able to find anything yet  Apologies if I missed something  I have pandas version 0100 and I have been experimenting with data of the following form:

import pandas
import numpy as np
import datetime
start_date = datetimedatetime(2009 3 1 6 29 59)
r = pandasdate_range(start_date  periods=12)
cols_1 = ['AAPL'  'AAPL'  'GOOG'  'GOOG'  'GS'  'GS']
cols_2 = ['close'  'rate'  'close'  'rate'  'close'  'rate']
dat = nprandomrandn(12  6)
cols = pandasMultiIndexfrom_arrays([cols_1  cols_2]  names=['ticker' 'field'])
dftst = pandasDataFrame(dat  columns=cols  index=r)
print dftst



ticker                   AAPL                GOOG                  GS          
field                   close      rate     close      rate     close      rate
2009-03-01 06:29:59  1956255 -2074371 -0200568  0759772 -0951543  0514577
2009-03-02 06:29:59  0069611 -2684352 -0310006  0730205 -0302949 -0830452
2009-03-03 06:29:59  2077130 -0903784  0449857 -1357464 -0469572 -0008757
2009-03-04 06:29:59  1585358 -2063672  0600889 -1741606 -0299875  0565253
2009-03-05 06:29:59  0269123  0226593  1132663  0485035  0796858 -0423112
2009-03-06 06:29:59  0094879 -1040069  0613450 -0175266 -0065172  3374658
2009-03-07 06:29:59 -1255167 -0326474  0437053 -0231594  0437703 -0256811
2009-03-08 06:29:59  0115454 -1096841 -1189211 -0208098 -0807860  0158198
2009-03-09 06:29:59  2142816  0173878 -0160932  0367309 -0449765 -0325400
2009-03-10 06:29:59  0470669 -0346805  1152648  0844632  1031602 -0012502
2009-03-11 06:29:59 -1366954  0452177  0010713 -1331553  0226781  0456900
2009-03-12 06:29:59  2182409  0890023 -0627318 -1516574 -1565416 -0694320


As you can see  I am trying to represent 3d timeseries data So I have a timeseries index and MultiIndex columns  I am pretty comfortable with slicing the data  If I wanted just a trailing mean of the close data  I can do the following:

pandasrolling_mean(dftstix[: ::2]  5)


ticker                   AAPL      GOOG        GS
field                   close     close     close
2009-03-01 06:29:59       NaN       NaN       NaN
2009-03-02 06:29:59       NaN       NaN       NaN
2009-03-03 06:29:59       NaN       NaN       NaN
2009-03-04 06:29:59       NaN       NaN       NaN
2009-03-05 06:29:59  0410966 -0412356  0722951
2009-03-06 06:29:59 -0103187 -0497165  0137731
2009-03-07 06:29:59  0000194 -0645375 -0298504
2009-03-08 06:29:59 -0074036 -0541717 -0035906
2009-03-09 06:29:59 -0391863 -0671918 -0554380
2009-03-10 06:29:59 -0336397 -0411845 -0992615
2009-03-11 06:29:59 -0251645 -0289512 -0458246
2009-03-12 06:29:59 -0138925  0244572 -0230743


What I cannot do is create a new field  like avg_close and assign to it  Ideally I would like to do something like the following:

dftst[: 'avg_close'] = pandasrolling_mean(dftstix[: ::2]  5)

Even if I swap the levels of my MultiIndex  I cannot make it work:

dftst = dftstswaplevel(1 0 axis=1)
print dftst['close']

ticker                   AAPL      GOOG        GS
2009-03-01 06:29:59  1178557 -0505672 -0336645
2009-03-02 06:29:59  0234305  0581429 -0232252
2009-03-03 06:29:59 -0734798  0117810  1658418
2009-03-04 06:29:59 -1555033 -0298322  0127408
2009-03-05 06:29:59  0244102 -1030041 -0562039
2009-03-06 06:29:59 -0297454  1150564 -1930883
2009-03-07 06:29:59  0818910 -0905296  1219946
2009-03-08 06:29:59  0586816  0965242  0928546
2009-03-09 06:29:59 -0357693  0071455  0072956
2009-03-10 06:29:59  0651803 -0685937  0805779
2009-03-11 06:29:59  0569802 -0062447 -1349261
2009-03-12 06:29:59 -1886335  0205778 -0864273

dftst['avg_close'] = pandasrolling_mean(dftst['close']  3)


----> 1 dftst['avg_close'] = pandasrolling_mean(dftst['close']  3)

/usr/local/lib/python27/dist-packages/pandas/core/framepyc in
__setitem__(self  key  value)    2041         else:    2042             # set column

-> 2043             self_set_item(key  value)    2044     2045     def _boolean_set(self  key  value):

/usr/local/lib/python27/dist-packages/pandas/core/framepyc in
_set_item(self  key  value)    2077         """"""    2078         value = self_sanitize_column(key  value)
-> 2079         NDFrame_set_item(self  key  value)    2080     2081     def insert(self  loc  column  value):

/usr/local/lib/python27/dist-packages/pandas/core/genericpyc in
_set_item(self  key  value)
    544 
    545     def _set_item(self  key  value):
--> 546         self_dataset(key  value)
    547         self_clear_item_cache()
    548 

/usr/local/lib/python27/dist-packages/pandas/core/internalspyc in set(self  item  value)
    951         except KeyError:
    952             # insert at end

--> 953             selfinsert(len(selfitems)  item  value)
    954 
    955         self_known_consolidated = False

/usr/local/lib/python27/dist-packages/pandas/core/internalspyc in insert(self  loc  item  value)
    963 
    964         # new block

--> 965         self_add_new_block(item  value  loc=loc)
    966 
    967         if len(selfblocks) > 100:

/usr/local/lib/python27/dist-packages/pandas/core/internalspyc in
_add_new_block(self  item  value  loc)
    992             loc = selfitemsget_loc(item)
    993         new_block = make_block(value  selfitems[loc:loc+1]copy() 
--> 994                                selfitems)
    995         selfblocksappend(new_block)
    996 

/usr/local/lib/python27/dist-packages/pandas/core/internalspyc in make_block(values  items  ref_items)
    463         klass = ObjectBlock
    464 
--> 465     return klass(values  items  ref_items  ndim=valuesndim)
    466 
    467 # TODO: flexible with index=None and/or items=None


/usr/local/lib/python27/dist-packages/pandas/core/internalspyc in
__init__(self  values  items  ref_items  ndim)
     30         if len(items) != len(values):
     31             raise AssertionError('Wrong number of items passed (%d vs %d)'
---> 32                                  % (len(items)  len(values)))
     33 
     34         self_ref_locs = None

AssertionError: Wrong number of items passed (1 vs 3)


If my columns were not MultiIndex  I could assign doing the following:

start_date = datetimedatetime(2009 3 1 6 29 59)
r = pandasdate_range(start_date  periods=12)
cols = ['AAPL'  'GOOG'  'GS']
dat = nprandomrandn(12  3)
dftst2 = pandasDataFrame(dat  columns=cols  index=r)
print dftst2

                         AAPL      GOOG        GS
2009-03-01 06:29:59  2476787  2386037 -0777566
2009-03-02 06:29:59 -0820647  1006159 -0590240
2009-03-03 06:29:59  0433960  0104458  0282641
2009-03-04 06:29:59  0300190 -0300786 -1780412
2009-03-05 06:29:59 -0247919  1616572  1145594
2009-03-06 06:29:59 -0779130  0695256  0845819
2009-03-07 06:29:59  0572073  0349394 -3557776
2009-03-08 06:29:59  2019885  0358346  1350812
2009-03-09 06:29:59  0472328 -0334223 -0605862
2009-03-10 06:29:59 -1570479  0410808  0616515
2009-03-11 06:29:59  1177562 -0240396 -2126951
2009-03-12 06:29:59  0311566 -1743213  0382617


To add a field  based on another field  I can do the following:

dftst2['GOOG_avg'] = pandasrolling_mean(dftst2['GOOG']  3)
print dftst2


                         AAPL      GOOG        GS  GOOG_avg
2009-03-01 06:29:59  2476787  2386037 -0777566       NaN
2009-03-02 06:29:59 -0820647  1006159 -0590240       NaN
2009-03-03 06:29:59  0433960  0104458  0282641  1165551
2009-03-04 06:29:59  0300190 -0300786 -1780412  0269944
2009-03-05 06:29:59 -0247919  1616572  1145594  0473415
2009-03-06 06:29:59 -0779130  0695256  0845819  0670347
2009-03-07 06:29:59  0572073  0349394 -3557776  0887074
2009-03-08 06:29:59  2019885  0358346  1350812  0467666
2009-03-09 06:29:59  0472328 -0334223 -0605862  0124506
2009-03-10 06:29:59 -1570479  0410808  0616515  0144977
2009-03-11 06:29:59  1177562 -0240396 -2126951 -0054604
2009-03-12 06:29:59  0311566 -1743213  0382617 -0524267


I have tried using a Panel object  but so far have not found a quick way to add a field where I have MultiIndex columns  ideally the other level of the columns would be broadcast  I apologize if there have been other posts that answer this question  Any suggestions would be much appreciated",1988295,,,,2013-02-01 05:41:32,add a field in pandas dataframe with MultiIndex columns,<data.frame><field><pandas><time-series><multi-index>,2.0,,2.0,
14429793,1,14431417.0,2013-01-20 21:55:59,2,53,"I have a Dataframe looking like this:

>>> import pandas
>>> df = pandasDataFrame({'region' : ['east'  'west'  'south'  'west' 
  'east'  'west'  'east'  'west'] 
  'item' : ['one'  'one'  'two'  'three' 
         'two'  'two'  'one'  'three'] 
         'quantity' : [3 3 4 5 12 14 3 8]  ""price"" : [50 50 12 35 10 10 12 12]})
>>> df
    item  price  quantity region
0    one     50         3   east
1    one     50         3   west
2    two     12         4  south
3  three     35         5   west
4    two     10        12   east
5    two     10        14   west
6    one     12         3   east
7  three     12         8   west


and what I want to do is modify the values in the quantity column Each new quantity value is caculated based on the number of different regions that exist for this row's combination of item  and price More concretly I want to take each quantity and multiply it by the weight of it's region returned by a function I wrote that takes a region and the list of other region composing the pool:

region_weight(region  list_of_regions) For this imaginary situation  let's say:

region east is worth 1
region west is worth 2
south worth is worth 3
Then the returned weight of east in the pool east  west is 03333333333333333 (1/3) The weight of south in pool east  west  south is 05 (1/2)

So for the first row  we look at what other rows there are of item one and price 50 There are 2 one with east and one with the west region The new quantity in the first row would be: 3 * region_weight(""east""  [""east""  ""west""])  or 3 * 03333333333333333

I want to apply the same process to the whole quantity column I don't know how to approach this problem with the pandas library other than looping through the Dataframe row by row",1092549,,,,2013-01-21 01:40:20,apply a function to a pandas Dataframe whose retuned value is based on other rows,<python><pandas>,1.0,,,
14431917,1,,2013-01-21 03:01:18,0,32,Is it possible to load a python panda dataframe from a pickle file into c# space? If yes could you please direct me to the documentation on this? If there is no direct way  is there a work around like running python in NET space and loading a panda dataframe and then converting it into C# data structure? What are the steps involved?,899811,,,,2013-01-21 03:01:18,can we load panda dataframe in c# .NET?,<c#><python><pandas>,,2.0,,
14438509,1,14438691.0,2013-01-21 12:21:45,1,55,"I'm about to try out Pytables for the first time and I need to write my data to the hdf file  per time step I'll have over 100 000 time steps When I'm done  I would like to sort my 100 000+ x 6 array by column 2  ie  I currently have everything sorted by time but now I need to sort the array by order of decreasing rain rates (col 2) I'm unsure how to even begin here I know that having the entire array in memory is unwise Any ideas how to doe this fast and efficiently? 

Appreciate any advice",1394513,,,,2013-01-21 12:36:54,Sorting very large 1D arrays,<numpy><pandas><pytables>,2.0,2.0,1.0,
14455746,1,14455831.0,2013-01-22 09:55:52,2,49,"I have a pandas Series with an integer index which I've sorted (by value)  how I access values by position in this Series

For example: 

s_original = pdSeries({0: -0000213  1: 000031399999999999999  2: -000024899999999999998  3: -26999999999999999e-05  4: 0000122})
s_sorted = npsort(s_original)

In [3]: s_original
Out[3]: 
0   -0000213
1    0000314
2   -0000249
3   -0000027
4    0000122

In [4]: s_sorted
Out[4]: 
2   -0000249
0   -0000213
3   -0000027
4    0000122
1    0000314

In [5]: s_sorted[3]
Out[5]: -26999999999999999e-05


But I would like to get the value 0000122 ie the item in position 3
How can I do this?",1795245,,1240268.0,2013-01-22 20:17:25,2013-01-22 21:59:02,Access value by location in sorted panda series with integer index,<python><pandas><series>,2.0,,,
14395678,1,14400659.0,2013-01-18 09:16:01,2,85,"I have a Series s with duplicate index :

>>> s
STK_ID  RPT_Date
600809  20061231    demo_str
        20070331    demo_str
        20070630    demo_str
        20070930    demo_str
        20071231    demo_str
        20060331    demo_str
        20060630    demo_str
        20060930    demo_str
        20061231    demo_str
        20070331    demo_str
        20070630    demo_str
Name: STK_Name  Length: 11


And I just want to keep the unique rows and only one copy of the duplicate rows by:

s[sindexunique()]


Pandas 0101dev-f7f7e13  give the below error msg

>>> s[sindexunique()]
Traceback (most recent call last):
  File """"  line 1  in 
  File ""d:\Python27\lib\site-packages\pandas\core\seriespy""  line 515  in __getitem__
    return self_get_with(key)
  File ""d:\Python27\lib\site-packages\pandas\core\seriespy""  line 558  in _get_with
    return selfreindex(key)
  File ""d:\Python27\lib\site-packages\pandas\core\seriespy""  line 2361  in reindex
    level=level  limit=limit)
  File ""d:\Python27\lib\site-packages\pandas\core\indexpy""  line 2063  in reindex
    limit=limit)
  File ""d:\Python27\lib\site-packages\pandas\core\indexpy""  line 2021  in get_indexer
    raise Exception('Reindexing only valid with uniquely valued Index '
Exception: Reindexing only valid with uniquely valued Index objects
>>> 


So how to drop extra duplicate rows of series  keep the unique rows and only one copy of the duplicate rows in an efficient way ? (better in one line)",1072888,,1072888.0,2013-01-18 13:19:46,2013-01-19 03:25:20,How to drop extra copy of duplicate index of Pandas Series?,<pandas>,2.0,,,
14408634,1,14408712.0,2013-01-18 22:22:10,3,77,"I'm currently iterating through a very large set of data ~85GB (~600M lines) and simply using newton-raphson to compute a new parameter As of right now my code is extremely slow  any tips on how to speed it up? The methods from BSCallClass & BSPutClass are closed-form  so there's nothing really to speed up there Thanks

class NewtonRaphson:

    def __init__(self  theObject):
        selftheObject = theObject

    def solve(self  Target  Start  Tolerance  maxiter=500):
        y = selftheObjectPrice(Start)
        x = Start
        i = 0
        while (abs(y - Target) > Tolerance):
            i += 1
            d = selftheObjectVega(x)
            x += (Target - y) / d
            y = selftheObjectPrice(x)
            if i > maxiter:
                x = nan
                break
        return x

    def main():
        for row in aiterrows():
            print row[1][""X1""]
            T = (row[1][""X7""] - row[1][""X8""])days
            Spot = row[1][""X2""]
            Strike = row[1][""X9""]
            MktPrice = abs(row[1][""X10""]-row[1][""X11""])/2
            CPflag = row[1][""X6""]

            if CPflag == 'call':
                option = BSCallClass(0  0  T  Spot  Strike)
            elif CPflag == 'put':
                option = BSPutClass(0  0  T  Spot  Strike)

            a[""X15""][row[0]] = NewtonRaphson(option)solve(MktPrice  05  0001)


EDIT:

For those curious  I ended up speeding this entire process significantly by using the scipy suggestion  as well as using the multiprocessing module",287950,,287950.0,2013-01-22 21:17:52,2013-01-22 21:17:52,Speeding up newton-raphson in pandas/python,<python><pandas><newtons-method>,1.0,2.0,2.0,
14487562,1,14487598.0,2013-01-23 19:14:55,1,44,"I have a Panda Series and based on a random number I want to pick a row (5 in the code example below) and drop that row When the row is dropped I want to create a new index for the remaining rows (0 to 8) The code below:

print 'Original series: '  sample_mean_series
print 'Length of original series'  len(sample_mean_series)
sample_mean_series = sample_mean_seriesdrop([5] axis=0)
print 'Series with item 5 dropped: '  sample_mean_series
print 'Length of modified series:'  len(sample_mean_series)
print sample_mean_seriesreindex(range(len(sample_mean_series)))


And this is the output:

Original series:  
0    0000074
1   -0000067
2    0000076
3   -0000017
4   -0000038
5   -0000051
6    0000125
7   -0000108
8   -0000009
9   -0000052
Length of original series 10
Series with item 5 dropped:  
0    0000074
1   -0000067
2    0000076
3   -0000017
4   -0000038
6    0000125
7   -0000108
8   -0000009
9   -0000052
Length of modified series: 9
0    0000074
1   -0000067
2    0000076
3   -0000017
4   -0000038
5         NaN
6    0000125
7   -0000108
8   -0000009


My problem is that the row number 8 is dropped I want to drop row ""5    NaN"" and keep -0000052 with an index 0 to 8 This is what I want it to look like:

0    0000074
1   -0000067
2    0000076
3   -0000017
4   -0000038
5    0000125
6   -0000108
7   -0000009
8   -0000052
",1795245,,,,2013-01-23 19:24:09,Drop row in Panda Series and clean up index,<python><pandas><series>,2.0,,1.0,
14450020,1,14450336.0,2013-01-22 01:13:46,2,42,"I have pandas DataFrame I would like to get single value from a column based on a condition involving two another column I am looking for the value from the column3 for which is the biggest distance in the column1 and 2

I build the simple example which works:

d = pdDataFrame({'c1':[1 3 113] 'c2':[3 6 6] 'c3':[8 8 109]})
print'data d=\n%s\n' % d                                               
x = float(dc3[abs(dc1-dc2)==max(abs(dc1-dc2))]values)
print 'the value of x= \n%s\n' % x


The output from this example is as I expect:

     c1   c2    c3
0   01  30   80
1   30  60   08
2  113  06  109

the value of x= 
109


I try to apply exactly the same logic to my original problem with large dataframe inside a class The code is:

yInit = float(selfDenFrameDepth[abs(selfDenFrameHper-selfDenFrameVper)==max(abs(selfDenFrameHper-selfDenFrameVper))]values)


but this code produce an error:


  File ""C:\Python27\lib\site-packages\pandas-090-py27-win32egg\pandas\core\seriespy""  line 73  in wrapper
return Series(na_op(selfvalues  othervalues) 
  File ""C:\Python27\lib\site-packages\pandas-090-py27-win32egg\pandas\core\seriespy""  line 59  in na_op
result[mask] = op(x[mask]  y[mask])
TypeError: unsupported operand type(s) for -: 'str' and 'str'


I found in here that there could be a problem with type of the columns but Depth is type numpyfloat64 Hper is type float Vper is type float so I understand how it can apply to my problem

I don't know from this point what to do as I understand the same code works in one case but not in another and I cannot spot the problem",1661173,,,,2013-01-22 02:11:22,unsupported operand in pandas dataframe operation,<python><pandas>,1.0,5.0,1.0,
14471515,1,,2013-01-23 02:33:14,0,47,"My previous code was list of list of pandas dataframes as follows

rowResults = [ [df  df  df]  [df  df  df]   [df  df  df] ]
results=resultsappend(rowResults)


Since all dataframes have exact same columns  when I appended above list  it converted the whole data structure into a single dataframe with same columns as individual dataframe

Now  I have converted the small dataframes into a dictionary because of performance issues If I create large number of dataframes  I see that there is some kind of memory leak in storing meta data information used by pandas dataframes This doesn't occur when I use a dictionary instead 

my new code looks as follows

rowResults = [ [dict  dict  dict]  [dict  dict  dict]   [dict  dict  dict] ]
results=resultsappend(rowResults)


Above code doesn't has same effect as in previous case which is normal How can I convert above list of list of dictionaries so that final pandas dataframes has same columns as that of dictionary keys?  In case of dictionaries  my output looks as follows

(Pdb) results

Int64Index: 799 entries  0 to 798
Data columns:
0                799  non-null values
1                799  non-null values
2                799  non-null values
column1            0  non-null values
column2            0  non-null values
column3            0  non-null values
column4            0  non-null values


Please advise",899811,,,,2013-01-23 03:52:18,how to convert a list of list of dictionaries into a single panda dataframe?,<python><pandas>,1.0,0.0,,2013-01-23 10:29:08
14490764,1,,2013-01-23 22:43:41,-3,35,"I just install ipython and ipython notebeook on my windows 7 computer Into CMD I type:

ipython notebook --pylab=inline


And I get the error message 

I google-ed around and this has to do with the fact that iPython saves notebooks into an file that is in sys32 and basically there is a property applied to all the folders inside sys32 that says that the any file within can only be read and not written into I am not sure how to fix this or how to go about this  ",1367204,,,,2013-01-23 22:43:41,iPython Notebook won't start on Windows 7 because of 'ERROR:root:500',<windows-7><python-2.7><pandas><ipython><ipython-notebook>,,2.0,,2013-01-24 08:30:48
14390224,1,14390487.0,2013-01-17 23:48:24,5,55,"It looks to me like a bug in pandasSeries

a = pdSeries([1 2 3 4])
b = areshape(2 2)
b


b has type Series but can not be displayed  the last statement gives exception  very lengthy  the last line is ""TypeError: %d format: a number is required  not numpyndarray"" bshape returns (2 2)  which contradicts its type Series I am guessing perhaps pandasSeries does not implement reshape function and I am calling the version from nparray? Anyone see this error as well? I am at pandas 091 ",458429,,,,2013-01-18 01:16:06,Reshape of pandas series?,<numpy><pandas>,1.0,2.0,,
14405975,1,14406096.0,2013-01-18 19:14:12,1,21,"I'm trying to use Pandas to solve an issue courtesy of an idiot DBA not doing a backup of a now crashed data set  so I'm trying to find differences between two columns  For reasons I won't get into  I'm using Pandas rather than a database  

What I'd like to do is  given:

Dataset A = [A  B  C  D  E]  
Dataset B = [C  D  E  F]


I would like to find values which are disjoint  

Dataset A!=C = [A  B  F]


In SQL  this is standard set logic  accomplished differently depending on the dialect  but a standard function  How do I elegantly apply this in Pandas?  I would love to input some code  but nothing I have is even remotely correct  It's a situation in which I don't know what I don't know  Pandas has set logic for intersection and union  but nothing for disjoint  

Thanks!  ",1524634,,,,2013-01-18 19:21:58,How do I do a SQL style disjoint or set difference on two Pandas DataFrame objects?,<python><pandas>,1.0,,,
14412181,1,14412246.0,2013-01-19 07:33:05,0,53,"Use the dframe from pandas module:

df = dframeresample('t'  how = 'sum')


And after that I want to write the data in a new file I use this:

with open('dframetxt'  'w') as fout:
   foutwrite(dfprice1) #it is the first column


But it doesn't work",1992474,,1199589.0,2013-01-19 08:09:10,2013-01-19 19:24:46,Writing DataFrame column to a file,<python><pandas>,3.0,1.0,,
14433039,1,14433084.0,2013-01-21 05:20:24,1,23,"I have a multilevel dataframe df :

>>> df
                   sales     cash
STK_ID RPT_Date                  
000568 20120630   51926   42845
       20120930   80093   57488
000596 20120630   22278   18247
       20120930   32585   26177
000799 20120630    9291    6513
       20120930   14784    8157


And I want to get the value list of sub_level index 'STK_ID'   which will return a list of ['000568' '000596' '000799']
Is there any direct function to do this (without using reset_index and getting the column value)?",1072888,,1240268.0,2013-01-21 07:01:32,2013-01-21 07:01:32,How to get the sub_level index value of Pandas dataframe?,<pandas>,1.0,,,
14444916,1,14446240.0,2013-01-21 18:26:31,1,22,"I need a smart and concise way to arrive from data_1 to data_3 dataframe
Right now I m arrived easily just to dataframe 2

DATA_1                        
key  SEGM1    SEGM2      VAL
A        K        X        1
B        K        X        2
C        K        X        3
D        K        Y        4
E        K        Y        5
F        J        Y        6
G        J        Z        7
H        J        Z        8
I        J        Z        9


DATA_2
SEGM1   SEGM2       VAL
    K       X         6
            Y         9
    J       Y         6
            Z        24

DATA_3
SEGM1   SEGM2        VAL
    K       X        40%
            Y        60%
    J       Y        20%
            Z        80%


Thanks a lot!

M",1937003,,243434.0,2013-01-21 19:31:46,2013-01-21 19:58:35,Pandas obtain share from DataFrame,<data.frame><grouping><pandas>,1.0,,,
14497777,1,,2013-01-24 09:22:11,1,37,"I have two Pandas TimeSeries: x  and y  which I would like to sync ""as of"" I would like to find for every element in x the latest (by index) element in y that preceeds it (by index value) For example  I would like to compute this new_x:

x       new_x
----    -----
13:01   13:00  
14:02   14:00

y
----
13:00
13:01
13:30
14:00


I am looking for a vectorized solution  not a Python loop The time values are based on Numpy datetime64 The y array's length is in the order of millions  so O(n^2) solutions are probably not practical",1579844,,1579844.0,2013-01-24 16:56:12,2013-01-24 16:56:12,Vectorized method to sync two arrays,<numpy><pandas><vectorization>,2.0,2.0,,
14471120,1,14471383.0,2013-01-23 01:36:39,0,37,"I am trying to write pandas DataTables and Series to xlwt Worksheet objects All goes well except if I try to write numpyint64 data  in which case xlwt gasps Changing int64 to float64 in my data and single-level indexes is simple  but what would be the right way of doing that for MultiIndexes?

t = pdDataFrame(nparray(npmat('0 1 0 1; 1 0 2 3; 1 1 2 4')))
arrays = [[1 2 3 4] [5 6 7 8]]
tuples = zip(*arrays)
index = pdMultiIndexfrom_tuples(tuples  names=['First' 'Second'])
tcolumns = index
wb = xlwtWorkbook()
ws_1 = wbadd_sheet('simple index'  cell_overwrite_ok=True)

In [137]: t
Out[137]: 
First   1  2  3  4
Second  5  6  7  8
0       0  1  0  1
1       1  0  2  3
2       1  1  2  4

In [157]: tix[0][1][5]
Out[157]: 0

In [158]: ws_1row(0)write(0  tix[0][1][5])
------------------------------------------------------------
Traceback (most recent call last):
  File """"  line 1  in 
  File ""C:\Python27\lib\site-packages\xlwt\Rowpy""  line 259  in write
    raise Exception(""Unexpected data type %r"" % type(label))
Exception: Unexpected data type 

In [159]: tdtypes
Out[159]: 
First  Second
1      5         int64
2      6         int64
3      7         int64
4      8         int64

In [160]: idx = tdtypes[tdtypes == npint64]index

In [161]: idx
Out[161]: 
MultiIndex
[(1  5)  (2  6)  (3  7)  (4  8)]

In [163]: for i in idx:
   :             t[i] = t[i]astype(npfloat64)
   : 

In [164]: tdtypes
Out[164]: 
First  Second
1      5         float64
2      6         float64
3      7         float64
4      8         float64

In [165]: ws_1row(0)write(0  tix[0][1][5])

In [167]: tcolumnslevels
Out[167]: [Int64Index([1  2  3  4]  dtype=int64)  Int64Index([5  6  7  8]  dtype=int64)]

In [168]: tcolumns
Out[168]: 
MultiIndex
[(1  5)  (2  6)  (3  7)  (4  8)]

In [169]: tcolumns[0][0]
Out[169]: 1

In [170]: ws_1row(0)write(0  tcolumns[0][0])
------------------------------------------------------------
Traceback (most recent call last):
  File """"  line 1  in 
  File ""C:\Python27\lib\site-packages\xlwt\Rowpy""  line 259  in write
    raise Exception(""Unexpected data type %r"" % type(label))
Exception: Unexpected data type ",1479269,,1479269.0,2013-01-23 01:46:47,2013-01-23 02:13:36,Change Int64Index to Index and dtype=int64 to dtype=object,<python><pandas><xlwt>,1.0,3.0,,
14488697,1,,2013-01-23 20:23:47,2,58,"Column y below should be ['Reg'  'Reg'  'Swp'  'Swp']

In [1]: pdread_csv('/tmp/test3csv')  
Out[1]:  
x y  
 ^@^@^@ Reg  
 ^@^@^@ Reg  
I Swp  
I Swp  

In [2]: ! cat /tmp/test3csv  
     x    y  
0  
1  NaN  NaN  
2    I  Swp  
3    I  Swp    

In [3]: f = open('/tmp/test3csv'  'rb'); print(repr(fread()))  
'x y\n \x00\x00\x00 Reg\n \x00\x00\x00 Reg\nI Swp\nI Swp\n'
",1827356,,919872.0,2013-01-23 20:56:01,2013-01-24 22:47:02,Pandas read_csv failing on columns with null characters,<python><pandas>,1.0,3.0,,
14507794,1,14508355.0,2013-01-24 18:03:11,4,89,"I have a data frame with a hierarchical index in axis 1 (columns) (from a groupbyagg operation):

          USAF   WBAN  year  month  day  s_PC  s_CL  s_CD  s_CNT  tempf       
                                     sum   sum   sum    sum   amax   amin
0  702730  26451  1993      1    1     1     0    12     13  3092  2498
1  702730  26451  1993      1    2     0     0    13     13  3200  2498
2  702730  26451  1993      1    3     1    10     2     13  2300   698
3  702730  26451  1993      1    4     1     0    12     13  1004   392
4  702730  26451  1993      1    5     3     0    10     13  1994  1094


I want to flatten it  so that it looks like this (names aren't critical - I could rename):

      USAF   WBAN  year  month  day  s_PC  s_CL  s_CD  s_CNT  tempf_amax    tmpf_amin   
0  702730  26451  1993      1    1     1     0    12     13  3092          2498
1  702730  26451  1993      1    2     0     0    13     13  3200          2498
2  702730  26451  1993      1    3     1    10     2     13  2300          698
3  702730  26451  1993      1    4     1     0    12     13  1004          392
4  702730  26451  1993      1    5     3     0    10     13  1994          1094


How do I do this? (I've tried a lot  to no avail) 

Per a suggestion  here is the head in dict form

{('USAF'  ''): {0: '702730' 
  1: '702730' 
  2: '702730' 
  3: '702730' 
  4: '702730'} 
 ('WBAN'  ''): {0: '26451'  1: '26451'  2: '26451'  3: '26451'  4: '26451'} 
 ('day'  ''): {0: 1  1: 2  2: 3  3: 4  4: 5} 
 ('month'  ''): {0: 1  1: 1  2: 1  3: 1  4: 1} 
 ('s_CD'  'sum'): {0: 120  1: 130  2: 20  3: 120  4: 100} 
 ('s_CL'  'sum'): {0: 00  1: 00  2: 100  3: 00  4: 00} 
 ('s_CNT'  'sum'): {0: 130  1: 130  2: 130  3: 130  4: 130} 
 ('s_PC'  'sum'): {0: 10  1: 00  2: 10  3: 10  4: 30} 
 ('tempf'  'amax'): {0: 30920000000000002 
  1: 320 
  2: 230 
  3: 10039999999999999 
  4: 19939999999999998} 
 ('tempf'  'amin'): {0: 2498 
  1: 2498 
  2: 69799999999999969 
  3: 39199999999999982 
  4: 10940000000000001} 
 ('year'  ''): {0: 1993  1: 1993  2: 1993  3: 1993  4: 1993}}
",1400991,,919872.0,2013-01-24 19:24:04,2013-01-24 19:26:28,Python Pandas - How to flatten a hierarchical index in columns,<python><pandas>,3.0,2.0,,
14492898,1,14493078.0,2013-01-24 02:25:00,4,46,"I have a DataFrame with daily OHLCV data

I can calculate the range with:

s['Range'] = s['High'] - s['Low']


Simple Now I would like to calculate a new column which I've called s['OIR'] (OIR = Open-In-Range)

The ['OIR'] column checks to see if we opened in range and it does this by testing if we opened above yesterdays low and below yesterday's high  I need to reference the previous rows and I'm not quite sure how to do it  The return values would be True/False

Thanks

edit:  I'm new to StackExchange and Python  Not sure where to drop sample data  Here's an image of the dataframe

http://i47tinypiccom/142eb2apng

Sample Data: Dictionary convert to DataFrame

{'High': {: 13845 
  : 13730} 
 'Last': {: 13650 
  : 13515} 
 'Low': {: 136425 
  : 13505} 
 'OIR': {: False 
  : False} 
 'Open': {: 13785 
  : 135675} 
 'Range': {: 2025 
 : 225} 
 'Volume': {: 1706906 
 : 1984041}}


Answer:  

s['OIR'] = ((s['Open']  s['Low']shift(1)))
",1569815,,1569815.0,2013-01-24 03:18:34,2013-01-24 03:18:34,Pandas Inter-row calculations,<pandas>,1.0,3.0,,
14509517,1,,2013-01-24 19:49:25,1,62,"I'm building a finance application in Python to do time series analysis on security prices (among other things) The heavy lifting will be done in Python mainly using Numpy  SciPy  and pandas (pandas has an interface for SQLite and MySQL) With a web interface to present results There will be a few hundred GB of data

I'm curious what is the better option for database in terms of performance  ease of accessing the data (queries)  and interface with Python I've seen the posts about the general pros and cons of SQLite v MySQL but I'm looking for feedback that's more specific to a Python application",687739,,,,2013-01-25 03:00:58,MySQL v. SQLite for Python based financial web app,<python><mysql><sqlite><pandas>,3.0,,,2013-01-25 07:56:14
14513006,1,14513351.0,2013-01-24 23:51:14,4,45,"I have a data file that includes several years' temperature records  I read-in the data file with Pandas and now it becomes a DataFrame below:

In [86]: tso
Out[86]: 

DatetimeIndex: 28170 entries  2005-05-20 13:28:42239999+00:00 to 2012-12-05           13:26:49919999+00:00
Data columns:
Day      28170  non-null values
Month    28170  non-null values
Year     28170  non-null values
Temp     28170  non-null values
dtypes: float64(1)  int64(3)


then I plot them according to 'Month'and 'year'columns:

ax=tsogroupby(['Month' 'Year'])mean()unstack()plot(linewidth=3 legend=False)
patches labels=axget_legend_handles_labels()
axlegend(unique(tso['Year']values) loc='best')
pltshow()




now I want the last year's temperatures are plotted by thick line What should I do?Is there any solution simple? Thank you!",1843099,,1301710.0,2013-01-27 12:05:17,2013-01-27 12:05:17,How to plot specified data in thick line,<python><matplotlib><pandas>,1.0,,,
14515239,1,14531449.0,2013-01-25 04:15:00,5,128,"I'm new to Pandas and Zipline  and I'm trying to learn how to use them (and use them with this data that I have) Any sorts of tips  even if no full solution  would be much appreciated I have tried a number of things  and have gotten quite close  but run into indexing issues  Exception: Reindexing only valid with uniquely valued Index objects  in particular [Pandas 0100  Python 27]

I'm trying to transform monthly returns data I have for thousands of stocks in postgres from the form: 

ticker_symbol :: String  monthly_return :: Float  date :: Timestamp


eg 

AAPL  0112  28/2/1992
GS  013  30/11/1981
GS  -023  22/12/1981


NB: The frequency of the reporting is monthly  but there is going to be considerable NaN data here  as not all of the over 6000 companies I have here are going to be around at the same time

to the form described below  which is what Zipline needs to run its backtester (I think Can Zipline's backtester work with monthly data like this  easily? I know it can  but any tips for doing this?)

The below is a DataFrame (of timeseries? How do you say this?)  in the format I need:

> data:


DatetimeIndex: 2268 entries  1993-01-04 00:00:00+00:00 to 2001-12-31 00:00:00+00:00
Data columns:
AA      2268  non-null values
AAPL    2268  non-null values
GE      2268  non-null values
IBM     2268  non-null values
JNJ     2268  non-null values
KO      2268  non-null values
MSFT    2268  non-null values
PEP     2268  non-null values
SPX     2268  non-null values
XOM     2268  non-null values
dtypes: float64(10)


The below is a TimeSeries  and is in the format I need

> dataAAPL:

Date
1993-01-04 00:00:00+00:00    7300
1993-01-05 00:00:00+00:00    7312


2001-12-28 00:00:00+00:00    3615
2001-12-31 00:00:00+00:00    3555
Name: AAPL  Length: 2268


Note  there isn't return data here  but prices instead They're adjusted (by Zipline's load_from_yahoothough  from reading the source  really by functions in pandas) for dividends  splits  etc  so there's an isomorphism (less the initial price) between that and my return data (so  no problem here) 

(EDIT: Let me know if you'd like me to write what I have  or attach my iPython notebook or a gist; I just doubt it'd be helpful  but I can absolutely do it if requested)",142240,,142240.0,2013-01-25 19:36:55,2013-01-25 22:36:43,Transforming financial data from postgres to pandas dataframe for use with Zipline,<python><pandas><etl><zipline>,1.0,,2.0,
14440187,1,14442343.0,2013-01-21 13:55:46,1,36,"I am new to Python and the Pandas library  so apologies if this is a trivial question I am trying to rank a Timeseries over a rolling window of N days I know there is a rank function but this function ranks the data over the entire timeseries I don't seem to be able to find a rolling rank function 
Here is an example of what I am trying to do:

           A

01-01-2013 100
02-01-2013 85
03-01-2013 110
04-01-2013 60
05-01-2013 20
06-01-2013 40


If I wanted to rank the data over a rolling window of 3 days  the answer should be:

           Ranked_A

01-01-2013 NaN
02-01-2013 Nan
03-01-2013 1
04-01-2013 3
05-01-2013 3
06-01-2013 2


Is there a built-in function in Python that can do this? Any suggestion?
Many thanks",1996758,,,,2013-01-21 15:52:58,rank data over a rolling window in pandas DataFrame,<pandas><time-series><rank>,2.0,,,
14446744,1,,2013-01-21 20:33:03,0,57,"I have enormous files that look like this:

05/31/2012 15:30:00029 130625 1 E 0  130625

05/31/2012 15:30:00029 130625 8 E 0  130625

I can easily read them using the following:

  pdread_csv(gzipopen(""myfilegz"")  header=None names=
  [""date"" ""time"" ""price"" ""size"" ""type"" ""zero"" ""empty"" ""last""]  parse_dates=[[0 1]])


Is there any way to efficiently parse dates like this into pandas timestamps?  If not  is there any guide for writing a cython function that can passed to date_parser= ?

I tried writing my own parser function and it still takes too long for the project I am working on  ",1064197,,1064197.0,2013-01-22 03:46:09,2013-01-22 03:46:09,How to efficiently handle pandas read_csv with datetime index,<python><pandas>,1.0,2.0,2.0,
14513638,1,,2013-01-25 00:56:37,0,31,"my data looks like this:

 date  cola  colb  colc
 1 10  
 2 11  
 3 12  
 4 13  
 1  14 
 2  15 
 3  16 
 4  17 
 1   17
 2   18
 3   19
 4 13  20


I'd like to merge the rows based on the first column and have the output look like this:

 date  cola  colb  colc
 1 10 14 17
 2 11 15 18
 3 12 16 19
 4 13 17 20


I can't guarantee there won't be any conflicts  so I'd like to be able to choose the max or mean",1647854,,,,2013-01-25 01:03:18,pandas: merge rows on timestamp,<python><merge><pandas><rows>,1.0,,,
14531544,1,14553291.0,2013-01-25 22:46:38,0,82,"I've built Matplotlib in Python such that when I import it or any part of it  it works fine and graphs fine

However  I've been going through ""Python for Data Analysis "" and in Ch2's IPython walkthrough of name trends  when I run the following

total_birthsplot(title='Total births by sex and year')


I get

ImportError: dlopen(/Library/Python/27/site-packages/matplotlib/_pngso  2): Symbol not found: _png_create_info_struct


Most of the fixes I've found around the Web solve the issue in Python with Matplotlib but not in IPython in particular

Below is the entire IPython output:

ImportError                               Traceback (most recent call last)
 in ()
----> 1 total_birthsplot(title='Total births by sex and year')

/Library/Python/27/site-packages/pandas/tools/plottingpyc in plot_frame(frame  x  y      subplots  sharex  sharey  use_index  figsize  grid  legend  rot  ax  style  title  xlim  ylim  logx  logy  xticks  yticks  kind  sort_columns  fontsize  secondary_y  **kwds)
   1453                      logy=logy  sort_columns=sort_columns 
   1454                      secondary_y=secondary_y  **kwds)
-> 1455     plot_objgenerate()
   1456     plot_objdraw()
   1457     if subplots:

/Library/Python/27/site-packages/pandas/tools/plottingpyc in generate(self)
    732         self_args_adjust()
    733         self_compute_plot_data()
--> 734         self_setup_subplots()
    735         self_make_plot()
    736         self_post_plot_logic()

/Library/Python/27/site-packages/pandas/tools/plottingpyc in _setup_subplots(self)
    781         else:
    782             if selfax is None:
--> 783                 fig = selfpltfigure(figsize=selffigsize)
    784                 ax = figadd_subplot(111)
    785                 ax = self_maybe_right_yaxis(ax)

/Library/Python/27/site-packages/pandas/libso in pandaslibcache_readonly__get__ (pandas/libc:27324)()

/Library/Python/27/site-packages/pandas/tools/plottingpyc in plt(self)
    859     @cache_readonly
    860     def plt(self):
--> 861         import matplotlibpyplot as plt
    862         return plt
    863 

/Library/Python/27/site-packages/matplotlib/pyplotpy in ()
     24 from matplotlibcbook import dedent  silent_list  is_string_like  is_numlike
     25 from matplotlib import docstring
---> 26 from matplotlibfigure import Figure  figaspect
     27 from matplotlibbackend_bases import FigureCanvasBase
     28 from matplotlibimage import imread as _imread

/Library/Python/27/site-packages/matplotlib/figurepy in ()
     30 
     31 from matplotlib import _image
---> 32 from matplotlibimage import FigureImage
     33 
     34 import matplotlibcolorbar as cbar

    /Library/Python/27/site-packages/matplotlib/imagepy in ()
         20 # For clarity  names from _image are given explicitly in this module:
         21 import matplotlib_image as _image
    ---> 22 import matplotlib_png as _png
         23 
         24 # For user convenience  the names from _image are also imported into

ImportError: dlopen(/Library/Python/27/site-packages/matplotlib/_pngso  2): Symbol not found: _png_create_info_struct
  Referenced from: /Library/Python/27/site-packages/matplotlib/_pngso
  Expected in: flat namespace
 in /Library/Python/27/site-packages/matplotlib/_pngso
",1321181,,380231.0,2013-01-26 03:13:26,2013-01-27 22:55:19,Matplotlib in IPython + Pandas: ImportError Symbol not found: _png_create_info_struct,<python><osx><matplotlib><pandas><ipython>,1.0,7.0,,
14422976,1,,2013-01-20 08:48:56,2,92,"I have installed Pandas on Python 33  and coded like this:

import csv
import pandas
from pandas import DataFrame

csvdata = pandasread_csv('datafilecsv')
df = DataFrame(csvdata)


It comes with the following error message:

cannot import name hashtable
Traceback (most recent call last):
  File ""C:\Users\document\test4py""  line 5  in 
    import pandas
  File ""C:\Python33\lib\site-packages\pandas\__init__py""  line 6  in 
    from  import hashtable  tslib  lib
ImportError: cannot import name hashtable


Could anyone help me figure out how to solve this error?  Python and Pandas were successfully installed  Thanks ",1994194,,,,2013-01-20 19:37:25,Python3.3_ImportError: cannot import name hashtable?,<python-3.x><pandas>,1.0,1.0,,
14511752,1,14512542.0,2013-01-24 22:12:04,2,28,"I create a pandas scatter-matrix usng the following code:

import numpy as np
import pandas as pd

a = nprandomnormal(1  3  100)
b = nprandomnormal(3  1  100)
c = nprandomnormal(2  2  100)

df = pdDataFrame({'A':a 'B':b 'C':c})
pdscatter_matrix(df  diagonal='kde')


This result in the following scatter-matrix:


The first row has no ytick labels  the 3th column no xtick labels  the 3th item 'C' is not labeled

Any idea how to complete this plot with the missing labels ?  ",1915862,,,,2013-01-25 04:54:03,pandas 3x3 scatter-matrix missing labels,<python><matplotlib><pandas>,1.0,,,
14511839,1,,2013-01-24 22:17:41,2,32,"I cannot seem to use read_clipboard() I have tried copying some of my data  the example data in the docs  and finally just the number 1 As you can see:

In [1]: %paste
1
## -- End pasted text --
Out[1]: 1

In [2]: pdread_clipboard()
Segmentation fault (core dumped)


What am I doing wrong? Has anyone else had trouble using this?

I use IPython 0121 and the latest dev build of pandas I'm on Ubuntu 1204

In [1]: pd__version__
Out[1]: '0101'


And just to be safe:

dallan@dielectric-pc:~/pandas-danielballan$ git fetch upstream master
From git://githubcom/pydata/pandas
 * branch            master     -> FETCH_HEAD
dallan@dielectric-pc:~/pandas-danielballan$ git merge upstream/master
Already up-to-date
dallan@dielectric-pc:~/pandas-danielballan$ git status
# On branch master
nothing to commit (working directory clean)
dallan@dielectric-pc:~/pandas-danielballan$ 
",1221924,,1221924.0,2013-01-25 05:32:08,2013-01-25 05:32:08,pd.read_clipboard() seg fault,<python><clipboard><pandas>,,5.0,,
14567210,1,14567693.0,2013-01-28 17:02:27,1,33,"I have used this code to compute values for the different quality metrics of each user in each cluster

>>> for name  group in dfgroupby([""Cluster_id""  ""User""]):
     print 'group name:'  name
     print 'group rows:'
     print group
     print 'counts of Quality values:'
     print group[""Quality""]value_counts()
     raw_input()
     


But now I get the output as 

group rows:
                tag                       user                    quality  cluster
676    black fabric  http://stevenl/user_1002          usefulness-useful        1
708      blond wood  http://stevenl/user_1002          usefulness-useful        1
709      blond wood  http://stevenl/user_1002    problematic-misspelling        1
1410         eames?  http://stevenl/user_1002      usefulness-not_useful        1
1411         eames?  http://stevenl/user_1002  problematic-misperception        1
3649  rocking chair  http://stevenl/user_1002          usefulness-useful        1
3650  rocking chair  http://stevenl/user_1002  problematic-misperception        1
counts of Quality Values:
usefulness-useful            3
problematic-misperception    2
usefulness-not_useful        1
problematic-misspelling      1


What I would like to do now is to have a check condition ie:

if quality==usefulness-useful:
 good = good + 1
else:
 bad = bad + 1


I tried writing the output:

counts of Quality Values:
usefulness-useful            3
problematic-misperception    2
usefulness-not_useful        1
problematic-misspelling      1


into a variable and tried traversing the variable row by row  but it does not work Can someone give me suggestions  on how to do computations on certain rows",1992696,,,,2013-01-28 17:28:30,How to perform functions over groupby results in pandas in python?,<python><csv><pandas>,1.0,1.0,,
14514046,1,14514757.0,2013-01-25 01:47:05,1,80,"I have a CSV file in the following format:

DATES  01-12-2010  01-12-2010  01-12-2010  02-12-2010  02-12-2010  02-12-2010
UNITS  Hz  kV  MW   Hz  kV  MW
Interval                                                      
00:15  4982  3373755  3465  4992  339009  3633 
00:30  499  337722  3534  4989  338382  3765 
00:45  4994  338316  335  5009  3407745  3741 
01:00  4986  3394875  3091  5018    3420945  3611 
01:15  4997  342243   2728   5011   343596  3324 
01:30   5002   343332  2691  5012   34452  3103 
01:45   5001   341286  3126  50  339306  3886 
02:00   5008   339141  3496  5014   3399165  3831 
02:15   5007   3384975  3533  5001  339537  3978 
02:30   4997   340263  3363  5007   338547  4148 


I would like to convert the above to a dataframe in the following format:

                    Hz      kV          MW
DATES_Interval
01-12-2010 00:15    4982   3373755    3465
01-12-2010 00:30    499    337722     3534
01-12-2010 00:45    4994   338316     335
01-12-2010 01:00    4986   3394875    3091
01-12-2010 01:15    4997   342243     2728
01-12-2010 01:30    5002   343332     2691
01-12-2010 01:45    5001   341286     3126
01-12-2010 02:00    5008   339141     3496
01-12-2010 02:15    5007   3384975    3533
01-12-2010 02:30    4997   340263     3363
02-12-2010 00:15    4992   339009     3633
02-12-2010 00:30    4989   338382     3765
02-12-2010 00:45    5009   3407745    3741
02-12-2010 01:00    5009   3407745    3741
02-12-2010 01:15    5011   343596     3324
02-12-2010 01:30    5012   34452      3103
02-12-2010 01:45    50      339306     3886
02-12-2010 02:00    5014   3399165    3831
02-12-2010 02:15    5001   339537     3978
02-12-2010 02:30    5007   338547     4148


How do I do this with pandas?",1104232,,,,2013-01-25 19:23:33,Convert csv file to pandas dataframe,<pandas>,2.0,0.0,,
14558387,1,14558806.0,2013-01-28 08:46:30,1,63,"I have a numpy array of dtype = object (which are actually lists of various data types) So it makes a 2D array because I have an array of lists (?) I want to copy every row & only certain columns of this array to another array I stored data in this array from a csv file This csv file contains several fields(columns) and large amount of rows Here's the code chunk I used to store data into the array

data = npzeros((401125 )  dtype = object)
for i  row in enumerate(csv_file_object):
    data[i] = row


data can be basically depicted as follows

column1  column2  column3  column4  column5 
1         none     2       'gona'    53
2         34       2       'gina'    55
3         none     2       'gana'    51
4         43       2       'gena'    50
5         none     2       'guna'    57
                  
                  
                  


There're unwanted fields in the middle that I want to remove Suppose I don't want column3
How do I remove only that column from my array? Or copy only relevant columns to another array?  ",1259621,,,,2013-01-28 09:31:45,how to read from an array without a particular column in python,<python><numpy><python-2.7><pandas><data-analysis>,3.0,3.0,,
14569223,1,,2013-01-28 19:03:20,3,33,"I use TimeGrouper from pandastseriesresample to sum monthly return to 6M as follows:

6m_return = monthly_returngroupby(TimeGrouper(freq='6M'))aggregate(numpysum)


where monthly_return is like:

2008-07-01    0003626
2008-08-01    0001373
2008-09-01    0040192
2008-10-01    0027794
2008-11-01    0012590
2008-12-01    0026394
2009-01-01    0008564
2009-02-01    0007714
2009-03-01   -0019727
2009-04-01    0008888
2009-05-01    0039801
2009-06-01    0010042
2009-07-01    0020971
2009-08-01    0011926
2009-09-01    0024998
2009-10-01    0005213
2009-11-01    0016804
2009-12-01    0020724
2010-01-01    0006322
2010-02-01    0008971
2010-03-01    0003911
2010-04-01    0013928
2010-05-01    0004640
2010-06-01    0000744
2010-07-01    0004697
2010-08-01    0002553
2010-09-01    0002770
2010-10-01    0002834
2010-11-01    0002157
2010-12-01    0001034


The 6m_return is like:

2008-07-31    0003626
2009-01-31    0116907
2009-07-31    0067688
2010-01-31    0085986
2010-07-31    0036890
2011-01-31    0015283


However I want to get the 6m_return starting 6m from 7/2008 like the following:

2008-12-31    
2009-06-31    
2009-12-31    
2010-06-31    
2010-12-31    


Tried the different input options (ie loffset) in TimeGrouper but doesn't work
Any suggestion will be really appreciated!",2019264,,1240268.0,2013-01-28 19:07:14,2013-01-28 19:07:14,"TimeGrouper, pandas",<group-by><data.frame><pandas>,,2.0,,
14530556,1,,2013-01-25 21:19:09,1,31,How do I resample a time series in pandas to a weekly frequency where the weeks start on an arbitrary day? I see that there's an optional keyword base but it only works for intervals shorter than a day,193277,,,,2013-01-25 22:11:16,Resample time series in pandas to a weekly interval,<pandas>,1.0,,,
14573453,1,14573561.0,2013-01-28 23:58:44,1,19,"I have a pandas dataframe like following

   mp  me        rt                                                         
0  0   1   1987366                                                         
1  1   1   1769593                                                         
2  2   1   1416274                                                         
3  3   1   1650428                                                         
4  4   1   1882780                                                         
5  0   2   1955086                                                         
6  1   2   1729387                                                         
7  2   2   1490797                                                         
8  3   2   1546333                                                         
9  4   2   1933006                 


I'd like to generate a new dataframe as folloing                        

   mp    me=1       me=2                                                    
0  0   1987366    1955086                                                 
1  1   1769593    1729387                                                 
2  2   1416274    1490797                                                 
3  3   1650428    1546333                                                 
4  4   1882780    1933006                                                 


I tried to use a for loop but unsuccessful                

for j in range(1 3):
    f[str(j)] = pdDataFrame(a[a['me']==j]['rt'])


Any idea how to do it and effectively?

Thanks
Dan",1591487,,,,2013-01-29 00:10:14,rearrange python pandas dataframe,<python><data.frame><pandas>,1.0,,,
14586898,1,,2013-01-29 15:55:01,1,27,"I haven't figure out how to do pickle load/save's between python 2 and 3 with pandas DataFrames There is a 'protocol' option in the pickler that I've played with unsuccessfully but I'm hoping someone has a quick idea for me to try Here is the code to get the error:

python27

>>> import pandas; from pylab import *
>>> a = pandasDataFrame(randn(10 10))
>>> asave('a2')
>>> a = pandasDataFrameload('a2')
>>> a = pandasDataFrameload('a3')
Traceback (most recent call last):
  File """"  line 1  in 
  File ""/usr/local/lib/python27/site-packages/pandas-0101-py27-linux-x86_64egg/pandas/core/genericpy""  line 30  in load
    return comload(path)
  File ""/usr/local/lib/python27/site-packages/pandas-0101-py27-linux-x86_64egg/pandas/core/commonpy""  line 1107  in load
    return pickleload(f)
ValueError: unsupported pickle protocol: 3


python3

>>> import pandas; from pylab import *
>>> a = pandasDataFrame(randn(10 10))
>>> asave('a3')
>>> a = pandasDataFrameload('a3')
>>> a = pandasDataFrameload('a2')
Traceback (most recent call last):
  File """"  line 1  in 
  File ""/usr/local/lib/python33/site-packages/pandas-0101-py33-linux-x86_64egg/pandas/core/genericpy""  line 30  in load
    return comload(path)
  File ""/usr/local/lib/python33/site-packages/pandas-0101-py33-linux-x86_64egg/pandas/core/commonpy""  line 1107  in load
    return pickleload(f)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xf4 in position 0: ordinal not in range(128)


Maybe expecting pickle to work between python version is a bit optimistic?",287238,,,,2013-01-29 15:55:01,pandas.DataFrame.load/save between python2 and python3: pickle protocol issues,<python><pandas>,,2.0,,
14525942,1,,2013-01-25 16:22:40,2,32,"Given:

foo = pdSeries(index=pdbdate_range(datetimedate(2000 1 1) datetimedate(2001 1 1)))
fooresample(""BMS"")
fooresample(""BMS"")resample(""B"")


The first resample starts at 2000-01-03 and the second resample starts at 2000-01-04

Is this a bug or am I missing a conceptual understanding of how resampling works?

Thanks in advance!",365781,,,,2013-01-25 16:22:40,Pandas resampling skips first date of timeseries,<python><pandas>,,3.0,,
14542145,1,14544216.0,2013-01-26 22:18:34,2,53,"I'm trying to transform a (well  many) column of return data to a column of closing prices In Clojure  I'd use reductions  which is like reduce  but returns a sequence of all the intermediate values 

eg

$ c

012
-13
023
017
029
-011

# something like this
$ creductions(init=1  lambda accumulator  ret: accumulator * (1 + ret)) 

112
097
120
140
181
161


NB: The actual closing price doesn't matter  hence using 1 as the initial value I just need a ""mock"" closing price

My data's actual structure is a DataFrame of named columns of TimeSeries I guess I'm looking for a function similar applymap  but I'd rather not do something hacky with that function and reference the DF from within it (which I suppose is one solution to this problem?)

Additionally  what would I do if I wanted to keep the returns data  but have the closing ""price"" with it? Should I return a tuple instead  and have the TimeSeries be of the type (returns  closing_price)?",142240,,,,2013-01-27 06:11:13,Reductions down a column in Pandas,<python><pandas>,2.0,2.0,1.0,
14550441,1,14550887.0,2013-01-27 18:09:06,1,35,"I am trying to read a csv file using pandas and the file has a column called Tags which consist of user provided tags and has tags like -   """"  '' 1950's  16th-century Since these are user provided  there are many special characters which are entered by mistake as well The issue is that I cannot open the csv file using pandas read_csv It shows error:Cparser  error tokenizing data Can someone help me with reading the csv file into pandas?",1992696,,,,2013-01-27 18:49:51,Problems reading CSV file with commas and characters in pandas,<python><csv><special-characters><pandas>,1.0,5.0,,
14552482,1,14554094.0,2013-01-27 21:25:28,1,85,"I have another problem with my data set Basically  there is a list of genes with associated features including position numbs (columns 3 and 4) and strand orientation (+ or -) I am trying to do a calculation with the positions to make them relative to the start codon TYPE (second column) for each gene  rather than the entire genome (as it is now) The problem is that  the calculation is only performed on the + STRAND sequences  the - STRAND sequences are not showing up in the output Below is a sample of the data set  my code  the output  and what I've tried

Here's the data set:

    GENE_ID TYPE    POS1    POS2    STRAND
PITG_00002  start_codon 10520   10522   -
PITG_00002  stop_codon  10097   10099   -
PITG_00002  exon    10474   10522   -
PITG_00002  CDS 10474   10522   -
PITG_00002  exon    10171   10433   -
PITG_00002  CDS 10171   10433   -
PITG_00002  exon    10097   10114   -
PITG_00002  CDS 10100   10114   -
PITG_00003  start_codon 38775   38777   +
PITG_00003  stop_codon  39069   39071   +
PITG_00003  exon    38775   39071   +
PITG_00003  CDS 38775   39068   +


Here is the code:

import numpy
import pandas
import pandas as pd
import sys

sysstdout = open(""outtry2txt""  ""w"")
data = pdread_csv('pinfestans-edited2csv'  sep='\t')
groups = datagroupby(['STRAND'  'GENE_ID'])

corrected = []

for (direction  gene_name)  group in groups:
    ##print direction gene_name
    if groupindex[groupTYPE=='start_codon']:
        start_exon = groupindex[groupTYPE=='exon'][0]
    if direction == '+':
        group['POSA'] = 1 + abs(groupPOS1 - groupPOS1[start_exon])
        group['POSB'] = 1 + abs(groupPOS2 - groupPOS1[start_exon])
    else:
        group['POSA'] = 1 - abs(groupPOS2 - groupPOS2[start_exon])
        group['POSB'] = 1 - abs(groupPOS1 - groupPOS2[start_exon])
    ##print group
    correctedappend(group)


Here is a sample of the output:

     + PITG_00003
    GENE_ID     TYPE         POS1   POS2   STRAND  POSA  POSB
8   PITG_00003  start_codon  38775  38777  +       1     3   
9   PITG_00003  stop_codon   39069  39071  +       295   297 
10  PITG_00003  exon         38775  39071  +       1     297 
11  PITG_00003  CDS          38775  39068  +       1     294 


Previously I was getting an array value error (Tab delimited dataset ValueError truth of array with more than one element is ambiguous error) but that has been taken care of So next I tried only doing this part:

import numpy
import pandas
import pandas as pd
import sys

##sysstdout = open(""outtry2txt""  ""w"")
data = pdread_csv('pinfestans-edited2csv'  sep='\t')# 
              #converters={'STRAND': lambda s: s[0]})
groups = datagroupby(['STRAND'  'GENE_ID'])

corrected = []

for (direction  gene_name)  group in groups:
    print direction gene_name


And the output printed out all the GENE_IDs and their STRAND symbol (+ or -)  and it did it for both the + and - sequences So somewhere below that it isn't selecting any of the sequences with - in the STRAND column

So I tried adding this to the original code:

if direction == '+':
    group['POSA'] = 1 + abs(groupPOS1 - groupPOS1[start_exon])
    group['POSB'] = 1 + abs(groupPOS2 - groupPOS1[start_exon])
elif direction == '-':
    group['POSA'] = 1 - abs(groupPOS2 - groupPOS2[start_exon])
    group['POSB'] = 1 - abs(groupPOS1 - groupPOS2[start_exon])
else:
    break
print group
# put into the result array
correctedappend(group)


and this is the very end of the output  it printed the first - and then froze for awhile before ending:

+
        GENE_ID     TYPE         POS1    POS2    STRAND  POSA  POSB
134991  PITG_23350  start_codon  161694  161696  +       516   518 
134992  PITG_23350  stop_codon   162135  162137  +       957   959 
134993  PITG_23350  exon         161179  162484  +       1     1306
134994  PITG_23350  CDS          161694  162134  +       516   956 
-
",1784467,,1784467.0,2013-01-28 00:43:55,2013-01-28 00:59:39,For loop skipping large part of data set,<python><numpy><pandas>,1.0,3.0,,
14606663,1,,2013-01-30 14:44:12,0,50,"I would like to create a new class which inherits from pandasSeries I usually don't have any problem creating a child class in python  but I am having problems in this case

Here is a simple inheritance scheme:

class Test(object):
    def __new__(cls  *args  **kwargs):
        print ""new Test""
        return object__new__(cls  *args  **kwargs)
    def __init__(self):
        print ""init Test""

class A(Test):
    def __new__(cls  *args  **kwargs):
        print ""new A""
        return Test__new__(cls  *args  **kwargs)
    def __init__(self):
        print ""init A""

print ""creating an instance of A""
a = A()
print ""type: ""  type(a)


which outputs:

creating an instance of A
new A
new Test
init A
type:    


now lets try it with a Series:

import pandas as pd
class subSeries(pdSeries):
    def __new__(cls  *args  **kwargs):
        print ""new subSeries""
        return pdSeries__new__(cls  *args  **kwargs)

    def __init__(self):
        print ""init subSeries""
print ""creating an instance of subSeries""
s = subSeries()
print ""type: ""  type(s)


and we get:

creating an instance of subSeries
new subSeries
type:  


why is s a Series and not a subSeries?",1300719,,,,2013-01-30 14:44:12,"In python, why can't I inherit pandas.Series in a simple manner?",<python><inheritance><pandas><series>,,3.0,,
14586608,1,,2013-01-29 15:39:16,2,36,"I'm learning how to write a pandas dataFrame to SqlLite db

I went in one example code:

import pandas as pd
import pandasiosql as pd_sql
import sqlite3 as sql

con = sqlconnect(""/home/msalese/Documents/ipyNotebooks/tmpdb"")
df =pdDataFrame({'TestData':[1 2 3 4 5 6 7 8 9]})
pd_sqlwrite_frame(df  ""tbldata2""  con)


But above code rise an exception:

---------------------------------------------------------------------------
InterfaceError                            Traceback (most recent call last)
 in ()
----> 1 pd_sqlwrite_frame(df  ""tbldata2""  con)

/opt/epdFree732/lib/python27/site-packages/pandas-0101-py27-linux-x86_64egg/pandas/io/sqlpyc in write_frame(frame  name  con  flavor  if_exists  **kwargs)
208     if func is None:
209         raise NotImplementedError
--> 210     func(frame  name  safe_names  cur)
211     curclose()
212     concommit()

/opt/epdFree732/lib/python27/site-packages/pandas-0101-py27-linux-x86_64egg/pandas/io/sqlpyc in _write_sqlite(frame  table  names  cur)
219         table  col_names  wildcards)
220     data = [tuple(x) for x in framevalues]
--> 221     curexecutemany(insert_query  data)
222 
223 def _write_mysql(frame  table  names  cur):

InterfaceError: Error binding parameter 0 - probably unsupported type


I think that the problem is on code line 220
If I try :

[tuple(x) for x in dfvalues]


the result is:

[(1 )  (2 )  (3 )  (4 )  (5 )  (6 )  (7 )  (8 )  (9 )]


may be commas give noise to sqlite db

I'm not sure  can someone give me an hint  please ?",1552041,,,,2013-01-29 15:39:16,Pandas DataFrame to SqlLite,<python><sqlite><pandas>,,2.0,1.0,
14590638,1,14591748.0,2013-01-29 19:24:51,0,35,"I have a script that reads system log files into pandas dataframes and produces charts from those The charts are fine for small data sets But when I face larger data sets due to larger timeframe of data gathering  the charts become too crowded to discern

I am planning to resample the dataframe so that if the dataset passes certain size  I will resample it so there are ultimately only the SIZE_LIMIT number of rows This means I need to filter the dataframe so every n = actual_size/SIZE_LIMIT rows would aggregated to a single row in the new dataframe The agregation can be either average value or just the nth row taken as is

I am not fully versed with pandas  so may have missed some obvious means  ",814907,,,,2013-01-29 20:35:41,Pandas dataframe resample at every nth row,<pandas>,2.0,,,
14614512,1,,2013-01-30 21:51:56,1,93,"I am using python for some data analysis I have two tables  the first (lets call it 'A') has 10 million rows and 10 columns and the second ('B') has 73 million rows and 2 columns They have 1 column with common ids and I want to intersect the two tables based on that column In particular I want the inner join of the tables
I could not load the table B on memory as a pandas dataframe to use the normal merge function on pandas I tried by reading the file of table B on chunks  intersecting each chunk with A and the concatenating these intersections (output from inner joins) This is OK on speed but every now and then this gives me problems and spits out a segmentation fault  no so great This error is difficult to reproduce but it happens on two different machines (Mac OS Snow Leopard and UNIX  RedHat) I finally tried with the combination of Pandas and PyTables by writing table B to disk and then iterating over table A and selecting from table B the matching rows This last options works but it is slow Table B on pytables has been indexed already by default
I wonder if you could suggest any other idea of how to tackle this problem 
Thanks in advance",2027051,,,,2013-01-31 03:35:09,merging two tables with millions of rows in python,<python><join><merge><pandas><pytables>,1.0,2.0,1.0,
14539992,1,14540509.0,2013-01-26 18:15:35,2,38,"I am trying to go through every row in a DataFrame index and remove all rows that are not between a certain time

I have been looking for solutions but none of them separate the Date from the Time  and all I want to do is drop the rows that are outside of a Time range",546624,,,,2013-01-26 20:09:52,Pandas Drop Rows Outside of Time Range,<python><pandas>,1.0,,,
14580684,1,14600682.0,2013-01-29 10:25:46,2,48,"I have an IPython Notebook that is using Pandas to back-test a rule-based trading system

I have a function that accepts various scalars and functions as parameters and outputs a stats pack as some tables and a couple of plots

For automation  I want to be able to format this nicely into a ""page"" and then call the function in a loop while varying the inputs and have it output a number of pages for comparison  all from a single notebook cell

The approach I am taking is to create IpyTables and then call _repr_html_()  building up the HTML output along the way so that I can eventually return it from the function that runs the loop

How can I capture the output of the plots this way - matplotlib subplot objects don't seem to implement _repr_html_()?

Feel free to suggest another approach entirely that you think might equally solve the problem

TIA",1583083,,,,2013-01-30 09:37:19,How to grab matplotlib plot as html in ipython notebook?,<python><matplotlib><pandas><ipython-notebook>,1.0,7.0,0.0,
14627380,1,14630250.0,2013-01-31 13:52:00,3,48,"I am trying to format a table  such that data in each column are formatted in a style depending on their values (similar to conditional formatting in spreadsheet programs) How can I achieve that in pandas using the HTML formatter?

A typical use case is highlighting significant values in a table For example:

    correlation  p-value
0   05          01
1   01          08
2   09          *001*


pandas allows to define custom formatters for HTML output - to obtain above output one could use:

from pandas core import format
from StringIO import StringIO
buf = StringIO()
df = pdDataFrame({'correlation':[05  01 09]  'p_value':[01 08 001]})
fmt = formatDataFrameFormatter(df  
          formatters={'p_value':lambda x: ""*%f*"" % x if x tags in the HTML output  which could be then formatted using CSS stylesheet The above would then become:


      correlation
      p_value
    0
       05
       010
    1
       01
       080
    2
       09
       001
    


Edit: As suggested by @Andy-Hayden I can add formatting by simply replacing stars with  in my example:

from pandas core import format
from StringIO import StringIO
buf = StringIO()
significant = lambda x: ""%f"" % x if x",74342,,74342.0,2013-01-31 16:34:30,2013-01-31 16:38:02,pandas: HTML output with conditional formatting,<python><html><css><data.frame><pandas>,1.0,,1.0,
14529838,1,14530027.0,2013-01-25 20:26:45,2,55,"The docs show how to apply multiple functions on a groupby object at a time using a dict with the output column names as the keys:

In [563]: grouped['D']agg({'result1' : npsum 
   :                   'result2' : npmean})
   :
Out[563]: 
      result2   result1
A                      
bar -0579846 -1739537
foo -0280588 -1402938


However  this only works on a Series groupby object And when a dict is similarly passed to a groupby DataFrame  it expects the keys to be the column names that the function will be applied to

What I want to do is apply multiple functions to several columns (but certain columns will be operated on multiple times) Also  some functions will depend on other columns in the groupby object (like sumif functions) My current solution is to go column by column  and doing something like the code above  using lambdas for functions that depend on other rows But this is taking a long time  (I think it takes a long time to iterate through a groupby object) I'll have to change it so that I iterate through the whole groupby object in a single run  but I'm wondering if there's a built in way in pandas to do this somewhat cleanly

For example  I've tried something like 

groupedagg({'C_sum' : lambda x: x['C']sum() 
             'C_std': lambda x: x['C']std() 
             'D_sum' : lambda x: x['D']sum()} 
             'D_sumifC3': lambda x: x['D'][x['C'] == 3]sum()  )


but as expected I get a KeyError (since the keys have to be a column if agg is called from a DataFrame)

Is there any built in way to do what I'd like to do  or a possibility that this functionality may be added  or will I just need to iterate through the groupby manually?

Thanks",386279,,,,2013-01-25 21:28:31,Apply multiple functions to multiple groupby columns,<python><group-by><aggregate-functions><pandas>,1.0,,,
14583576,1,14586194.0,2013-01-29 13:02:37,0,60,"I have a pandas DataFrame with a structure as follows:

data = DataFrame({'Cat1':['A'  'B'  'B'  'C']  'Cat2': ['X'  'Y'  'Z'  'X']  'Counter': [0  4  1  5]})


Now I want to add a separate column with a ranking by Cat1 (so in this case: 1 3 2 4 as new column) My first try was:

data['ranking'] = data['ranking'] + data[data['Cat1'] == 'A']['Counter']rank(ascending=0)fillna(0)


However  when I add the second Category (data['Cat1']=='B' as condition)  it overrides the existing values This is what I expected  as I have to use add() as far as I understand However  the same happens with the following script:

data['ranking']add(data[data['Cat1']=='A']['Counter']rank(ascending=0))


Also overrides all values where Cat1==B with NA How can I avoid this?

Thanks in advance!

-----------------------EDIT!!------------------

Let's say this is my table:



And ordinary rank would give me a ranking of all numbers 1 through 12 Now what I need is a ranking based on the category and as an additional column in the original python DataFrame 

Hence  the last column should look say:
2 (second-ranked value of a)
3 (third-ranked value of a)
1 (first-ranked value of a)
1 (first-ranked value of b)
1 (first-ranked value of c)
5
2
",1563867,,1563867.0,2013-01-29 16:53:54,2013-01-29 18:51:05,Sorting panda DataFrames based on criteria,<python><pandas>,1.0,7.0,,
14631139,1,,2013-01-31 16:59:30,2,54,"Consider you've got some unevenly time series data:

import pandas as pd
import random as randy
ts = pdSeries(range(1000) index=randysample(pddate_range('2013-02-01 09:00:00000000' periods=1e6 freq='U') 1000))sort_index()
print tshead()


2013-02-01 09:00:00002895    995
2013-02-01 09:00:00003765    499
2013-02-01 09:00:00003838    797
2013-02-01 09:00:00004727    295
2013-02-01 09:00:00006287    253


Let's say I wanted to do the rolling sum over a 1ms window to get this:

2013-02-01 09:00:00002895    995
2013-02-01 09:00:00003765    499 + 995
2013-02-01 09:00:00003838    797 + 499 + 995
2013-02-01 09:00:00004727    295 + 797 + 499
2013-02-01 09:00:00006287    253


Currently  I cast everything back to longs and do this in cython  but is this possible in pure pandas? I'm aware that you can do something like asfreq('U') and then fill and use the traditional functions but this doesn't scale once you've got more than a toy # of rows

As a point of reference  here's a hackish  not fast Cython version:

%%cython
import numpy as np
cimport cython
cimport numpy as np

ctypedef npdouble_t DTYPE_t

def rolling_sum_cython(npndarray[long ndim=1] times  npndarray[double ndim=1] to_add  long window_size):
    cdef long t_len = timesshape[0]  s_len = to_addshape[0]  i =0  win_size = window_size  t_diff  j  window_start
    cdef npndarray[DTYPE_t  ndim=1] res = npzeros(t_len  dtype=npdouble)
    assert(t_len==s_len)
    for i in range(0 t_len):
        window_start = times[i] - win_size
        j = i
        while times[j]>= window_start and j>=0:
            res[i] += to_add[j]
            j-=1
    return res   


Demonstrating this on a slightly larger series:

ts = pdSeries(range(100000) index=randysample(pddate_range('2013-02-01 09:00:00000000' periods=1e8 freq='U') 100000))sort_index()

%%timeit
res2 = rolling_sum_cython(tsindexastype(int64) tsvaluesastype(double) long(1e6))
1000 loops  best of 3: 156 ms per loop
",1784599,,1784599.0,2013-02-01 18:11:38,2013-02-01 18:11:38,Pandas Rolling Computations on Sliding Windows (Unevenly spaced),<pandas>,2.0,,1.0,
14575129,1,,2013-01-29 03:20:30,2,51,"I have a DataFrame that is indexed with DateTime and on the histogram it shows up with counts  Each count is 20 secs
How can I change the y axis to display mins  hours  or days  instead of counts?

Here's the top five rows of my DataFrame:

from pandaslib import Timestamp
df = pdDataFrame({'A': {Timestamp('2013-01-20 00:00:16726000'): 014746094 
                         Timestamp('2013-01-20 00:00:36726002'): 013964844 
                         Timestamp('2013-01-20 00:00:56726004'): 013574219 
                         Timestamp('2013-01-20 00:01:16725997'): 014355469 
                         Timestamp('2013-01-20 00:01:36725999'): 014746094} 
                   'B': {Timestamp('2013-01-20 00:00:16726000'): 35716574 
                         Timestamp('2013-01-20 00:00:36726002'): 35716574 
                         Timestamp('2013-01-20 00:00:56726004'): 35716574 
                         Timestamp('2013-01-20 00:01:16725997'): 35716574 
                         Timestamp('2013-01-20 00:01:36725999'): 35716574}})
dfplot()


matplotlib v120
pandas 0101



bump",1625012,,1625012.0,2013-02-02 15:03:15,2013-02-02 15:03:15,Pandas Histogram - y axis in time,<matplotlib><pandas><histogram>,,5.0,,
14602739,1,,2013-01-30 11:19:21,1,59,"I have a pandas DataFrame like:
 AB
'2010-01-01'  10  20
'2010-02-01'  20  30
'2010-03-01'  30  10  

I need to apply some function for every columns and create new columns in this DataFrame with special name

ABA1B1
'2010-01-01'  10  202040
'2010-02-01'  20  304060
'2010-03-01'  30  106020

So I need to make two additional columns with names A1 and B2 based on columns A and B ( like name A1  = str(A) + str(1)) by multiplying by two Is it possible to do this using DataFrameapply() or other construction? ",1627810,,,,2013-01-30 17:19:48,Creating of new columns in pandas.DataFrame using apply() function,<python><pandas>,2.0,,,
14629578,1,,2013-01-31 15:45:10,1,34,"I have a pandas dataframe df:

Out[16]:

DatetimeIndex: 269850 entries  2012-12-19 16:15:36 to 2012-12-20 14:36:55
Data columns:
X1    269850  non-null values
X2      269848  non-null values
X3      269848  non-null values
dtypes: float64(2)  object(1)


And I would like to slice the dataframe to return a four hour window of data from 2012-12-20 05:00:00 to 2012-12-20 09:00:00

When I try:

Slicedf = dftruncate(before='12/20/2012 05:00:00' after='12/20/2012 09:00:00')


The following error occurs

---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
/Users/millerc/Dropbox/CODE/KentValeDataProcessor/1-17-13-files/ in ()
      2 y=datetime(2012 12 20 9 0 0)
      3 #Slicedf = dfix[x:y]

----> 4 Slicedf = dftruncate(before='12/20/2012 05:00:00' after='12/20/2012 09:00:00')

/Library/Frameworks/Pythonframework/Versions/73/lib/python27/site-packages/pandas/core/genericpyc in truncate(self  before  after  copy)
   1008                                  (before  after))
   1009 
-> 1010     result = selfix[before:after]
   1011 
   1012     if isinstance(selfindex  MultiIndex):

/Library/Frameworks/Pythonframework/Versions/73/lib/python27/site-packages/pandas/core/indexingpyc in __getitem__(self  key)
     32             return self_getitem_tuple(key)
     33         else:
---> 34             return self_getitem_axis(key  axis=0)
     35 
     36     def _get_label(self  label  axis=0):

/Library/Frameworks/Pythonframework/Versions/73/lib/python27/site-packages/pandas/core/indexingpyc in _getitem_axis(self  key  axis)
    325         labels = selfobj_get_axis(axis)
    326         if isinstance(key  slice):
--> 327             return self_get_slice_axis(key  axis=axis)
    328         elif _is_list_like(key) and not (isinstance(key  tuple) and
    329                                          isinstance(labels  MultiIndex)):

/Library/Frameworks/Pythonframework/Versions/73/lib/python27/site-packages/pandas/core/indexingpyc in _get_slice_axis(self  slice_obj  axis)
    574         else:
    575             try:
--> 576                 i  j = labelsslice_locs(start  stop)
    577                 slicer = slice(i  j  slice_objstep)
    578             except Exception:

/Library/Frameworks/Pythonframework/Versions/73/lib/python27/site-packages/pandas/tseries/indexpyc in slice_locs(self  start  end)
   1151                 pass
   1152 
-> 1153         return Indexslice_locs(self  start  end)
   1154 
   1155     def __getitem__(self  key):

/Library/Frameworks/Pythonframework/Versions/73/lib/python27/site-packages/pandas/core/indexpyc in slice_locs(self  start  end)
   1123         else:
   1124             try:
-> 1125                 beg_slice = selfget_loc(start)
   1126                 if isinstance(beg_slice  slice):
   1127                     beg_slice = beg_slicestart

/Library/Frameworks/Pythonframework/Versions/73/lib/python27/site-packages/pandas/tseries/indexpyc in get_loc(self  key)
   1121                 return self_engineget_loc(Timestamp(key))
   1122             except (KeyError  ValueError):
-> 1123                 raise KeyError(key)
   1124 
   1125     def _get_string_slice(self  key):

KeyError: datetimedatetime(2012  12  20  5  0)


I have also tried (from Pandas DataFrame slicing by day/hour/minute):

from datetime import datetime
x=datetime(2012 12 20 5 0 0)
y=datetime(2012 12 20 9 0 0)
Slicedf = dfix[x:y]


which returns the exact same error",655733,,655733.0,2013-02-01 10:52:01,2013-02-01 10:52:01,Pandas DatetimeIndex truncate error,<python><pandas><slicing>,,5.0,1.0,
14655172,1,14655778.0,2013-02-01 21:16:52,2,36,"I've got 7000 data frames with columns 

Date  X_1
Date  X_2



Each dataframe has around 2500 rows

The dates sometimes overlap  but are not guaranteed to do so

I'd like to combine them into a dataframe of the form

Date  X_1  X_2 etc


I tried applying combine_first 7000 times  but it was really slow  as it had to create 7000 new objects  each slightly bigger than the last one

Is there a more efficient way to combine multiple dataframes?",1984745,,487339.0,2013-02-01 21:32:48,2013-02-02 20:34:35,How do I efficiently combine similar dataframes in Pandas into one giant dataframe,<python><pandas><time-series>,2.0,,,
14657241,1,14657511.0,2013-02-02 00:22:08,2,47,"I have a list of items that likely has some export issues  I would like to get a list of the duplicate items so I can manually compare them  When I try to use pandas duplicated method  it only returns the first duplicate  Is there a a way to get all of the duplicates and not just the first one?

A small subsection of my dataset looks like this:

ID ENROLLMENT_DATE TRAINER_MANAGING TRAINER_OPERATOR FIRST_VISIT_DATE
1536D 12-Feb-12 ""06DA1B3-Lebanon NH""  15-Feb-12
F15D 18-May-12 ""06405B2-Lebanon NH""  25-Jul-12
8096 8-Aug-12 ""0643D38-Hanover NH"" ""0643D38-Hanover NH"" 25-Jun-12
A036 1-Apr-12 ""06CB8CF-Hanover NH"" ""06CB8CF-Hanover NH"" 9-Aug-12
8944 19-Feb-12 ""06D26AD-Hanover NH""  4-Feb-12
1004E 8-Jun-12 ""06388B2-Lebanon NH""  24-Dec-11
11795 3-Jul-12 ""0649597-White River VT"" ""0649597-White River VT"" 30-Mar-12
30D7 11-Nov-12 ""06D95A3-Hanover NH"" ""06D95A3-Hanover NH"" 30-Nov-11
3AE2 21-Feb-12 ""06405B2-Lebanon NH""  26-Oct-12
B0FE 17-Feb-12 ""06D1B9D-Hartland VT""  16-Feb-12
127A1 11-Dec-11 ""064456E-Hanover NH"" ""064456E-Hanover NH"" 11-Nov-12
161FF 20-Feb-12 ""0643D38-Hanover NH"" ""0643D38-Hanover NH"" 3-Jul-12
A036 30-Nov-11 ""063B208-Randolph VT"" ""063B208-Randolph VT"" 
475B 25-Sep-12 ""06D26AD-Hanover NH""  5-Nov-12
151A3 7-Mar-12 ""06388B2-Lebanon NH""  16-Nov-12
CA62 3-Jan-12   
D31B 18-Dec-11 ""06405B2-Lebanon NH""  9-Jan-12
20F5 8-Jul-12 ""0669C50-Randolph VT""  3-Feb-12
8096 19-Dec-11 ""0649597-White River VT"" ""0649597-White River VT"" 9-Apr-12
14E48 1-Aug-12 ""06D3206-Hanover NH""  
177F8 20-Aug-12 ""063B208-Randolph VT"" ""063B208-Randolph VT"" 5-May-12
553E 11-Oct-12 ""06D95A3-Hanover NH"" ""06D95A3-Hanover NH"" 8-Mar-12
12D5F 18-Jul-12 ""0649597-White River VT"" ""0649597-White River VT"" 2-Nov-12
C6DC 13-Apr-12 ""06388B2-Lebanon NH""  
11795 27-Feb-12 ""0643D38-Hanover NH"" ""0643D38-Hanover NH"" 19-Jun-12
17B43 11-Aug-12   22-Oct-12
A036 11-Aug-12 ""06D3206-Hanover NH""  19-Jun-12


My code looks like this currently:

df_bigdata_duplicates = df_bigdata[df_bigdataduplicated(cols='ID')]


There area a couple duplicate items But  when I use the above code  I only get the first item  In the API reference  I see how I can get the last item  but I would like to have all of them so I can visually inspect them to see why I am getting the discrepancy  So  in this example I would like to get all three A036 entries and both 11795 entries and any other duplicated entries  instead of the just first one  Any help is most appreciated",1467553,,1467553.0,2013-02-02 00:29:22,2013-02-02 01:01:09,How do I get a list of all the duplicate items using pandas in python?,<python><pandas>,1.0,,1.0,
14506583,1,14506690.0,2013-01-24 16:54:14,0,25,"Is it possible to suppress the array output when plotting a histogram in ipython?:

For example:

plthist(OIR['Range']  bins  named=True  histtype='bar')


outputs/prints the array information before displaying the graph

",1569815,,1240268.0,2013-01-24 18:02:07,2013-01-24 18:02:07,Suppress output of object when plotting in ipython,<pandas><ipython>,1.0,,,
14513339,1,14513503.0,2013-01-25 00:26:24,1,36,"first  if I use DataReader to read in the data  and then plot  everything is good

In [55]: t = DataReader('SPY' 'yahoo'  start=datetimedatetime(1990 1 1))

In [56]: t
Out[56]: 

Index: 5033 entries  1993-01-29 00:00:00 to 2013-01-23 00:00:00
Data columns:
Open         5033  non-null values
High         5033  non-null values
Low          5033  non-null values
Close        5033  non-null values
Volume       5033  non-null values
Adj Close    5033  non-null values
dtypes: float64(5)  int64(1)

In [58]: tplot()
Out[58]: 


However  if I save it as a csv file and reload it again  I got error message and the plot is not quite right either  

In [62]: tto_csv('spycsv')

In [63]: s = pdread_csv('spycsv'  na_values=["" ""])

In [64]: sset_index('Date')
Out[64]: 

Index: 5033 entries  1993-01-29 00:00:00 to 2013-01-23 00:00:00
Data columns:
Open         5033  non-null values
High         5033  non-null values
Low          5033  non-null values
Close        5033  non-null values
Volume       5033  non-null values
Adj Close    5033  non-null values
dtypes: float64(5)  int64(1)

In [66]: splot()                                                            
--------------------------------------------------------------------------- 
AttributeError                            Traceback (most recent call last) 
/home/dli/pythonTest/pandas/ in ()   
----> 1 splot()                                                            

/usr/lib/pymodules/python27/pandas/core/framepyc in plot(self  subplots  sharex  sharey  use_index  figsize  grid  legend  rot  ax  kind  **kwds)
3748                     axlegend(loc='best')                              
3749                 else:                                                  
-> 3750                     axplot(x  y  label=str(col)  **kwds)           
3751                                                                        
3752                 axgrid(grid)                                          

/usr/lib/pymodules/python27/matplotlib/axespyc in plot(self  *args  **kwargs)
3891         lines = []                                                     
3892                                                                        
-> 3893         for line in self_get_lines(*args  **kwargs):               
3894             selfadd_line(line)                                        
3895             linesappend(line)                                         

/usr/lib/pymodules/python27/matplotlib/axespyc in _grab_next_args(self  *args  **kwargs)
    320                 return                                              
    321             if len(remaining)  322                 for seg in self_plot_args(remaining  kwargs):      
    323                     yield seg                                       
    324                 return                                              

/usr/lib/pymodules/python27/matplotlib/axespyc in _plot_args(self  tup  kwargs)
    279         ret = []                                                    
    280         if len(tup) > 1 and is_string_like(tup[-1]):                
--> 281             linestyle  marker  color = _process_plot_format(tup[-1])
    282             tup = tup[:-1]                                          
    283         elif len(tup) == 3:                                         

/usr/lib/pymodules/python27/matplotlib/axespyc in _process_plot_format(fmt)
    93     # handle the multi char special cases and strip them from the    

    94     # string                                                         

---> 95     if fmtfind('--')>=0:                                           
    96         linestyle = '--'                                             
    97         fmt = fmtreplace('--'  '')                                  

AttributeError: 'numpyndarray' object has no attribute 'find'            


Any idea how to fix it?

Thanks
Dan",1591487,,,,2013-01-25 22:00:26,Date is not displayed correctly when plot pandas dataframe,<matplotlib><data.frame><pandas>,1.0,,,
14568070,1,14568392.0,2013-01-28 17:51:55,2,69,"I would like to build pandas from source rather than use a package manager because I am interested in contributing The first time I tried to build pandas  these were the steps I took:

1) created the virtualenv
mkvirtualenv --no-site-packages pandas

2) activated the virtualenv

3) installed Anaconda CE However  this was installed in ~/anaconda

4) cloned pandas

5) built C extensions in place

(pandas)ems ~/virtualenvs/pandas/localrepo/pandas> ~/anaconda/bin/python setuppy build_ext --inplace

6) built pandas

(pandas)ems ~/virtualenvs/pandas/localrepo/pandas> ~/anaconda/bin/python setuppy build

7) ran nosetests on master branch

Tests failed:
(pandas)ems ~/virtualenvs/pandas/localrepo/pandas> nosetests pandas
    E
    ======================================================================
    ERROR: Failure: ValueError (numpydtype has the wrong size  try recompiling)
    ----------------------------------------------------------------------
    Traceback (most recent call last):
    File ""/Users/EmilyChen/virtualenvs/pandas/lib/python27/site-packages/nose/loaderpy""      line 390  in loadTestsFromName
    addrfilename  addrmodule)
    File ""/Users/EmilyChen/virtualenvs/pandas/lib/python27/site-packages/nose/importerpy""  line 39  in importFromPath
    return selfimportFromDir(dir_path  fqname)
    File ""/Users/EmilyChen/virtualenvs/pandas/lib/python27/site-packages/nose/importerpy""  line 86  in importFromDir
    mod = load_module(part_fqname  fh  filename  desc)
    File ""/Users/EmilyChen/virtualenvs/pandas/localrepo/pandas/pandas/initpy""  line 6  in 
    from  import hashtable  tslib  lib
    File ""numpypxd""  line 156  in init pandashashtable (pandas/hashtablec:20354)
    ValueError: numpydtype has the wrong size  try recompiling

Ran 1 test in 0001s

FAILED (errors=1)

Someone on the PyData mailing list said:


  It looks like you have NumPy installed someplace else on your machine and AnacondaCE is not playing nicely in the virtualenv The error you are getting is a Cython error message which occurs when the NumPy version it built against doesn't match the installed version on your system-- I had thought that 17x was supposed to be ABI compatible with 16x (so this would not happen) but I guess not Sigh


The numpy version in Anaconda CE library is 170b2 and my system numpy installation is version 151 Setuppy linked to the numpy in the Anaconda distribution's libraries when it built pandas but my guess is it's linking to my system version when nosetests runs /pandas/initpy

Next  I repeated the steps outside a virtualenv  but got the same error Finally  I decided to install all the dependencies in a new virtualenv instead of using the Anaconda distribution to build pandas This way  I can see that dependencies like numpy reside in the lib directory of the virtualenv python installation  which takes precedent when pandasinit runs import statements This is what I did:

1) installed numpy  dateutil  pytz  cython  scipy  matplotlib and openpyxl using pip

2) built c extensions in place

3) pandas install output here: http://pastebincom/3CKf1f9i

4) pandas did not install correctly

(pandas)ems ~/virtualenvs/pandas/localrepo/pandas> python
Python 271 (r271:86832  Jul 31 2011  19:30:53) 
[GCC 421 (Based on Apple Inc build 5658) (LLVM build 23351500)] on darwin
Type ""help""  ""copyright""  ""credits"" or ""license"" for more information
>>> import pandas
 cannot import name hashtable
Traceback (most recent call last):
File """"  line 1  in 
File ""pandas/__init__py""  line 6  in 
from  import hashtable  tslib  lib
ImportError: cannot import name hashtable


I took a look at this question but cython installed in my case  and I am trying to build successfully from source rather than using pip like the answer recommended

(pandas)ems ~/virtualenvs/pandas/localrepo/pandas> which cython
/Users/EmilyChen/virtualenvs/pandas/bin/cython
",1644251,,,,2013-01-28 18:12:24,Pandas installation on Mac OS X: ImportError (cannot import name hashtable),<python><hashtable><pandas><cython><importerror>,1.0,2.0,,
14570400,1,,2013-01-28 20:14:49,1,38,"I have a DataFrame 'tso':

tso=DataFrame(temp index=datet)
#tso['Month']=tsoindexmonth
tso['Year']=tsoindexyear
tso['Day']=tsoindexdayofyear
ax=tsogroupby(['Day' 'Year'])mean()unstack()plot()


here is the picture:


I want to change x-axis as 'month' instead of 'Day' And when I zoom-in/out the figure the x-axis can corresponding changes",1843099,,173292.0,2013-01-28 20:15:32,2013-01-29 18:48:17,Changing x-axis without changing index in pandas,<python><pandas>,1.0,2.0,0.0,
14616809,1,,2013-01-31 01:06:01,0,29,"I have a simple csv file downloaded from the Australian Bureau of Statistics It has the standard double quotes for strings and is separated by commas However  it has '\r\n' (carriage return and newline) as line separator  and I pandas under Windows is not parsing it properly What would be the most convenient way of reading this file and creating a dataframe from it?

In [153]: df = pdread_csv('CP2006RA_2006POAtxt')

In [154]: df[:5]
Out[154]: 
              RA_2006_CODE               RA_2006_NAME  POA_2006  POA_20061                      RATIO                   PERCENT
0                       \r                         10       NaN         NaN  Major Cities of Australia  ajor Cities of Australia
1  jor Cities of Australia  Major Cities of Australia      2000        2000                          1                       100
2                       \r                        NaN       NaN         NaN                        NaN                       NaN
3                       10  Major Cities of Australia      2006        2006                          1                       100
4                       \r                        NaN       NaN         NaN                        NaN                       NaN
",1479269,,1479269.0,2013-01-31 01:37:47,2013-01-31 03:05:21,DataFrame.to_csv failing,<python><pandas>,3.0,1.0,,
14663004,1,,2013-02-02 14:40:31,1,19,"I have pandas dataframe df1 and df2 (df1 is vanila dataframe  df2 is indexed by 'STK_ID' & 'RPT_Date') :

>>> df1
    STK_ID  RPT_Date  TClose   sales  discount
0   000568  20060331    369   5975       NaN
1   000568  20060630    914  10143       NaN
2   000568  20060930    949  13854       NaN
3   000568  20061231   1584  19262       NaN
4   000568  20070331   1700   6803       NaN
5   000568  20070630   2631  12940       NaN
6   000568  20070930   3912  19977       NaN
7   000568  20071231   4594  29269       NaN
8   000568  20080331   3875  12668       NaN
9   000568  20080630   3009  21102       NaN
10  000568  20080930   2600  30769       NaN

>>> df2
                 TClose   sales  discount  net_sales    cogs
STK_ID RPT_Date                                             
000568 20060331    369   5975       NaN      5975   2591
       20060630    914  10143       NaN     10143   4363
       20060930    949  13854       NaN     13854   5901
       20061231   1584  19262       NaN     19262   8407
       20070331   1700   6803       NaN      6803   2815
       20070630   2631  12940       NaN     12940   5418
       20070930   3912  19977       NaN     19977   8452
       20071231   4594  29269       NaN     29269  12606
       20080331   3875  12668       NaN     12668   3958
       20080630   3009  21102       NaN     21102   7431


I can get the last 3 rows of df2 by:

>>> df2ix[-3:]
                 TClose   sales  discount  net_sales    cogs
STK_ID RPT_Date                                             
000568 20071231   4594  29269       NaN     29269  12606
       20080331   3875  12668       NaN     12668   3958
       20080630   3009  21102       NaN     21102   7431


while df1ix[-3:] give all the rows: 

>>> df1ix[-3:]
    STK_ID  RPT_Date  TClose   sales  discount
0   000568  20060331    369   5975       NaN
1   000568  20060630    914  10143       NaN
2   000568  20060930    949  13854       NaN
3   000568  20061231   1584  19262       NaN
4   000568  20070331   1700   6803       NaN
5   000568  20070630   2631  12940       NaN
6   000568  20070930   3912  19977       NaN
7   000568  20071231   4594  29269       NaN
8   000568  20080331   3875  12668       NaN
9   000568  20080630   3009  21102       NaN
10  000568  20080930   2600  30769       NaN


Why ? How to get the last 3 rows of df1 (dataframe without index) ?
Pandas 0101",1072888,,,,2013-02-03 05:02:23,How to get the last n row of pandas dataframe?,<pandas>,1.0,3.0,,
14639825,1,,2013-02-01 04:38:00,3,62,"I'm working on a python program to compute a numerical coding of mutated residues and positions of a set of strings (protein sequences)   stored in fasta format file with each protein sequence is separated by comma I'm trying to find the position and sequences which are mutated

My fasta file is as follows:

MTAQDDSYSDGKGDYNTIYLGAVFQLN MTAQDDSYSDGRGDYNTIYLGAVFQLN MTSQEDSYSDGKGNYNTIMPGAVFQLN MTAQDDSYSDGRGDYNTIMPGAVFQLN MKAQDDSYSDGRGNYNTIYLGAVFQLQ MKSQEDSYSDGRGDYNTIYLGAVFQLN MTAQDDSYSDGRGDYNTIYPGAVFQLN MTAQEDSYSDGRGEYNTIYLGAVFQLQ MTAQDDSYSDGKGDYNTIMLGAVFQLN MTAQDDSYSDGRGEYNTIYLGAVFQLN


Example:
The following figure (based on another set of fasta file) will explain the algorithm behind this In this figure first box represents alignment of input file sequences The last box represents the output file How can I do this with my fasta file in Python?

example input file:

MTAQDD MTAQDD MTSQED MTAQDD MKAQHD


        positions  1  2  3  4  5  6                  1  2  3  4  5  6    
protein sequence1  M  T  A  Q  D  D                     T  A     D
protein sequence2  M  T  A  Q  D  D                     T  A     D    
protein sequence3  M  T  S  Q  E  D                     T  S     E    
protein sequence4  M  T  A  Q  D  D                     T  A     D    
protein sequence5  M  K  A  Q  H  D                     K  A     H

     PROTEIN SEQUENCE ALIGNMENT                   DISCARD NON-VARIABLE REGION    

        positions  2  2  3  3  5  5  5    
protein sequence1  T     A     D       
protein sequence2  T     A     D       
protein sequence3  T        S     E    
protein sequence4  T     A     D       
protein sequence5     K  A           H


MUTATED RESIDUE IS SPLITED TO SEPARATE COLUMN

Output file should be like this:

position+residue   2T  2K  3A  3S  5D  5E  5H    
       sequence1   1   0   1   0   1   0   0    
       sequence2   1   0   1   0   1   0   0    
       sequence3   1   0   0   1   0   1   0    
       sequence4   1   0   1   0   1   0   0    
       sequence5   0   1   1   0   0   0   1

    (RESIDUES ARE CODED 1 IF PRESENT  0 IF ABSENT)


Here are two ways I have tried to do it:

ls= 'MTAQDDSYSDGKGDYNTIYLGAVFQLN MTAQDDSYSDGRGDYNTIYLGAVFQLN MTSQEDSYSDGKGNYNTIMPGAVFQLN MTAQDDSYSDGRGDYNTIMPGAVFQLN MKAQDDSYSDGRGNYNTIYLGAVFQLQ MKSQEDSYSDGRGDYNTIYLGAVFQLN MTAQDDSYSDGRGDYNTIYPGAVFQLN MTAQEDSYSDGRGEYNTIYLGAVFQLQ MTAQDDSYSDGKGDYNTIMLGAVFQLN MTAQDDSYSDGRGEYNTIYLGAVFQLN'split(' ')
pos = [set(enumerate(x  1)) for x in ls]
a=set()union(*pos)
alle = sorted(set()union(*pos))
print '\t'join(str(x) + y for x  y in alle)
for p in pos:
    print '\t'join('1' if key in p else '0' for key in alle)


(here I'm getting columns of mutated as well as non-mutated residues  but I want only columns for mutated residues)

from pandas import *
data = 'MTAQDDSYSDGKGDYNTIYLGAVFQLN MTAQDDSYSDGRGDYNTIYLGAVFQLN MTSQEDSYSDGKGNYNTIMPGAVFQLN MTAQDDSYSDGRGDYNTIMPGAVFQLN MKAQDDSYSDGRGNYNTIYLGAVFQLQ MKSQEDSYSDGRGDYNTIYLGAVFQLN MTAQDDSYSDGRGDYNTIYPGAVFQLN MTAQEDSYSDGRGEYNTIYLGAVFQLQ MTAQDDSYSDGKGDYNTIMLGAVFQLN MTAQDDSYSDGRGEYNTIYLGAVFQLN'  
df = DataFrame([list(row) for row in datasplit(' ')])
df = DataFrame({str(col+1)+val:(df[col]==val)apply(int) for col in dfcolumns for val in set(df[col])})
print dfselect(lambda x: not df[x]all()  axis = 1)


(here it is giving output  but not in orderly ie  first 2K then 2T then 3A like that)

How should I be doing this?",2020442,,1240268.0,2013-02-03 04:34:54,2013-02-03 04:34:54,protein sequence coding,<python><pandas>,1.0,1.0,,
5515021,1,5516153.0,2011-04-01 14:50:44,4,1430,"Greetings all  I have two series of data: daily raw stock price returns (positive or negative floats) and trade signals (buy=1  sell=-1  no trade=0)

The raw price returns are simply the log of today's price divided by yesterday's price:

log(p_today / p_yesterday)


An example:

raw_return_series = [ 00063 -00031 00024   -00221 00097 -00015]


The trade signal series looks like this:

signal_series = [-1 0 -1 -1 0 0 -1 0 0 0]


To get the daily returns based on the trade signals:

daily_returns = [raw_return_series[i] * signal_series[i+1] for i in range(0  len(signal_series)-1)]


These daily returns might look like this:

[00  000316  -00024  00  00  00023  00  00  00] # results in daily_returns; notice the 0s


I need to use the daily_returns series to compute a compounded returns series However  given that there are 0 values in the daily_returns series  I need to carry over the last non-zero compound return ""through time"" to the next non-zero compound return

For example  I compute the compound returns like this (notice I am going ""backwards"" through time):

compound_returns = [(((1 + compounded[i + 1]) * (1 + daily_returns[i])) - 1) for i in range(len(compounded) - 2  -1  -1)]


and the resulting list:

[00  00  00023  00  00  -00024  00031  00] # (notice the 0s)


My goal is to carry over the last non-zero return to the accumulate these compound returns That is  since the return at index i is dependent on the return at index i+1  the return at index i+1 should be non-zero Every time the list comprehension encounters a zero in the daily_return series  it essentially restarts",687739,,667301.0,2011-05-30 22:28:56,2011-05-30 22:28:56,Compute a compounded return series in Python,<python><list-comprehension><time-series><pandas>,2.0,6.0,6.0,
14642435,1,,2013-02-01 08:40:19,0,28,"Ive been working on this as a beginner for a while  Overall I want to read in a NetCDF file and import multiple (~50) columns (and 17520 cases) into a Pandas DataFrame At the moment I have set it up for a list of 4 variables but I want to be able to expand that somehow I made a start  but any help on how to loop through to make this happen with 50 variables would be great  It does work using the code below for 4 variables  I know its not pretty so spankings are welcome  still learning

Another question I have it that when I try to read the numpy arrays directly into Pandas DataFrame it doesnt work and instead creates a DataFrame that is 17520 columns large  It should be the other way (transposed)  If I create a series it works fine  So I have had to use the following lines to get around this  Not even sure why it works  Any suggestions of a better way?  Especially when it comes to 50 variables 

d={vnames[0] :vartemp[0]  vnames[1] :vartemp[1]  vnames[2] :vartemp[2]  vnames[3] :vartemp[3]}
hs = pdDataFrame(d index=times)


The whole code is pasted below

Thanks a lot
Jason

import pandas as pd
import datetime as dt
import xlrd
import numpy as np
import netCDF4


def excel_to_pydate(exceldate):
    datemode=0           # datemode: 0 for 1900-based  1 for 1904-based
    pyear  pmonth  pday  phour  pminute  psecond = xlrdxldate_as_tuple(exceldate  datemode)
    py_date = dtdatetime(pyear  pmonth  pday  phour  pminute  psecond)
    return(py_date)

def main():
    filename='HowardSprings_2010_L4nc'
#Define a list of variables names we want from the netcdf file
    vnames = ['xlDateTime'  'Fa'  'Fh'  'Fg']

# Open the NetCDF file
    nc = netCDF4Dataset(filename) 

#Create some lists of size equal to length of vnames list
    temp=list(xrange(len(vnames)))
    vartemp=list(xrange(len(vnames)))

#Enumerate the list and assign each NetCDF variable to an element in the lists  
# First get the netcdf variable object assign to temp
# Then strip the data  from that and add to temporary variable (vartemp)
    for index  variable in enumerate(vnames):               
        temp[index]= ncvariables[variable]
        vartemp[index] = temp[index][:]   

# Now call the function to convert to datetime from excel Assume datemode: 0
    times = [excel_to_pydate(elem) for elem in vartemp[0]]

#Dont know why I cant just pass a list of variables ie [vartemp[0]  vartemp[1]  vartemp[2]]
#But this is only thing that worked
#Create Pandas dataframe using times as index
    d={vnames[0] :vartemp[0]  vnames[1] :vartemp[1]  vnames[2] :vartemp[2]  vnames[3] :vartemp[3]}
    theDataFrame = pdDataFrame(d index=times)

#Define missing data value and apply to DataFrame
    missing=-9999
    theDataFrame1=theDataFramereplace({vnames[0] :missing  vnames[1] :missing  vnames[2] :missing  vnames[3] :missing} 'NaN')

main()
",1911866,,,,2013-02-01 18:21:50,Stuck importing NetCDF file into Pandas DataFrame,<data.frame><pandas><netcdf>,1.0,,,
5486226,1,5486285.0,2011-03-30 12:26:50,4,1128,"I have some stock data based on daily close values  I need to be able to insert these values into a python list and get a median for the last 30 closes  Is there a python library that does this?

Thank you~
Yueer",683866,,667301.0,2011-05-30 22:28:24,2011-12-09 08:59:40,Rolling median in python,<python><finance><quantitative-finance><pandas>,4.0,4.0,1.0,
5558607,1,6468964.0,2011-04-05 21:13:50,2,2484,"The pandas DataFrame object has a sort method but pandas DataMatrix object does not

What is the best way to sort this DataMatrix object by index (the date column) in ascending order?

>>> dm
               compound_ret
2/16/2011 0:00  0006275682
2/15/2011 0:00  0003098208
2/14/2011 0:00  00055039
2/13/2011 0:00  0011471506
2/12/2011 0:00  0011853712
2/11/2011 0:00  0009558739
2/10/2011 0:00  0014127912
2/9/2011 0:00   002042923
2/8/2011 0:00   0023308062


The result should be the DataMatrix with 2/8/2011 as the first entry and 2/16/2011 as the last entry The entries in the compound_ret column should follow their date in the sort So the result should look something like this:

>>>dm_sorted
                  compound_ret
2/8/2011 0:00    0023308062
2/9/2011 0:00    002042923
2/10/2011 0:00  0014127912
2/11/2011 0:00  0009558739
2/12/2011 0:00  0011853712
2/13/2011 0:00  0011471506
2/14/2011 0:00  00055039
2/15/2011 0:00  0003098208
2/16/2011 0:00  0006275682
",687739,,101167.0,2013-01-06 21:17:09,2013-01-06 21:17:09,Sort a pandas DataMatrix in ascending order,<python><sorting><numpy><pandas>,3.0,,1.0,
14667196,1,,2013-02-02 22:28:38,0,5,"I'm new to py3k and I was trying to install pandas on my Mac OS X106 I wasn't able to install it and because I need setuptools/distribute for Py3k So I downloaded distribute-0634 package and tried to install it with setuppy I have got following errors 


  Traceback (most recent call last):   File
  ""/Users/Downloads/distribute-0634/setuppy""  line 22  in 
      dir_utilcreate_tree(tmp_src  flfiles)   File ""/Library/Frameworks/Pythonframework/Versions/32/lib/python32/distutils/dir_utilpy"" 
  line 97  in create_tree
      mkpath(dir  mode  verbose=verbose  dry_run=dry_run)   File ""/Library/Frameworks/Pythonframework/Versions/32/lib/python32/distutils/dir_utilpy"" 
  line 66  in mkpath
      loginfo(""creating %s""  head)   File ""/Library/Frameworks/Pythonframework/Versions/32/lib/python32/distutils/logpy"" 
  line 44  in info
      self_log(INFO  msg  args)   File ""/Library/Frameworks/Pythonframework/Versions/32/lib/python32/distutils/logpy"" 
  line 30  in _log
      if streamerrors == 'strict': AttributeError: errors


Does anyone know how I can solve this problem? Thanks a lot in advance!",2036062,,464770.0,2013-02-02 22:33:23,2013-02-02 22:33:23,distribute-0.6.34 installation error,<osx><installation><pandas><distutils><python-3.3>,,,,
7766400,1,,2011-10-14 10:33:54,1,383,"Pandas is producing 'module' object has no attribute 'core' when being imported under django and mod_wsgi inside a virtual environment  It works fine running under the django development server inside the virtual environment 

Other modules eg: numpy have no problems so I assume this means the virtual environment is set up correctly with mod_wsgi  Any advice would be appreciated

stagingwsgi

import os
import sys
import site

PROJECT_ROOT = ospathdirname(ospathdirname(ospathdirname(__file__)))
site_packages = ospathjoin(PROJECT_ROOT  'env/openportfolio/lib/python27/site-packages')
siteaddsitedir(ospathabspath(site_packages))
syspathinsert(0  PROJECT_ROOT)
syspathinsert(0  ospathdirname(ospathdirname(__file__)))
osenviron['DJANGO_SETTINGS_MODULE'] = 'openportfoliosettings_staging'

import pandas #triggers error
import djangocorehandlerswsgi
application = djangocorehandlerswsgiWSGIHandler()


Error

Traceback (most recent call last):
  File ""/usr/local/web/django/www/staging/openportfolio/apache/stagingwsgi""  line 22  in 
    import pandas
  File ""/usr/local/web/django/www/staging/env/openportfolio/lib/python27/site-packages/pandas/__init__py""  line 12  in 
    from pandascoreapi import *
  File ""/usr/local/web/django/www/staging/env/openportfolio/lib/python27/site-packages/pandas/core/apipy""  line 6  in 
    import pandascoredatetools as datetools
  AttributeError: 'module' object has no attribute 'core'  


Python Path

['/usr/local/web/django/www/staging/openportfolio' 
 '/usr/local/web/django/www/staging' 
 '/Library/Python/27/site-packages/pip-102-py27egg' 
 '/usr/local/web/django/www/staging/env/openportfolio/lib/python27/site-packages/setuptools-06c11-py27egg' 
 '/usr/local/web/django/www/staging/env/openportfolio/lib/python27/site-packages/pip-102-py27egg' 
 '/usr/local/web/django/www/staging/env/openportfolio/lib/python27/site-packages/matplotlib-110-py27-macosx-107-intelegg' 
 '/System/Library/Frameworks/Pythonframework/Versions/27/lib/python27zip' 
 '/System/Library/Frameworks/Pythonframework/Versions/27/lib/python27' 
 '/System/Library/Frameworks/Pythonframework/Versions/27/lib/python27/plat-darwin' 
 '/System/Library/Frameworks/Pythonframework/Versions/27/lib/python27/plat-mac' 
 '/System/Library/Frameworks/Pythonframework/Versions/27/lib/python27/plat-mac/lib-scriptpackages' 
 '/System/Library/Frameworks/Pythonframework/Versions/27/Extras/lib/python' 
 '/System/Library/Frameworks/Pythonframework/Versions/27/lib/python27/lib-tk' 
 '/System/Library/Frameworks/Pythonframework/Versions/27/lib/python27/lib-old' 
 '/System/Library/Frameworks/Pythonframework/Versions/27/lib/python27/lib-dynload' 
 '/System/Library/Frameworks/Pythonframework/Versions/27/Extras/lib/python/PyObjC' 
 '/Library/Python/27/site-packages' 
 '/usr/local/web/django/www/staging/env/openportfolio/lib/python27/site-packages']
",995182,,190597.0,2011-10-14 10:42:05,2011-10-24 15:10:26,Pandas + Django + mod_wsgi + virtualenv,<django><mod-wsgi><virtualenv><pandas>,1.0,2.0,1.0,
14623935,1,14624704.0,2013-01-31 10:48:43,1,38,"I have the following  DataFrame

                           Qtr Premium      Claim     Rate

Type    Code                                           
A        3                  14  355277      1009917  0004017
         3                  15  561067      1057636  0004017
         3                  16  646322      1077406  0004017
         4                  17  612991      1069677  0005638
         4                  18  468865      1036256  0005638
         4                  19  215894      9775966  0005638
         4                  20  854077      8936972  0005638


I have constant ""c""

I'm looking to carry out a row by row calculation that uses the relevant values from Qtr and Rate but updates the values of Premium and Claim

Example:

Premium = Premium / (1+Rate)^(c-Qtr)
Claim = Claim / (1+Rate)^(c-Qtr)


In reality I have a lot of columns that I want this calculation carried out on",2028710,,,,2013-01-31 16:27:19,Pandas DataFrame - Calculated Column based on subset of Columns,<data.frame><pandas><computed-columns>,1.0,,,
14639551,1,14639627.0,2013-02-01 04:05:58,0,32,"I have 2 pandasSeries:

s
-999900    26371
24000       1755
13899          2

s2
-999900    26371
24000       1755
11303          6
10000          4


I have tried concat:

-999900    26371
 24000      1755
 13899         2
-999900    26371
 24000      1747
 11303         6
 10000         4


and s+s2:

-999900    52742
 10000       NaN
 11303       NaN
 13899       NaN
 24000      3502


but I need the output that adds values of existing indices and keeps new indices if they appear  so a mix of concat and '+' How can I do that? My expected output is

swanted:

-999900    52742
 10000         4
 11303         6
 13899         2
 24000      3502
",680232,,,,2013-02-01 04:36:55,Join pandas series while keeping new indices,<python><pandas>,1.0,,,
14656852,1,14657647.0,2013-02-01 23:32:46,1,47,I'd like to use pandas for all my analysis along with numpy but use Rpy2 for plotting my data Is it possible to translate a pandas dataframe into an R dataframe? thanks,248237,,,,2013-02-02 01:23:46,Converting pandas dataframe into R dataframes,<python><r><pandas><rpy2>,1.0,3.0,2.0,
7813132,1,7821507.0,2011-10-18 20:16:12,0,1066,"I am trying to do something very similar to that previous question but I meet error
I have a pandas dataframe containing features and label I need to do some convertion to send the features and the label variable into a machine learning object:

import pandas
import milk
from scikitsstatsmodelstools import categorical


then I have:

trainedData=bigdata[bigdata['meta']=15]
#print trainedData
#extract two columns from trainedData
#convert to numpy array
features=trainedDataix[: ['ratio' 'area']]as_matrix(['ratio' 'area'])
un_features=untrainedix[: ['ratio' 'area']]as_matrix(['ratio' 'area'])
print 'features'
print features[:5]
##label is a string:single  touching nuclei dust
print 'labels'

labels=trainedDataix[: ['type']]as_matrix(['type'])
print labels[:5]
#convert single to 0  touching to 1  nuclei to 2  dusts to 3
#
tmp=categorical(labels drop=True)
targets=categorical(labels drop=True)argmax(1)
print targets


The output console yields first:

features
[[ 038846334  097681855]
[ 38318634   05724734 ]
[ 067710876  101816444]
[ 112024943  091508699]
[ 751749674  100156707]]
labels
[[single]
[touching]
[single]
[single]
[nuclei]]


I meet then the following error:

Traceback (most recent call last):
File ""/home/claire/Applications/ProjetPython/projet particule et objet/karyotyper/DAPI-Trainer02-MILKpy""  line 83  in 
tmp=categorical(labels drop=True)
File ""/usr/local/lib/python26/dist-packages/scikitsstatsmodels-030rc1-py26egg/scikits/statsmodels/tools/toolspy""  line 206  in categorical
tmp_dummy = (tmp_arr[: None]==data)astype(float)
AttributeError: 'bool' object has no attribute 'astype'


Is it possible to convert the category variable 'type' within the dataframe into int ? 'type' can take the values 'single'  'touching' 'nuclei' 'dusts' and I need to convert with int values such 0  1  2  3

Thanks for advices

Jean-Patrick",601314,,,,2011-10-19 12:43:50,Convert array of string (category) to array of int from a pandas dataframe,<python><numpy><pandas>,2.0,,1.0,
14646336,1,,2013-02-01 12:29:00,1,50,"I have an intra day series of log returns over multiple days that I would like to downsample to daily ohlc I can do something like

hi = seriesresample('B'  how=lambda x: npmax(npcumsum()))
low = seriesresample('B'  how=lambda x: npmin(npcumsum())) 


But it seems inefficient to compute cumsum on each call Is there a way to first compute the cumsums and then apply 'ohcl' to the data?

1999-08-09 12:30:00-04:00   -0000486
1999-08-09 12:31:00-04:00   -0000606
1999-08-09 12:32:00-04:00   -0000120
1999-08-09 12:33:00-04:00   -0000037
1999-08-09 12:34:00-04:00   -0000337
1999-08-09 12:35:00-04:00    0000100
1999-08-09 12:36:00-04:00    0000219
1999-08-09 12:37:00-04:00    0000285
1999-08-09 12:38:00-04:00   -0000981
1999-08-09 12:39:00-04:00   -0000487
1999-08-09 12:40:00-04:00    0000476
1999-08-09 12:41:00-04:00    0000362
1999-08-09 12:42:00-04:00   -0000038
1999-08-09 12:43:00-04:00   -0000310
1999-08-09 12:44:00-04:00   -0000337

1999-09-28 06:45:00-04:00    0000000
1999-09-28 06:46:00-04:00    0000000
1999-09-28 06:47:00-04:00    0000000
1999-09-28 06:48:00-04:00    0000102
1999-09-28 06:49:00-04:00   -0000068
1999-09-28 06:50:00-04:00    0000136
1999-09-28 06:51:00-04:00    0000566
1999-09-28 06:52:00-04:00    0000469
1999-09-28 06:53:00-04:00    0000000
1999-09-28 06:54:00-04:00    0000000
1999-09-28 06:55:00-04:00    0000000
1999-09-28 06:56:00-04:00    0000000
1999-09-28 06:57:00-04:00    0000000
1999-09-28 06:58:00-04:00    0000000
1999-09-28 06:59:00-04:00    0000000
",220120,,,,2013-02-01 15:17:24,Pandas - grouping intra day timeseries by date,<python><numpy><pandas><time-series>,2.0,,1.0,
7776679,1,,2011-10-15 08:21:17,6,2913,"I try to merge dataframes by rows doing:

bigdata=data1append(data2)


and I get the following error:

Exception: Index cannot contain duplicate values!


The index of the first data frame starts from 0 to 38 and the second one from 0 to 48 I didn't understand that I have to modify the index of one of the data frame before merging  but I don't know how to

Thank you

These are the two dataframes:

data1:

    meta  particle  ratio   area    type    
0   2     part10    1348   08365  touching
1   2     part18    1558   08244  single  
2   2     part2     1893   0894   single  
3   2     part37    06695  1005   single  
clip
36  2     part23    1051   08781  single  
37  2     part3     8054   09714  nuclei  
38  2     part34    1071   09337  single  


data2:

    meta  particle  ratio    area    type    
0   3     part10    04756   1025   single  
1   3     part18    004387  1232   dusts   
2   3     part2     1132    08927  single  
clip
46  3     part46    1371    1001   nuclei  
47  3     part3     07439   09038  single  
48  3     part34    04349   09956  single 


the first column is the index",601314,,667301.0,2011-10-15 10:30:18,2012-10-20 06:56:19,append two data frame with pandas,<python><pandas>,2.0,7.0,1.0,
7837722,1,7837947.0,2011-10-20 14:46:14,12,4228,"I want to perform my own complex operations on financial data in dataframes in a sequential manner

For example I am using the following MSFT CSV file taken from Yahoo Finance:

Date Open High Low Close Volume Adj Close

2011-10-19 2737 2747 2701 2713 42880000 2713

2011-10-18 2694 2740 2680 2731 52487900 2731

2011-10-17 2711 2742 2685 2698 39433400 2698

2011-10-14 2731 2750 2702 2727 50947700 2727




I then do the following:

#!/usr/bin/env python
from pandas import *

df = read_csv('tablecsv')

for i  row in enumerate(dfvalues):
    date = dfindex[i]
    open  high  low  close  adjclose = row
    #now perform analysis on open/close based on date  etc


Is that the most efficient way? Given the focus on speed in pandas  I would assume there must be some special function to iterate through the  values in a manner that one also retrieves the index (possibly through a generator to be memory efficient)? dfiteritems unfortunately only iterates column by column",1005409,,577088.0,2012-07-23 17:15:25,2012-07-29 04:53:26,What is the most efficient way to loop through dataframes with pandas?,<python><pandas>,4.0,2.0,8.0,
14474093,1,14474271.0,2013-01-23 06:59:11,1,48,"Data:

date                                              price1

2001/11/01 00:00:01am                               10

2001/11/02 00:00:02am                               20

2001/11/03 00:00:03am                               15

2001/11/04 00:00:04am                               30


Using a pandas data frame (df) we can take the values of the first column with this way:

dfprice1


If we want to take the days what could you do?

dfdate


is not working",1992474,,,,2013-01-23 07:24:41,Time series through pandas,<python><pandas>,1.0,,,
14536747,1,,2013-01-26 12:04:50,0,52,"How can I compute the eigenvalues and eigenvectors of a masked NumPy array (for an unmasked array  this can be achieved by scipylinalgeig)

Edit:

Turns out you can do this after all

# a list of numpy matrices
import numpy as np
import numpyma as npma

import numpymatlib as npm
import scipy as sp
import scipylinalg as splin

import pandas as pd

# read in numpy data
import urllib
dataURL = 'http://archiveicsuciedu/ml/machine-learning-databases/arrhythmia/arrhythmiadata'
dataFile = urlliburlopen(dataURL)

# read in the data as a NumPy dataset
aArr = npgenfromtxt(dataFile  dtype = npfloat 
                         delimiter = ' '  missing_values = '?')

aArrMasked = npmamasked_array(aArr  npisnan(aArr))
aArrMaskedCenter = aArrMasked - npmamean(aArrMasked  axis=0)
print splineig(npmacov(aArrMaskedCenter)) 
",1414455,,1414455.0,2013-01-26 12:51:00,2013-01-26 12:51:00,Eigenvalues of masked array (NumPy),<python><numpy><scipy><pandas>,,2.0,,
14591855,1,,2013-01-29 20:41:42,2,41,"I created a file by using:

store = pdHDFStore('/home//datah5')


and stored some tables using:

store['firstSet'] = df1
storeclose()


I closed down python and reopened in a fresh environment

How do I reopen this file?

When I go:

store = pdHDFStore('/home//datah5')


I get the following error

Traceback (most recent call last):
  File """"  line 1  in 
  File ""/misc/apps/linux/python-261/lib/python26/site-packages/pandas-0100-py26-linux-x86_64egg/pandas/io/pytablespy""  line 207  in __init__
    selfopen(mode=mode  warn=False)
  File ""/misc/apps/linux/python-261/lib/python26/site-packages/pandas-0100-py26-linux-x86_64egg/pandas/io/pytablespy""  line 302  in open
    selfhandle = _tables()openFile(selfpath  selfmode)
  File ""/apps/linux/python-261/lib/python26/site-packages/tables/filepy""  line 230  in openFile
    return File(filename  mode  title  rootUEP  filters  **kwargs)
  File ""/apps/linux/python-261/lib/python26/site-packages/tables/filepy""  line 495  in __init__
    self_g_new(filename  mode  **params)
  File ""hdf5Extensionpyx""  line 317  in tableshdf5ExtensionFile_g_new (tables/hdf5Extensionc:3039)
tablesexceptionsHDF5ExtError: HDF5 error back trace

  File ""H5Fc""  line 1582  in H5Fopen
    unable to open file
  File ""H5Fc""  line 1373  in H5F_open
    unable to read superblock
  File ""H5Fsuperc""  line 334  in H5F_super_read
    unable to find file signature
  File ""H5Fsuperc""  line 155  in H5F_locate_signature
    unable to find a valid file signature

End of HDF5 error back trace

Unable to open/create file '/home//datah5'


What am I doing wrong here?  Thank you",1911092,,1240268.0,2013-01-29 20:44:32,2013-01-29 20:44:32,pandas HDFStore - how to reopen?,<python><pandas>,,6.0,,
14650341,1,14658413.0,2013-02-01 16:07:53,0,46,"i am having some trouble masking a panel in the same way that I would a DataFrame  What I want to do feels simple  but I have not found a way looking at the docs and online forums  I have a simple example below:

import pandas
import numpy as np
import datetime
start_date = datetimedatetime(2009 3 1 6 29 59)
r = pandasdate_range(start_date  periods=12)
cols_1 = ['AAPL'  'AAPL'  'GOOG'  'GOOG'  'GS'  'GS']
cols_2 = ['close'  'rate'  'close'  'rate'  'close'  'rate']
dat = nprandomrandn(12  6)

dftst = pandasDataFrame(dat  columns=pandasMultiIndexfrom_arrays([cols_1  cols_2]  names=['ticker' 'field'])  index=r)
pn = dftstTto_panel()transpose(2 0 1)
print pn

Out[14]: 

Dimensions: 2 (items) x 12 (major_axis) x 3 (minor_axis)
Items axis: close to rate
Major_axis axis: 2009-03-01 06:29:59 to 2009-03-12 06:29:59
Minor_axis axis: AAPL to GS


I now have a Panel object  if I take a slice along the items axis  I get a DataFrame

close_p = pn['close']
print close_p

Out[16]: 
ticker                   AAPL      GOOG        GS
2009-03-01 06:29:59 -0082203 -0286354  1227193
2009-03-02 06:29:59  0340005 -0688933 -1505137
2009-03-03 06:29:59 -0525567  0321858 -0035047
2009-03-04 06:29:59 -0123549 -0841781 -0616523
2009-03-05 06:29:59 -0407504  0188372  1311262
2009-03-06 06:29:59  0272883  0817179  0584664
2009-03-07 06:29:59 -1767227  1168876  0443096
2009-03-08 06:29:59 -0685501 -0534373 -0063906
2009-03-09 06:29:59  0851820  0068740  0566537
2009-03-10 06:29:59  0390678 -0012422 -0152375
2009-03-11 06:29:59 -0985585 -0917705 -0585091
2009-03-12 06:29:59  0067498 -0764343  0497270


I can filter this data in two ways:

1) I create a mask and mask the data as follows:

msk = close_p > 0
close_p = close_pmask(msk)


2) I can just slice by the boolean operator in msk above

close_p = close_p[close_p > 0]
Out[28]: 
ticker                   AAPL      GOOG        GS
2009-03-01 06:29:59       NaN       NaN  1227193
2009-03-02 06:29:59  0340005       NaN       NaN
2009-03-03 06:29:59       NaN  0321858       NaN
2009-03-04 06:29:59       NaN       NaN       NaN
2009-03-05 06:29:59       NaN  0188372  1311262
2009-03-06 06:29:59  0272883  0817179  0584664
2009-03-07 06:29:59       NaN  1168876  0443096
2009-03-08 06:29:59       NaN       NaN       NaN
2009-03-09 06:29:59  0851820  0068740  0566537
2009-03-10 06:29:59  0390678       NaN       NaN
2009-03-11 06:29:59       NaN       NaN       NaN
2009-03-12 06:29:59  0067498       NaN  0497270


What I cannot figure out how to do is filter all of my data based on a mask without a for loop  I can do the following:

msk = (pn['rate'] > 0) & (pn['close'] > 0)
def mask_panel(pan  msk):
    for item in panitems:
        pan[item] = pan[item]mask(msk)
    return pan
print pn['close']

Out[32]: 
ticker                   AAPL      GOOG        GS
2009-03-01 06:29:59 -0082203 -0286354  1227193
2009-03-02 06:29:59  0340005 -0688933 -1505137
2009-03-03 06:29:59 -0525567  0321858 -0035047
2009-03-04 06:29:59 -0123549 -0841781 -0616523
2009-03-05 06:29:59 -0407504  0188372  1311262
2009-03-06 06:29:59  0272883  0817179  0584664
2009-03-07 06:29:59 -1767227  1168876  0443096
2009-03-08 06:29:59 -0685501 -0534373 -0063906
2009-03-09 06:29:59  0851820  0068740  0566537
2009-03-10 06:29:59  0390678 -0012422 -0152375
2009-03-11 06:29:59 -0985585 -0917705 -0585091
2009-03-12 06:29:59  0067498 -0764343  0497270

mask_panel(pn  msk)

print pn['close']

Out[34]: 
ticker                   AAPL      GOOG        GS
2009-03-01 06:29:59 -0082203 -0286354       NaN
2009-03-02 06:29:59       NaN -0688933 -1505137
2009-03-03 06:29:59 -0525567       NaN -0035047
2009-03-04 06:29:59 -0123549 -0841781 -0616523
2009-03-05 06:29:59 -0407504       NaN       NaN
2009-03-06 06:29:59       NaN       NaN       NaN
2009-03-07 06:29:59 -1767227       NaN       NaN
2009-03-08 06:29:59 -0685501 -0534373 -0063906
2009-03-09 06:29:59       NaN       NaN       NaN
2009-03-10 06:29:59       NaN -0012422 -0152375
2009-03-11 06:29:59 -0985585 -0917705 -0585091
2009-03-12 06:29:59       NaN -0764343       NaN


So the above loop does the trick  I know there is a faster vectorized way of doing this using the ndarray  but I have not put that together yet  It also seems like this should be functionality that is built into the pandas library  If there is a way to do this that I am missing  any suggestions would be much appreciated",1988295,,,,2013-02-02 03:55:38,boolean mask in pandas panel,<python><panel><pandas><mask>,1.0,3.0,,
8966871,1,,2012-01-23 03:21:00,0,192,"I'm trying to run a script at work which uses the Python labeled array module Pandas which is built on top of Numpy but Python crashes every time with an ""Unhandled Exception"" error pointing Numpy/core/multiarraypyc  This happens when I execute the code through IDLE  Python  or PythonW  I wasn't doing anything complicated or working with very much data so  as a test  I took the code home and ran it on my personal computer with the same versions of Python/Numpy/Pandas installed and it works fine  I have an older and extremely secure computer at work and I'm wondering if anyone knows tips about numpy that may cause it to crash if running in this type of environment

I'm using Python 262 with Numpy 161 and Pandas 070  At work I have a Dell running 32bit Windows XP SP3 with an Intel Duo 2 e8400

Are there firewall settings  registry settings or something else that might not be set that could be the cause of the problem?",1164240,,,,2012-01-23 03:21:00,Running Python/Numpy/Pandas on older secure computer,<python><numpy><pandas>,,3.0,,
5955695,1,6178993.0,2011-05-10 19:55:08,11,6454,Is anyone aware of a good tutorial  list of examples  or guide on using pandas with PyTables?,687739,,,,2012-07-22 02:04:09,Tutorial on PANDAS and PYTABLES,<python><pytables><pandas>,3.0,,6.0,2012-12-04 04:36:45
7577546,1,7580456.0,2011-09-28 01:58:38,5,960,"I am trying to subsample rows of a DataFrame according to a grouping  Here is an example  Say I define the following data:

from pandas import *
df = DataFrame({'group1' : [""a"" ""b"" ""a"" ""a"" ""b"" ""c"" ""c"" ""c"" ""c"" 
                            ""c"" ""a"" ""a"" ""a"" ""b"" ""b"" ""b"" ""b""] 
                'group2' : [1 2 3 4 1 3 5 6 5 4 1 2 3 4 3 2 1] 
                'value'  : [""apple"" ""pear"" ""orange"" ""apple"" 
                            ""banana"" ""durian"" ""lemon"" ""lime"" 
                            ""raspberry"" ""durian"" ""peach"" ""nectarine"" 
                            ""banana"" ""lemon"" ""guava"" ""blackberry"" ""grape""]})


If I group by group1 and group2  then the number of rows in each group is here:

In [190]: dfgroupby(['group1' 'group2'])['value']agg({'count':len})
Out[190]: 
      count
a  1  2    
   2  1    
   3  2    
   4  1    
b  1  2    
   2  2    
   3  1    
   4  1    
c  3  1    
   4  1    
   5  2    
   6  1    


(If there is an even more concise way to compute that  please tell)

I now want to construct a DataFrame that has one randomly selected row from each group  My proposal is to do it like so:

In [215]: from random import choice
In [216]: grouped = dfgroupby(['group1' 'group2'])
In [217]: subsampled = groupedapply(lambda x: dfreindex(index=[choice(range(len(x)))]))
In [218]: subsampledindex = range(len(subsampled))
In [219]: subsampled
Out[219]: 
    group1  group2  value
0   b       2       pear 
1   a       1       apple
2   b       2       pear 
3   a       1       apple
4   a       1       apple
5   a       1       apple
6   a       1       apple
7   a       1       apple
8   a       1       apple
9   a       1       apple
10  a       1       apple
11  a       1       apple


which works  However  my real data has about 25 million rows and 12 columns  If I do this the dirty way by building my own data structures  I can complete this operation in a matter of seconds  However  my implementation above does not finish within 30 minutes (and does not appear to be memory-limited)  As a side note  when I tried implementing this in R  I first tried plyr  which also did not finish in a reasonable amount of time; however  a solution using datatable finished very rapidly

How do I get this to work rapidly with pandas?  I want to love this package  so please help!

Thanks!

Uri",510187,,403310.0,2012-10-24 10:24:35,2012-10-24 10:24:35,"Using pandas, how do I subsample a large DataFrame by group in an efficient manner?",<python><r><numpy><pandas><data.table>,1.0,,2.0,
8916302,1,8916746.0,2012-01-18 19:41:27,5,1415,"I have a dataframe df in pandas that was built using pandasread_table from a csv file The dataframe has several columns and it is indexed by one of the columns (which is unique  in that each row has a unique value for that column used for indexing) 

How can I select rows of my dataframe based on a ""complex"" filter applied to multiple columns? I can easily select out the slice of the dataframe where column colA is greater than 10 for example:

df_greater_than10 = df[df[""colA""] > 10]


But what if I wanted a filter like: select the slice of df where any of the columns are greater than 10? 

Or where the value for colA is greater than 10 but the value for colB is less than 5?

How are these implemented in pandas?
Thanks",248237,,,,2012-11-16 15:12:56,selecting across multiple columns with python pandas?,<python><csv><numpy><tab-delimited><pandas>,2.0,,2.0,
9339184,1,9339501.0,2012-02-18 06:26:50,1,462,"This is Pandas dataframe
I want to convert 1D data into 2D array form 

How to convert from 

  'A'  'B'  'C'
1  10   11   a  
2  10   12   b
3  10   13   c 
4  20   11   d  
5  20   12   e
6  20   13   f

to this 2d array as the following

   11 12 13
10  a  b  c 
20  d  e  f
",1088821,,1088821.0,2012-02-18 07:57:42,2012-11-05 06:03:43,"Python: Pandas, Dataframe, Convert 1column data into 2D data format",<python><transform><data.frame><pandas>,2.0,,,
8991709,1,8992714.0,2012-01-24 17:59:53,26,1884,"I recently came across the pandas library for python  which according to this benchmark performs very fast in-memory merges  It's even faster than the datatable package in R (my language of choice for analysis)

Why is pandas so much faster than datatable?  Is it because of an inherent speed advantage python has over R  or is there some tradeoff I'm not aware of?  Is there a way to perform inner and outer joins in datatable without resorting to merge(X  Y  all=FALSE) and merge(X  Y  all=TRUE)?



Here's the R code and the Python code used to benchmark the various packages",345660,,345660.0,2012-01-24 18:07:05,2012-01-25 05:27:58,Why are pandas merges in python faster than data.table merges in R?,<python><r><join><data.table><pandas>,2.0,12.0,14.0,
9283166,1,9283497.0,2012-02-14 19:42:19,1,1817,"I'm just getting started with Python/Pandas and put together the following code to plot the S&P 500

from pandasiodata import DataReader
sp500 = DataReader(""^GSPC""  ""yahoo""  start=datetimedatetime(1990  1  1)) # returns a DataFrame
sp500plot(figsize = (12 8))


It looks like this is plotting the high  low  open  close  adj close  and volume all on one graph  Is there an easy way to plot just the adj close in one graph and the volume right below it like they do on Yahoo and most other financial sites?  I'd also be interested in an example of plotting an OHLC candlestick chart",64421,,,,2012-02-15 01:10:28,Plotting a stock chart with Pandas in IPython,<matplotlib><pandas>,1.0,1.0,1.0,
9516422,1,9526061.0,2012-03-01 12:40:33,1,400,"I'm new to pandas and I have started by trying to read a table completely composed by 1s and 0s and I'm using the read_csv function to do it Everything goes OK and I get a DataFrame with int64 as column types The problem appears when I introduce NaN values In that case I get a DataFrame with column types as float64 Is this the expected behaviour? Is the NaN value incompatible with the int type?

I have also tried to cast the DataFrame with the float column by doing DataFrame(data  dtype=numpyint64)  but in this case I get something like:

                    col1   col2
row1 -9223372036854775808      1
row2                    1      0
",1242647,,151019.0,2012-03-01 12:41:49,2012-03-01 23:52:59,"nan, floats and ints",<pandas>,1.0,,,
9550867,1,9551068.0,2012-03-03 23:42:13,4,810,"I am trying to do a pivot table of frequency counts using Panda 

I have the following code:  

 from pandas import pivot_table  DataFrame  crosstab
 import numpy as np
 df=DataFrame({'Y':[99999991  99999992  99999993  99999994  99999995  99999996  99999997  99999998  99999999] 'X':[1  2  3  4  5  6     7  8  9] 'X2':[1  2  3  4  5  6  7  8  9]})
 print pivot_table(df rows=['Y']  cols=['X'] aggfunc=npsum)


This is my output:  

 X          1   2   3   4   5   6   7   8   9
 Y                                         
  99999991   1 NaN NaN NaN NaN NaN NaN NaN NaN
  99999992 NaN   2 NaN NaN NaN NaN NaN NaN NaN
  99999993 NaN NaN   3 NaN NaN NaN NaN NaN NaN
  99999994 NaN NaN NaN   4 NaN NaN NaN NaN NaN
  99999995 NaN NaN NaN NaN   5 NaN NaN NaN NaN
  99999996 NaN NaN NaN NaN NaN   6 NaN NaN NaN
  99999997 NaN NaN NaN NaN NaN NaN   7 NaN NaN
  99999998 NaN NaN NaN NaN NaN NaN NaN   8 NaN
  99999999 NaN NaN NaN NaN NaN NaN NaN NaN   9


This is my desired output:  

 X          1   2   3   4   5   6   7   8   9
 X2                                         
   1   99999991 NaN NaN NaN NaN NaN NaN NaN NaN
   2 NaN   99999992 NaN NaN NaN NaN NaN NaN NaN
   3 NaN NaN   99999993 NaN NaN NaN NaN NaN NaN
   4 NaN NaN NaN   99999994 NaN NaN NaN NaN NaN
   5 NaN NaN NaN NaN   99999995 NaN NaN NaN NaN
   6 NaN NaN NaN NaN NaN   99999996 NaN NaN NaN
   7 NaN NaN NaN NaN NaN NaN   99999997 NaN NaN
   8 NaN NaN NaN NaN NaN NaN NaN   99999998 NaN
   9 NaN NaN NaN NaN NaN NaN NaN NaN   99999999


This is what I keep getting:  


Int64Index: 9 entries  1 to 9
Data columns:
('Y'  1L)    1  non-null values
('Y'  2L)    1  non-null values
('Y'  3L)    1  non-null values
('Y'  4L)    1  non-null values
('Y'  5L)    1  non-null values
('Y'  6L)    1  non-null values
('Y'  7L)    1  non-null values
('Y'  8L)    1  non-null values
('Y'  9L)    1  non-null values
dtypes: float64(9)


Does anyone know why?  Is the output too big I can't seem to find anything on it",1026987,,815724.0,2012-03-04 00:46:29,2012-03-09 01:52:25,Python Panda Pivot Table,<python><python-2.7><pivot-table><pandas>,2.0,1.0,2.0,
14593304,1,,2013-01-29 22:34:51,0,37,"I am using pandas for graphing data for a cluster of nodes I find that pandas is repeating color values for the different series  which makes them indistinguishable

I tried giving custom color values like this and passed the my_colors to the colors field in plot:

  my_colors = []
  for node in nodes_list:
    my_colorsappend(rand_color())


rand_color() is defined as follows:

  def rand_color():
    from random import randrange
    return ""#%s"" % """"join([hex(randrange(16  255))[2:] for i in range(3)])


But here also I need to avoid color values that are too close to distinguish I sometimes have as many as 60 nodes (series)  Most probably a hard-coded list of color values would be best option?",814907,,,,2013-01-30 13:31:49,Colors for pandas timeline graphs with many series,<matplotlib><pandas>,1.0,2.0,,
14661701,1,14661768.0,2013-02-02 12:03:46,1,21,"I have a dataframe df :

>>> df
                  sales  discount  net_sales    cogs
STK_ID RPT_Date                                     
600141 20060331   2709       NaN      2709   2245
       20060630   6590       NaN      6590   5291
       20060930  10103       NaN     10103   7981
       20061231  15915       NaN     15915  12686
       20070331   3196       NaN      3196   2710
       20070630   7907       NaN      7907   6459


Then I want to drop rows with certain sequence numbers which indicated in a list  suppose here is [1 2 4]  then left:

                  sales  discount  net_sales    cogs
STK_ID RPT_Date                                     
600141 20060331   2709       NaN      2709   2245
       20061231  15915       NaN     15915  12686
       20070630   7907       NaN      7907   6459


How or what function can do that ?",1072888,,733291.0,2013-02-02 12:25:47,2013-02-02 12:34:41,How to drop a list of rows from Pandas dataframe?,<python><pandas>,1.0,,,
14665828,1,14665875.0,2013-02-02 19:52:18,0,21,"I've been trying to find out how to select a certain value based on multiple other values in the same tuple of a dataframe  The data looks like this(copied from the current dataframe)

    DealID      PropId LoanId   ServicerId    ServicerPropId
0   BAC98765      15   000015    30220144       010-002-001
1   BAC98765      16   000016    30220092       010-003-001
2   BAC98765      45   000045    30220155       010-045-001
3   BAC98765      48   000048    30220157       010-048-001


In SQL terms what I would like to accomplish is this:  

Select ServicerPropId from dataframe
 where DealID = 'BAC98765' and ServicerId = '30220144'  


I've tried a few different ways to slice the data  but can't seem to figure out how to get multiple selection criteria to work and return only 1 value into a variable",1478684,,733291.0,2013-02-02 20:09:24,2013-02-02 20:22:10,How can I select a certain value based on 2(or more) other values in a pandas dataframe,<python><pandas>,1.0,,,
8451327,1,,2011-12-09 20:27:24,0,799,"I utilize python's map() function to pass parameters to a trading model and output the results  I use itertoolsproduct to find all the possible combinations of the two parameters  then pass the combination to my function named ""run""  The function run returns a pandas dataframe of returns  The Column header is a tuple of the two parameters and the sharpe ratio of the returns See below:

def run((x y)): 
ENTRYMULT = x
PXITR1PERIOD = y

create_trade()
pull_settings()    
pull_marketdata()
create_position()
create_pnl_output()

return DataFrame(DF3['NETPNL']values  index=DF3index  columns=[(ENTRYMULT PXITR1PERIOD SHARPE)])


My main() function uses the Pool() capability to run map() on all 8 cores:

if __name__ == '__main__':    
global DF3
pool = Pool()    
test1 =poolmap(run list(itertoolsproduct([x * 01 for x in range(10 12)]  range(100 176 25))))
print test1


I realize the map function can only output lists The output is a list of the header from the returned dataframe My output from print test1 looks like this:

[(10  150  -85010673966997263)
2011-11-17  1863                          
2011-11-18  1786                          
2011-11-21  1701                          
2011-11-22  1592                          
2011-11-23  1556                          
2011-11-24  1556                          
2011-11-25  1536                          
2011-11-28  1518                          
2011-11-29  1584                          
2011-11-30  NaN                                          (10  175  -94016837593189102)
2011-11-17  2263                          
2011-11-18  2203                          
2011-11-21  2136                          
2011-11-22  1993                          
2011-11-23  1977                          
2011-11-24  1977                          
2011-11-25  1968                          
2011-11-28  1916                          
2011-11-29  1956                          
2011-11-30  NaN                                          (11  100  -20255968672741457)
2011-11-17  1203                          
2011-11-18  1095                          
2011-11-21  1003                          
2011-11-22  9003                          
2011-11-23  8221                          
2011-11-24  8221                          
2011-11-25  7903                          
2011-11-28  7709                          
2011-11-29  6444                          
2011-11-30  NaN                                          (11  125  -18178187305758119)
2011-11-17  1464                          
2011-11-18  1376                          
2011-11-21  1289                          
2011-11-22  1185                          
2011-11-23  1134                          
2011-11-24  1134                          
2011-11-25  1116                          
2011-11-28  1106                          
2011-11-29  1014                          
2011-11-30  NaN                                          (11  150  -14486791104380069)
2011-11-17  2625                          
2011-11-18  2557                          
2011-11-21  2476                          
2011-11-22  2374                          
2011-11-23  2348                          
2011-11-24  2348                          
2011-11-25  2343                          
2011-11-28  2338                          
2011-11-29  2293                          
2011-11-30  NaN                                          (11  175  -12118290962161304)
2011-11-17  2466                          
2011-11-18  2421                          
2011-11-21  2357                          
2011-11-22  2214                          
2011-11-23  2206                          
2011-11-24  2206                          
2011-11-25  2211                          
2011-11-28  2164                          
2011-11-29  2124                          
2011-11-30  NaN                            ] 


My end goal is to have a pandas dataframe with an index (same for all returns)  column headers of the (ENTRYMULT  PXITR1PERIOD  SHARPE) with the corresponding returns below  I will then be doing a pairwise correlation calculation across all of the return series  ",1023631,,894284.0,2011-12-09 20:33:38,2011-12-12 18:50:53,Python map() function output into Pandas DataFrame,<python><pandas>,1.0,,1.0,
9588331,1,9620832.0,2012-03-06 17:01:47,6,489,"I stumbled across pandas and it looks ideal for simple calculations that I'd like to do (I have a SAS background and was thinking it'd replace proc freq)  and it looks like it'll scale to what I may want to do in the future However  I just can't seem to get my head around a simple task (I'm not sure if I'm supposed to look at pivot/crosstab/indexing - whether I should have a Panel or DataFrames etc) I'm hoping if someone wouldn't mind giving me some pointers on how to do the following:

I have two CSV files (one for year 2010  one for year 2011 - simple transactional data) - eg: [cols are category and amount)

2010:
AB 10000
AB 20000
AC 15000
AD 50000

2011:
AB 50000
AC 25000
AX 90000

These are loaded into separate DataFrame objects

What I'd like to do is get the category  the sum of the category  and the frequency of the category  eg:

2010:
AB 30000 2
AC 15000 1
AD 50000 1

2011:
AB 50000 1
AC 25000 1
AX 90000 1

I'm sure it's simple  but I can't work out whether I should be using pivot/crosstab/groupby/an index etc I can get either the sum or the frequency - I can't seem to get both It gets a bit more complex because I would like to do it on a month by month basis  but I think if someone would be so kind to point me to the right technique/direction I'll be able to go from there

Kind regards 

Jon",1252759,,,,2012-03-08 19:49:23,Simple cross-tabulation in pandas,<pandas>,2.0,2.0,1.0,
6467832,1,6468875.0,2011-06-24 12:31:45,4,2035,"I have two sets of temperature date  which have readings at regular (but different) time intervals I'm trying to get the correlation between these two sets of data

I've been playing with Pandas to try to do this I've created two timeseries  and am using TimeSeriesAcorr(TimeSeriesB) However  if the times in the 2 timeSeries do not match up exactly (they're generally off by seconds)  I get Null as an answer I could get a decent answer if I could:

a)  Interpolate/fill missing times in each TimeSeries (I know this is possible in Pandas  I just don't know how to do it)

b) strip the seconds out of python datetime objects (Set seconds to 00  without changing minutes) I'd lose a degree of accuracy  but not a huge amount 

c) Use something else in Pandas to get the correlation between two timeSeries

d) Use something in python to get the correlation between two lists of floats  each float having a corresponding datetime object  taking into account the time

Anyone have any suggestions?",814005,,,,2011-06-24 14:01:15,How to get the correlation between two timeseries using Pandas,<python><statistics><correlation><pandas>,1.0,,5.0,
9189425,1,9198129.0,2012-02-08 07:42:11,1,548,"I'm having trouble writing the entries of a pandas dataframe to a stringbuffer 

It's possible to initialize a dataframe by passing a stringbuffer to the read_csv function

In [80]: buf = StringIO('a b\n1 2\n')
In [81]: df = pandasread_csv(buf)
In [82]: df
Out[82]: 
   a  b
0  1  2


To do the opposite is not straight forward as the DataFrameto_csv function only accepts a string file path

Is there any good reason for this behaviour? What's the best way to serialize a pandas DataFrame without storing the contents on disk first?",81657,,,,2012-02-08 23:15:46,Pandas DataFrame serialization,<python><io><pandas>,1.0,,,
9556892,1,9557224.0,2012-03-04 16:58:45,2,1093,"This is my first time trying Pandas  I think I have a reasonable use case  but I am stumbling I want to load a tab delimited file into a Pandas Dataframe  then group it by Symbol and plot it with the xaxis indexed by the TimeStamp column  Here is a subset of the data:

Symbol Price M1 M2 Volume TimeStamp
TBET 219 3 805 1124179 9:59:14 AM
FUEL 3949 9 115 109674 9:59:11 AM
SUNH 437 6 009 24394 9:59:09 AM
FUEL 39099 8 111 105265 9:59:09 AM
TBET 218 2 803 1121629 9:59:05 AM
ORBC 34 2 022 10509 9:59:02 AM
FUEL 38599 7 107 102116 9:58:47 AM
FUEL 38544 6 105 100116 9:58:40 AM
GBR 383 4 046 64251 9:58:24 AM
GBR 38 3 045 63211 9:58:20 AM
XRA 36167 3 012 42310 9:58:08 AM
GBR 375 2 034 47521 9:57:52 AM
MPET 142 3 026 44600 9:57:52 AM


Note two things about the TimeStamp column; 1) it has duplicate values and 2) the intervals are irregular

I thought I could do something like this

from pandas import *
import pylab as plt

df = read_csv('datatxt' index_col=5)
dfsort(ascending=False)

dfplot()
pltshow()


But the read_csv method raises an exception ""Tried columns 1-X as index but found duplicates"" Is there an option that will allow me to specify an index column with duplicate values?  

I would also be interested in aligning my irregular timestamp intervals to one second resolution  I would still wish to plot multiple events for a given second  but maybe I could introduce a unique index  then align my prices to it?",963033,,487339.0,2012-03-04 17:00:46,2012-04-12 18:14:41,Pandas DataFrame - desired index has duplicate values,<python><data.frame><pandas>,1.0,,1.0,
9652832,1,9652858.0,2012-03-11 06:00:56,1,469,"I'm new to python and pandas and coming from R  I'm trying to get a tsv file loaded into a pandas DataFrame  

This is what I'm trying and the error I'm getting:

>>> df1 = DataFrame(csvreader(open('c:/~/trainSetRel3txt')  delimiter='\t'))

Traceback (most recent call last):
  File """"  line 1  in 
    df1 = DataFrame(csvreader(open('c:/~/trainSetRel3txt')  delimiter='\t'))
  File ""C:\Python27\lib\site-packages\pandas\core\framepy""  line 318  in __init__
    raise PandasError('DataFrame constructor not properly called!')
PandasError: DataFrame constructor not properly called!
",914308,,,,2012-03-11 15:34:23,Python Pandas How Do I load a tsv file into a Pandas DataFrame?,<python><r><pandas><tsv>,2.0,,,
8270129,1,9300391.0,2011-11-25 13:55:02,0,368,"I have a pandas panel of investment pricing data to which I want to add two new minor axis columns (portfolio holding and benchmark holding)

The initial panel is:



Dimensions: 4 (items) x 463 (major) x 8 (minor)
Items: ListedEquity:BHP Billiton:BHPAX to SavingsAccount:ING Australia Savings Maximiser
Major axis: 2010-01-04 00:00:00 to 2011-10-31 00:00:00
Minor axis: content_type to xrate


which conceptually looks like:


         Investment 1   Investment 2
         c1 c2 c3    c1 c2 c3
Date 1
Date 2



Is it possible create a matching panel that only has these columns and then somehow merge the two?

Thoughts on possible alternative ways of achieving this?

The documentation on the Panel data structure is pretty bare

EDIT:

I created the 2nd panel and tried p1join(p2) but this generates a columns overlap error

Here's the 2nd panel I would like to append:



Dimensions: 4 (items) x 463 (major) x 2 (minor)
Items: ListedEquity:BHP Billiton:BHPAX to SavingsAccount:Comsec Cash Management Account
Major axis: 2010-01-04 00:00:00 to 2011-10-31 00:00:00
Minor axis: benchmark to portfolio
",995182,,995182.0,2011-11-25 14:27:35,2012-02-15 19:58:06,Pandas Panel for share portfolio,<panel><quantitative-finance><pandas>,1.0,,,
8273092,1,8281757.0,2011-11-25 18:39:02,1,835,"I have the academic distribution of EPD 71 for MacOS 106x  which has pandas 03 version THe latest official version is 050 so I though I would upgrade to the latest 

Here is what I did:

saved the zip file of the source from here
executed sudo python setuppy install
ran tests by executing nosetests pandas
Please keep in mind I am a total newbie to python Any insights on why my install failed would be greatly appreciated Below is a snippet of the errors The entire log can be found at Error log

Results:

Ran 1498 tests in 55011s
FAILED (SKIP=4  errors=91  failures=14)

======================================================================
ERROR: test_generate (test_daterangeTestGeneration)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/Library/Frameworks/EPD64framework/Versions/71/lib/python27/site-packages/pandas/core/tests/test_daterangepy""  line 20  in test_generate
    rng2 = list(generate_range(START  END  timeRule='WEEKDAY'))
TypeError: generate_range() got an unexpected keyword argument 'timeRule'


@wesm - thanks for your quick reply Next time I will definitely use the mailing list So I actually backtracked installed the official 050 release and compiled from source I received 3 errors (see below) I did not experience any problems running sudo python setuppy install

======================================================================
ERROR: testForSeries (pandasstatsteststest_olsTestPanelOLS)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/Library/Frameworks/EPD64framework/Versions/71/lib/python27/site-packages/pandas/stats/tests/test_olspy""  line 472  in testForSeries
    selfseries_x  selfseries_y)
  File ""/Library/Frameworks/EPD64framework/Versions/71/lib/python27/site-packages/pandas/stats/tests/test_olspy""  line 565  in checkForSeries
    reference = ols(y=series_y  x=series_x  **kwds)
  File ""/Library/Frameworks/EPD64framework/Versions/71/lib/python27/site-packages/pandas/stats/interfacepy""  line 133  in ols
    return klass(**kwargs)
  File ""/Library/Frameworks/EPD64framework/Versions/71/lib/python27/site-packages/pandas/stats/olspy""  line 36  in __init__
    import scikitsstatsmodelsapi as sm
ImportError: No module named api

======================================================================
ERROR: testNonPooled (pandasstatsteststest_olsTestPanelOLS)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/Library/Frameworks/EPD64framework/Versions/71/lib/python27/site-packages/pandas/stats/tests/test_olspy""  line 522  in testNonPooled
    selfcheckNonPooled(y=selfpanel_y  x=selfpanel_x)
  File ""/Library/Frameworks/EPD64framework/Versions/71/lib/python27/site-packages/pandas/stats/tests/test_olspy""  line 528  in checkNonPooled
    result = ols(y=y  x=x  pool=False  **kwds)
  File ""/Library/Frameworks/EPD64framework/Versions/71/lib/python27/site-packages/pandas/stats/interfacepy""  line 133  in ols
    return klass(**kwargs)
  File ""/Library/Frameworks/EPD64framework/Versions/71/lib/python27/site-packages/pandas/stats/plmpy""  line 780  in __init__
    nw_overlap=nw_overlap)
  File ""/Library/Frameworks/EPD64framework/Versions/71/lib/python27/site-packages/pandas/stats/interfacepy""  line 133  in ols
    return klass(**kwargs)
  File ""/Library/Frameworks/EPD64framework/Versions/71/lib/python27/site-packages/pandas/stats/olspy""  line 36  in __init__
    import scikitsstatsmodelsapi as sm
ImportError: No module named api

======================================================================
ERROR: test_auto_rolling_window_type (pandasstatsteststest_olsTestPanelOLS)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/Library/Frameworks/EPD64framework/Versions/71/lib/python27/site-packages/pandas/stats/tests/test_olspy""  line 604  in test_auto_rolling_window_type
    window_model = ols(y=y  x=data  window=20  min_periods=10)
  File ""/Library/Frameworks/EPD64framework/Versions/71/lib/python27/site-packages/pandas/stats/interfacepy""  line 133  in ols
    return klass(**kwargs)
  File ""/Library/Frameworks/EPD64framework/Versions/71/lib/python27/site-packages/pandas/stats/olspy""  line 521  in __init__
    OLS__init__(self  y=y  x=x  **self_args)
  File ""/Library/Frameworks/EPD64framework/Versions/71/lib/python27/site-packages/pandas/stats/olspy""  line 36  in __init__
    import scikitsstatsmodelsapi as sm
ImportError: No module named api
",668624,,668624.0,2011-11-25 20:39:11,2011-11-26 22:18:07,python: pandas install errors,<python><compiler-errors><install><pandas>,2.0,,,
8842114,1,8842419.0,2012-01-12 20:52:41,5,636,"I'm playing with pandas and trying to apply string slicing on a Series of strings object
Instead of getting the strings sliced  the series gets sliced:

In [22]: s = pSeries(data=['abcdef']*20)
In [23]: sapply(lambda x:x[:2])
Out[24]:
0    abcdef
1    abcdef


On the other hand:

In [25]: sapply(lambda x:x+'qwerty')
Out[25]:
0     abcdefqwerty
1     abcdefqwerty
2     abcdefqwerty



I got it to work by using the map function instead  but I think I'm missing something about how it's supposed to work

Would very much appreciate a clarification",490288,,,,2012-01-12 21:23:57,How to apply slicing on pandas Series of strings,<python><pandas>,2.0,2.0,1.0,
9421412,1,9421580.0,2012-02-23 21:14:19,2,310,"If I use the following methodology to construct a pandasDataFrame  I get an output that (I think) is peculiar:

import pandas  numpy

df = pandasDataFrame(numpyrandomrand(100 2)  index = numpyarange(100)  columns = ['s1' 's2'])
smoothed = pandasDataFrame(pandasewma(df  span = 21)  index = dfindex  columns = ['smooth1' 'smooth2'])


When I go to look at the smoothed values  I get:

>>> smoothedtail()
smooth1  smooth2
95      NaN      NaN
96      NaN      NaN
97      NaN      NaN
98      NaN      NaN
99      NaN      NaN


This seems like it an aggregation of the following fragmented calls  which yield different results:

smoothed2 = pandasDataFrame(pandasewma(df  span = 21))
smoothed2index = dfindex
smoothed2columns = ['smooth1' 'smooth2']


Again using the DataFrametail() invocation I get:

>>> smoothed2tail()
smooth1   smooth2
95  0496021  0501153 
96  0506118  0507541
97  0516655  0544621
98  0520212  0543751
99  0518170  0572429


Can anyone provide rationale as to why these to DataFrame construction methodologies should be different?",963989,,173292.0,2012-02-23 21:34:29,2012-02-23 21:34:29,trying to explain pandas.DataFrame behavior,<python><numpy><pandas>,1.0,,1.0,
9762193,1,9762583.0,2012-03-18 20:51:48,3,494,"I see that Pandas does not allow duplicate time series indexes yet (https://githubcom/pydata/pandas/issues/643)  but will be added soon  I am wondering if there is a good way to apply rolling window means to a dataset with duplicate times by a multi-index tag/column

Basically I have a csv of non-ordered events that consist of epochtime   hierarchical tags (tag1  tag2)  and time taken A small sample:

 epochTimeMS event tag timeTakenMS
 1331782842801 event1 tag1 16
 1331782841535 event1 tag2 1278
 1331782842801 event1 tag1 17
 1331782842381 event2 tag1 436


What I want to do is build and graph rolling means with varying ms windows  by event and event+tag This seems like it should be accomplished in Pandas  but not sure if I will need to wait until the duplicate time-series indexes first  Any thoughts on hacking this in place now?",1277360,,,,2012-03-18 21:43:52,Pandas rolling median for duplicate time series data,<python><matplotlib><pandas>,1.0,,1.0,
9794697,1,9846881.0,2012-03-20 20:49:24,1,922,"I'm running daily simulations in a batch: I do 365 simluations to get results for a full year  After every run  I want to extract some arrays from the results and add them to a pandasDataFrame for analysis later 

I have a rough model (doing an optimisation) and a more precise model for a post-simulation  so I can get the same variable from two sources In case the post-simulation is done  the results may overwrite the optimization results
To make it more complicated  the optimization model has a smaller output interval  depending on the discretisation settings  but the final analysis will happen on the larger interval of the post-simulation) 

What is the best way to construct this DataFrame?  

This was my first appraoch:

creation of an empty DataFrame df for the full year  with DateRange index with the larger post- simulation interval (=15 minutes)
do optimization for 1 day ==> create temporary df_temp with DateRange as index with smaller interval
downsample this DataFrame to 15 minutes as described here: 
update df with df_temp (rows in df are still empty  except for the last row of the previous run  so I have to take df_temp[1:])
do simulation for same day ==> create temporary df_temp2 with interval = 15min
overwrite the corresponding rows in df with df_temp2
Which methods should I use in step 4) and 6)?  Or is there a better way from the start?
Thanks 
Roel",566942,,,,2012-03-23 21:45:21,Best way to construct a pandas.DataFrame composed of different chunks,<python><pandas>,1.0,,1.0,
9826431,1,,2012-03-22 16:23:22,0,669,"I am trying to insert a pandas (pandaspydataorg) DataFrame into a Postgresql DB (91) in the most efficient way (using Python 27)
Using ""cursorexecute_many"" is really slow  so is ""DataFrameto_csv(buffer )"" together with ""copy_from""
I found an already much! faster solution on the web (http://eatthedotsblogspotde/2008/08/faking-read-support-for-psycopgshtml) which I adapted to work with pandas 
My code can be found below
My question is whether the method of this related question (using ""copy from stdin with binary"") can be easily transferred to work with DataFrames and if this would be much fasterUse binary COPY table FROM with psycopg2
Unfortunately my Python skills aren't sufficient to understand the implementation of this approach
This is my approach:


import psycopg2
import connectDB # this is simply a module that returns a connection to the db
from datetime import datetime

class ReadFaker:
    """"""
    This could be extended to include the index column optionally Right now the index
    is not inserted
    """"""
    def __init__(self  data):
        selfiter = dataitertuples()

    def readline(self  size=None):
        try:
            line = selfiternext()[1:]  # element 0 is the index
            row = '\t'join(xencode('utf8') if isinstance(x  unicode) else str(x) for x in line) + '\n'
        # in my case all strings in line are unicode objects
        except StopIteration:
            return ''
        else:
            return row

    read = readline

def insert(df  table  con=None  columns = None):

    time1 = datetimenow()
    close_con = False
    if not con:
        try:
            con = connectDBgetCon()   ###dbLoader returns a connection with my settings
            close_con = True
        except psycopg2Error  e:
            print epgerror
            print epgcode
            return ""failed""
    inserted_rows = dfshape[0]
    data = ReadFaker(df)

    try:
        curs = concursor()
        print 'inserting %s entries into %s ' % (inserted_rows  table)
        if columns is not None:
            curscopy_from(data  table  null='nan'  columns=[col for col in columns])
        else:
            curscopy_from(data  table  null='nan')
        concommit()
        cursclose()
        if close_con:
            conclose()
    except psycopg2Error  e:
        print epgerror
        print epgcode
        conrollback()
        if close_con:
            conclose()
        return ""failed""

    time2 = datetimenow()
    print time2 - time1
    return inserted_rows
",942591,,,,2012-05-28 08:41:04,Fast insertion of pandas DataFrame into Postgres DB using psycopg2,<python><postgresql><psycopg2><pandas>,1.0,,,
8914992,1,8915588.0,2012-01-18 18:00:16,3,453,"I have a list of csv files (""file1""  ""file2""  "") that have two columns but do not have header labels I'd like to assign them header labels and them as a DataFrame which is indexed by file and then indexed by those column labels For example  I tried:

import pandas

mydict = {}
labels = [""col1""  ""col2""]
for myfile in [""file1""  ""file2""]:
  my_df = pandasread_table(myfile  names=labels)
  # build dictionary of dataframe records
  mydict[myfile] = my_df

test = pandasDataFrame(mydict)


this produces a DataFrame  test  indexed by ""myfile1""  ""myfile2"" however  I'd like each of those to be indexed by ""col1"" and ""col2"" as well My questions are:

how can I make it so the first index is file  and second index are columns I assigned (in the variable labels)? So that I can write:

test[""myfile1""][""col1""]
right now  test[""myfile1""] only gives me a series of records

also  how can I then reindex things so that the first indices are the column labels of each file and the second is the filename? So that I can write:

test[""col1""][""myfile1""]
or print test[""col1""] and then see the value of ""col1"" shown for myfile1  myfile2  etc",248237,,,,2012-01-18 18:45:57,indexing several csv files with pandas from records?,<python><csv><numpy><tab-delimited><pandas>,1.0,1.0,1.0,
9641916,1,,2012-03-09 22:36:52,5,545,"Hi I'm an inexperienced python user trying to get my code (running in eclipse) to import pandas

I get the following error: ""ImportError: numpycoremultiarray failed to import"" when I try to import pandas I'm using python27  pandas 071  and numpy 151

anybody have any thoughts?",1260307,,1301710.0,2012-08-11 15:44:04,2012-08-17 14:46:24,Python Pandas: can't find numpy.core.multiarray when importing pandas,<python><numpy><pandas>,4.0,2.0,,
9695668,1,9712271.0,2012-03-14 03:50:45,2,1079,"I have some text files whose format looks like below:


000423||     300|1|015000|            |
000425||     600|1|015000|            |
000503||     400|1|015000|            |
000522||        |2|       |    1982080|
000527||     900|1|015000|            |
000528|    |     300|1|015000|            |  




when I use read_csv to load them into DataFrame  it doesn't generate correct dtype for some columns For example  the first column is parsed as int  not unicode str  the third column is parsed as unicode str  not int  because of one missing data Is there a way to preset the dtype of the DataFrame  just like the numpygenfromtxt does?

Updates:
I used read_csv like this which caused the problem:

data = pandasread_csv(StringIO(etf_info)  sep='|'  skiprows=14  index_col=0  
                       skip_footer=1  names=['ticker'  'name'  'vol'  'sign'  
                       'ratio'  'cash'  'price']  encoding='gbk')


In order to solve both the dtype and encoding problems  I need to use unicode() and numpygenfromtxt first:

etf_info = unicode(urllib2urlopen(etf_url)read()  'gbk')
nd_data = npgenfromtxt(StringIO(etf_info)  delimiter='|'  
                        skiprows=14  skip_footer=1  dtype=ETF_DTYPE)
data = pandasDataFrame(nd_data  index=nd_data['ticker'] 
                        columns=['name'  'vol'  'sign'  
                                 'ratio'  'cash'  'price'])


It would be nice if read_csv can add dtype and usecols settings Sorry for my greed ^_^",1267974,,1267974.0,2012-03-15 02:25:00,2012-03-15 02:25:00,How to specify dtype when using pandas.read_csv to load data from csv files?,<python><pandas>,1.0,1.0,3.0,
9927711,1,9941582.0,2012-03-29 14:42:42,0,460,"I am using pandas to read a csv file The data are numbers but stored in the csv file as text Some of the values are non-numeric when they are bad or missing How do I filter out these values and convert the remaining data to integers 

I assume there is a better/faster way than looping over all the values and using isdigit() to test for them being numeric 

Does pandas or numpy have a way of just recognizing bad values in the reader? If not  what is the easiest way to do it? Do I have to specific the dtypes to make this work?",1024495,,,,2012-03-30 10:54:02,Reading csv in python pandas and handling bad values,<python><numpy><pandas>,3.0,,2.0,
8957175,1,8957495.0,2012-01-21 22:12:10,1,321,"The following code should do what I want but it takes 10gb of ram by the time it is 20% done with the loop 

# In [4]: type(pd)
# Out[4]: pandassparseframeSparseDataFrame
memid = unique(pdMember)
pan = {}
for mem in memid:
    pan[mem] = pd[pdMember==mem]
goal = pandasPanel(pan)
",1162837,,,,2012-01-21 23:05:12,DataFrame to Panel indexed by nonunique column with Pandas,<python><data.frame><panels><pandas>,1.0,4.0,,
9555635,1,9557319.0,2012-03-04 14:25:36,10,1218,"I used Enthought's python distribution as a graduate student for data analysis and really enjoyed it  But I've recently taken a job which takes away my ability to use it  

I prefer python for initial scoping and cleaning the data  and R for the stats side  Part of the impetus for wanting this though  is trying out pandas  And other other part is I don't have proper licensure (or the means to pay)  which is clearly a problem

So is there some other well put together easy to install python distribution that I can get numpy  scipy  sci-kits  and all the other goodness?

Thanks",402468,,,,2013-01-27 11:25:23,Open Source Enthought Python Alternative,<python><numpy><scipy><enthought><pandas>,7.0,4.0,3.0,
9877391,1,9918868.0,2012-03-26 18:11:04,3,407,"DataFrame I have:

            A   B   C 
2012-01-01  1   2   3 
2012-01-05  4   5   6 
2012-01-10  7   8   9 
2012-01-15  10  11  12 


What I am using now:

date_after = dtdatetime( 2012  1  7 )
frameix[date_after:]ix[0:1]
Out[1]: 
            A  B  C
2012-01-10  7  8  9


Is there any better way of doing this?  I do not like that I have to specify ix[0:1] instead of ix[0]  but if I don't the output changes to a TimeSeries instead of a single row in a DataFrame  I find it harder to work with a rotated TimeSeries back on top of the original DataFrame

Without ix[0:1]:

frameix[date_after:]ix[0]
Out[1]: 
A    7
B    8
C    9
Name: 2012-01-10 00:00:00


Thanks 

John",1279469,,,,2012-03-29 03:54:52,How to get the closest single row after a specific datetime index using Python Pandas,<python><pandas>,1.0,,,
9892044,1,,2012-03-27 14:54:46,2,494,"I'm having some trouble parsing the dates of a file when reading it with Pandas

I'm using python(x y)  version 27

The file i'm trying to read has the following format:

""
SomethingSomethig 
SomethingSomethig 
SomethingSomethig 
Est dir Vmed raj Vmin desvpadro date 
555 162 530 1010 650 067 200901010000 
555 135 610 1090 640 067 200901010010 
555 156 590 1100 590 076 200901010020 
555 178 690 1090 530 096 200901010030 
555 200 980 1120 610 096 200901010040 
555 100 970 1140 570 096 200901010050 ""


Using the following line of code:


  dados = read_csv(file  sep="" ""  skiprows=3  index_col=6 parse_dates=True)


the output is:

""""
Int64Index: 157968 entries  200901010000 to 201112312350
Data columns:
Est            157968  non-null values
dir            157968  non-null values
Vmed           157968  non-null values
raj            157968  non-null values
Vmin           157968  non-null values
desvpadr?o    157968  non-null values
Unnamed: 7     157968  non-null values
dtypes: float64(4)  int64(2)  object(1)
""""


With the dates not parsed And I get an error when tryin to use the dates to perform any kind of calculations I dont know how to use the converter and could really use your help",1295828,,1137055.0,2012-03-28 14:22:26,2012-04-04 21:02:27,Pandas Date converter,<parsing><date><pandas>,2.0,,,
9941288,1,9941490.0,2012-03-30 10:30:59,1,230,"Recently I'm doing some work with two Series in pandas:

The first Series contains purely numerical data
The second Series contains categorical data: ""Plus""  ""Minus""  and NaN
Example data:

first_series = pandasSeries([0000003  0004991  0004991])
second_series = pandasSeries([""Plus""  ""Minus""  npnan]  dtype=""object"" 
                              index=first_seriesindex)


(in the real-world scenario  the second Series is built programmatically using the same index as the first one  but here it's just a simplified example)

I'm doing first some manipulation:

first_series = nplog2(1 / first_series)


Then I'd need to invert the sign (multiply by -1) of the corresponding ""Minus"" entries  and turn to NaN the ones that in the second series are NaN

The latter part works OK:

first_series[npinvert(second_seriesnotnull())] = npnan

print first_series

0    18567557
1     7646459
2          NaN
Name: Example data


However I'm kind of stuck with the former part How can I use the information in the second Series (given that they are identically-indexed) to switch the sign in the first Series?

As a reference  after the application  first_series should become like this:

0    18567557
1    -7646459
2          NaN
Name: Example data
",241515,,,,2012-03-30 10:46:02,pandas: Change one Series by using a second one with the same index,<python><pandas>,1.0,,,
9943848,1,9966145.0,2012-03-30 13:22:40,0,795,"I have been using the scikitsstatsmodels OLS predict function to forecast fitted data but would now like to shift to using Pandas

The documentation refers to OLS as well as to a function called y_predict but I can't find any documentation on how to use it correctly

By way of example:

exogenous = {""1998"": ""4760"" ""1999"": ""5904"" ""2000"": ""4504"" ""2001"": ""9808"" ""2002"": ""4241"" ""2003"": ""4086"" ""2004"": ""4687"" ""2005"": ""7686"" ""2006"": ""3740"" ""2007"": ""3075"" ""2008"": ""3753"" ""2009"": ""4679"" ""2010"": ""5468"" ""2011"": ""7154"" ""2012"": ""4292"" ""2013"": ""4283"" ""2014"": ""4595"" ""2015"": ""9194"" ""2016"": ""4221"" ""2017"": ""4520""}
endogenous = {""1998"": ""691""  ""1999"": ""1580""  ""2000"": ""80""  ""2001"": ""1450""  ""2002"": ""555""  ""2003"": ""956""  ""2004"": ""877""  ""2005"": ""614""  ""2006"": ""468""  ""2007"": ""191""}

import numpy as np
from pandas import *

ols_test = ols(y=Series(endogenous)  x=Series(exogenous))


However  while I can produce a fit:

>>> ols_testy_fitted
1998     675268299
1999     841176837
2000     638141913
2001    1407354228
2002     600000352
2003     577521485
2004     664681478
2005    1099611292
2006     527342854
2007     430901264


Prediction produces nothing different:

>>> ols_testy_predict
1998     675268299
1999     841176837
2000     638141913
2001    1407354228
2002     600000352
2003     577521485
2004     664681478
2005    1099611292
2006     527342854
2007     430901264


In scikitsstatsmodels one would do the following:

import scikitsstatsmodelsapi as sm

ols_model = smOLS(endogenous  npcolumn_stack(exogenous))
ols_results = ols_modfit()
ols_pred = ols_modpredict(npcolumn_stack(exog_prediction_values))


How do I do this in Pandas to forecast the endogenous data out to the limits of the exogenous?

UPDATE: Thanks to Chang  the new version of Pandas (073) now has this functionality as standard",295606,,295606.0,2012-04-13 13:43:28,2012-04-13 13:43:28,Forecasting using Pandas OLS,<python><pandas><scikits>,1.0,,1.0,
10015284,1,,2012-04-04 16:22:28,1,1217,"How do I create an datetime index ""foo"" to use with raw data series 
(Example would ""as of"" every 15 seconds 'foo' and and every 30 seconds 'foo2') If raw series can be inserted into a 'base' dataframe  I would like to use 'foo' to recast the dataframe

If wanted series to combine combine df ""foo"" and df ""foo2""  what would be the memory hits 
Would it be better to fill the foo index with the raw data series    

EDIT:
after import pandas   datetimetimedelta stops working",428862,,428862.0,2012-04-04 19:36:45,2012-04-07 20:11:27,How you create a datetime index in pandas,<python><numpy><pandas>,1.0,,1.0,
10017938,1,10021367.0,2012-04-04 19:27:14,2,303,"I am trying do use a pandas multiindex to select a partial slice at the top level index (date)  and apply a list to the second level index (stock symbol)  Ie below I want the data for AAPL and MSFT in the range d1:d2

The partial slice works fine  however it is not clear how to select both AAPL and MSFT from the second index  while avoiding GOOG in the middle  

If I swap the levels it works with a single symbol  but not a list

In [93]: print df
                 f1  f2  c1
date       sym
2012-01-01 AAPL  5  2   3
           GOOG  1  2   3
           MSFT  4  2   3
2012-01-02 AAPL  8  2   3
           GOOG  6  2   3
           MSFT  7  2   3
2012-01-03 AAPL  11  2   3
           GOOG  9  2   3
           MSFT  10  2   3

In [94]: print dfix[d1:d2]swaplevel(0 1)ix['AAPL']
            f1  f2  c1
date
2012-01-01  5   2   3
2012-01-02  8   2   3

In [95]: print dfix[d1:d2]swaplevel(0 1)ix[['AAPL'  'MSFT']]

TypeError: Expected tuple  got str


I want to AVOID building a long tuple list ie:

t = [(d1  'AAPL')  (d1  'MSFT')  (d2  'AAPL')  (d2  'MSFT')]


which DOES work when passed to ix  Below is my desired output

In [103]: print dfix[t]
                 f1  f2  c1
date       sym
2012-01-01 AAPL  5   2   3
           MSFT  4   2   3
2012-01-02 AAPL  8   2   3
           MSFT  7   2   3


Thanks 
John",1279469,,487339.0,2012-04-04 19:28:21,2012-04-05 01:04:36,How to index with a list of values with only one label in a Pandas MultiIndex,<python><pandas><multi-index>,1.0,,1.0,
9788299,1,9794823.0,2012-03-20 14:00:00,2,545,"I have a pandasDataFrame df1  indexed with a pandasDateRange object

If I have a d1 and d2  as datetimes  why does df[d1:d2]not work  and how can i obtain this slice?

Roel",566942,,,,2012-12-19 03:07:09,convenient slicing of DataFrames with datetime indexes in pandas,<python><pandas>,2.0,,,
9962114,1,10763488.0,2012-04-01 05:25:20,3,392,"I currently use R routinely for statistical process control With this I can produce control charts such as EWMA  Shewhart  CUSUM and GAM / Loess smoothing

Does anyone know of the best way to do these types of charts using Python? I initially looked at scikitstimeseries but it has been canned to contribute to pandas

I had a look at pandas and although it does have EWMA functionality  I need a little bit more ",390388,,2683.0,2012-04-09 00:10:00,2012-05-26 04:17:13,Control Charts in Python,<python><charts><pandas>,2.0,,,
10003171,1,10003697.0,2012-04-03 23:59:41,1,198,"With doBy package in R  we do a summary on group and get results in the same shape and order as the original data:

> require(doBy)
> df  df
  first second      data
1   bar    one -0424972
2   bar    two  0567020*emphasized text*
3   baz    one  0276232
4   baz    two -1087401
5   foo    one -0673690
6   foo    two  0113648
7   qux    one -1478427
8   qux    two  0524988
> df['datasum'] = summaryBy(data~first  data=df  FUN=sum  fulldimension=T)['datasum']
> df
  first second      data  datasum
1   bar    one -0424972  0142048
2   bar    two  0567020  0142048
3   baz    one  0276232 -0811169
4   baz    two -1087401 -0811169
5   foo    one -0673690 -0560042
6   foo    two  0113648 -0560042
7   qux    one -1478427 -0953439
8   qux    two  0524988 -0953439


Is there a way to do the same in pandas  when the DataFrame is grouped by one of multiple indexes?

>>> from pandas import DataFrame
>>> df = DataFrame({ 
                 'first': ['bar'  'bar'  'baz'  'baz'  'foo'  'foo'  'qux'  'qux'] 
                 'second': ['one'  'two'  'one'  'two'  'one'  'two'  'one'  'two'] 
                 'data': [-0424972  0567020  0276232  -1087401  -0673690  0113648  -1478427  0524988] })
>>> df = dfset_index(['first'  'second'])
>>> s = dfgroupby(level='first')['data']sum()
>>> dfjoin(s  on='first'  rsuffix='sum')

KeyError: 'no item named first'
",688693,,688693.0,2012-04-04 00:44:11,2012-04-07 20:24:34,"What is an efficient way in pandas to do summaryBy(...,full.dimension=T)",<python><r><pandas>,2.0,1.0,,
10020591,1,11603242.0,2012-04-04 23:17:23,1,932,"I have a times series with temperature and radiation in a pandas dataframe The time resolution is 1 minute in regular steps

import datetime
import pandas as pd
import numpy as np

date_times = pddate_range(datetimedatetime(2012  4  5  8  0) 
                           datetimedatetime(2012  4  5  12  0) 
                           freq='1min')
tamb = nprandomsample(date_timessize) * 100
radiation = nprandomsample(date_timessize) * 100
frame = pdDataFrame(data={'tamb': tamb  'radiation': radiation} 
                     index=date_times)
frame

DatetimeIndex: 241 entries  2012-04-05 08:00:00 to 2012-04-05 12:00:00
Freq: T
Data columns:
radiation    241  non-null values
tamb         241  non-null values
dtypes: float64(2)


How can I down-sample this dataframe to a resolution of one hour  computing the hourly mean for the temperature and the hourly sum for radiation? ",1301710,,1301710.0,2012-10-13 08:31:46,2013-01-10 09:50:34,How to downsample a dataframe with different methods?,<python><numpy><time-series><pandas>,4.0,2.0,2.0,
9938130,1,10064444.0,2012-03-30 06:41:28,2,575,"This is my first attempt at playing with Pandas library after attending Wesley's tutorial at pycon

After poking around a bit with the dataframe I am glad I was able to massage the data in the way I wanted but having trouble in plotting it I guess it also points to my naiveness with the matplotlib library

What I have is pandas Series object with the following data I would like to plot as a barplot with col 1 ('file') as the labels oriented vertically

sample data here:
http://pastebincom/y2w0uJPQ       

Thanks!
-Abhi",369541,,,,2012-04-08 16:37:48,plotting stacked barplots on a panda data frame,<python><pandas>,2.0,1.0,,
10065051,1,,2012-04-08 18:01:13,11,1768,"The documentation for Pandas has numerous examples of best practices for working with data stored in various formats

However  I am unable to find any good examples for working with databases like MySQL for example

Can anyone point me to links or give some code snippets of how to convert query results using mysql-python to data frames in Panda efficiently ?",1320615,,,,2013-01-09 22:48:13,python-pandas and databases like mysql,<python><pandas>,4.0,,7.0,
10130734,1,,2012-04-12 19:51:25,0,294,"I want to use a pandas dataframe to keep track of some market data I will be downloading live during the trading day

Let's say I want to log the prices of AAPL and GOOG I start by creating a dataframe:

prices = DataFrame(columns = ['AAPL'  'GOOG']) 


Let's say the first datapoint comes in at at time t1 and price 5550 for AAPL  And then a few seconds later at t2  a price of 4300 comes in for GOOG

One of course can't do:

prices['AAPL'][t1] = 5550
prices['GOOG'][t2] = 4300


Is there an easy/fast way in pandas to accomplish this though besides pulling the index  modifying it  reindexing the dataframe and then inserting each scalar price as it comes in?

Thanks!",1330060,,4913.0,2012-04-12 19:56:36,2012-04-12 22:04:48,Best way to insert a new value,<python><pandas>,1.0,,,
10133021,1,10149202.0,2012-04-12 22:56:34,4,969,"I have the following data frame

In[45]: data[:10]  
Out[45]:
   Z    A    beta2    M      shell
0  100  200  03112   1972 -4213
1  100  200 -04197   202   -1143
2  100  200  003205  203    0    
3  100  201  02967   191   -4434
4  100  201 -04893   1961 -4691
5  100  202  03084   1834 -4134
6  100  202 -04873   1882 -475 
7  100  202 -02483   1884 -1106
8  100  203  03069   1771 -4355
9  101  203 -04956   1825 -5217


My question is  how can I group/transform the data in such a way that I have a MultiIndex with (Z A) as indexes(or MultiIndexes) having into account that the data is not unique? To clear my goal this is what I expect to achieve:

             beta2[1] beta2[2]  beta2[3]   M[1]   M[2]   M[3]   shell[1]   shell[2]  shell[3]
   Z    A 
0  100  200  03112   -04197   003205    1972  202    203    -4213     -1143    0
1  100  201  02967   04893    NaN        191    1961  NaN    -4434     -4691    NaN
2  100  202  03084   -04873   NaN        1834  1882  NaN    -4134     -475     NaN
3  100  203  03069   NaN       NaN        1771  NaN    NaN    -4355     NaN       NaN 
4  101  203  -04956  NaN       NaN        1825  NaN    NaN    -5217     NaN       NaN


I understand that this involves at least two steps  one for the uniqueness and one for the indexing in Z A so any help in one of those steps is appreciated  also  is there some data structure which is maybe more appropiate for this problem?  

Edit: I have found that the line:


  data=dataset_index(('Z' 'A'))  


solves the the problem of the indexing in Z A Unfortunately this only works if (Z A) pairs are unique",1330293,,1330293.0,2012-04-13 20:52:13,2012-04-13 22:40:25,Multiindex from array in Pandas with non unique data,<python><pandas>,1.0,,1.0,
9721429,1,9730620.0,2012-03-15 14:08:31,5,404,"I just got my hands on pandas and am figuring out how I can read a file The file is from WRDS database and is the SP500constituentslist all the way back to 1960s I checked the file and no matter what I do to import it using 'read_csv'  i still cant display the data correctly

df = read_csv('sp500-sbtxt')

df


Int64Index: 1231 entries  0 to 1230
Data columns: gvkeyx   from   thru   conm
                    gvkey   co_conm
(the column names)
dtypes: object(1)


What does the above chunk of output mean? Anything would be helpful",1610626,,1610626.0,2012-03-16 05:40:19,2012-03-17 19:53:59,How do I read a fix width format text file in pandas,<python><pandas>,3.0,2.0,,
9758450,1,9762084.0,2012-03-18 12:53:06,1,880,"I have manipulated some data using pandas and now I want to carry out a batch save back to the database This requires me to convert the dataframe into an array of tuples  with each tuple corresponding to a ""row"" of the dataframe

My DataFrame looks something like:

In [182]: data_set
Out[182]: 
  index data_date   data_1  data_2
0  14303 2012-02-17  2475   2503 
1  12009 2012-02-16  2500   2507 
2  11830 2012-02-15  2499   2515 
3  6274  2012-02-14  2468   2505 
4  2302  2012-02-13  2462   2477 
5  14085 2012-02-10  2438   2461 


and I want to convert it to an array of tuples like:

[(datetimedate(2012 2 17) 2475 2503) 
(datetimedate(2012 2 16) 2500 2507) 
etc ]


Any suggestion on how I can efficiently do this?",939715,,,,2012-12-05 19:42:50,Pandas convert dataframe to array of tuples,<python><pandas>,2.0,,1.0,
9762935,1,9772031.0,2012-03-18 22:34:26,3,2624,"I'm a beginning pandas user  and after studying the documentation I still can't find a straightforward way to do the following 

I have a DataFrame with a pandasDateRange index  and I want to add a column with values for part of the same DateRange  

Suppose I have 

df

                            A         B
2010-01-01 00:00:00  0340717  0702432
2010-01-01 01:00:00  0649970  0411799
2010-01-01 02:00:00  0932367  0108047
2010-01-01 03:00:00  0051942  0526318
2010-01-01 04:00:00  0518301  0057809
2010-01-01 05:00:00  0779988  0756221
2010-01-01 06:00:00  0597444  0312495


and 
    df2

                     C
2010-01-01 03:00:00  5
2010-01-01 04:00:00  5
2010-01-01 05:00:00  5


How can I obtain something like this:

                            A         B    C
2010-01-01 00:00:00  0340717  0702432    nan
2010-01-01 01:00:00  0649970  0411799    nan
2010-01-01 02:00:00  0932367  0108047    nan
2010-01-01 03:00:00  0051942  0526318    5
2010-01-01 04:00:00  0518301  0057809    5
2010-01-01 05:00:00  0779988  0756221    5
2010-01-01 06:00:00  0597444  0312495    nan


Thanks a lot in advance 

Roel",566942,,,,2012-03-19 14:30:06,Add indexed column to DataFrame with pandas,<python><pandas>,1.0,,,
10145224,1,10147050.0,2012-04-13 17:08:02,3,522,"I'm doing something wrong with merge and I can't understand what it is I've done the following to estimate a histogram of a series of integer values:

import pandas as pnd
import numpy  as np

series = pndSeries(nprandompoisson(5  size = 100))
tmp  = {""series"" : series  ""count"" : npones(len(series))}
hist = pndDataFrame(tmp)groupby(""series"")sum()
freq = (hist / histsum())rename(columns = {""count"" : ""freq""})


If I print hist and freq this is what I get:

> print hist
        count
series       
0           2
1           4
2          13
3          15
4          12
5          16
6          18
7           7
8           8
9           3
10          1
11          1

> print freq 
        freq
series      
0       002
1       004
2       013
3       015
4       012
5       016
6       018
7       007
8       008
9       003
10      001
11      001


They're both indexed by ""series"" but if I try to merge:

> df   = pndmerge(freq  hist  on = ""series"")


I get a KeyError: 'no item named series' exception If I omit on = ""series"" I get a IndexError: list index out of range exception

I don't get what I'm doing wrong May be ""series"" is an index and not a column so I must do it differently? ",114388,,,,2012-04-13 19:22:11,Pandas: trouble understading how merge works,<python><pandas>,1.0,,,
9597681,1,9608269.0,2012-03-07 07:56:37,2,416,"I'm trying to read a csv file that holds several values in every cell and I want to encode them to a single int formatted byte to be stored in a pandas cell  (eg (1  1) -> 771) For that I would like to use the converters parameter of the read_csv function The problem is that I don't know the names of the columns before hand and the value to be passed to the converters should be a dict with the column names as keys In fact I want to convert all columns with the same converter function For that it would be better to write:

read_csv(fhand  converter=my_endocing_function)


than:

read_csv(fhand  converters={'col1':my_endocing_function 
                            'col2':my_endocing_function 
                            'col3':my_endocing_function })


Is something like that possible? Right now to solve the issue I'm doing:

dataframe = read_csv(fhand)
enc_func = numpyvectorize(encoderencode_genotype)
dataframe = dataframeapply(enc_func  axis=1)


But I guess that this approach might be less efficient
By the way I have similar doubts with the formatters used by the to_string method",1242647,,,,2012-03-07 20:04:31,read_csv converters for unkown columns,<pandas>,1.0,,,
9621362,1,9624150.0,2012-03-08 16:42:30,3,1111,"Using pandas I can compute

simple moving average SMA using pandasstatsmomentsrolling_mean
exponential moving average EMA using pandasstatsmomentsewma
But how do I compute a weighted moving average (WMA) as described in wikipedia http://enwikipediaorg/wiki/Exponential_smoothing  using pandas?

Is there a pandas function to compute a WMA?  ",1257502,,1257502.0,2012-03-08 18:55:44,2012-03-08 20:09:06,how do I compute a weighted moving average using pandas,<python><pandas>,1.0,2.0,,
9723000,1,9739828.0,2012-03-15 15:33:47,4,533,"I have a csv file where one of the columns is a date/time string  How do I parse it correctly with pandas?  I don't want to make that column the index  Thanks!

Uri",510187,,,,2012-03-16 15:13:30,"How do I tell pandas to parse a particular column as a datetime object, but not make it an index?",<python><parsing><datetime><pandas>,1.0,,3.0,
10145025,1,10149110.0,2012-04-13 16:52:55,2,481,"I'm trying to index data by their probability (estimated with a simple histogram) The objective is to select items in the series with a probability less then some threshold

I have a series of integer values  for example:

import pandas as pnd
import numpy  as np

series = pndSeries(nprandompoisson(5  size = 100))


then I calculate their histogram like this:

tmp  = {""series"" : series  ""count"" : npones(len(series))}
hist = pndDataFrame(tmp)groupby(""series"")sum()
freq = hist / histsum()


So now I have the frequencies of each result indexed by the result  and the series of results I have now two questions:

Is there a way to index series by the mapping of result/frequency defined by freq?
If I manage to do this  how do I select only results with frequency greater than some value?
Thanks  ",114388,,,,2012-04-13 22:27:10,Pandas: index data by a histogram result,<python><statistics><pandas>,1.0,,2.0,
10158613,1,,2012-04-15 00:37:31,4,858,"I had 071 pandas before today I tried to update and I simply ran the exe file supplied by the website

now I tried "" import pandas"" but then it gives me an error

ImportError: C extensions not built: if you installed already verify that you are not importing from the source directory

I am new to python and pandas in general Anything will help 

thanks ",1610626,,1610626.0,2012-08-13 12:57:27,2012-08-22 08:03:40,Importing Confusion Pandas,<python><pandas>,3.0,1.0,,
10194482,1,10195347.0,2012-04-17 15:45:10,3,633,"I am starting to render plots with matplotlib as I learn both python and this interesting plotting library I need help with a custom plot for a problem I am working on May be there is an inbuilt function already for this

Problem:
I am trying to draw a table(rectangle) as a plot with 96 individual cells ( 8 rows X 12 cols) Color each alternative cell with a specific color ( like a chess board : instead of black/white I will use some other color combination) and insert value for each cell from a pandas data frame or python dictionary Show the col and row labels on the side

Sample Data:   http://pastebincom/N4A7gWuH

I would like the plot to look something like this substituting the values in the cells from a numpy/pandas ds

Sample Plot:   http://picpastecom/sample-E0DZaoXkpng

Appreciate your input

PS: did post the same on mathplotlib's mailing list

Thanks!
-Abhi",369541,,369541.0,2012-04-17 17:06:12,2012-04-17 21:21:13,custom matplotlib plot : chess board like table with colored cells,<python><matplotlib><pandas>,1.0,3.0,2.0,
9787853,1,9794891.0,2012-03-20 13:36:09,5,449,"I want to perform a join/merge/append operation on a dataframe with datetime index

Let's say I have df1 and I want to add df2 to it  df2 can have fewer or more columns  and overlapping indexes  For all rows where the indexes match  if df2 has the same column as df1  I want the values of df1 be overwritten with those from df2 

How can I obtain the desired result? ",566942,,192839.0,2012-03-20 15:48:45,2012-03-20 21:02:32,join or merge with overwrite in pandas,<python><pandas>,1.0,,1.0,
9850954,1,,2012-03-24 10:26:50,4,305,"I have the following dataframe:

   obj_id   data_date   value
0  4        2011-11-01  59500    
1  2        2011-10-01  35200 
2  4        2010-07-31  24860   
3  1        2009-07-28  15860
4  2        2008-10-15  200200


I want to get a subset of this data so that I only have the most recent (largest 'data_date') 'value' for each 'obj_id' 

I've hacked together a solution  but it feels dirty I was wondering if anyone has a better way I'm sure I must be missing some easy way to do it through pandas 

My method is essentially to group  sort  retrieve  and recombine as follows:

row_arr = []
for grp  grp_df in dfgroupby('obj_id'):
    row_arrappend(dfgsort('data_date'  ascending = False)[:1]values[0])

df_new = DataFrame(row_arr  columns = ('obj_id'  'data_date'  'value'))
",939715,,243434.0,2013-01-29 04:59:23,2013-01-29 04:59:23,pandas - get most recent value of a particular column indexed by another column (get maximum value of a particular column indexed by another column),<python><pandas>,3.0,,1.0,
9851018,1,,2012-03-24 10:35:14,1,137,"I'm interested in configuring or patching pandas so that its memory overhead is as low as possible In an experiment I created 2 numpy arrays  each containing 50 million uint32 values Storing these arrays in numpy format needs 200 + 200 = 400 Mbytes If I wrap one of the arrays into a Series object (with index=None)  then it consumes ~600 Mbytes of memory If I wrap the two arrays into a DataFrame object (with index=None)  then the memory requirement is ~1600 Mbytes

It seems that the extra memory requirement is #rows * 8 bytes for Series storage  and #rows * (#columns + 1) * 8 bytes for DataFrame storage Can you explain what extra data exactly is stored in the Series and the DataFrame object along with the original numpy arrays?",1289813,,1289813.0,2012-03-24 10:48:56,2012-03-24 21:51:42,What extra data is stored in Series and DataFrame objects?,<pandas>,1.0,,,
10114399,1,,2012-04-11 21:40:38,0,317,"I like to think I'm not an idiot  but maybe I'm wrong Can anyone explain to me why this isn't working? I can achieve the desired results using 'merge' But I eventually need to join multiple Pandas DataFrames so I need to get this method working

In [2]: left = pandasDataFrame({'ST_NAME': ['Oregon'  'Nebraska']  'value': [4685  2491]})

In [3]: right = pandasDataFrame({'ST_NAME': ['Oregon'  'Nebraska']  'value2': [6218  0001]})

In [4]: leftjoin(right  on='ST_NAME'  lsuffix='_left'  rsuffix='_right')
Out[4]: 
  ST_NAME_left  value ST_NAME_right  value2
0       Oregon  4685           NaN     NaN
1     Nebraska  2491           NaN     NaN


I apologize in advance for the noob-like nature of this question",1327660,,,,2012-04-11 22:06:26,Pandas: simple 'join' not working?,<pandas>,1.0,,,
10158060,1,,2012-04-14 23:00:05,0,247,"I have a dataset that looks below which I could stream into python from a file 
I would like to use pandas to create a HLOC chart of data for every one minute starting with time zero being 9:46 using the asof method I would also like to know how to stream data into a pandas dataframe as updates Is this possible?    

2012-03-15 09:45:00  1398000
2012-03-15 09:45:11  1397900
2012-03-15 09:45:22  1397850
2012-03-15 09:45:33  1398100
2012-03-15 09:45:44  1398000
2012-03-15 09:45:55  1398000
2012-03-15 09:46:06  1398100
2012-03-15 09:46:16  1398500
2012-03-15 09:46:27  1398400
2012-03-15 09:46:38  1398300
2012-03-15 09:46:49  1398000
2012-03-15 09:46:59  1398200
2012-03-15 09:47:10  1398200
2012-03-15 09:47:21  1398500
2012-03-15 09:47:32  1398600
2012-03-15 09:47:42  1398680
2012-03-15 09:47:53  1398600
2012-03-15 09:48:04  1398620
2012-03-15 09:48:15  1398500
2012-03-15 09:48:25  1398600
2012-03-15 09:48:36  1398400
2012-03-15 09:48:47  1398197
2012-03-15 09:48:58  1398200
2012-03-15 09:49:08  1398000
2012-03-15 09:49:19  1398300
2012-03-15 09:49:30  1398199
2012-03-15 09:49:41  1398300
2012-03-15 09:49:52  1398600
2012-03-15 09:50:02  1398600
2012-03-15 09:50:13  1398800
2012-03-15 09:50:24  1399000
2012-03-15 09:50:35  1399200
2012-03-15 09:50:45  1399300
2012-03-15 09:50:56  1399300
2012-03-15 09:51:07  1399290
2012-03-15 09:51:18  1399100
2012-03-15 09:51:28  1399200
2012-03-15 09:51:39  1399200
2012-03-15 09:51:50  1399370
2012-03-15 09:52:01  1399386
2012-03-15 09:52:11  1399400
2012-03-15 09:52:22  1399590
2012-03-15 09:52:33  1399650
2012-03-15 09:52:44  1399600
2012-03-15 09:52:54  1399800
2012-03-15 09:53:05  1399900
2012-03-15 09:53:16  1399800
2012-03-15 09:53:27  1399800
2012-03-15 09:53:37  1399700
2012-03-15 09:53:48  1399900
2012-03-15 09:53:59  1399884
2012-03-15 09:54:10  1399900
2012-03-15 09:54:20  1399900
2012-03-15 09:54:31  1400100
2012-03-15 09:54:42  1400000
2012-03-15 09:54:53  1399850
2012-03-15 09:55:03  1399900
2012-03-15 09:55:14  1400000
2012-03-15 09:55:25  1400090
2012-03-15 09:55:36  1400000
2012-03-15 09:55:47  1399890
2012-03-15 09:55:57  1399900
2012-03-15 09:56:08  1399900
2012-03-15 09:56:19  1400000
2012-03-15 09:56:30  1400400
2012-03-15 09:56:40  1400200
2012-03-15 09:56:51  1400200
2012-03-15 09:57:02  1400300
2012-03-15 09:57:13  1400400
2012-03-15 09:57:23  1400390
2012-03-15 09:57:34  1400300
2012-03-15 09:57:45  1400200
2012-03-15 09:57:56  1400200
2012-03-15 09:58:06  1400400
2012-03-15 09:58:17  1400300
2012-03-15 09:58:28  1400400
2012-03-15 09:58:39  1400300
2012-03-15 09:58:49  1400300
2012-03-15 09:59:00  1400500
2012-03-15 09:59:11  1400400
2012-03-15 09:59:22  1400200
2012-03-15 09:59:32  1400300
2012-03-15 09:59:43  1400300
2012-03-15 09:59:54  1400200
2012-03-15 10:00:05  1400100
2012-03-15 10:00:15  1400100
2012-03-15 10:00:26  1400700
2012-03-15 10:00:37  1400900
2012-03-15 10:00:48  1400899
2012-03-15 10:00:58  1400700
2012-03-15 10:01:09  1400800
2012-03-15 10:01:20  1400300
2012-03-15 10:01:31  1400200
2012-03-15 10:01:41  1400100
2012-03-15 10:01:52  1399800
2012-03-15 10:02:03  1399300
2012-03-15 10:02:14  1399900
2012-03-15 10:02:25  1400200
2012-03-15 10:02:35  1400000
2012-03-15 10:02:46  1399700
2012-03-15 10:02:57  1399300
2012-03-15 10:03:08  1399300
2012-03-15 10:03:18  1399200
2012-03-15 10:03:29  1399700
2012-03-15 10:03:40  1399700
2012-03-15 10:03:51  1399600
2012-03-15 10:04:01  1399700
2012-03-15 10:04:12  1399200
2012-03-15 10:04:23  1399100
2012-03-15 10:04:34  1399200
2012-03-15 10:04:44  1399100
2012-03-15 10:04:55  1399100
2012-03-15 10:05:06  1398900
2012-03-15 10:05:17  1399000
2012-03-15 10:05:27  1399900
2012-03-15 10:05:38  1399700
2012-03-15 10:05:49  1399521
2012-03-15 10:06:00  1399700
2012-03-15 10:06:10  1399800
2012-03-15 10:06:21  1400200
2012-03-15 10:06:32  1400400
2012-03-15 10:06:43  1400400
2012-03-15 10:06:53  1400300
2012-03-15 10:07:04  1400400
2012-03-15 10:07:15  1399893
2012-03-15 10:07:26  1400100
2012-03-15 10:07:36  1400100
2012-03-15 10:07:47  1400010
2012-03-15 10:07:58  1399900
2012-03-15 10:08:09  1400100
2012-03-15 10:08:19  1399800
2012-03-15 10:08:30  1399899
2012-03-15 10:08:41  1400000
2012-03-15 10:08:52  1400000
2012-03-15 10:09:03  1399710
2012-03-15 10:09:13  1399710
2012-03-15 10:09:24  1399700
2012-03-15 10:09:35  1399700
2012-03-15 10:09:46  1399700
2012-03-15 10:09:56  1399614
2012-03-15 10:10:07  1399700
2012-03-15 10:10:18  1399400
2012-03-15 10:10:29  1399100
2012-03-15 10:10:39  1399300
2012-03-15 10:10:50  1399400
2012-03-15 10:11:01  1399800
2012-03-15 10:11:12  1400000
2012-03-15 10:11:22  1399700
2012-03-15 10:11:33  1399400
2012-03-15 10:11:44  1398900
2012-03-15 10:11:55  1398800
2012-03-15 10:12:05  1399000
2012-03-15 10:12:16  1399100
2012-03-15 10:12:27  1399100
2012-03-15 10:12:38  1399000
2012-03-15 10:12:48  1399300
2012-03-15 10:12:59  1399200
2012-03-15 10:13:10  1399300
2012-03-15 10:13:21  1399500
2012-03-15 10:13:31  1399500
2012-03-15 10:13:42  1399700
2012-03-15 10:13:53  1399600
2012-03-15 10:14:04  1399700
2012-03-15 10:14:14  1399400
2012-03-15 10:14:25  1400300
",1333852,,,,2012-04-25 03:17:16,How to create high low open close chart with pandas,<python><pandas>,1.0,,,
10202570,1,10202789.0,2012-04-18 03:59:55,1,1531,"How can I find the row for which the value of a specific column is maximal?

dfmax() will give me the maximal value for each column  I don't know how to get the corresponding row",7650,,,,2012-04-18 15:51:32,Pandas DataFrame - Find row where values for column is maximal,<python><pandas>,2.0,,,
9944436,1,9968147.0,2012-03-30 13:58:29,2,763,"Could someone please point me in the right direction with respect to OHLC data timeframe conversion with Pandas? What I'm trying to do is build a Dataframe with data for higher timeframes  given data with lower timeframe 

For example  given I have the following one-minute (M1) data:

                       Open    High     Low   Close  Volume
Date                                                       
1999-01-04 10:22:00  11801  11819  11801  11817       4
1999-01-04 10:23:00  11817  11818  11804  11814      18
1999-01-04 10:24:00  11817  11817  11802  11806      12
1999-01-04 10:25:00  11807  11815  11795  11808      26
1999-01-04 10:26:00  11803  11806  11790  11806       4
1999-01-04 10:27:00  11801  11801  11779  11786      23
1999-01-04 10:28:00  11795  11801  11776  11788      28
1999-01-04 10:29:00  11793  11795  11782  11789      10
1999-01-04 10:31:00  11780  11792  11776  11792      12
1999-01-04 10:32:00  11788  11792  11788  11791       4


which has Open  High  Low  Close (OHLC) and volume values for every minute I would like to build a set of 5-minute readings (M5) which would look like so:

                       Open    High     Low   Close  Volume
Date                                                       
1999-01-04 10:25:00  11807  11815  11776  11789      91
1999-01-04 10:30:00  11780  11792  11776  11791      16


So the workflow is that:

Open is the Open of the first row in the timewindow
High is the highest High in the timewindow
Low is the lowest Low
Close is the last Close
Volume is simply a sum of Volumes
There are few issues though:

the data has gaps ( note there is no 10:30:00 row)
the 5-minute intervals have to start at round time  eg M5 starts at 10:25:00 not 10:22:00
first  incomplete set can be omitted like in this example  or included (so we could have 10:20:00 5-minute entry)
The Pandas documentation on up-down sampling gives an example  but they use mean value as the value of up-sampled row  which won't work here I have tried using groupby and agg but to no avail For one getting highest High and lowest Low might be not so hard  but I have no idea how to get first Open and last Close

What I tried is something along the lines of:

grouped = slicegroupby( dr5minuteasof )agg( 
    { 'Low': lambda x : xmin()[ 'Low' ]  'High': lambda x : xmax()[ 'High' ] } 
)


but it results in following error  which I don't understand:

In [27]: grouped = slicegroupby( dr5minuteasof )agg( { 'Low' : lambda x : xmin()[ 'Low' ]  'High' : lambda x : xmax()[ 'High' ] } )
---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
/work/python/fxcruncher/ in ()
----> 1 grouped = slicegroupby( dr5minuteasof )agg( { 'Low' : lambda x : xmin()[ 'Low' ]  'High' : lambda x : xmax()[ 'High' ] } )

/usr/lib/python27/site-packages/pandas/core/groupbypyc in agg(self  func  *args  **kwargs)
    242         See docstring for aggregate
    243         """"""
--> 244         return selfaggregate(func  *args  **kwargs)
    245 
    246     def _iterate_slices(self):

/usr/lib/python27/site-packages/pandas/core/groupbypyc in aggregate(self  arg  *args  **kwargs)
   1153                     colg = SeriesGroupBy(obj[col]  column=col 
   1154                                          grouper=selfgrouper)
-> 1155                     result[col] = colgaggregate(func)
   1156 
   1157             result = DataFrame(result)

/usr/lib/python27/site-packages/pandas/core/groupbypyc in aggregate(self  func_or_funcs  *args  **kwargs)
    906                 return self_python_agg_general(func_or_funcs  *args  **kwargs)
    907             except Exception:
--> 908                 result = self_aggregate_named(func_or_funcs  *args  **kwargs)
    909 
    910             index = Index(sorted(result)  name=selfgroupernames[0])

/usr/lib/python27/site-packages/pandas/core/groupbypyc in _aggregate_named(self  func  *args  **kwargs)
    976             grp = selfget_group(name)
    977             grpname = name
--> 978             output = func(grp  *args  **kwargs)
    979             if isinstance(output  npndarray):
    980                 raise Exception('Must produce aggregated value')

/work/python/fxcruncher/ in (x)
----> 1 grouped = slicegroupby( dr5minuteasof )agg( { 'Low' : lambda x : xmin()[ 'Low' ]  'High' : lambda x : xmax()[ 'High' ] } )

IndexError: invalid index to scalar variable


So any help on doing that would be greatly appreciated If the path I chose is not going to work  please suggest other relatively efficient approach (I have millions of rows) Some resources on using Pandas for financial processing would also be nice

Thank you :)",680238,,,,2012-04-01 20:42:23,Converting OHLC stock data into a different timeframe with python and pandas,<python><stock><pandas>,1.0,3.0,3.0,
9962822,1,9966022.0,2012-04-01 07:56:42,2,452,"I have a pandas DataFrame with a date column It is not an index

I want to make a pivot_table on the dataframe using counting aggregate per month for each location

The data look like this:

['INDEX']                 DATE LOCATION  COUNT
0          2009-01-02 00:00:00      AAH      1
1          2009-01-03 00:00:00      ABH      1
2          2009-01-03 00:00:00      AAH      1
3          2009-01-03 00:00:00      ABH      1
4          2009-01-04 00:00:00      ACH      1

I used:

pivot_table(cdiff  values='COUNT'  rows=['DATE' 'LOCATION']  aggfunc=npsum) 

to pivot the values I need a way to convert cdiffDATE to a month rather than a date
I hope to end up with something like:
The data look like this:

  
  MONTH LOCATION  COUNT
January      AAH      2
January      ABH      2
January      ACH      1


I tried all manner of strftime methods on cdiffDATE with no success It wants to apply the to strings  not series object",390388,,390388.0,2012-04-01 09:16:08,2012-04-01 16:21:23,Pandas pivot_table on date,<python><datetime><pandas>,1.0,,3.0,
10175068,1,10182172.0,2012-04-16 13:27:44,5,489,"I have the following Pandas Dataframe with a MultiIndex(Z A):

             H1       H2  
   Z    A 
0  100  200  03112   -04197   
1  100  201  02967   04893    
2  100  202  03084   -04873   
3  100  203  03069   NaN        
4  101  203  -04956  NaN       


Question: How can I select all items with A=203?
I tried df[: 'A']  but it doesn't work Then I found this in the online documentation so I tried:dfxs(203 level='A')
but I get:
""TypeError: xs() got an unexpected keyword argument 'level'""
Also I dont see this parameter in the installed doc(dfxs?):
""Parameters ---------- key : object Some label contained in the index  or partially in a MultiIndex axis : int  default 0 Axis to retrieve cross-section on copy : boolean  default True Whether to make a copy of the data""
Note:I have the development version

Edit: I found this thread They recommend something like:

dfselect(lambda x: x[1]==200  axis=0)  


I still would like to know what happened with dfxs with the level parameter or what is the recommended way in the current version",1330293,,1330293.0,2012-04-18 08:17:02,2012-04-18 08:17:02,Select data at a particular level from a MultiIndex,<python><pandas>,1.0,3.0,3.0,
10376647,1,10377863.0,2012-04-29 22:41:28,3,1294,"This is a beginner python installation question  This the first time I have tried to install and call a package  I've got pip installed  and I tried to install two modules - numpy and pandas  

In terminal  I ran the following commands:

sudo pip install numpy

sudo pip install pandas


Both commands returned with a success message  Here is the pandas success message (it's the second package I installed and was still in my terminal history):

Successfully installed pandas
Cleaning up


pip returned a similar message after numpy was installed  

Now  when I launch python and try to call it with:

import pandas


I get this error message:

Traceback (most recent call last):
  File """"  line 1  in 
ImportError: No module named pandas


Same when I try numpy  

Can anyone tell me what I'm doing incorrectly?  ",854739,,,,2012-04-30 15:59:40,Installed Python Modules - Python can't find them,<python><numpy><pandas>,1.0,14.0,,
9647656,1,,2012-03-10 15:35:26,1,271,"In Pandas it seems I can't store a dataframe of mixed types:

store = HDFStore('playh5')
df = DataFrame([{'a': 1  'b': 'hello'}  {'a': 5  'b': 'world'}])
storeput('df'  df  table=True  compression='zlib')


This gives an Exception: Cannot currently store mixed-type DataFrame objects in Table format

I wonder: is this due to some inherent limitation of Pandas or just a future nice-to-have? It seems that HDFStore would not be very useful with this limitation  as many dataframes will be mixed-type Thanks! ",946197,,,,2012-03-10 17:32:24,Pandas dataframe in mixed mode can't serialize to hdf5?,<python><hdf5><pandas>,1.0,,,
10002181,1,,2012-04-03 22:10:49,1,336,"There are at least four data struts in pandas 

->Slice

->DateFrame

->DateMatrix

->Panel 

What are the use cases for these The documents seem to highlight slice and DataFrame
Please give examples of use cases I know where the doc is located   ",428862,,428862.0,2012-04-03 23:44:06,2012-04-09 02:20:10,What python's Pandas data struts used for?,<python><pandas>,1.0,,,
10009468,1,10058000.0,2012-04-04 10:38:56,2,502,"Is it possible to directly compute the product (or for example sum) of two columns without usinggroupedapply(lambda x: (xa*xb)sum()?
It is much (less than half the time on my machine) faster to use
    
    df['helper'] = dfa*dfb
    grouped= dfgroupby(something)
    grouped['helper']sum()
    dfdrop('helper'  axis=1)
    

But I don't really like having to do this 
It is for example useful to compute the weighted average per group Here the lambda approach would be 

groupedapply(lambda x: (xa*xb)sum()/(dfb)sum())

and again is much slower than dividing the helper by bsum()",942591,,,,2012-04-07 20:18:37,"GroupBy functions in Python Pandas like SUM(col_1*col_2), weighted average etc",<python><pandas>,2.0,,1.0,
10027719,1,10037032.0,2012-04-05 11:28:00,0,183,"I'm trying to get Pandas installed with Python 25 on a machine running Windows XP

The installation seems to finish OK  but when I try

from pandas import *


I get the following error:

Traceback (most recent call last):
  File """"  line 1  in 
  File ""c:\python25\lib\site-packages\pandas-072-py25-win32egg\pandas\__init__py""  line 24  in 
    from pandasstatsapi import *
  File ""c:\python25\lib\site-packages\pandas-072-py25-win32egg\pandas\stats\apipy""  line 7  in 
    from pandasstatsmoments import *
  File ""c:\python25\lib\site-packages\pandas-072-py25-win32egg\pandas\stats\momentspy""  line 388  in 
    rolling_max = _rolling_func(_tseriesroll_max  'Moving maximum')
  File ""c:\python25\lib\site-packages\pandas-072-py25-win32egg\pandas\stats\momentspy""  line 379  in _rolling_func
    def f(arg  window  min_periods=None  time_rule=None):
  File ""C:\Python25\Lib\functoolspy""  line 35  in update_wrapper
    getattr(wrapper  attr)update(getattr(wrapped  attr))
AttributeError: 'builtin_function_or_method' object has no attribute '__dict__'


As far as I can tell  I've got all of the dependencies installed correctly  and the problem is something to do with functoolspy  which is part of the Python Standard Library(?) 

Does anyone have any suggestions  please? Pandas is supposed to work with Python 25  so I assume it's a problem with my Python configuration

Thanks!",505698,,,,2012-04-05 22:43:43,Installing Pandas with Python 2.5 on Windows,<python><pandas>,1.0,,,
10422438,1,10429674.0,2012-05-02 22:12:45,1,210,"I am using pandas HDFSTore object to open a hdf5 file and store DataFrame objects But before I do that  I want to find out if the file is empty Is there a way to find out if my

In[12]:
import pandas
store = pandasiopytablesHDFStore('storeh5')
Out[12]:

File path: storeh5
Empty


Is there a way to get a browse the hierarchy tree in storeh5 to check if the object is empty I would like to get a list of objects in storeh5",1257953,,,,2012-05-03 10:37:38,Check if HDF5 Store object is empty?,<python><hdf5><pandas>,1.0,,,
10393447,1,10393475.0,2012-05-01 04:12:13,0,79,"I have an API for analysing my exercise data (which I scrape runkeeper's website)

My main class is a subclass of a pandasDataFrame  which is basically a container for tabular data It supports indexing by column name  returning an array of the column values

I would like to add some convenience properties based on the types of 'fitness activities' that are present in the data So for example I'd like to add a property 'running':

@property
def running(self):
    return self[self['type'] == 'running']


Which would return all rows of the DataFrame which have 'running' in the 'type' column

I tried to do this dynamically for all types present in the data Here's what I naively did:

class Activities(pandasDataFrame):
    def __init__(self data):
        pandasDataFrame__init__(self data)
        # The set of unique types in the 'type' column:
        types = set(self['type'])
        for type in types:
            method = property(lambda self: self[self['type'] == type])
            setattr(self__class__ type method)


The result was that all of these properties ended up returning tables of data for the same type of activity ('walking') 

What's happening is that when the properties are accessed  the lambdas are called and they look in the scope they were defined in for the name 'type' They find that it is bound to the string 'walking'  since that was the last iteration of the for loop Each iteration of the for loop doesn't have its own namespace  so all the lambdas see only the last iteration  rather than the value that 'type' had when they were actually defined

Can anyone thing of a way around this? I can think of two  but they don't seem particularly ideal:

define __getattr__ to check that the attribute is an activity type and return the appropriate rows
use a recursive function call instead of a for loop  so that each level of recursion has its own namespace
Both of these are a little too clever for my tastes  and pandasDataFrame already has a __getattr__ that I'd have to cautiously interact with if I made one too And recursion would work  but feels very wrong since the set of types doesn't have any intrinsic tree-like structure to it It's flat  and should look flat in the code!",437537,,437537.0,2012-05-01 04:31:10,2012-05-01 21:17:26,Scope gotcha when dynamically adding methods in a loop,<python><pandas>,2.0,,,
10457584,1,10458386.0,2012-05-05 00:00:12,2,1192,"I am trying to re-index a pandas dataFrame object  like so 

From:
            a   b   c
        0   1   2   3
        1  10  11  12
        2  20  21  22

To :
           b   c
       1   2   3
      10  11  12
      20  21  22


I am going about this as shown below and am getting the wrong answer Any clues on how to do this?

>>> col = ['a' 'b' 'c']
>>> data = DataFrame([[1 2 3] [10 11 12] [20 21 22]] columns=col)
>>> data
    a   b   c
0   1   2   3
1  10  11  12
2  20  21  22
>>> idx2 = dataavalues
>>> idx2
array([ 1  10  20]  dtype=int64)
>>> data2 = DataFrame(data index=idx2 columns=col[1:])
>>> data2
     b   c
1   11  12
10 NaN NaN
20 NaN NaN


Any idea why this is happening?",1257953,,1257953.0,2012-05-05 00:34:57,2012-05-05 02:44:12,Redefining the Index in a Pandas DataFrame Object,<python><pandas>,1.0,1.0,0.0,
10518803,1,,2012-05-09 15:05:08,1,459,"I have two sets of data and only want to keep the data where both sets have common dates I imported the data sets with read_csv()  calling them df1  df2

Then run:

DF=df1align(df2  join='inner' axis=0)


After checking  it appears that DF is a tuple The goal is to export the aligned data (on common dates only) using DFto_csv(path) It fails with the message that 'tuple' object has no attribute 'to_csv' I don't understand why the join has created a tuple Shouldn't this still be a dataframe that can be exported to a CSV? 

Is there a better command to use  so that it can easily be exported to a csv file?",1374969,,95852.0,2012-05-09 15:39:21,2012-05-18 19:10:37,Cannot export to CSV after data alignment,<python><pandas>,2.0,3.0,,
10214692,1,,2012-04-18 17:28:36,3,429,"Pandas depends on numpy and there is an open build issue 
https://githubcom/pydata/pandas/issues/507 to install pandas dependencies Regardless  any ideas why pip quits on numpy in the below example? Also occurs if using a requirements file

$virtualenv /tmp/pandatest
$source /tmp/pandatest/bin/activate
$pip install numpy pandas



Then in middle of numpy setup panda setup gets triggered

  DeprecationWarning)
C compiler: /usr/bin/gcc-40 -DNDEBUG -g -O3 -m32

compile options: '-Inumpy/core/src/private -Inumpy/core/src -Inumpy/core -     Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/include -I/Library/Frameworks/Pythonframework/Versions/26/include/python26 -c'
gcc-40: _configtestc
/usr/bin/gcc-40 _configtesto -o _configtest
_configtest
failure
removing: _configtestc _configtesto _configtest
building data_files sources
build_src: building npy-pkg config files
Downloading/unpacking pandas
Running setuppy egg_info for package pandas
# numpy needed to finish setup  run:

    $ pip install numpy  # or easy_install numpy


 and when testing for numpy

(pandatest)$ python
Python 262 (r262:71600  Apr 16 2009  09:17:39) 
[GCC 401 (Apple Computer  Inc build 5250)] on darwin
Type ""help""  ""copyright""  ""credits"" or ""license"" for more information
>>> import numpy
Traceback (most recent call last):
   File """"  line 1  in 
ImportError: No module named numpy
",1342051,,1301710.0,2012-04-26 21:22:31,2012-04-26 21:22:31,pip install numpy pandas fails?,<python><pip><pandas>,1.0,2.0,1.0,
10445549,1,10462487.0,2012-05-04 08:43:55,1,225,"I have a Pandas DataFrame using a MultiIndex on the rows:

index = pandasMultiIndexfrom_tuples(list(itertoolsproduct(range(3)  range(3))))
df = pandasDataFrame(numpyrandomrandn(9 3)  index=index  columns=['A'  'B'  'C'])

            A         B         C
0 0  2400417  0698638  1231540
  1 -0023154 -2110450  0774964
  2 -1282392 -0062794  1471655
1 0 -1081853  0261876 -1771075
  1 -2013747 -0377957 -0393802
  2  1711172 -0552468  1018727
2 0  0155821 -0222691  0496586
  1  0563638 -0756709  1050212
  2 -1446159 -0891549  0256695


I would like to shuffle this DataFrame on the first level of the index  so a possible result would be:

            A         B         C
1 0 -1081853  0261876 -1771075
  1 -2013747 -0377957 -0393802
  2  1711172 -0552468  1018727
0 0  2400417  0698638  1231540
  1 -0023154 -2110450  0774964
  2 -1282392 -0062794  1471655
2 0  0155821 -0222691  0496586
  1  0563638 -0756709  1050212
  2 -1446159 -0891549  0256695
",319584,,,,2012-05-05 13:49:37,Pandas shuffle rows at a certain level,<python><pandas>,2.0,,1.0,
10545957,1,10546350.0,2012-05-11 05:36:56,3,531,"I am trying to create a pandas data frame and it works fine for a single file If I need to build it for multiple files which have the same data structure So instead of single file name I have a list of file names from which I would like to create the data frame

Not sure whats the way to append to current data frame in pandas or is there a way for pandas to suck a list of files into a data frame

Thanks!
-Abhi",369541,,,,2013-01-23 22:58:43,creating pandas data frame from multiple files,<python><pandas>,4.0,1.0,2.0,
10591000,1,,2012-05-14 21:01:58,3,720,I am just getting started with Pandas and I am reading in a csv file using the read_csv() method The difficulty I am having is preventing pandas from converting my telephone numbers to large numbers  instead of keeping them as strings I defined a converter which just left the numbers alone  but then they still converted to numbers When I changed my converter to prepend a 'z' to the phone numbers  then they stayed strings Is there some way to keep them strings without modifying the values of the fields?,1394684,,1301710.0,2012-05-29 11:31:04,2012-05-29 11:31:04,Specifying data type in Pandas csv reader,<python><pandas>,1.0,1.0,1.0,
10264739,1,10269848.0,2012-04-22 02:35:06,6,487,"I grabbed the KDD track1 dataset from Kaggle and decided to load a ~25GB 3-column CSV file into memory  on my 16GB high-memory EC2 instance: 

data = nploadtxt('rec_log_traintxt')


the python session ate up all my memory (100%)  and then got killed 

I then read the same file using R (via readtable) and it used less than 5GB of ram  which collapsed to less than 2GB after I called the garbage collector

My question is why did this fail under numpy  and what's the proper way of reading a file into memory Yes I can use generators and avoid the problem  but that's not the goal :)",190894,,190894.0,2012-04-22 03:11:39,2012-04-26 00:26:27,major memory problems reading in a csv file using numpy,<python><r><numpy><pandas>,3.0,1.0,4.0,
10373660,1,10374456.0,2012-04-29 16:10:35,3,761,"I'm starting with input data like this

df1 = pandasDataFrame( { 
    ""Name"" : [""Alice""  ""Bob""  ""Mallory""  ""Mallory""  ""Bob""   ""Mallory""]   
    ""City"" : [""Seattle""  ""Seattle""  ""Portland""  ""Seattle""  ""Seattle""  ""Portland""] } )


Which when printed appears as this:

   City     Name
0   Seattle    Alice
1   Seattle      Bob
2  Portland  Mallory
3   Seattle  Mallory
4   Seattle      Bob
5  Portland  Mallory


Grouping is simple enough:

g1 = df1groupby( [ ""Name""  ""City""] )count()


and printing yields a GroupBy object:

                  City  Name
Name    City
Alice   Seattle      1     1
Bob     Seattle      2     2
Mallory Portland     2     2
        Seattle      1     1


But what I want eventually is another DataFrame object that contains all the rows in the GroupBy object In other words I want to get the following result:

                  City  Name
Name    City
Alice   Seattle      1     1
Bob     Seattle      2     2
Mallory Portland     2     2
Mallory Seattle      1     1


I can't quite see how to accomplish this in the pandas documentation Any hints would be welcome",468604,,,,2012-04-29 17:50:33,Converting a Pandas GroupBy object to DataFrame,<python><pandas>,1.0,,3.0,
10484106,1,,2012-05-07 14:46:14,1,1777,"Hei I'm trying to read in pandas the csv file you can download from http://wwweuribor-ebfeu/assets/modules/rateisblue/processed_files/hist_EURIBOR_2012csv (euribor rates I think you can imagine the reason I would like to have this file!) The file is a CSV file but it is somehow strangely oriented If you import it in Excel file has the format 

   02/01/2012 03/01/2012 04/01/2012     
1w 0652 0626 0606    
2w 0738 0716 0700    


act with first column going up to 12m (but I have give you the link where you can download a sample) I would like to read it in pandas but I'm not able to read it in the correct way Pandas has a built-in function for reading csv files but somehow it expect to be row oriented rather than column oriented What I would like to do is to obtain the information on the row labeled 3m and having the values and the date in order to plot the time variation of this index But I can't handle this problem I know I can read the data with 

import pandas 
data = pandasioread_csv(""filecsv"" parse_dates=True) 


but it would work if the csv file would be somehow transpose H",462901,,,,2012-05-07 18:34:52,pandas reading csv orientation,<python><csv><pandas>,2.0,,2.0,
10591796,1,,2012-05-14 22:17:20,0,102,"For some reason a script I wrote and worked with 071 did not work with 073

I reverted to 071 and everything worked fine The problem seems to be with the aggregate method I'm using a dictionary of different aggregation methods to agg different columns (npmean  npsumetc) When running with 073 this threw a ""No numeric values to aggregate"" error

Any thoughts on this? I would like to get everything working in the 073 versionmaybe some syntax changed?

Thanks",1260307,,,,2012-05-14 22:17:20,Python Pandas: Aggregate changed from 0.7.1 to 0.7.3,<python-2.7><pandas>,,2.0,,
10464738,1,10465162.0,2012-05-05 18:25:59,4,438,I have dataframe say volatility surface with index as time and column as strike how do I do two dimensional interpolation I can reindex but how do i deal with NaN I know we can fillna(method='pad') but it is not even linear interpolation Is there a way we can plug in our own method to do interpolation?,1377107,,,,2012-05-05 19:16:19,interoplation on DataFrame in pandas,<python><pandas>,1.0,,2.0,
10532501,1,10533776.0,2012-05-10 10:53:48,0,392,"I have following data structure: 

2011-01-01 00:00  2011-01-20 00:00  200   # days-range
2011-01-20 00:00  2011-03-08 00:00  1288  # days-range
2011-04-11 00:00  2012-01-08 00:00  5987  # days-range

2012-02-01 00:00  2012-02-01 01:00  7     # hourly-range
2012-02-01 02:00  2012-02-01 02:30  3     # hourly-range


This is interval with start date  end date and value (some metric recorded between dates)

For further data analysis I need to generate time series with required frequency:
monthly/daily/hourly/half-hourly time series For example  hourly data: 

2011-01-01 00:00  2 
2011-01-01 01:00  6
2011-01-01 02:00  5



Is there any python lib which can help to implement this kind of data transformation?",585753,,,,2012-05-10 12:16:19,Convert interval datetime values to any-frequency time series,<python><time-series><pandas><data-analysis>,1.0,3.0,1.0,
10565282,1,10565742.0,2012-05-12 16:05:17,2,979,"I worked now for quite some time using python and pandas for analysing a set of hourly data and find it quite nice (Coming from Matlab)

Now I am kind of stuck I created my DataFrame like that:

SamplingRateMinutes=60
index = DateRange(initialTime finalTime  offset=datetoolsMinute(SamplingRateMinutes))
ts=DataFrame(data  index=index)


What I want to do now is to select the Data for all days at the hours 10 to 13 and 20-23 to use the data for further calculations
So far I sliced the data using

 selectedData=ts[begin:end]


And I am sure to get some kind of dirty looping to select the data needed But there must be a more elegant way to index exacly what I want I am sure this is a common problem and the solution in pseudocode should look somewhat like that:

myIndex=tsindex[10",1391241,,,,2012-12-28 00:16:44,"pandas, python - how to select specific times in timeseries",<python><indexing><time-series><pandas>,3.0,,4.0,
10575251,1,10592962.0,2012-05-13 21:05:23,0,43,"When i am trying to read the fixed width file  it gives me that

Exception: Index (columns 0) have duplicated values ['12345']


The thing is  even though I have duplicated values  I still want the data incorporated How could one get around this?

thanks ",1610626,,,,2012-05-15 01:09:03,Why is there this read_fwf() error?,<pandas>,1.0,,,
10601041,1,,2012-05-15 12:49:25,2,396,"I face the problem of memory leaks using pandas library in python I create pandasdataframe objects in my class and I have method  that change dataframe size according my conditions After changing dataframe size and creating new pandas object I rewrite original pandasdataframe in my class But memory usage is very high even after significally reducing of initial table Some code for short example (I didn't write process manager see task manager):

import time  string  pandas  numpy  gc
class temp_class ():

    def __init__(self  nrow = 1000000  ncol = 4  timetest = 5):

        selfnrow = nrow
        selfncol = ncol
        selftimetest = timetest

    def createDataFrame(self):

        print('Check memory before dataframe creating')
        timesleep(selftimetest)
        selfdf = pandasDataFrame(numpyrandomrandn(selfnrow  selfncol) 
            index = numpyrandomrandn(selfnrow)  columns = list(stringletters[0:selfncol]))
        print('Check memory after dataFrame creating')
        timesleep(selftimetest)

    def changeSize(self  from_ = 0  to_ = 100):

        df_new = selfdf[from_:to_]copy()
        print('Check memory after changing size')
        timesleep(selftimetest)

        print('Check memory after deleting initial pandas object')
        del selfdf
        timesleep(selftimetest)

        print('Check memory after deleting copy of reduced pandas object')
        del df_new
        gccollect()
        timesleep(selftimetest)

if __name__== '__main__':

    a = temp_class()
    acreateDataFrame()
    achangeSize()


Before dataframe creating I have approx 15 mb of memory usage
After creating - 67mb
After changing size - 67 mb
After deleting original dataframe - 35mb 
After deleting reduced table - 31 mb
16 mb?

I use python 272(x32) on Windows 7 (x64) machine  pandasversion is 073 numpyversion is 161",1396036,,,,2012-05-15 14:22:04,Pandas: where's the memory leak here?,<python><pandas>,1.0,1.0,,
10256229,1,,2012-04-21 04:32:55,2,127,"I have a data set with 8 attributes (which is sorted according to the first attribute) and is of the following format (Just an example  it is tab separated) 

AX  0123  December 20  2010  1  2  80  hello this
AX  2313  April 19  2009  2  3  40  hi there
AX  4532  December 19  2010  6  2  80  nice tie
AX  1244  January 10  2011  3  4  80  king tale
BX  0214  September 10  2009  2  3  90 this king
BX  0114  February 9  2003  4  9  40  his brought
BX  3214  September 1  2006  1  3  30 is great
MG  980   April 20  2007  2  4  71  not available
MG  246   May 8  2005  5  1  21  make goat  


Now  that the file is sorted according first attribute  now i need to sort internally according to date based on first attribute  the output should be like this  (I don't want to use the database  this is a huge file (2 GB) so I think a special python code might be required (Not sure if one can do this with a simple code)

AX  2313  April 19  2009  2  3  40  hi there
AX  4532  December 19  2010  6  2  80  nice tie
AX  0123  December 20  2010  1  2  80  hello this
AX  1244  January 10  2011  3  4  80  king tale
BX  0114  February 9  2003  4  9  40  his brought
BX  3214  September 1  2006  1  3  30 is great
BX  0214  September 10  2009  2  3  90 this king
MG  246   May 8  2005  5  1  21  make goat
MG  980   April 20  2007  2  4  71  not available


Any replies are greatly appreciated Let me know if you have any other questions",1090816,,1301710.0,2012-05-19 00:34:08,2012-05-19 00:34:08,Python Sorting by date internally based on 1st attribute,<python><sorting><numpy><pandas>,2.0,8.0,1.0,
10594515,1,,2012-05-15 05:17:50,0,37,"This is the file content (named sampletxt)

gvkeyx        from        thru    conm                gvkey     co_conm                      co_tic
123453    19661214    19890426    S&P 500 Comp-Ltd    010490    TEXAS EASTERN CORP           PEL4    
123453    19670101               S&P 500 Comp-Ltd    001078    ABBOTT LABORATORIES          ABT     
123453    19670101               S&P 500 Comp-Ltd    001300    HONEYWELL INTERNATIONAL INC  HON     
123453    19670101               S&P 500 Comp-Ltd    001356    ALCOA INC                    AA      
123453    19670101               S&P 500 Comp-Ltd    001408    FORTUNE BRANDS INC           FO 


The code I entered to read it

In [16]: colspecs = [(0  9)  (10  21)  (22  33)  (34  53)  (54  63)  (64  92)  (93  99)]

In [17]: df = read_fwf('sampletxt'  colspecs = colspecs  header=None  index_col=None)

In [18]: df[:2]

Out[19]:      

Int64Index: 2 entries  0 to 1
Data Columns:
X1    2    non-null values
X2    2    non-null values
X3    2    non-null values
X4    2    non-null values
X5    2    non-null values
X6    2    non-null values
X7    2    non-null values
dtypes: object(7)


I am having trouble to understand this output as its entirely different from file Any comments and advice would help Thanks",1610626,,,,2012-05-18 18:42:10,Why isn't read_fwf() output correct content of files?,<pandas>,1.0,,,
10636024,1,12036847.0,2012-05-17 12:48:00,3,653,"I'm using the Pandas package and it creates a DataFrame object  which is basically a labeled matrix Often I have columns that have long string fields  or dataframes with many columns  so the simple print command doesn't work well I've written some text output functions  but they aren't great

What I'd really love is a simple GUI that lets me interact with a dataframe / matrix / table Just like you would find in a SQL tool Basically a window that has a read-only spreadsheet like view into the data I can expand columns  page up and down through long tables  etc

I would suspect something like this exists  but I must be Googling with the wrong terms It would be great if it is pandas specific  but I would guess I could use any matrix-accepting tool (BTW - I'm on Windows)

Any pointers?

Or  conversely  if someone knows this space well and knows this probably doesn't exist  any suggestions on if there is a simple GUI framework / widget I could use to roll my own? (But since my needs are limited  I'm reluctant to have to learn a big GUI framework and do a bunch of coding for this one piece)

Thanks",1400991,,,,2012-08-20 13:28:26,Python / Pandas - GUI for viewing a DataFrame or Matrix,<python><gui><pandas>,4.0,4.0,,
10484424,1,10485998.0,2012-05-07 15:04:58,0,109,"I'm trying to align my index values between multiple DataFrames or Series and I'm using 
Seriesinterpolate but it doesn't seem to interpolate correctly  Or perhaps I am misunderstanding something Here's a small example:

x1 = nparray([0  025  077  12  14  26  31])
y1 = nparray([0  11  05  15  12  21  24])
x2 = nparray([0  025  066  10  12  14  31])
y2 = nparray([0  02  08  11  22  01  24])

df1 = DataFrame(data=y1  index=x1  columns=['A'])
df1plot(marker='o')

df2 = DataFrame(data=y2  index=x2  columns=['A'])
df2plot(marker='o')

df3=df1 - df2
df3plot(marker='o')
print df3

def resample(signals):
    aligned_x_vals = reduce(lambda s1  s2: s1indexunion(s2index)  signals)
    return map(lambda s: sreindex(aligned_x_vals)apply(Seriesinterpolate)  signals)

sig1  sig2 = resample([df1  df2])
sig3 = sig1 - sig2
pltplot(df1index  df1values  marker='D')
pltplot(sig1index  sig1values  marker='o')
pltgrid()
pltfigure()
pltplot(df2index  df2values  marker='o')
pltplot(sig2index  sig2values  marker='o')
pltgrid()


I expect sig1 and sig2 to have more points than df1 and df2 but with the values interpolated  There are a few points that are not overlapping  Is this a bug or user error?  I'm using v073

Thanks",792849,,,,2012-05-07 16:59:02,Possible bug in Series.interpolate,<python><pandas>,2.0,,,
10556568,1,,2012-05-11 18:17:19,0,124,"I'd like to use a new project to learn Pandas and Matplotlib I've run into a snag as described in the ""STATUS"" section below

DATA

date idnum idtype|
2012-01-01 1 A|
2012-01-01 1 A|
2012-01-01 1 A|
2012-01-01 2 A|
2012-01-02 2 A|
2012-01-03 1 A|
2012-01-03 3 B|

GOAL

I want to create graphs that look similar to page 20 in this lab on longitudinal data:
http://remphuclaedu/rob/mv/rcode/lab8pdf

Each line corresponds to a single ""idnum""
Line color is determined by ""idtype""
the horizontal (x) axis is month/year
the vertical (y) axis is the count of rows for that ""idnum"" in that month/year
STATUS

I load the data using dateutilparser


  
    data = read_csv(""D:/path/filecsv"" converters={0:parse})
  


I create a ""monthyear"" column set to the first day of the month


  
    data['monthyear']=datadateapply(lambda x: datetime(xyear xmonth 1))
  


Now I need to create the counts crosstab produces a table where each column is a date and each cell a count:


  
    cross1=crosstab(datauserid datamonthyear)
  


But  I'm not sure how to graph the output or even if that is a useful format for future work I'd appreciate some advice

thank you 
-david",1383352,,1383352.0,2012-05-11 18:22:36,2012-05-11 18:22:36,aggregating longitudinal data in Pandas DataFrame for mapping in matplotlib,<date><matplotlib><aggregate><pandas>,,2.0,,
10705715,1,,2012-05-22 16:00:59,0,147,"I'm trying to obtain the average (over 24 hours at 30 minute intervals) of a section of time-series data

Currently I have:

# Data of interest
START = datetime(2010 2 1 0 0 0)
END = datetime(2010 2 8 23 59 59)

# Function to group by time of day
def minutes(date):
    time = datestrftime(""%H:%M:%S"")
    dt = datetimestrptime(""2009-04-01 "" + time  ""%Y-%m-%d %H:%M:%S"")
    return dt

# Truncate data to desired section
telford = storeget_data(""spd_mw"")TELRtruncate(START END)

# Group and average
telfordgroupby(minutes)mean()


I've tried using asfreq(datetoolsBDay()) but it removes the half-hourly data Is it a case of writing my own DateOffset?

Is there a cleaner was to do the above thing grouping by minutes?

On an aside: does truncate need to be existing indexes in the data? I'm getting the following error (on a different set of data to above):

Traceback (most recent call last):
  File ""power-weatherpy""  line 21  in 
    egph_temp = storeget_data(""weather_EGPH"")TemperatureCtruncate(START END)
  File ""/usr/local/lib/python26/dist-packages/pandas-073-py26-linux-x86_64egg/pandas/core/genericpy""  line 702  in truncate
    result = selfix[before:after]
  File ""/usr/local/lib/python26/dist-packages/pandas-073-py26-linux-x86_64egg/pandas/core/indexingpy""  line 35  in __getitem__
    return self_getitem_axis(key  axis=0)
  File ""/usr/local/lib/python26/dist-packages/pandas-073-py26-linux-x86_64egg/pandas/core/indexingpy""  line 167  in _getitem_axis
    return self_get_slice_axis(key  axis=axis)
  File ""/usr/local/lib/python26/dist-packages/pandas-073-py26-linux-x86_64egg/pandas/core/indexingpy""  line 372  in _get_slice_axis
    i  j = labelsslice_locs(start  stop)
  File ""/usr/local/lib/python26/dist-packages/pandas-073-py26-linux-x86_64egg/pandas/core/indexpy""  line 842  in slice_locs
    beg_slice = selfget_loc(start)
  File ""/usr/local/lib/python26/dist-packages/pandas-073-py26-linux-x86_64egg/pandas/core/indexpy""  line 523  in get_loc
    return self_engineget_loc(key)
  File ""enginespyx""  line 101  in pandas_enginesDictIndexEngineget_loc (pandas/src/enginesc:2498)
  File ""enginespyx""  line 108  in pandas_enginesDictIndexEngineget_loc (pandas/src/enginesc:2460)
KeyError: datetimedatetime(2010  2  1  0  0)
",1410646,,1410646.0,2012-05-23 13:26:39,2012-08-10 06:46:01,averaging half-hourly data for weekdays only,<python><pandas>,1.0,2.0,,
10198295,1,,2012-04-17 20:03:13,0,99,"Suppose I have a dataframe looks like:

      a      b
0    11      A
1    -2      A
2     3      A
3    NA      A
4   05      B
5    NA      B
6    -9      B


I can create a group by 'b' Is there a fast way to get the last non-NA value in 'a' of each group? In this case would be 3 for group A and -9 for group B

(In this case the series 'a' is sorted as given  but it might not be the case There could be another column 'c'  according which the 'last' is defined)

I wrote my own loop code by looking into the groupedgroups dict But apparently that's very inefficient given my huge dataset I think this could be done very straightforwardly -- maybe I am just too novice with pandas :-)",1339666,,,,2012-04-18 01:07:15,Last non-NaN value in a group,<pandas>,1.0,,,
10215259,1,,2012-04-18 18:04:04,0,656,"I have aggregated data using pandas data frame  Below is some actual data shown and how I aggregated it

fdfgroupby(['row' col'])['percent']sum()

http://pastebincom/R8XWpgtU

What I would like to do is create a 2d numpy array of this (rows = row  columns = col) Any slick way to do this ?

Another way I did something similar was create a pivot table

pivot_table(fdf values='percent' rows='row' cols='col'  aggfunc=npsum)

In this case I want to convert this pivot table to 2d numpy array Is there a way for me to index into each cell of this table If so then I probably will be Ok with the table itself 

Thanks!
-Abhi",369541,,369541.0,2012-04-18 19:16:52,2012-04-18 21:50:13,pandas aggregated data to a numpy array : data structure conversion,<python><pandas>,1.0,,2.0,
10475488,1,10619525.0,2012-05-07 00:38:47,1,134,"I have periodic data with the index being a floating point number like so:

time =    [0  01  021  031  040  049  051  06  071  082  093]
voltage = [1   -1   11  -09     1    -1   09 -12  095  -11  111]
df = DataFrame(data=voltage  index=time  columns=['voltage'])
dfplot(marker='o')


I want to create a cross(df  y_val  direction='rise' | 'fall' | 'cross') function that returns an array of times (indexes) with all the 
interpolated points where the voltage values equal y_val  For 'rise' only the values where the slope is positive are returned; for 'fall' only the values with a negative slope are retured; for 'cross' both are returned  So if y_val=0 and direction='cross' then an array with 10 values would be returned with the X values of the crossing points (the first one being about 0025) 

I was thinking this could be done with an iterator but was wondering if there was a better way to do this  

Thanks  I'm loving Pandas and the Pandas community",792849,,972208.0,2012-05-07 02:01:48,2012-05-16 16:05:16,Calculating crossing (intercept) points of a Series or DataFrame,<python><pandas>,1.0,1.0,1.0,
10723847,1,,2012-05-23 16:08:37,0,67,"In [139]: pandas__version__
Out[139]: '073'


I have two aligned series in a DataFrame which have several unmatched ""NaN"" I would like to print the intersection between them removing all ""NaN's""  but without loose alignment That is  I want to remove the rows from both series whem I find a ""NaN"" in one of them It sounds simple  but I not doing any operation between the series to dropna's afterwards and cannot dropna's from the series separately I couldn't figure out the right df function to do this - several are not documented
Just an example  I want to take this:

10         NaN     -1200
11         NaN     -1324
12    0000585        NaN
13    0000573     -1453
14         NaN     -2006


and print this:

13    0000573     -1453
",1289107,,1301710.0,2012-05-24 10:25:45,2012-05-24 13:23:19,Print out inner series,<python><pandas>,1.0,7.0,,
10449663,1,,2012-05-04 13:20:52,0,422,"I am running Windows 70  with Python27 I installed pandas before installing ipython0121(stable) 

When I run 'import pandas' in IPythonexe I get the error message: 'No module named pandas'

If I call IPython from the start menu  via cmd it doesn't automatically run in C:\Python27 where pandas is located It appears to run from a different path C:\Users\mycomputername\ipython (It seems unreasonable that ipython would not be able to detect that the pandas library exists on the machine) 

That said  I also cd into  C:\Python27\dist\ipython-012 folder: to run ipythonexe from there--where the appropriate libraries ie distribute-0626 and pyreadline-171 were installed 

I still receive that error message
Does anyone know if the order of installation is particular on Windows7? ",1374969,,635608.0,2012-05-04 18:14:18,2012-05-04 18:14:18,Pandas cannot be imported into IPython,<python><windows><ipython><pandas>,1.0,2.0,,
10458493,1,10466763.0,2012-05-05 03:10:23,0,527,"I have a series of data indexed by time values (a float) and I want to take chunks of the series and plot them on top of each other  So for example  lets say I have stock prices taken about every 10 minutes for a period of 20 weeks and I want to see the weekly pattern by plotting 20 lines of the stock prices  So my X axis is one week and I have 20 lines (corresponding to the prices during the week)

Updated

The index is not a uniformly spaced value and it is a floating point  It is something like:

t = nparange(0 12e-9 12e-9/10000)
noise = nprandomrandn(1000)/1e12
cn = noisecumsum()
t_noise = t+cn
y = sin(2*mathpi*36e7*t_noise) + noise
df = DataFrame(y index=t_noise columns=[""A""])
dfplot(marker='')
pltaxis([0 02e-8 0 1])


So the index is not uniformly spaced  I'm dealing with voltage vs time data from a simulator  I would like to know how to create a window of time  T  and split df into chunks of T long and plot them on top of each other  So if the data was 20*T long then I would have 20 lines in the same plot

Sorry for the confusion; I used the stock analogy thinking it might help

Thanks",792849,,792849.0,2012-05-06 05:39:19,2012-05-06 05:39:19,Pandas: how to plot yearly data on top of each other,<python><pandas>,2.0,,,
10511024,1,10511545.0,2012-05-09 06:45:29,2,1032,"I am trying to plot some data using pandas in Ipython Notebook  and while it gives me the object  it doesn't actually plot the graph itself So it looks like this:

In [7]:

pledgeAmountplot()

Out[7]:




The graph should follow after that  but it simply doesn't appear I have imported matplotlib  so that's not the problem Is there any other module I need to import ?",442158,,,,2012-05-09 07:24:51,"in Ipython notebook, Pandas is not displying the graph I try to plot.",<python><ipython><pandas>,2.0,2.0,2.0,
10728757,1,,2012-05-23 22:27:09,0,366,"I want to compute the aggregated average of a signal over time  in a certain period I don't know how this is called scientifically 

Example: I have an electricity consumption for a full year in 15 minute values  I want to know my average consumption by hour of the day (24 values)  But it is more complex: there are more measurements in between the 15-minute steps  and I cannot foresee where they are  However  they should be taken into account  with a correct 'weight'

I wrote a function that works  but it is extremely slow Here is a test setup:

import numpy as np

signal = nparange(6)
time = nparray([0  2  35  4  6  8])
period = 4
interval = 2

def aggregate(signal  time  period  interval):
    pass

aggregated = aggregate(signal  time  period  interval)
# This should be the result: aggregated = array([ 2      3125])


aggregated should have period/interval values  This is the manual computation:

aggregated[0] = (nptrapz(y=nparray([0  1])  x=nparray([0  2]))/interval + \
               nptrapz(y=nparray([3  4])  x=nparray([4  6]))/interval) / (period/interval)
aggregated[1] = (nptrapz(y=nparray([1  2  3])  x=nparray([2  35  4]))/interval + \
                nptrapz(y=nparray([4  5])  x=nparray([6  8]))/interval) / (period/interval)


I hope this makes sense  

One last detail: it has to be efficient  thats why my own solution is not useful  Maybe I'm overlooking a numpy or scipy method?  Or is this something pandas can do? 
Thanks a lot for your help ",566942,,566942.0,2012-05-23 23:56:47,2012-07-06 13:42:29,numpy function to aggregate a signal for time?,<python><numpy><signals><pandas>,3.0,4.0,,
10671227,1,10676196.0,2012-05-20 06:14:57,0,137,"I'm getting a floating point error on a simple time series in pandas  I'm trying to do shift operations but this also happens with the window functions like rolling_mean

EDIT: For some more info I tried to actually build this from source yesterday prior to the error  I'm not sure if the error would've occurred prior the build attempt  as I'd never messed around w/ these functions

EDIT2: I thought I'd fixed this  but when I run this inside python it works  but when it's in ipython I get the error

EDIT3: Numpy 170  iPython 013  pandas 073

In [35]: ts = Series(nparange(12)  index=DateRange('1/1/2000'  periods=12  freq='T'))

In [36]: tsshift(0)
Out[36]: 
2000-01-03     0
2000-01-04     1
2000-01-05     2
2000-01-06     3
2000-01-07     4
2000-01-10     5
2000-01-11     6
2000-01-12     7
2000-01-13     8
2000-01-14     9
2000-01-17    10
2000-01-18    11

In [37]: tsshift(1)
Out[37]: ---------------------------------------------------------------------------
FloatingPointError                        Traceback (most recent call last)
/Users/trenthauck/Repository/work/SQS/analysis/campaign/tv2/data/ in ()
----> 1 tsshift(1)

/Library/Python/27/site-packages/ipython-013dev-py27egg/IPython/core/displayhookpyc in __call__(self  result)
    236             selfstart_displayhook()
    237             selfwrite_output_prompt()
--> 238             format_dict = selfcompute_format_data(result)
    239             selfwrite_format_data(format_dict)
    240             selfupdate_user_ns(result)

/Library/Python/27/site-packages/ipython-013dev-py27egg/IPython/core/displayhookpyc in compute_format_data(self  result)
    148             MIME type representation of the object
    149         """"""
--> 150         return selfshelldisplay_formatterformat(result)
    151 
    152     def write_format_data(self  format_dict):

/Library/Python/27/site-packages/ipython-013dev-py27egg/IPython/core/formatterspyc in format(self  obj  include  exclude)
    124                     continue
    125             try:
--> 126                 data = formatter(obj)
    127             except:
    128                 # FIXME: log the exception

/Library/Python/27/site-packages/ipython-013dev-py27egg/IPython/core/formatterspyc in __call__(self  obj)
    445                 type_pprinters=selftype_printers 
    446                 deferred_pprinters=selfdeferred_printers)
--> 447             printerpretty(obj)
    448             printerflush()
    449             return streamgetvalue()

/Library/Python/27/site-packages/ipython-013dev-py27egg/IPython/lib/prettypyc in pretty(self  obj)
    353                 if callable(obj_class_repr_pretty_):
    354                     return obj_class_repr_pretty_(obj  self  cycle)
--> 355             return _default_pprint(obj  self  cycle)
    356         finally:
    357             selfend_group()

/Library/Python/27/site-packages/ipython-013dev-py27egg/IPython/lib/prettypyc in _default_pprint(obj  p  cycle)
    473     if getattr(klass  '__repr__'  None) not in _baseclass_reprs:
    474         # A user-provided repr
--> 475         ptext(repr(obj))
    476         return
    477     pbegin_group(1  ' 50 
--> 698                                     name=True)
    699         else:
    700             result = '%s' % ndarray__repr__(self)

/Library/Python/27/site-packages/pandas/core/seriespyc in _get_repr(self  name  print_header  length  na_rep  float_format)
    756                                         length=length  na_rep=na_rep 
    757                                         float_format=float_format)
--> 758         return formatterto_string()
    759 
    760     def __str__(self):

/Library/Python/27/site-packages/pandas/core/formatpyc in to_string(self)
     99 
    100         fmt_index  have_header = self_get_formatted_index()
--> 101         fmt_values = self_get_formatted_values()
    102 
    103         maxlen = max(len(x) for x in fmt_index)

/Library/Python/27/site-packages/pandas/core/formatpyc in _get_formatted_values(self)
     90         return format_array(selfseriesvalues  None 
     91                             float_format=selffloat_format 
---> 92                             na_rep=selfna_rep)
     93 
     94     def to_string(self):

/Library/Python/27/site-packages/pandas/core/formatpyc in format_array(values  formatter  float_format  na_rep  digits  space  justify)
    431                         justify=justify)
    432 
--> 433     return fmt_objget_result()
    434 
    435 

/Library/Python/27/site-packages/pandas/core/formatpyc in get_result(self)
    528 
    529             # this is pretty arbitrary for now
--> 530             has_large_values = (npabs(selfvalues) > 1e8)any()
    531 
    532             if too_long and has_large_values:

FloatingPointError: invalid value encountered in absolute

In [38]: tsshift(-1)
Out[38]: ---------------------------------------------------------------------------
FloatingPointError                        Traceback (most recent call last)
/Users/myusername/Repository/work/SQS/analysis/campaign/tv2/data/ in ()
----> 1 tsshift(-1)

/Library/Python/27/site-packages/ipython-013dev-py27egg/IPython/core/displayhookpyc in __call__(self  result)
    236             selfstart_displayhook()
    237             selfwrite_output_prompt()
--> 238             format_dict = selfcompute_format_data(result)
    239             selfwrite_format_data(format_dict)
    240             selfupdate_user_ns(result)

/Library/Python/27/site-packages/ipython-013dev-py27egg/IPython/core/displayhookpyc in compute_format_data(self  result)
    148             MIME type representation of the object
    149         """"""
--> 150         return selfshelldisplay_formatterformat(result)
    151 
    152     def write_format_data(self  format_dict):

/Library/Python/27/site-packages/ipython-013dev-py27egg/IPython/core/formatterspyc in format(self  obj  include  exclude)
    124                     continue
    125             try:
--> 126                 data = formatter(obj)
    127             except:
    128                 # FIXME: log the exception

/Library/Python/27/site-packages/ipython-013dev-py27egg/IPython/core/formatterspyc in __call__(self  obj)
    445                 type_pprinters=selftype_printers 
    446                 deferred_pprinters=selfdeferred_printers)
--> 447             printerpretty(obj)
    448             printerflush()
    449             return streamgetvalue()

/Library/Python/27/site-packages/ipython-013dev-py27egg/IPython/lib/prettypyc in pretty(self  obj)
    353                 if callable(obj_class_repr_pretty_):
    354                     return obj_class_repr_pretty_(obj  self  cycle)
--> 355             return _default_pprint(obj  self  cycle)
    356         finally:
    357             selfend_group()

/Library/Python/27/site-packages/ipython-013dev-py27egg/IPython/lib/prettypyc in _default_pprint(obj  p  cycle)
    473     if getattr(klass  '__repr__'  None) not in _baseclass_reprs:
    474         # A user-provided repr
--> 475         ptext(repr(obj))
    476         return
    477     pbegin_group(1  ' 50 
--> 698                                     name=True)
    699         else:
    700             result = '%s' % ndarray__repr__(self)

/Library/Python/27/site-packages/pandas/core/seriespyc in _get_repr(self  name  print_header  length  na_rep  float_format)
    756                                         length=length  na_rep=na_rep 
    757                                         float_format=float_format)
--> 758         return formatterto_string()
    759 
    760     def __str__(self):

/Library/Python/27/site-packages/pandas/core/formatpyc in to_string(self)
     99 
    100         fmt_index  have_header = self_get_formatted_index()
--> 101         fmt_values = self_get_formatted_values()
    102 
    103         maxlen = max(len(x) for x in fmt_index)

/Library/Python/27/site-packages/pandas/core/formatpyc in _get_formatted_values(self)
     90         return format_array(selfseriesvalues  None 
     91                             float_format=selffloat_format 
---> 92                             na_rep=selfna_rep)
     93 
     94     def to_string(self):

/Library/Python/27/site-packages/pandas/core/formatpyc in format_array(values  formatter  float_format  na_rep  digits  space  justify)
    431                         justify=justify)
    432 
--> 433     return fmt_objget_result()
    434 
    435 

/Library/Python/27/site-packages/pandas/core/formatpyc in get_result(self)
    528 
    529             # this is pretty arbitrary for now
--> 530             has_large_values = (npabs(selfvalues) > 1e8)any()
    531 
    532             if too_long and has_large_values:

FloatingPointError: invalid value encountered in absolute
",402468,,402468.0,2012-05-20 15:44:23,2012-05-20 18:51:26,Pandas FloatingPoint Error,<python><ipython><pandas>,1.0,5.0,,
10717504,1,10718007.0,2012-05-23 09:53:22,2,276,"I have a csv file that looks like so:

TEST
2012-05-01 00:00:00203 ON 1
2012-05-01 00:00:11203 OFF 0
2012-05-01 00:00:22203 ON 1
2012-05-01 00:00:33203 OFF 0
2012-05-01 00:00:44203 OFF 0
TEST
2012-05-02 00:00:00203 OFF 0
2012-05-02 00:00:11203 OFF 0
2012-05-02 00:00:22203 OFF 0
2012-05-02 00:00:33203 OFF 0
2012-05-02 00:00:44203 ON 1
2012-05-02 00:00:55203 OFF 0  

and cannot get rid of the ""TEST"" string
Is it possible to check whether a line starts with a date and read only those that do?",1412286,,1301710.0,2012-05-29 12:22:21,2012-05-29 12:22:21,Is it possible to use read_csv to read only specific lines?,<python><csv><pandas>,3.0,0.0,,
10791661,1,10795379.0,2012-05-29 00:06:52,0,520,"I mean something like this:

I have a DataFrame with columns that may be categorical or nominal  For each observation (row)  I want to generate a new row where every possible value for the variables is now its own binary variable  For example  this matrix (first row is column labels)

'a'     'b'     'c'
one     02     0
two     04     1
two     09     0
three   01     2
one     00     4
two     02     5


would be converted into something like this:

'a'              'b'                                                    'c'
one  two  three  [00 02)  [02 04)  [04 06)  [06 08)  [08 10]   0   1   2   3   4   5

 1    0     0        0          1          0          0          0       1   0   0   0   0   0
 0    1     0        0          0          0          0          1       0   1   0   0   0   0
 0    1     0        0          0          0          0          1       1   0   0   0   0   0
 0    0     1        1          0          0          0          0       0   0   1   0   0   0
 1    0     0        1          0          0          0          0       0   0   0   0   1   0
 0    1     0        0          1          0          0          0       0   0   0   0   0   1


Each variable (column) in the initial matrix get binned into all the possible values  If it's categorical  then each possible value becomes a new column  If it's a float  then the values are binned some way (say  always splitting into 10 bins)  If it's an int  then it can be every possibel int value  or perhaps also binning

FYI: in my real application  the table has up to 2 million rows  and the full ""expanded"" matrix may have hundreds of columns

Is there an easy way to perform this operation?

Separately  I would also be willing to skip this step  as I am really trying to compute a Burt table (which is a symmetric matrix of the cross-tabulations)  Is there an easy way to do something similar with the crosstab function?  Otherwise  computing the cross tabulation is just a simple matrix multiplication

Thanks!

Uri",510187,,,,2012-06-12 21:52:56,How do I discretize values in a pandas DataFrame and convert to a binary matrix?,<python><data><pandas>,2.0,,,
10797734,1,10816150.0,2012-05-29 10:54:38,1,463,"I am new in python and I am currenlt struggly to do simple things with pandas I would like to apply the same function to each item of a given dataset but using a time-dependent parameter

I am working with pandas DataFrame with timestamps as index

Let's say :

a(i j) is ith element in column j in a dataframe A (timestamp/index = i and column = j)

b(i) is the ith element in a dataframe B (with a single column)

I want to compute:

c(i j)=fct(a(i j) b(i))

where fct is a function with two arguments z=fct(x y)

I wrote a code that does it correcly but it is likely not optimal (very slow) 
For the example I just used a simple function fct (but in reallity it is more complex)

Inputs:

df_data: pandasDataFrame with index=timestamps and several columns
df_parameter: pandasDataFrame with 1 column containing the time-dependent parameter
Here is the code:

# pconcat is required as timestamps are not identical in df_data & df_parameters
import numpy as np
import pandas as p

temp = pconcat([df_data  df_parameter]  join='inner'  axis=1)
index = tempindex
np_data = temp[nacelleWindSpeedscolumns]values
np_parameter = temp[airDensitycolumns]values

import math 

def fct(x  y):
    return mathpow(x  y)

def test(np_data  np_parameter):
    np_result = npempty(np_datashape  dtype=float)
    it = npnditer(np_data  flags=['multi_index'])

    while not itfinished:
        np_result[itmulti_index] = fct(it[0]item() 
                                        np_parameter[itmulti_index[0]][0])
        ititernext()

    df_final=pDataFrame(data=np_result  index=index)
    return df_final

final=test(np_data  np_parameter)   

finalto_csv(r'C:\temp\testcsv'  sep=';')


Here is some example data:

df_data

01/03/2010 00:00  ;  9  ;  5  ;  7  
01/03/2010 00:10  ;  9  ;  1  ;  4  
01/03/2010 00:20  ;  5  ;  3  ;  8  
01/03/2010 00:30  ;  7  ;  7  ;  1  
01/03/2010 00:40  ;  8  ;  2  ;  3  
01/03/2010 00:50  ;  0  ;  3  ;  4     
01/03/2010 01:00  ;  4  ;  3  ;  2  
01/03/2010 01:10  ;  6  ;  2  ;  2  
01/03/2010 01:20  ;  6  ;  8  ;  5  
01/03/2010 01:30  ;  7  ;  7  ;  0  


df_parameter  

01/03/2010 00:00  ;  2  
01/03/2010 00:10  ;  5  
01/03/2010 00:20  ;  2  
01/03/2010 00:30  ;  3  
01/03/2010 00:40  ;  0  
01/03/2010 00:50  ;  2  
01/03/2010 01:00  ;  4  
01/03/2010 01:10  ;  3  
01/03/2010 01:20  ;  3  
01/03/2010 01:30  ;  1  


final  

01/03/2010 00:00  ;  81  ;  25  ;  49  
01/03/2010 00:10  ;  59049  ;  1  ;  1024  
01/03/2010 00:20  ;  25  ;  9  ;  64  
01/03/2010 00:30  ;  343  ;  343  ;  1  
01/03/2010 00:40  ;  1  ;  1  ;  1  
01/03/2010 00:50  ;  0  ;  9  ;  16  
01/03/2010 01:00  ;  256  ;  81  ;  16  
01/03/2010 01:10  ;  216  ;  8  ;  8  
01/03/2010 01:20  ;  216  ;  512  ;  125  
01/03/2010 01:30  ;  7  ;  7  ;  0  


Thank you very very much in advance for your help 

Patrick",1415325,,1301710.0,2012-05-30 13:56:06,2012-06-04 14:01:14,How to optimally apply a function on all items of a dataframe using inputs from another dataframe?,<python><numpy><pandas>,1.0,2.0,1.0,
10595327,1,10606901.0,2012-05-15 06:34:02,1,439,"Just trying out Pandas for the first time  and I am trying to sort a pivot table first by an index  then by the values in a series

So far I've tried:

table = pivot_table(sheet1  values='Value'  rows=['A' 'B']  aggfunc=npsum)

# Sorts by value ascending  can't change to descending
tablecopy()sort()
table

# The following gives me the correct ordering in values  but ignores index 
sorted_table = tableorder(ascending=False)
sorted_table

# The following brings me back to the original ordering
sorted_table = tableorder(ascending=False)
sorted_table2 = sorted_tablesortlevel(0)
sorted_table2


What's the correct way to sort a pivot table by index then value?",599251,,,,2012-05-15 18:43:27,Pandas: Sort pivot table,<python><pandas>,1.0,1.0,1.0,
10697423,1,10708261.0,2012-05-22 07:14:56,0,393,"Using Ipython notebook and the pandas module  I have a bit of code that iterates through a series and makes a number of bar charts (or is supposed to) It only produces the last chart that it should The data is the funds raised by day and radio show  and I want a chart for each day I suspect this may be a combo pandas/ipython problem  but I don't know how to approach it   

The code is this:

print pledge[pledgeDate==k[0]]groupby('Break')['Amount']sum()plot(kind='bar')
kcount =0;
for k  v in groupedAmountiteritems():
    if k[0]  kcount:
        kcount=k[0]
        print k[0];
        print pledge[pledgeDate==k[0]]groupby('Break')['Amount']sum()plot(kind='bar')


and the output I get is 

05/02/2012 

Axes(0125 0125;0775x0775)

05/03/2012

Axes(0125 0125;0775x0775) 

05/04/2012 

Axes(0125 0125;0775x0775)

05/05/2012 

Axes(0125 0125;0775x0775) 

05/06/2012

Axes(0125 0125;0775x0775)




With only a single chart at the end  of the last iteration ",442158,,442158.0,2012-06-07 17:57:52,2012-06-07 17:57:52,How do I display multiple charts in pandas python,<python><ipython><pandas>,2.0,1.0,,
10715519,1,10726275.0,2012-05-23 07:41:57,0,468,"I have a DataFrame with a few columns One columns contains a symbol for which currency is being used  for instance a euro or a dollar sign Another column contains a budget value So for instance in one row it could mean a budget of 5000 in euro and in the next row it could say a budget of 2000 in dollar

In pandas I would like to add an extra column to my DataFrame  normalizing the budgets in euro So basically  for each row the value in the new column should be the value from the budget column * 1 if the symbol in the currency column is a euro sign  and the value in the new column should be the value of the budget column * 078125 if the symbol in the currency column is a dollar sign

I know how to add a column  fill it with values  copy values from another column etc but not how to fill the new column conditionally based on the value of another column 

Any suggestions?",493499,,,,2012-05-23 19:05:57,Conditionally fill column values based on another columns value in pandas,<data.frame><pandas>,1.0,,3.0,
10729210,1,10739432.0,2012-05-23 23:21:18,1,2052,"Possible Duplicate:What is the most efficient way to loop through dataframes with pandas?  




I'm looking to iterate row by row through a pandas DataFrame  The way I'm doing it so far is as follows:

for i in dfindex:
    do_something(dfix[i])


Is there a more performant and/or more idiomatic way to do this?  I know about apply  but sometimes it's more convenient to use a for loop  Thanks in advance",1413778,,1413778.0,2012-05-23 23:27:42,2012-05-24 14:24:52,iterating row by row through a pandas dataframe,<pandas>,1.0,2.0,1.0,2012-05-29 13:14:21
10824906,1,,2012-05-30 22:03:44,2,121,"I have a data frame (let's call it ""csv"") that I want to group and get a value of the first element of the group Example:

A   B   C  D
foo bar happy yellow
foo bar sad   green
foo ape last  laugh


I would like this as output:

A   B   C
foo bar happy
foo ape last


I currently do this:

grp1 = csvgroupby(['A' 'B'])
lst = [(A B csvix[group[0]]['C']) for (A B) group in grp1groupsitems()]
df = DataFrame(lst columns=['A' 'B' 'C'])
dfto_csv('grpcsv' cols=['A' 'B' 'C'] index=False)


But this seems inefficient Do I really have to create a list first  and then create a dataframe from that? Isn't there a way to just create a dataframe directly  or do some sort of indexing or something on the original dataframe so that i can just work with the first record in each group?",1427057,,1301710.0,2012-06-01 07:59:04,2012-06-01 07:59:04,How to create a dataframe from grouped data,<python><pandas>,1.0,,1.0,
10665889,1,10677896.0,2012-05-19 14:11:42,3,1218,"I load a some machine learning data from a csv file The first 2 columns are observations and the remaining columns are features

Currently  I do the following :

data = pandasread_csv('mydatacsv')


which gives something like:

data = pandasDataFrame(nprandomrand(10 5)  columns = list('abcde'))


I'd like to slice this dataframe in two dataframes: one containing the columns a and b and one containing the columns c  d and e

It is not possible to write something like 

observations = data[:'c']
features = data['c':]


I'm not sure what the best method is Do I need a panel?

By the way  I find dataframe indexing pretty inconsistent: data['a'] is permitted  but data[0] is not On the other side  data['a':] is not permitted but data[0:] is
Is there a practical reason for this? This is really confusing if columns are indexed by Int  given that data[0] != data[0:1]",1350862,,,,2012-05-20 22:51:30,How to slice by columns in pandas,<python><numpy><statistics><pandas>,2.0,2.0,1.0,
10715965,1,10716007.0,2012-05-23 08:12:31,9,3238,"I understand that pandas is designed to load fully populated DataFrame but I need to create an empty DataFrame then add rows  one by one
What is the best way to do this ?

I successfully created an empty DataFrame with :

res = DataFrame(columns=('lib'  'qty1'  'qty2'))


Then I can add a new row and fill a field with :

res = resset_value(len(res)  'qty1'  100)


It works but seems very odd :-/ (it fails for adding string value)

How can I add a new row to my DataFrame (with different columns type) ?

Answer from @aix hint :

row = pandasDataFrame([dict(lib='hello'  qty1=40  qty2=1000)  ])
res = resappend(row  ignore_index=True)
",31335,,31335.0,2012-05-23 20:17:28,2012-10-17 19:38:13,add one row in a pandas.DataFrame,<python><pandas>,1.0,2.0,6.0,
10771745,1,10793167.0,2012-05-27 04:53:45,0,472,"I have two pandas DataFrames and I want to join them together such that I get the outer join with the duplicates removed  My problem is that drop_duplicates() ignores the index when finding duplicates  If the index is different then it shouldn't be a duplicate  How do I remove duplicates if the row index and columns are duplicates?  The only thing I can think of is using dfto_dict() and then create a new DataFrame (very inefficient)  

Update:

As requested here is an example of my data:

from pandas import *
index1 = ['2012-05-2' + str(i) for i in range(0 6)]
data1 = {'rate': range(0 6)}
a = DataFrame(data1  index1)

index2 = ['2012-05-2' + str(i) for i in range(3 9)]
data2 = {'rate': range(3 9)}
b = DataFrame(data2  index2)


Glen",792849,,792849.0,2012-05-29 04:42:54,2012-05-29 04:43:14,Removing duplicates from dataframe with index + row matching,<python><pandas>,1.0,2.0,1.0,
10883805,1,,2012-06-04 15:22:52,0,338,"I am starting out learning this wonderful tool  and I am stuck at the simple task of loading several time series and aligning them with a ""master"" date vector 

For example: I have a csv file: Datacsv where the first row contains the headers ""Date1  Rate1  Date2  Rate2"" where Date1 is the dates of the Rate1 and Date2 are the dates of Rate2

In this case  Rate2 has more observations (the start date is the same as Date1  but the end date is furhter apart then the end date in Date1  and there are less missing values)  and everything should be indexed according to Date2

What is the preferred way to get the following DataFrame? (or accomplishing something similar)

index(Date2) Rate1 Rate2
11/12/06     15   18
12/12/06     NaN   19
13/12/06     16   19
etc
etc
11/10/06     NaN   12
12/10/06     NaN   11
13/10/06     NaN   13


I have tried to follow the examples in the official pandaspdf and Googling  but to no avail (I even bought the Pre-Edition of Mr McKinneys Pandas book  but the chapters concering Pandas where not ready yet :( )

Is there a nice recipe for this?

Thank you very much

EDIT: Concerning the answer of separating the series into two CSV files:
But what if I have very many time series  eg

Date1 Rate1 Date2 Rate2  DateN RateN

And all I know is that the dates should be almost the same  with exceptions coming from series that contain missing values (where there is no Date or Rate entry) (this would be an example of some financial economics time series  by the way)

Is the preferred way to load this dataset still to split every series into a separate CSV?

EDIT2 archlight is completely right  just doing ""csv_read"" will mess things up

Essentially my question would then boil down to: how to join several unaligned time series  where each series has a date column  and column for the series itself (CSV file exported from Excel)

Thanks again",1338403,,1338403.0,2012-06-05 11:42:49,2012-06-06 06:51:17,"Load un-aligned time series into a DataFrame, with one index?",<python><pandas>,2.0,,1.0,
10706533,1,10706942.0,2012-05-22 16:54:49,0,306,"I am trying to install the Pandas library on Python 32 on Windows 7 64bit

The pip log is here
http://pastebincom/Vuitwaz9",362750,,,,2012-05-22 17:22:44,"Error Installing Pandas using pip, Windows 7 64bit Python 3.2",<python><pandas>,1.0,,,
10839701,1,,2012-05-31 19:04:59,3,422,"What's the most efficient way to calculate the time-weighted average of a TimeSeries in Pandas 08? For example  say I want the time-weighted average of dfy - dfx as created below:

import pandas
import numpy as np
times = npdatetime64('2012-05-31 14:00') + nptimedelta64(1  'ms') * npcumsum(10**3 * nprandomexponential(size=10**6))
x = nprandomnormal(size=10**6)
y = nprandomnormal(size=10**6)
df = pandasDataFrame({'x': x  'y': y}  index=times)


I feel like this operation should be very easy to do  but everything I've tried involves several messy and slow type conversions",1429196,,,,2012-06-01 19:15:41,Time-weighted average with Pandas,<python><time-series><pandas>,1.0,,1.0,
10844493,1,10844760.0,2012-06-01 04:40:16,0,515,"I'm having a bit of trouble altering a duplicated pandas DataFrame and not having the edits apply to both the duplicate and the original DataFrame 

Here's an example Say I create an arbitrary DataFrame from a list of dictionaries:

In [67]: d = [{'a':3  'b':5}  {'a':1  'b':1}]

In [68]: d = DataFrame(d)

In [69]: d

Out[69]: 
   a  b
0  3  5
1  1  1


Then I assign the 'd' dataframe to variable 'e' and apply some arbitrary math to column 'a' using apply:

In [70]: e = d

In [71]: e['a'] = e['a']apply(lambda x: x + 1)


The problem arises in that the apply function apparently applies to both the duplicate DataFrame 'e' and original DataFrame 'd'  which I cannot for the life of me figure out:

In [72]: e # duplicate DataFrame
Out[72]: 
   a  b
0  4  5
1  2  1

In [73]: d # original DataFrame  notice the alterations to frame 'e' were also applied
Out[73]:  
   a  b
0  4  5
1  2  1


I've searched both the pandas documentation and Google for a reason why this would be so  but to no avail I can't understand what is going on here at all 

I've also tried the math operations using a element-wise operation (eg  e['a'] = [i + 1 for i in e['a']] )  but the problem persists Is there a quirk in the pandas DataFrame type that I'm not aware of? I appreciate any insight someone might be able to offer",1429853,,,,2012-06-01 05:27:19,DataFrame.apply in python pandas alters both original and duplicate DataFrames,<python><pandas>,1.0,,,
10947968,1,10948591.0,2012-06-08 11:28:57,0,179,"I have a XML file with thousands of lines like:

WORD


I want to convert it (all it's attributes) to pandas dataframe To do that i could loop through the file using beautiful soup and insert the values row by row or create lists to be inserted as columns However I would like to know if there's a more pythonic way of accomplishing what I described Thank you in advance

Code example:

x1list=[]
x2list=[]

for word in souppagefindAll('word'):
    x1listappend(int(word['x1']))
    x2listappend(int(word['x2']))
df=DataFrame({'x1':x1list 'x2':x2list})
",1199589,,1199589.0,2012-06-08 11:37:12,2012-06-08 12:09:32,XML to pandas dataframe,<python><xml><data.frame><pandas>,1.0,2.0,,
10972410,1,10972557.0,2012-06-10 21:12:43,2,844,"I have a pandas DataFrame that has multiple columns in it:

Index: 239897 entries  2012-05-11 15:20:00 to 2012-06-02 23:44:51
Data columns:
foo                   11516  non-null values
bar                   228381  non-null values
Time_UTC              239897  non-null values
dtstamp               239897  non-null values
dtypes: float64(4)  object(1)


where foo and bar are columns which contain the same data yet are named differently Is there are a way to move the rows which make up foo into bar  ideally whilst maintaining the name of bar? 

In the end the DataFrame should appear as:

Index: 239897 entries  2012-05-11 15:20:00 to 2012-06-02 23:44:51
Data columns:
bar                   239897  non-null values
Time_UTC              239897  non-null values
dtstamp               239897  non-null values
dtypes: float64(4)  object(1)


That is the NaN values that made up bar were replaced by the values from foo",983310,,,,2012-06-10 21:38:40,pandas: combine two columns in a DataFrame,<python><data.frame><pandas>,1.0,,2.0,
10751127,1,,2012-05-25 08:35:27,3,511,"I'm using a Pandas DataFrame to do a row-wise t-test as per this example:

import numpy
import pandas

df = pandasDataFrame(numpylog2(numpyrandn(1000  4)  
                      columns=[""a""  ""b""  ""c""  ""d""])

df = dfdropna()


Now  supposing I have ""a"" and ""b"" as one group  and ""c"" and ""d"" at the other  I'm performing the t-test row-wise This is fairly trivial with pandas  using apply with axis=1 However  I can either return a DataFrame of the same shape if my function doesn't aggregate  or a Series if it aggregates

Normally I would just output the p-value (so  aggregation) but I would like to generate an additional value based on other calculations (in other words  return two values) I can of course do two runs  aggregating the p-values first  then doing the other work  but I was wondering if there is a more efficient way to do so as the data is reasonably large

As an example of the calculation  a hypotethical function would be:

from scipystats import ttest_ind

def t_test_and_mean(series  first  second):
    first_group = series[first]
    second_group = series[second]
    _  pvalue = ttest_ind(first_group  second_group)

    mean_ratio = second_groupmean() / first_groupmean()

    return (pvalue  mean_ratio)


Then invoked with 

dfapply(t_test_and_mean  first=[""a""  ""b""]  second=[""c""  ""d""]  axis=1)


Of course in this case it returns a single Series with the two tuples as value

Instead  ny expected output would be a DataFrame with two columns  one for the first result  and one for the second Is this possible or I have to do two runs for the two calculations  then merge them together?",241515,,241515.0,2012-05-25 08:40:41,2012-05-25 23:48:46,Returning multiple values from pandas apply on a DataFrame,<python><pandas>,1.0,2.0,1.0,
10760364,1,10761312.0,2012-05-25 19:33:39,3,517,"I'm building some interactive workflows in IPython using the fantastic Notebook for interactive analysis and Pandas

Some of the tables I'm displaying would be much easier to read with a little bit of formatting I'd really like something like ""zebra tables"" where every other row is shaded I read here about how this formatting can be implemented via css Is there a really straight forward way to apply a css to an IPython Notebook and then have tables rendered using the style sheet? ",37751,,,,2012-12-27 06:32:31,"""Zebra Tables"" in IPython Notebook?",<ipython><pandas>,2.0,,1.0,
10867028,1,11005208.0,2012-06-03 00:38:37,3,856,"I'm using the pandas library to read in some CSV data  In my data  certain columns contain strings  The string ""nan"" is a possible value  as is an empty string  I managed to get pandas to read ""nan"" as a string  but I can't figure out how to get it not to read an empty value as NaN  Here's sample data and output

One Two Three
a 1 one
b 2 two
 3 three
d 4 nan
e 5 five
nan 6 
g 7 seven

>>> pandasread_csv('testcsv'  na_values={'One': []  ""Three"": []})
    One  Two  Three
0    a    1    one
1    b    2    two
2  NaN    3  three
3    d    4    nan
4    e    5   five
5  nan    6    NaN
6    g    7  seven


It correctly reads ""nan"" as the string ""nan'  but still reads the empty cells as NaN  I tried passing in str in the converters argument to read_csv (with converters={'One': str}))  but it still reads the empty cells as NaN

I realize I can fill the values after reading  with fillna  but is there really no way to tell pandas that an empty cell in a particular CSV column should be read as an empty string instead of NaN?",1427416,,,,2012-06-25 22:35:25,Get pandas.read_csv to read empty values as empty string instead of nan,<python><csv><pandas>,2.0,,,
11004088,1,11039349.0,2012-06-12 20:11:53,3,346,"I have a Multindex DataFrame with the following structure:

       0     1     2     ref
A  B             
21 45  001  056  023  002
22 45  030  088  053  087
23 46  045  023  090  023


What I want to do with it is:
From the columns [0:2] choose the closest value to the column 'ref'  so the expected result would be:

       closest
A  B             
21 45  001
22 45  088
23 46  023 
",1330293,,,,2012-06-14 18:39:53,indexing a pandas DataFrame,<python><pandas>,1.0,,,
10857924,1,,2012-06-01 22:05:44,7,1217,"I have a dataFrame in pandas and several of the columns have all null values Is there a built in function which will let me remove those columns?

Thank you!",1165294,,1301710.0,2012-06-02 08:34:18,2012-07-24 14:30:10,Remove NULL columns in a dataframe Pandas?,<python><pandas>,1.0,,1.0,
11005012,1,,2012-06-12 21:17:34,1,182,"I am new to Pandas so please forgive me if I'm overlooking something obvious  but I can't seem to find the answer in previous questions

I'm trying to import a table into Pandas that has an arbitrary number of whitespaces as the delimiters  Here's an example of the data:

*PRODUCT : Backscatter Ratio - 10640 nm ^

Altitude                  2010/03/23 17:01:00       2010/03/23 17:03:00       

150                      1                         1                         

450                      1                         1                         

                                                  

                                                  

                                                  *


The actual size of the table is 1310 columns by 6009 rows  I don't necessarily want to use the fixed width parser as the spacing might change for these tables in the future so I used the following:

df = pandasioparsersread_csv(filepath sep='s*' header=2 index_col=None  skiprows=2)


When I do this I get the following error:


  File ""C:\Python27\lib\site-packages\pandas\io\parserspy""  line 187 
  in read_csv
      return _read(TextParser  filepath_or_buffer  kwds)
  File ""C:\Python27\lib\site-packages\pandas\io\parserspy""  line 160  in
  _read
      return parserget_chunk()
  File ""C:\Python27\lib\site-packages\pandas\io\parserspy""  line 613  in
  get_chunk
      raise Exception(err_msg) 
  
  Exception: Index (columns 0) have duplicate values [nan]


Notice I have set the index column to None  I've tried this using the first column as the index column and get the same error  There are no empty lines in the data set or duplicated values in the first column",1452228,,1452228.0,2012-06-12 23:30:38,2012-10-18 18:21:53,Importing data table using Pandas - Exception: Index (columns 0) have duplicate values [nan],<io><pandas>,1.0,2.0,0.0,
11012981,1,,2012-06-13 10:29:47,2,176,"I recently learned about Pandas and was happy to see its analytics functionality  I am trying to convert Excel array functions into the Pandas equivalent to automate spreadsheets that I have created for the creation of performance attribution reports  In this example  I created a new column in Excel based on conditions within other columns:

={SUMIFS($F$10:$F$4518 $A$10:$A$4518 $C$4 $B$10:$B$4518 0 $C$10:$C$4518 "" "" $D$10:$D$4518 $D10 $E$10:$E$4518 $E10)}


The formula is summing up the values in the ""F"" array (security weights) based on certain conditions  ""A"" array (portfolio ID) is a certain number  ""B"" array (security id) is zero  ""C"" array (group description) is "" ""  ""D"" array (start date) is the date of the row that I am on  and ""E"" array (end date) is the date of the row that I am on

In Pandas  I am using the DataFrame  Creating a new column on a dataframe with the first three conditions is straight forward  but I am having difficult with the last two conditions  

reportAggregateDF['PORT_WEIGHT'] = reportAggregateDF['SEC_WEIGHT_RATE']
          [(reportAggregateDF['PORT_ID'] == portID) &
           (reportAggregateDF['SEC_ID'] == 0) &
           (reportAggregateDF['GROUP_LIST'] == "" "") & 
           (reportAggregateDF['START_DATE'] == reportAggregateDF['START_DATE']ix[:]) & 
           (reportAggregateDF['END_DATE'] == reportAggregateDF['END_DATE']ix[:])]sum()


Obviously the ix[:] in the last two conditions is not doing anything for me  but is there a way to make the sum conditional on the row that I am on without looping?  My goal is to not do any loops  but instead use purely vector operations",1452305,,,,2012-06-15 19:18:50,Create Excel-like SUMIFS in Pandas,<python><pandas>,1.0,0.0,,
11041411,1,11042986.0,2012-06-14 21:11:12,2,995,"I want to be able to create a Pandas DataFrame with MultiIndexes for the rows and the columns index and read it from an ASCII text file  My data looks like:

col_indx = MultiIndexfrom_tuples([('A'   'B'   'C')  ('A'   'B'   'C2')  ('A'   'B'   'C3')  
                                   ('A'   'B2'  'C')  ('A'   'B2'  'C2')  ('A'   'B2'  'C3')  
                                   ('A'   'B3'  'C')  ('A'   'B3'  'C2')  ('A'   'B3'  'C3')  
                                   ('A2'  'B'   'C')  ('A2'  'B'   'C2')  ('A2'  'B'   'C3')  
                                   ('A2'  'B2'  'C')  ('A2'  'B2'  'C2')  ('A2'  'B2'  'C3')  
                                   ('A2'  'B3'  'C')  ('A2'  'B3'  'C2')  ('A2'  'B3'  'C3')]  
                                   names=['one' 'two' 'three']) 
row_indx = MultiIndexfrom_tuples([(0   'North'  'M')  
                                   (1   'East'   'F')  
                                   (2   'West'   'M')  
                                   (3   'South'  'M')  
                                   (4   'South'  'F')  
                                   (5   'West'   'F')  
                                   (6   'North'  'M')  
                                   (7   'North'  'M')  
                                   (8   'East'   'F')  
                                   (9   'South'  'M')]  
                                   names=['n'  'location'  'sex'])
size=len(row_indx)  len(col_indx)
data = nprandomrandint(0 10  size)
df = DataFrame(data  index=row_indx  columns=col_indx)
print df


I've tried dfto_csv() and read_csv() but they don't keep the index  

I was thinking of maybe creating a new format using extra delimeters  For example  using a row of ---------------- to mark the end of the column indexes and a | to mark the end of a row index So it would look like this:

one            | A   A   A   A   A   A   A   A   A  A2  A2  A2  A2  A2  A2  A2  A2  A2
two            | B   B   B  B2  B2  B2  B3  B3  B3   B   B   B  B2  B2  B2  B3  B3  B3
three          | C  C2  C3   C  C2  C3   C  C2  C3   C  C2  C3   C  C2  C3   C  C2  C3
--------------------------------------------------------------------------------------
n location sex :                                                                      
0 North    M   | 2   3   9   1   0   6   5   9   5   9   4   4   0   9   6   2   6   1
1 East     F   | 6   2   9   2   7   0   0   3   7   4   8   1   3   2   1   7   7   5
2 West     M   | 5   8   9   7   6   0   3   0   2   5   0   3   9   6   7   3   4   9
3 South    M   | 6   2   3   6   4   0   4   0   1   9   3   6   2   1   0   6   9   3
4 South    F   | 9   6   0   0   6   1   7   0   8   1   7   6   2   0   8   1   5   3
5 West     F   | 7   9   7   8   2   0   4   3   8   9   0   3   4   9   2   5   1   7
6 North    M   | 3   3   5   7   9   4   2   6   3   2   7   5   5   5   6   4   2   9
7 North    M   | 7   4   8   6   8   4   5   7   9   0   2   9   1   9   7   9   5   6
8 East     F   | 1   6   5   3   6   4   6   9   6   9   2   4   2   9   8   4   2   4
9 South    M   | 9   6   6   1   3   1   3   5   7   4   8   6   7   7   8   9   2   3


Does Pandas have a way to write/read DataFrames to/from ASCII files with MultiIndexes?",792849,,,,2013-01-15 18:38:55,How to write/read a Pandas DataFrame with MultiIndex from/to an ASCII file?,<python><pandas>,2.0,1.0,,
11057222,1,11065664.0,2012-06-15 19:45:33,1,168,"Thanks to the response to my initial question  I now have a multi-indexed DataFrame the way that I want it  Now that I have the data in the data structure  I'm trying to get it out and wonder if there is a better way to do this  My two problems are related  but may have separate ""ideal"" solutions:

Sample DataFrame (truncated)

Experiment           IWWGCW         IWWGDW       
Lead Time                24     48      24     48
2010-11-27 12:00:00   0997  0991   0998  0990
2010-11-28 12:00:00   0998  0987   0997  0990
2010-11-29 12:00:00   0997  0992   0997  0992
2010-11-30 12:00:00   0997  0987   0997  0987
2010-12-01 12:00:00   0996  0986   0996  0986


Iteration

I'd like to be able to loop over this DataFrame where the iteration would take me down only 1 index dimension  ie an iteritems behavior that would return [('IWWGCW'  df['IWWGCW'])  ('IWWGDW'  df['IWWGDW'])] and yield 2 DataFrames with Lead Time columns  My brute-force solution is to use a wrapper routine that basically does [(key  df[key] for key in dfcolumnslevels[0]]  Is there a better way to do this?

Apply

I'd also like to do things like ""subtract the IWWGDW entries from everybody else"" to compute paired differences  I tried to do dfapply(lambda f: f - df['IWWGDW']) but get a KeyError: ('IWWGDW'  'occurred at index 2010-11-26 12:00:00') regardless of if I use axis=1 or axis=0  I've tried rebuilding a new DataFrame using the iteration workaround identified above  but I always worry when I brute-force things  Is there a more ""pandasic"" way to do this sort of computation?",24895,,,,2012-06-16 17:38:48,How can I iterate and apply a function over a single level of a DataFrame with MultiIndex?,<python><pandas>,1.0,,,
11015974,1,11128080.0,2012-06-13 13:32:55,0,158,"I have the data below saved as a pandas dataframe With this data  I would like to calculate the bid ask spread for a specific second However  as you can see there are many times more asks than bids and vice versa So my goal is to do the following: I would like to take only data such that it is a bid followed by an ask  or an ask followed by the bid for the same timestamp and then calculate the spread and how many spreads there were

In the below data  it would look like the following  I would take row 1 and row 2  and calculate the spread which is 0 Then I would take row 3 and row 4 and have a spread of 2 

        time quote   price  volume
0   07:00:00     B  39505       5
1   07:00:00     B  39500       4
2   07:00:00     A  39500       7
3   07:00:00     B  39480      17
4   07:00:00     A  39500      20
5   07:00:00     A  39500      31
6   07:00:00     A  39500      44
7   07:00:00     A  39500      57
8   07:00:00     A  39500      67
9   07:00:00     A  39500      57
10  07:00:00     A  39500      67
11  07:00:00     A  39500      80
12  07:00:00     A  39500      90
13  07:00:00     A  39500      99
14  07:00:01     B  39480      15
15  07:00:01     A  39500      89
16  07:00:01     A  39495       1
17  07:00:02     A  39500      89
18  07:00:03     B  39480      12
19  07:00:03     A  39490       1
20  07:00:03     B  39480       9
21  07:00:03     B  39485       4
22  07:00:04     A  39495       5
23  07:00:04     B  39485       2
24  07:00:05     B  39485       1


This is my desired output:

       time spread num_spread
   07:00:00      2          2 
   07:00:01      2          1  
   07:00:03      1          1
   07:00:04      1          1
",1449148,,,,2012-06-20 20:57:25,"Vectorize High Frequency Data, Calculate Spread,",<python><time-series><pandas>,1.0,4.0,,
11018120,1,11037996.0,2012-06-13 15:19:52,2,279,"I am currently fighting to work with the reasmpling function from pandas 080b1

For example  when I try to aggregate (using 'mean') 10min values to monthly values  the function seems to use the last day of data from one month in the mean of the next month

Here is an example with a simple time serie of 3 month of 10 minutes data with

january 2012 : all values = 1
february 2012 : all values = 2
march 2012 : all values = 3
The monthly means I get using dfresample('M' how='mean') are :

Out[454]: 

0
2012-01-31  1000000
2012-02-29  1965757
2012-03-31  2967966
2012-04-30  3000000


but I would like to get something like:

0
2012-02-01  1000000
2012-03-01  2000000
2012-04-01  3000000


Here is the code:

january = pddate_range(pddatetime(2012 1 1) pddatetime(2012 1 31 23 50) freq='10min')
february = pddate_range(pddatetime(2012 2 1) pddatetime(2012 2 29 23 50) freq='10min')
march = pddate_range(pddatetime(2012 3 1) pddatetime(2012 3 31 23 50) freq='10min')
data_jan = npzeros(size(january))+1
data_feb = npzeros(size(february))+2
data_march = npzeros(size(march))+3
df1 = pdDataFrame(data_jan index=january)
df2 = pdDataFrame(data_feb index=february)
df3 = pdDataFrame(data_march index=march)
df = pdconcat([df1 df2 df3])
dfresample('M' how='mean')


If now  I remove the last day by :

january = pddate_range(pddatetime(2012 1 1) pddatetime(2012 1 31 00 00) freq='10min')
february = pddate_range(pddatetime(2012 2 1) pddatetime(2012 2 29 00 00) freq='10min')
march = pddate_range(pddatetime(2012 3 1) pddatetime(2012 3 31 00 00) freq='10min')


I get (nearly) what I want:

Out[474]: 
            0
2012-01-31  1
2012-02-29  2
2012-03-31  3


Could you help me ???? Is it a bug ???",1415325,,,,2012-06-14 19:08:55,Bug in resampling with pandas 0.8?,<python><pandas><resampling>,1.0,,,
11067027,1,11067072.0,2012-06-16 21:05:01,5,1323,"I have a dataframe with over 200 columns(don't ask why) The issue is as they were generated the order is
['Q13' 'Q61' 'Q12' 'Q11' ]

I need to re-order the columns as follows:
['Q11' 'Q12' 'Q13' 'Q61' ]

Is there someway for me to do this within python?

Thanks!",1419123,,,,2012-07-08 18:56:47,Python Pandas - Re-ordering columns in a dataframe based on column name,<python><pandas><reorder>,3.0,,3.0,
10767163,1,10781127.0,2012-05-26 14:41:22,1,347,"I am using pandas groupBy and was wondering how to implement the following:

Dataframes A and B have the same variable to index on  but A has 20 unique index values and B has 5 
I want to create a dataframe C that contains rows whose indices are present in A and not in B 
Assume that the 5 unique index values in B are all present in A C in this case would have only those rows associated with  index values in A and not in B (ie 15)
Using inner  outer  left  and right do not do this (unless I misread something)
In SQL I might do this as where Aindex  (not equal) Bindex

My Left handed solution:

a) get the respective index columns from each data set  say x and y

def match(x y compareCol):

""""""

x and y are series

compare col is the name to the series being returned 

It is the same name as the name of x and y in their respective dataframes""""""

x = xunique()

y = yunique()

"""""" Need to compare arrays xunique() returns arrays""""""

new = []

for item in (x):

    if item not in y:

        newappend(item)

returnADataFrame = paDataFrame(paSeries(new  name = compareCol))

return returnADataFrame


b) now do a left join on this on the data set A

I am reasonably confident that my elementwise comparison is slow as a tortoise on weed with no 
motivation",1419123,,1419123.0,2012-05-28 00:44:21,2012-05-28 07:51:50,Python Pandas GroupBy equivalent of  If A and not B  where clause in SQL,<python><sql><group-by><pandas>,1.0,,2.0,
10841538,1,10872241.0,2012-05-31 21:28:05,1,518,"I got stuck trying to get the resulting names of a pivot table The table printed using to_string() looks as below

I want to create a list with the names of the columns('a_Zero'  'b_Inst') I've been looking for a couple of days but I am still stuck trying to do that 

I am using pandas 07

                            sales             
        finprod              a_Zero       b_Inst
        year month                              
        2012 a_January   314623237  1344165922
              b_February  333532618  1349365653
              c_March     347457938  1443616732
              d_April     303391051  1424464116
              e_May       140377007   675714250
",1402211,,1301710.0,2012-06-01 07:57:27,2012-06-03 17:02:25,Pandas: List of Column names in a pivot table,<python><pivot><pandas>,1.0,,1.0,
10951341,1,,2012-06-08 15:01:32,5,1223,"Is there a way to write an aggregation function used in DataFrameagg method that would have access to more then one column of the data being aggregated? Typical use cases would be weighted average  weighted standard deviation funcs

I would like to be able to write something like

def wAvg(c  w):
    return ((c * w)sum() / wsum())

df = DataFrame() # df has columns c and w  i want weighted average
                     # of c using w as weight
dfagregate ({""c"": wAvg}) # and somehow tell it to use w column as weights 
",1444817,,1301710.0,2012-06-12 09:11:27,2012-06-12 09:11:27,Pandas DataFrame aggregate function using multiple columns,<python><pandas>,1.0,,5.0,
11105728,1,,2012-06-19 16:57:17,1,362,"This is a simple question for which answer are surprisingly difficult to find online Here's the situation:

>>> A
[('hey'  'you'  4)  ('hey'  'not you'  5)  ('not hey'  'you'  2)  ('not hey'  'not you'  6)]
>>> A_p = pandasDataFrame(A)
>>> A_p
         0        1  2
0      hey      you  4
1      hey  not you  5
2  not hey      you  2
3  not hey  not you  6
>>> B_p = A_ppivot(0 1 2)
>>> B_p
1        not you  you
0                    
hey            5    4
not hey        6    2


This isn't quite what's suggested in the documentation for pivot -- there  it shows results without the 1 and 0 in the upper-left-hand corner And that's what I'm looking for  a DataFrame object that prints as

         not you  you
hey            5    4
not hey        6    2


The problem is that the normal behavior results in a csv file whose first line is

0 not you you


when I really want

not you  you


When the normal csv file (with the preceding ""0 "") reads into R  it doesn't properly set the column and row names from the frame object  resulting in painful manual manipulation to get it in the right format Is there a way to get pivot to give me a DataFrame object without that additional information in the upper-left corner?",1467068,,321622.0,2012-06-19 17:35:58,2012-09-18 15:32:35,Pivoting a DataFrame in Pandas for output to CSV,<python><data.frame><pandas>,1.0,3.0,,
10816994,1,10818587.0,2012-05-30 13:10:38,1,493,"My data looks like this:

TEST
2012-05-01 00:00:00203 OFF 0
2012-05-01 00:00:11203 OFF 0
2012-05-01 00:00:22203 ON 1
2012-05-01 00:00:33203 ON 1
2012-05-01 00:00:44203 OFF 0
TEST
2012-05-02 00:00:00203 OFF 0
2012-05-02 00:00:11203 OFF 0
2012-05-02 00:00:22203 OFF 0
2012-05-02 00:00:33203 ON 1
2012-05-02 00:00:44203 ON 1
2012-05-02 00:00:55203 OFF 0


Ultimately  I want to be able to downsample data like this to individual days  using  mean  min  max -values  for example
I cannot get it to work for my data and get this error:

TypeError: unhashable type: 'list'


Perhaps it has something to do with the date format in the data frame since an index line looks like this:

[datetimedatetime(2012  5  1  0  0  0  203000)]   OFF  0


Can anyone help
My code so far is this:

import time
import dateutilparser
from pandas import *
from pandascoredatetools import *



t0 = timeclock()

filename = ""testdatadat""

index = []
data = []

with open(filename) as f:
    for line in f:
        if not linestartswith('TEST'):
            line_content =  linesplit(' ')

            mydatetime =  dateutilparserparse(line_content[0] +  "" "" + line_content[1])

            del line_content[0] # delete the date
            del line_content[0] # delete the time so that only values remain

            index_row = [mydatetime]
            data_row = []
            for item in line_content:
                data_rowappend(item)

            indexappend(index_row)
            dataappend(data_row)


df = DataFrame(data  index = index)
print dfhead()
print dftail()

print
date_from =  index[0] # first datetime entry in data frame
print date_from
date_to =  index[len(index)-1] #last datetime entry in date frame
print date_to

print date_to[0] - date_from[0]
dayly= DateRange(date_from[0]  date_to[0]  offset=datetoolsDateOffset())
print dayly

grouped = dfgroupby(daylyasof)
#print groupedmean()
#df2 = dfgroupby(dailyasof)agg({'2':np_mean})


time2 = timeclock() - t0
print time2
",1412286,,1412286.0,2012-05-30 13:29:55,2012-05-30 14:41:42,Python Pandas: Data Downsampling,<python><pandas><downsampling>,2.0,1.0,,
10982266,1,10982599.0,2012-06-11 14:38:45,0,347,"I am working on python (pandas specifically) to analyze a dataset (Python is too awesome  the power of open source is amazing) I am having trouble with a specific part of my dataset

I have the following data set 

time contract ticker expiry strike quote price volume
08:01:08 C PXA 20100101 4000 A 578 60
08:01:11 C PXA 20100101 4000 A 584 60
08:01:12 C PXA 20100101 4000 A 58 60
08:01:16 C PXA 20100101 4000 A 584 60
08:01:16 C PXA 20100101 4000 A 58 60
08:01:21 C PXA 20100101 4000 A 584 60
08:01:21 C PXA 20100101 4000 A 58 60


and it goes on 

I am using pandas to load the data After this  I would like to be able to do the following  take a volume weighted average of the time there are duplicates 

ie since there are two asks at time 08:01:16  I would like to calculate the average price based on volume which would be (584*60 + 58*60)/(60+60) and an average of the volume on the volume column which would be (60+60)/2",1449148,,1370085.0,2012-06-12 09:11:18,2012-06-12 09:11:18,How to aggregate duplicate timestamps with pandas?,<python><indexing><time-series><pandas>,1.0,2.0,,
11106823,1,11112419.0,2012-06-19 18:11:14,5,424,"I have two dataframes  both indexed by timeseries  I need to add the elements together to form a new dataframe  but only if the index and column are the same  If the item does not exist in one of the dataframes then it should be treated as a zero

I've tried using add but this sums regardless of index and column  Also tried a simple combined_data = dataframe1 + dataframe2 but this give a NaN if both dataframes don't have the element

Any suggestions?

Thanks",1223860,,849425.0,2012-06-19 20:39:25,2012-06-20 03:28:12,adding two pandas dataframes,<python><pandas>,2.0,1.0,,
11039985,1,,2012-06-14 19:25:11,0,30,"(The following happens in both 073 and 08-dev versions)

When converting a properly multi-indexed DataFrame to a Panel using the to_panels() method  resulting ""cells"" that did not exist in the DataFrame are now populated with random  giant numbers (sometimes negative) For example: if the DataFrame is indexed on width and length with values from 1 to 5 for both  but there is no 2 3 entry in the DataFrame  the resulting 2 3 major-axis/minor-axis ""cell"" of the Panel will have the huge random numbers in every Item of the Panel

Is this a bug? Or is there something we can do to prevent this?

Thanks",1342094,,,,2012-06-14 19:25:11,to_panel fills nonexistent values with (seemingly) huge random numbers,<python><pandas>,,,,
11129478,1,11138040.0,2012-06-20 23:11:41,0,117,"Newbie pandas user here I am trying to find out which edges from a graph are bidirectional Each row is an edge For each starting node A  I am searching each corresponding end node B if they have node A as an ending point: 

for ending_point_B in nodeA:
    nodeA in ending_points_of_B


Disregard for now repeated entries in df['S'] How can I optimize this search? I suspect something along the lines of groupby This way takes too much time for my real graph 

Thank you

from pandas import *

def missing_node(node):
    set1 = set(df[dfE == node]Svalues)
    set2 = set(dfE[dfS == node]values)
    return list(set1difference(set2))

x = [1 1 2 2 3]
y = [2 3 1 3 1]

df = DataFrame([x y])T
dfcolumns = ['S' 'E'] #Start & End

df['Missing'] = dfSapply(missing_node)

df:

   S  E Missing
0  1  2      []
1  1  3      []
2  2  1      []
3  2  3      []
4  3  1     [2]
",1470604,,,,2012-06-21 12:22:08,Directed graph in pandas,<python><pandas>,2.0,,,
11131647,1,,2012-06-21 04:48:00,1,125,"My problem is as follows: I want to create a time series of the valuation of a stock portfolio  by aggregating time series valuation data on the individual stock holdings of that portfolio The problem I have is that on certain dates there may not be a valuation for a given stock holding and thus aggregating on that date would produce erroneous results 

The solution I have come up with is to exclude dates for which valuation (actually price) data doesn't exist for a given holding and then aggregate on these dates where I have complete data The procedure I use is as follows:

# Get the individual holding valuation data
valuation = get_valuation(portfolio = portfolio  df = True)

# Then next few lines retrieve the dates for which I have complete price data for the
# assets that comprise this portflio
# First get a list of the assets that this portfolio contains (or has contained)
unique_assets = valuation['asset']unique()tolist()

# Then I get the price data for these assets
ats = get_ats(assets = unique_assets  df = True )[['data_date' 'close_price']]

# I mark those dates for which I have a 'close_price' for each asset:
ats = atsgroupby('data_date')['close_price']agg({'data_complete':lambda x: len(x) == len(unique_assets)} )reset_index()

# And extract the corresponding valid dates
valid_dates = ats['data_date'][ats['data_complete']]

# Filter the valuation data for those dates for which I have complete data:
valuation = valuation[valuation['data_date']apply(lambda x: x in valid_datesvalues)]

# Group by date  and sum the individual hodling valuations by date  to get the Portfolio valuation
portfolio_valuation = valuation[['data_date' 'valuation']]groupby('data_date')agg(lambda df: sum(df['valuation']))reset_index()


My question is two fold:

1) The above approach feels quite convoluted  and I am confident that Pandas has a better way of implementing my solution Any suggestions?

2) The approach I have used isn't ideal The best method is that for those dates for which we have no valuation data (for a given holding) we should use the most recent valuation for that holding So let's say I am calculating the valuation of the portfolio on the 21 June 2012 and have valuation data for GOOG on that date but for APPL only on the 20 June 2012 Then the valuation for the portfolio on 21 June 2012 should still be the sum of these two valuations Is there a efficient way to do this in Pandas? I want to avoid having to iterate through the data",939715,,1301710.0,2012-06-24 15:53:36,2012-09-27 07:39:03,Pandas - Aggregating grouped data provided certain criteria is met.,<python><pandas>,1.0,,1.0,
10908295,1,10916118.0,2012-06-06 04:46:19,3,1149,"I have data from multiple choice questions and it is formatted like so:

      Sex Qu1  Qu2  Qu3
Name
Bob    M   1    2    1
John   M   3    3    5
Alex   M   4    1    2
Jen    F   3    2    4
Mary   F   4    3    4


The data is a rating from 1 to 5 for the 3 multiple choice questions  I want rearrange the data so that the index is range(1 6) where 1='bad'  2='poor'  3='ok'  4='good'  5='excellent'  the columns are the same and the data is the count of the number occurrences of the values (excluding the Sex column)  This is basically a histogram of fixed bin sizes and the x-axis labeled with strings  I like the output of dfplot() much better than  dfhist() for this but I can't figure out how to rearrange the table to give me a histogram of data  Also  how do you change x-labels to be strings?",792849,,,,2012-06-06 14:19:52,How to convert pandas dataframe so that index is the unique set of values and data is the count of each value?,<python><pandas>,1.0,,1.0,
10943478,1,10943545.0,2012-06-08 05:24:57,6,1310,"Is it possible to reindex a pandas DataFrame using a column made up of datetime objects?

I have a DataFrame df with the following columns:

Int64Index: 19610 entries  0 to 19609
Data columns:
cntr                  19610  non-null values  #int
datflt                19610  non-null values  #float
dtstamp               19610  non-null values  #datetime object
DOYtimestamp          19610  non-null values  #float
dtypes: int64(1)  float64(2)  object(1)


I can reindex the df easily along DOYtimestamp with: dfreindex(index=dfdtstamp)
and DOYtimestamp has the following values:

>>> df['DOYtimestamp']values
    array([ 15376252315   15376253472   1537625463      15398945602 
    15398946759   15398947917])


but I'd like to reindex the DataFrame along dtstamp which is made up of datetime objects so that I generate different timestamps directly from the index The dtstamp column has values which look like:

 >>> df['dtstamp']values
     array([2012-06-02 18:18:02  2012-06-02 18:18:03  2012-06-02 18:18:04   
     2012-06-02 23:44:49  2012-06-02 23:44:50  2012-06-02 23:44:51]  
     dtype=object)


When I try and reindex df along dtstamp I get the following:

>>> dfreindex(index=dfdtstamp)
    TypeError: can't compare datetimedatetime to long


I'm just not sure what I need to do get the index to be of a datetime type Any thoughts?",983310,,,,2012-06-08 05:34:25,pandas reindex DataFrame with datetime objects,<python><data.frame><pandas><reindex>,1.0,,1.0,
11077023,1,11077215.0,2012-06-18 04:45:48,9,924,They both seem exceedingly similar and I'm curious as to which package would be more beneficial for financial data analysis ,1462733,,,,2012-06-18 19:32:13,What are the differences between Pandas and NumPy+SciPy in Python?,<python><numpy><scipy><pandas>,2.0,1.0,,
11174367,1,11194573.0,2012-06-24 01:32:34,1,610,"I have 30 csv data files from 30 replicate runs of an experiment I ran I am using pandas' read_csv() function to read the data into a list of DataFrames I would like to create a single DataFrame out of this list  containing the average of the 30 DataFrames for each column Is there a built-in way to accomplish this?

To clarify  I'll expand on the example in the answers below Say I have two DataFrames:

>>> x
          A         B         C
0 -0264438 -1026059 -0619500
1  0927272  0302904 -0032399
2 -0264273 -0386314 -0217601
3 -0871858 -0348382  1100491
>>> y
          A         B         C
0  1923135  0135355 -0285491
1 -0208940  0642432 -0764902
2  1477419 -1659804 -0431375
3 -1191664  0152576  0935773


What is the merging function I should use to make a 3D array of sorts with the DataFrame? eg 

>>> automagic_merge(x  y)
                      A                      B                      C
0 [-0264438   1923135] [-1026059   0135355] [-0619500  -0285491]
1 [ 0927272  -0208940] [ 0302904   0642432] [-0032399  -0764902]
2 [-0264273   1477419] [-0386314  -1659804] [-0217601  -0431375]
3 [-0871858  -1191664] [-0348382   0152576] [ 1100491   0935773]


so I can calculate average  sem  etc on those lists instead of the entire column",1383444,,1383444.0,2012-06-24 07:09:31,2012-06-25 17:49:49,Averaging data from multiple data files in Python with pandas,<python><statistics><pandas>,3.0,,,
11138859,1,,2012-06-21 13:07:26,0,388,"I have a python panel that is index by integer values 
in dict form it would look like this:

{1:{1:series 2: series 3:series 4:series} 2:{1:series 2:series 3:series 4:series}}


I would like to roll through my data by date and on each date take a time slice in the past apply a function to every time series so I get a result such as this where X is the output of the function of timeslice

  1 2 3 4 
1 X X X X
2 X X X X
3 X X X X
4 X X X X


I thought pandasPanelapply(func) would do this but it does not  I  only get a result in 1 seemingly random column   I can iterate with for loops but i was hoping there was a faster and easier way of doing this

I have a panel that looks like this:

 
 Dimensions: 1000 (items) x 3714 (major) x 1000 (minor)
 Items: 1 to 1000
 Major axis: 1997-09-10 00:00:00 to 2012-06-19 00:00:00  
 Minor axis: 1 to 1000
",1064197,,1064197.0,2012-06-25 21:34:03,2012-06-25 21:34:03,Pandas panel data,<python><pandas>,2.0,,,
13445973,1,13446322.0,2012-11-19 00:04:20,3,71,"I have a very large dataframe (~17MM rows x 6 columns) A simplified example of the relevant data is:

City        Borough

Brooklyn    Brooklyn
Astoria     Queens
Astoria     Unspecified
Ridgewood   Unspecified
Ridgewood   Queens


So I'm trying to fill the 'Unspecified' values based on the information from the City column So for example  the City Ridgewood is in an Unspecified Borough in one instance  but correctly has the Borough listed as Queens elsewhere in the dataset

I've already explored Panda's fillna  but it doesn't seem to meet my needs I've also considered the npwhere method  but I'm not sure how'd it work in this situation I'm pretty new to Pandas  but maybe the map/apply function are what I need? This can probably be accomplished a thousand different ways  but looking for something that won't crawl given the size of the data

EDIT: I was able to create a dictionary which contains the highest occurring ""pairs"" between cities and boroughs with the following code:

specified = data[['Borough' 'City']][data['Borough']!= 'Unspecified']
paired = specifiedBoroughgroupby(specifiedCity)max()
paired = pairedto_dict()


The paired dict has the city as the key and the borough as the value Now the last step is to apply/map it back to the borough columnhow do I do that?",1438637,,1438637.0,2012-11-19 01:12:19,2012-11-20 07:51:20,Conditionally replacing values based on values in another column,<python><pandas>,2.0,2.0,,
11170653,1,11170956.0,2012-06-23 15:22:48,5,535,"Similar to this R question  I'd like to apply a function to each item in a Series (or each row in a DataFrame) using Pandas  but want to use as an argument to this function the index or id of that row As a trivial example  suppose one wants to create a list of tuples of the form [(index_i  value_i)    (index_n  value_n)] Using a simple Python for loop  I can do:


In [1] L = []
In [2] s = Series(['six'  'seven'  'six'  'seven'  'six'] 
           index=['a'  'b'  'c'  'd'  'e'])
In [3] for i  item in enumerate(s):
           Lappend((i item))
In [4] L
Out[4] [(0  'six')  (1  'seven')  (2  'six')  (3  'seven')  (4  'six')]


But there must be a more efficient way to do this? Perhaps something more Panda-ish like Seriesapply? In reality  I'm not worried (in this case) about returning anything meaningful  but more for the efficiency of something like 'apply' Any ideas?

Cheers 

Carson",1256988,,,,2012-12-11 20:47:51,pandas row specific apply,<python><pandas>,2.0,,,
11197690,1,11214168.0,2012-06-25 21:34:19,1,68,"I am using the pandasols function from version 073 I am interested in doing a moving regression  such as:

model = pandasols(y = realizedData  x = pastData  intercept = 0  window_type=""rolling""  window = 80  min_periods = 80)


The inputs contain data for about 600 dates  of which 15 are NA values  But the output only contains regression results for about 120 dates  The issue is that whenever the window contains even one NA value  there is no output for that window  The problem disappears if I change window_type to expanding and I get about 500 output points as expected  but I don't want to do an expanding regression

Can you tell me how to fix this?

Thanks 

-- Preyas",983487,,696023.0,2012-06-26 01:58:27,2012-06-26 18:56:50,pandas MovingOLS doesn't support NA values?,<python><pandas>,1.0,,,
10982089,1,10982198.0,2012-06-11 14:28:49,1,205,"I would like to shift a column in a Pandas DataFrame  but I havent been able to find a method to do it from the documentation without rewriting the whole DF Does anyone know how to do it? Help much appriciated
DataFrame:

##    x1   x2
##0  206  214
##1  226  234
##2  245  253
##3  265  272
##4  283  291


Desired output:

##    x1   x2
##0  206  nan
##1  226  214
##2  245  234
##3  265  253
##4  283  271
##5  nan  291
",1199589,,,,2012-06-11 14:35:02,How to shift a column in Pandas DataFrame,<python><data.frame><pandas>,1.0,0.0,1.0,
11017862,1,11083005.0,2012-06-13 15:06:07,2,122,"I am using pyodbc to query data from SQL Server and then perform operations on it in pandas What I have noticed is that some calculated numeric columns are returned as decimalDecimal and others as float This is problematic as arithmetic operands can't performed on results of two different types

What is the best way to resolve it? Is there a pydobc setting to coerce decimalDecimal to float? Should I do the conversion myself? Something else?",1074399,,,,2012-06-18 12:34:23,Pyodbc default types for numbers,<python><sql-server><types><pyodbc><pandas>,2.0,,,
11073609,1,11073962.0,2012-06-17 18:07:39,3,198,"I have some data from log files and would like to group entries by a minute:


def gen(date  count=10):
    while count > 0:
        yield date  ""event{}""format(randint(1 9))  ""source{}""format(randint(1 3))
        count -= 1
        date += DateOffset(seconds=randint(40))

df = DataFramefrom_records(list(gen(datetime(2012 1 1 12  30)))  index='Time'  columns=['Time'  'Event'  'Source'])



df:


Event Source
2012-01-01 12:30:00    event3  source1
2012-01-01 12:30:12    event2  source2
2012-01-01 12:30:12    event2  source2
2012-01-01 12:30:29    event6  source1
2012-01-01 12:30:38    event1  source1
2012-01-01 12:31:05    event4  source2
2012-01-01 12:31:38    event4  source1
2012-01-01 12:31:44    event5  source1
2012-01-01 12:31:48    event5  source2
2012-01-01 12:32:23    event6  source1



I tried these options:

dfresample('Min') is too high level and wants to aggregate
dfgroupby(date_range(datetime(2012 1 1 12  30)  freq='Min' 
periods=4)) fails with exception
dfgroupby(TimeGrouper(freq='Min')) works fine and returns a DataFrameGroupBy object for further processing  eg:

grouped = dfgroupby(TimeGrouper(freq='Min'))
groupedSourcevalue_counts()
2012-01-01 12:30:00  source1    1
2012-01-01 12:31:00  source2    2
                     source1    2
2012-01-01 12:32:00  source2    2
                     source1    2
2012-01-01 12:33:00  source1    1



However  the TimeGrouper class is not documented

What is the correct way to group by a period of time? How can I group the data by a minute AND by the Source column  eg groupby([TimeGrouper(freq='Min')  dfSource])?",1461554,,1370085.0,2012-06-18 05:41:34,2012-06-18 05:41:34,How to group DataFrame by a period of time?,<python><pandas>,1.0,,1.0,
11232275,1,11246087.0,2012-06-27 17:58:38,1,154,"On Pandas documentation of the pivot method  we have:

    Examples
    --------
    >>> df
        foo   bar  baz
    0   one   A    1
    1   one   B    2
    2   one   C    3
    3   two   A    4
    4   two   B    5
    5   two   C    6

    >>> dfpivot('foo'  'bar'  'baz')
         A   B   C
    one  1   2   3
    two  4   5   6


My DataFrame is structured like this:

   name   id     x
----------------------
0  john   1      0
1  john   2      0
2  mike   1      1
3  mike   2      0


And I want something like this:

      1    2   # (this is the id as columns)
----------------------
mike  0    0   # (and this is the 'x' as values)
john  1    0


But when I run the pivot method  it is saying:

*** ReshapeError: Index contains duplicate entries  cannot reshape


Which doesn't makes sense  even in example there are repeated entries on the foo column I'm using the name column as the index of the pivot  the first argument of the pivot method call",179372,,,,2012-12-12 05:32:09,Pandas pivot warning about repeated entries on index,<python><pandas>,2.0,,,
11139757,1,,2012-06-21 13:54:52,1,165,"The code mentioned below seems to work
Calling 'tail' function on a sorted group is able to give me the last n rows of a group
Is this the documented behavior of the tail function for a groupThe pandas documentation does not mentioned it and I am worried that in the next version this behavior may change
Also is there some other way to do the followingUsing Apply function seems to be very slow for large dataset
The pandas version used in 073

df1=pdsDataFrame({'A' : ['CU' 'CU' 'CU' 'CU' 'CU' 'AU' 'AU' 'AU' 'AU' 'AU'] 'B':[1 2 3 4 5 1 2 3 4 5]})sort(['A'])reset_index()drop(['index'] axis=1)
df2=df1groupby(['A'])
df3=df2tail(2)groupby(['A'])
df3mean()
",1472282,,,,2012-06-21 14:03:44,using pandas how do I get the mean of last two rows in a group,<python><pandas>,1.0,0.0,1.0,
11199437,1,,2012-06-26 00:52:19,2,232,"I have a pandas dataframe that has columns: 

'video' and 'link' of click values 

with an index of datetime For some reason  when I use semilogy and boxplot with the video series  I get the error

ValueError: Data has no positive values  and therefore can not be log-scaled


but when I do it on the 'link' series I can draw the boxplot correctly

I have verified that both the 'video' and 'link' series has NaN values and positive values

Any thoughts on why this is occurring? Below is what I've done to verify that this is the case

Below is sample code:

#get all the not null values of video to show that there are positive
temp=atypes_pivot[atypes_pivot['video']notnull()]
print temp

#get a count of all the NaN values to show both 'video' and 'link' has NaN
count = 0 
for item in atypes_pivot['video']:
    if(itemis_integer() == False):
        count += 1

#try to draw the plots
print ""there is %s nan values in video"" % (count)

fig=pltfigure(figsize=(6 6) dpi=50)
ax=figadd_subplot(111)
axsemilogy()
pltboxplot(atypes_pivot['video']values)


Here is relevant output from the code for video series
    
    type                       link   video
    created_time
    2011-02-10 15:00:51+00:00   NaN   5
    2011-02-17 17:50:38+00:00   NaN   5
    2011-03-22 14:04:56+00:00   NaN   5

there is 5463 nan values in video
    
I run the same exact code except I do 

atypes_pivot['link'] 


and I am able to draw the boxplot

Below is the relevant output from the link series
    

Index: 5269 entries  2011-01-24 20:03:58+00:00 to 2012-06-22 16:56:30+00:00
Data columns:
link        5269  non-null values
photo       0  non-null values
question    0  non-null values
status      0  non-null values
swf         0  non-null values
video       0  non-null values
dtypes: float64(6)

there is 216 nan values in link
    

Using the describe function

atypes_pivot['video']describe()


count    22000000
mean     16227273
std      15275040
min       1000000
25%       5250000
50%       9500000
75%      23000000
max      58000000
",821428,,,,2012-10-31 01:44:35,issue with pandas and semilog for boxplot,<matplotlib><pandas>,1.0,2.0,,
11233502,1,12335205.0,2012-06-27 19:21:52,1,119,"I am using the pandasols function from version 073 I seem to be getting inconsistent values for adjusted $R^2$ when using the simple regression vs window regression  For instance  if realizedData and pastData have 600 entries then

model = pandasols(y = realizedData  x = pastData  intercept = 0  window = 600)


produces the following output:-

-------------------------Summary of Regression Analysis-------------------------

Formula: Y ~  +  + 

Number of Observations:         596
Number of Degrees of Freedom:   3

R-squared:         06914
Adj R-squared:     06904

Rmse:            6994880

F-stat (3  593):   6643691  p-value:     00000

Degrees of Freedom: model 2  resid 593

-----------------------Summary of Estimated Coefficients------------------------
      Variable       Coef    Std Err     t-stat    p-value    CI 25%   CI 975%
--------------------------------------------------------------------------------
             1     04171     00428       975     00000     03333     05010
            10     04362     00688       634     00000     03014     05709
         90000     00623     00319       195     00517    -00003     01249
---------------------------------End of Summary---------------------------------


while just using 

model = pandasols(y = realizedData  x = pastData  intercept = 0)


gives:-

-------------------------Summary of Regression Analysis-------------------------

Formula: Y ~  +  + 

Number of Observations:         596
Number of Degrees of Freedom:   3

R-squared:         06914
Adj R-squared:     03053

Rmse:            6994880

F-stat (3  593):     17909  p-value:     01477

Degrees of Freedom: model 2  resid 593

-----------------------Summary of Estimated Coefficients------------------------
      Variable       Coef    Std Err     t-stat    p-value    CI 25%   CI 975%
--------------------------------------------------------------------------------
             1     04171     00428       975     00000     03333     05010
            10     04362     00688       634     00000     03014     05709
         90000     00623     00319       195     00517    -00003     01249
---------------------------------End of Summary---------------------------------


Note that the output is identical except for the adjsuted $R^2$ value

Is this a bug or am I doing something wrong?

Thanks ",983487,,776560.0,2012-06-27 22:02:04,2012-09-08 23:09:44,Inconsistent R^2 adjusted values in pandas OLS,<python><pandas>,1.0,,,
11194610,1,12335136.0,2012-06-25 17:52:09,1,103,"I have a multi-indexed dataframe with names attached to the column levels  I'd like to be able to easily shuffle the columns around so that they match the order specified by the user  Since this is down the pipeline  I'm not able to use this recommended solution and order them properly at creation time

I have a data table that looks (something) like

Experiment           BASE           IWWGCW         IWWGDW
Lead Time                24     48      24     48      24     48
2010-11-27 12:00:00   0997  0991   0998  0990   0998  0990
2010-11-28 12:00:00   0998  0987   0997  0990   0997  0990
2010-11-29 12:00:00   0997  0992   0997  0992   0997  0992
2010-11-30 12:00:00   0997  0987   0997  0987   0997  0987
2010-12-01 12:00:00   0996  0986   0996  0986   0996  0986


I want to take in a list like ['IWWGCW'  'IWWGDW'  'BASE'] and reorder this to be:

Experiment           IWWGCW         IWWGDW         BASE           
Lead Time                24     48      24     48      24     48  
2010-11-27 12:00:00   0998  0990   0998  0990   0997  0991  
2010-11-28 12:00:00   0997  0990   0997  0990   0998  0987  
2010-11-29 12:00:00   0997  0992   0997  0992   0997  0992  
2010-11-30 12:00:00   0997  0987   0997  0987   0997  0987  
2010-12-01 12:00:00   0996  0986   0996  0986   0996  0986  


with the caveat that I don't always know at what level ""Experiment"" will be  I tried (where df is the multi-indexed frame shown above)

df2 = dfreindex_axis(['IWWGCW'  'IWWGDW'  'BASE']  axis=1  level='Experiment')


but that didn't seem to work - it completed successfully  but the DataFrame that was returned had its column order unchanged

My workaround is to have a function like:

def reorder_columns(frame  column_name  new_order):
    """"""Shuffle the specified columns of the frame to match new_order""""""

    index_level  = framecolumnsnamesindex(column_name)
    new_position = lambda t: new_orderindex(t[index_level])
    new_index    = sorted(framecolumns  key=new_position)
    new_frame    = framereindex_axis(new_index  axis=1)
    return new_frame


where reorder_columns(df  'Experiment'  ['IWWGCW'  'IWWGDW'  'BASE']) does what I expect but it feels like I'm doing extra work  Is there an easier way to do this?",24895,,,,2012-09-08 22:59:04,How can I reorder multi-indexed dataframe columns at a specific level,<python><pandas>,1.0,,,
11237557,1,11246539.0,2012-06-28 03:07:25,0,106,"I am attempting to execute the following within python:

from pandas import *
tickdata = read_csv('/home/user/sourcefilecsv' index_col=0 parse_dates='TRUE')


The csv files has rows that look like:

2011/11/23 23:56:00554389 11652500

2011/11/23 23:56:02310943 11655000

2011/11/23 23:56:05564009 11652500

On pandas 7  this executes fine  On pandas 80rc2  I get the error below  Because I have 7 and 8 installed on two different systems  I have not ruled out a dependency or python version difference  Any ideas on how to get this to execute under 8 are appreciated

Traceback (most recent call last):
  File """"  line 1  in 
  File ""/usr/local/lib/python27/dist-packages/pandas-080rc2-py27-linux-x86_64egg/pandas/io/parserspy""  line 225  in read_csv
return _read(TextParser  filepath_or_buffer  kwds)

File ""/usr/local/lib/python27/dist-packages/pandas-080rc2-py27-linux-x86_64egg/pandas/io/parserspy""  line 192  in _read
return parserget_chunk()
File ""/usr/local/lib/python27/dist-packages/pandas-080rc2-py27-linux-x86_64egg/pandas/io/parserspy""  line 728  in get_chunk
index = self_agg_index(index)
File ""/usr/local/lib/python27/dist-packages/pandas-080rc2-py27-linux-x86_64egg/pandas/io/parserspy""  line 846  in _agg_index
if try_parse_dates and self_should_parse_dates(selfindex_col):
File ""/usr/local/lib/python27/dist-packages/pandas-080rc2-py27-linux-x86_64egg/pandas/io/parserspy""  line 874  in _should_parse_dates
return i in to_parse or name in to_parse
TypeError: 'in ' requires string as left operand  not int
",1487345,,,,2012-06-28 14:11:36,"TypeError on read_csv, working in pandas .7, error in .8.0rc2, possible dependency error?",<pandas>,2.0,,,
11193212,1,,2012-06-25 16:22:35,3,266,"Is there any way to prevent pandas from changing the default print format for numpy arrays?

With plain numpy  I get:

>>> numpyarray([12345  006])
array([  123450000e+02    600000000e-02])


After I import pandas  I get:

>>> numpyarray([12345  006])
array([ 12345     006])


Can I stop it from doing this as a configuration setting? I don't want to have to wrap every ""import pandas"" with a ""foo=npget_printoptions(); import pandas; npset_printoptions(**foo)""  but that's the best I can come up with

As it is  if I import pandas in one place  I get doctest errors from another",250839,,,,2012-09-08 23:02:53,Pandas changes default numpy array format,<python><numpy><pandas>,2.0,,,
11264307,1,11518797.0,2012-06-29 15:09:28,1,231,"Let's assume I have a DataFrame df with a MultiIndex and it has the level L

Is there a way to remove L from the index and add it again?

df = dfindexdrop('L') removes L completely from the DataFrame ( unlike df= dfreset_index() which has a drop argument)
I could of course do df = dfreset_index()set_index(everything_but_L  inplace=True)

Now  let us assume the index contains everything but L  and I want to add L
dfindexinsert(0  dfL) doesn't work
Again  I could of course call df= dfreset_index()set_index(everything_including_L  inplace=True) but it doesn't feel right

Why do I need this?  Since indices need not be unique  it can occur that I want to add a new column so the index becomes unique Dropping may be useful in situations where after splitting data one level of the index does not contain any information anymore (say my index is A B and I operate on a df with A=x but I do not want to lose A which would occur with indexdroplevel('A'))",942591,,,,2012-07-17 08:34:29,"Adding levels to MultiIndex, removing without losing",<pandas>,1.0,,1.0,
11295147,1,11297642.0,2012-07-02 13:55:35,2,161,"I am trying to run a Winsorized regression in Pandas for Python  The very helpful user manual offers this example code:

winz = retscopy()
std_1year = rolling_std(rets  250  min_periods=20)
cap_level = 3 * npsign(winz) * std_1year
winz[npabs(winz) > 3 * std_1year] = cap_level
winz_model = ols(y=winz['AAPL']  x=winzix[:  ['GOOG']] window=250)


The fourth line looks wrong to me: shouldn't the RHS be cap_level[npabs(winz) > 3 * std_1year]?

Thanks for the help!  I'm still new to using the Pandas dataframe and want to make sure I'm understanding right",1476254,,567292.0,2012-07-02 19:51:58,2012-07-02 19:56:42,Winsorize data in Pandas for Python,<python><regression><pandas>,1.0,,,
11306167,1,11306265.0,2012-07-03 07:16:53,1,198,"times = [datetime(2011  01  03  0  10 + i  0) for i in range(5)]
series = pdTimeSeries(range(5)  index=times)
seriesix[datetime(2011  01  03  0  10  0):datetime(2011  01  03  0  13  0)]

2011-01-03 00:10:00    0
2011-01-03 00:11:00    1
2011-01-03 00:12:00    2
2011-01-03 00:13:00    3


but

x = range(5)
x[0:3]
[0  1  2]


Also 

times = [datetimedatetime(2000  1  1) + datetimetimedelta(minutes=i) for i in range(1000000 - 1)]
df = pdTimeSeries(range(1000000 - 1)  times)
len(dfix[datetimedatetime(1900 1 1):datetimedatetime(2100 1 1)])

999999


But

times = [datetimedatetime(2000  1  1) + datetimetimedelta(minutes=i) for i in range(1000000)]
df = pdTimeSeries(range(1000000)  times)
len(dfix[datetimedatetime(1900 1 1):datetimedatetime(2100 1 1)])


Traceback (most recent call last):

  File ""C:\Program Files (x86)\Wing IDE 41\src\debug\tserver\_sandboxpy""  line 1  in 
    # Used internally for debug sandbox under external interpreter
  File ""C:\dev\Python26\Lib\site-packages\pandas\core\indexingpy""  line 35  in __getitem__
    return self_getitem_axis(key  axis=0)
  File ""C:\dev\Python26\Lib\site-packages\pandas\core\indexingpy""  line 234  in _getitem_axis
    return self_get_slice_axis(key  axis=axis)
  File ""C:\dev\Python26\Lib\site-packages\pandas\core\indexingpy""  line 460  in _get_slice_axis
    i  j = labelsslice_locs(start  stop)
  File ""C:\dev\Python26\Lib\site-packages\pandas\tseries\indexpy""  line 949  in slice_locs
    return Indexslice_locs(self  start  end)
  File ""C:\dev\Python26\Lib\site-packages\pandas\core\indexpy""  line 1057  in slice_locs
    end_slice = selfget_loc(end) + 1
  File ""C:\dev\Python26\Lib\site-packages\pandas\tseries\indexpy""  line 919  in get_loc
    return self_engineget_loc(key)
  File ""C:\dev\Python26\Lib\site-packages\pandas\libpyd""  line 378  in pandaslibDatetimeEngineget_loc (pandas\src\tseriesc:108808)
  File ""C:\dev\Python26\Lib\site-packages\pandas\libpyd""  line 390  in pandaslibDatetimeEngineget_loc (pandas\src\tseriesc:108422)
  File ""C:\dev\Python26\Lib\site-packages\pandas\libpyd""  line 32  in utilget_value_at (pandas\src\tseriesc:112091)
IndexError: index out of bounds
",1497828,,781150.0,2012-07-03 07:22:40,2012-07-03 07:23:21,Pandas slicing on a timeseries seems inconsistent with list slicing,<python><pandas><slicing>,1.0,3.0,,
11230071,1,,2012-06-27 15:41:10,2,255,"I'm reading in timeseries data that contains only the available times  This leads to a timeseries with no missing values  but an unequally spaced index  I'd like to convert this to a timeseries with an equally spaced index with missing values  Since I don't know a priori what the spacing will be  I'm currently using a function like

min_dt      = npdiff(seriesindexvalues)min()
new_spacing = pandasDateOffset(days=min_dtdays  seconds=min_dtseconds 
                                microseconds=min_dtmicroseconds)
series      = seriesasfreq(new_spacing)


to compute what the spacing should be (note that this is using Pandas 073 - the 08 beta code looks slightly differently since I have to use seriesindexto_pydatetime() for correct behavior with Numpy 16)

Is there an easier way to do this operation using the pandas library?",24895,,,,2012-12-12 22:37:15,Resampling pandas timeseries without computing a new offset,<python><pandas>,1.0,,,
11289670,1,,2012-07-02 07:52:09,0,119,"I recently installed Numpy with ease using the exe installer for Python 27 However  when I attempted to install IPython  Pandas or Matplotlib using the exe file  I consistently get a variant of the following error right after the installation commeces (pandas in the following case):

pandas-080win32-py27[1]exe has stopped working

A problem caused the program to stop working correctly Windows will close the program and notify you if a solution is available

NumPy just worked fine when I installed it This is extremely frustrating and I would appreciate any insight 

Thanks",1462733,,,,2012-07-23 16:02:57,".EXE installer crashes when installing Python modules: IPython, Pandas and Matplotlib",<python><numpy><matplotlib><ipython><pandas>,2.0,1.0,,
11346283,1,11346337.0,2012-07-05 14:21:15,7,2422,"Apologies if this is pretty simple  but I couldn't seem to find this anywhere

I currently have a datatable using pandas and column labels that I need to edit and then replace the original column labels So for instance I'd like to change the column names in a data table A 
where 
original column names = ['$a' '$b'  '$c'  '$d'  '$e'] 
to 
edited column names = ['a' 'b' 'c' d' 'e']

I currently have edited the column names and stored it in a list  but I'm not sure how to complete the final step to replace the column names 

Thanks! ",1504276,,,,2012-11-28 12:25:20,Renaming columns in pandas,<python><replace><rename><pandas>,2.0,,,
11350488,1,11354319.0,2012-07-05 18:38:37,1,330,"I am very new to PythonHere is a copy of what one of many txt files looks like 

Class 1:
Subject A:
posX posY posZ  x(%)  y(%)
  0   2    0    81    72
  0   2   180   63    38
 -1  -2    0    79    84
 -1  -2   180   85    95
                  
Subject B:
posX posY posZ  x(%)   y(%)
  0   2     0    71     73
 -1  -2     0    69     88   
                    
Subject C:
posX  posY posZ x(%)   y(%)
  0    2    0    86     71
 -1   -2    0    81     55
                    
Class 2:
Subject A:
posX posY posZ  x(%)  y(%)
  0   2    0    81    72
 -1  -2    0    79    84
                  


The number of classes  subjects  row entries all vary
Class1-Subject A always has posZ entries that have 0 alternating with 180
Calculate average of x(%)  y(%) by class and by subject
Calculate standard deviation of x(%)  y(%) by class and by subject
Also ignore the posZ of 180 row when calculating averages and std_deviations
I have developed an unwieldly solution in excel (using macro's and VBA) but I would rather go for a more optimal solution in python 

numpy is very helpful but the mean()  std() functions only work with arrays- I am still researching some more into it as well as the panda's groupby function

I would like the final output to look as follows (1 By Class  2 By Subject)

 1 By Class                 
             X     Y                      
 Average                        
 std_dev     

 2 By Subject  
             X     Y
 Average
 std_dev                   
",1504774,,1504774.0,2012-07-05 22:20:08,2012-07-06 00:21:14,"Efficient way to calculate averages, standard deviations from a txt file",<numpy><python-2.7><python><pandas>,1.0,2.0,,
11235604,1,,2012-06-27 22:21:04,0,44,"I got the following message while running nosetests pandas:

C:\Python27\lib\site-packages\pandas\core\algorithmspy:125: RuntimeWarning: tp_compare didn't return -1 or -2 for exception
  sorter = uniquesargsort()


Is this a serious issue that I need to fix?",1487028,,1487028.0,2012-06-27 22:23:00,2012-06-28 13:44:35,nosetests: algorithms.py:125: RuntimeWarning: tp_compare didn't return -1 or -2 for exception,<pandas><nosetests>,1.0,,,
11285613,1,11287278.0,2012-07-01 21:03:16,6,453,"I have data in different columns but I don't know how to extract it to save it in another variable

index  a   b   c
1      2   3   4
2      3   4   5


How do I select b  c and save it in to df1?

I tried 

df1 = df['a':'b']
df1 = dfix[:  'a':'b']


None seem to work Any ideas would help thanks",1610626,,,,2012-10-31 20:06:18,Selecting columns,<pandas>,3.0,,,
11337437,1,,2012-07-05 03:10:24,1,140,"I'm querying an underlying PostgreSQL database using Pandas 08  Pandas is returning the DataFrame properly but the underlying timestamp column in my database is being returned as a generic ""object"" type in Pandas  As I would eventually like to do seasonal normalization of my data and I am curious as to how to convert this generic ""object"" column to something that is appropriate for analysis

Here is my current code to retrieve the data:

# get timestamp with time zone Pandas example
import pandasiosql as psql
import psycopg2

# define query
QRY = """"""
    select 
        i i  
        i * random() f 
        case when random() > 05 
        then 
            true 
        else 
            false 
        end b  
        (current_date - (i*random())::int)::timestamp with time zone tsz 
    from 
        generate_series(1 1000) as s(i)
    order by
        4
    ;
""""""
CONN_STRING = ""host='localhost' port=5432 dbname='postgres' user='postgres'""

# connect to db
conn = psycopg2connect(CONN_STRING)

# get some data set index on relid column
df = psqlframe_query(QRY  con=conn)

print ""Row count retrieved: %i"" % (len(df) )


Result in Python:


Int64Index: 1000 entries  0 to 999
Data columns:
i      1000  non-null values
f      1000  non-null values
b      1000  non-null values
tsz    1000  non-null values
dtypes: bool(1)  float64(1)  int64(1)  object(1)


Interesting to note that the first column  ""i""  is an Integer col in PG  I'm not sure why Pandas thinks this is a ""bool"" type column  My real issue though is the ""object"" column which I think I need to be of some type of timestamp

Thanks for any help you can render

M",655832,,655832.0,2012-07-05 03:15:38,2012-07-08 16:37:49,Convert object to DateRange,<python><numpy><pandas>,1.0,1.0,,
11360675,1,,2012-07-06 10:43:35,1,1184,"I'm using pandas to analyse financial records

I have a DataFrame that comes from a csv file that looks like this:


DatetimeIndex: 800 entries  2010-10-27 00:00:00 to 2011-07-12 00:00:00
Data columns:
debit                      800  non-null values
transaction_type           799  non-null values
transaction_date_raw       800  non-null values
credit                     800  non-null values
transaction_description    800  non-null values
account_number             800  non-null values
sort_code                  800  non-null values
balance                    800  non-null values
dtypes: float64(3)  int64(1)  object(4)


I am selecting a subset based on transaction amount:

c1 = df['credit']map(lambda x: x > 1000)
milestones = df[c1]sort()


and want to create slices of the original df based on the dates between the milestones:

delta = dttimedelta(days=1)
for i in range(len(milestonesindex)-1):
        start = milestonesindex[i]date()
        end = milestonesindex[i+1]date() - delta
        rng = date_range(start  end)


this generates a new series with the dates between my milestones 


[2010-11-29 00:00:00    2010-12-30 00:00:00]
Length: 32  Freq: D  Timezone: None


I have followed several approaches to slice my df using these new series (rng) but have failed:

dfix[start:end] or
dfix[rng]


this raises: IndexError: invalid slice

dfreindex(rng) or dfreindex(index=rng)


raises: Exception: Reindexing only valid with uniquely valued Index objects

x = [v for v in rng if v in dfindex]
df[x]
dfix[x]
dfindex[x]


this also raises invalid slice  and so does:

dftruncate(start  end)


I'm new to pandas  I'm following the early release of the book from Oreilly  and really enjoying it Any pointers would be appreciated",240068,,,,2012-07-10 15:28:07,slicing pandas dataframe on date range,<python><time-series><pandas>,2.0,,,
11136006,1,11182373.0,2012-06-21 10:15:21,2,833,"My data looks like so: 

TEST
2012-05-01 00:00:00203 OFF 0
2012-05-01 00:00:11203 OFF 0
2012-05-01 00:00:22203 ON 1
2012-05-01 00:00:33203 ON 1
2012-05-01 00:00:44203 OFF 0
TEST
2012-05-02 00:00:00203 OFF 0
2012-05-02 00:00:11203 OFF 0
2012-05-02 00:00:22203 OFF 0
2012-05-02 00:00:33203 ON 1
2012-05-02 00:00:44203 ON 1
2012-05-02 00:00:55203 OFF 0


I'm using pandas read_table to read a pre-parsed string (which gets rid of the ""TEST"" lines) like so:

df = pandasread_table(buf  sep=' '  header=None  parse_dates=[[0  1]]  date_parser=dateParser  index_col=[0])


So far  i've tried several date parsers  the uncommented one being the fastest

def dateParser(s):
#return datetimestrptime(s  ""%Y-%m-%d %H:%M:%S%f"")
return datetime(int(s[0:4])  int(s[5:7])  int(s[8:10])  int(s[11:13])  int(s[14:16])  int(s[17:19])  int(s[20:23])*1000)
#return npdatetime64(s)
#return pandasTimestamp(s  ""%Y-%m-%d %H:%M:%S%f""  tz='utc' )


Is there anything else I can do to speed things up? I need to read large amounts of data - several Gb per file",1412286,,,,2012-06-25 00:49:26,Python Pandas: What is the fastest way to create a datetime index?,<python><performance><parsing><datetime><pandas>,1.0,2.0,1.0,
11314693,1,11330247.0,2012-07-03 15:53:33,1,567,"I am using pymssql and the Pandas sql package to load data from SQL into a Pandas dataframe with frame_query  

I would like to send it back to the SQL database using write_frame  but I haven't been able to find much documentation on this  In particular  there is a parameter flavor='sqlite'  Does this mean that so far Pandas can only export to SQLite?  My firm is using MS SQL Server 2008 so I need to export to that",1476254,,,,2013-02-02 09:59:16,Can I export a Python Pandas dataframe to MS SQL?,<python><sql><sql-server-2008><pandas>,3.0,,2.0,
11334098,1,11335177.0,2012-07-04 18:43:10,1,1014,"Open     High    Low    Close   Volume      Adj Close
Date                        
1990-01-02 00:00:00  3525   3750   3500   3725   6555600     870
1990-01-03 00:00:00  3800   3800   3750   3750   7444400     876
1990-01-04 00:00:00  3825   3875   3725   3763   7928800     879
1990-01-05 00:00:00  3775   3825   3700   3775   4406400     882
1990-01-08 00:00:00  3750   3800   3700   3800   3643200     888


How can I get rid of the Date index name in the above dataframe? It should be in the same row as the other column names but its not which is causing problems

Thanks",1610626,,1301710.0,2012-09-24 19:29:00,2012-09-24 19:29:00,How to take out the column index name in dataframe,<python><pandas>,2.0,,0.0,
11361985,1,11362056.0,2012-07-06 12:12:49,4,1992,"I have a csv file with the name paramscsv I opened up ipython qtconsole and created a pandas dataframe using:

import pandas
paramdata = pandasread_csv('paramscsv'  names=paramnames)


where  paramnames is a python list of string objects Example of paramnames (the length of actual list is 22):

paramnames = [""id"" 
""fc"" 
""mc"" 
""markup"" 
""asplevel"" 
""aspreview"" 
""reviewpd""]


At the ipython prompt if I type paramdata and press enter then I do not get the dataframe with columns and values as shown in examples on Pandas website Instead  I get information about the dataframe I get:

In[35]: paramdata
Out[35]: 

Int64Index: 59 entries  0 to 58
Data columns:
id                    59  non-null values
fc                    59  non-null values
mc                    59  non-null values
markup                59  non-null values
asplevel              59  non-null values
aspreview             59  non-null values
reviewpd              59  non-null values


If I type paramdata['mc'] then I do get the values as expected for the mc column I have two questions:

(1) In the examples on the pandas website (see  for example  the output of df here: http://pandassourceforgenet/indexinghtml#additional-column-access) typing the name of the dataframe gives the actual data Why am I getting information about the dataframe as shown above instead of the actual data? Do I need to set some output options somewhere?

(2) How do I output all columns in the dataframe to the screen without having to type their names  ie  without having to type something like paramdata[['id' 'fc' 'mc']] 

I am using pandas version 08 

Thank you",316357,,,,2012-11-05 18:13:42,Output data from all columns in a dataframe in pandas,<python><numpy><pandas>,4.0,,1.0,
11157450,1,,2012-06-22 13:47:00,1,212,"I am using pandas and ipython notebook inline I drew a figure with pandas dataframe

figure()
subplot = df['likes']hist()
subplotset_title(""Likes"")
display(fig)
draw()


I am trying to add a title to the histogram for example and I'd like to redraw it  but ipython notebook does not redraw 

Does anyone have a suggestion? In ipython when I use draw() it redraws and adds a title I want to format my pandas plots

Thanks!",821428,,,,2012-07-12 08:55:11,How do you force a figure redraw ipython notebook inline?,<ipython><pandas>,1.0,,0.0,
11253390,1,,2012-06-28 21:47:30,1,166,"I am currently rolling up numbers with the following code  For each element in the dataframe  I am setting a few conditions for which to sum by  but it is the slowest part of a report that has been created  Is there a faster way to identify all the elements in the dataframe that start with a certain string?

for idx  eachRecord in attributionCalcDFTiteritems():        
   if (attributionCalcDF['SEC_ID']ix[idx] == 0):

       currentGroup = lambda x:  str(x)startswith(attributionCalcDF['GROUP_LIST']ix[idx])
       currentGroupArray = attributionCalcDF['GROUP_LIST']map(currentGroup)

       attributionCalcDF['ROLLUP_DAILY_TIMING_IMPACT']ix[idx] = (
                                                         attributionCalcDF['DAILY_TIMING_IMPACT'][(attributionCalcDF['SEC_ID'] != 0) & 
                                                        (currentGroupArray) & 
                                                        (attributionCalcDF['START_DATE'] == attributionCalcDF['START_DATE']ix[idx])]sum())

       attributionCalcDF['ROLLUP_DAILY_STOCK_TO_GROUP_IMPACT']ix[idx] = (
                                                         attributionCalcDF['DAILY_STOCK_TO_GROUP_IMPACT'][(attributionCalcDF['SEC_ID'] != 0) & 
                                                        (currentGroupArray) & 
                                                        (attributionCalcDF['START_DATE'] == attributionCalcDF['START_DATE']ix[idx])]sum())
",1452305,,,,2012-06-29 14:10:32,Find all elements in dataframe column that startswith string,<python><pandas>,1.0,,,
11265116,1,11265413.0,2012-06-29 15:58:11,1,213,"I have a DataFrame with columns like this:

[""A_1""  ""A_2""  ""A_3""  ""B_1""  ""B_2""  ""B_3""]


What I'd like to to do is to ""collapse"" the various A and B columns in a single column each  by calculating their mean value In short  at the end of the operation I'd get:

[""A""  ""B""]


where ""A"" is the column-wise mean of all ""A"" columns and ""B"" the mean of all ""B"" columns

As far as I understood  groupby is not suited for this task  or perhaps I'm using it incorrectly:

grouped = datagroupby([item for item in data if ""A"" not in item])


If I use axis=1  all I get is an empty DataFrame when calling mean()  and if not I'm not getting the desired effect I would like to avoid building a separate DataFrame to be fillled with the means via iteration (eg by calculating means separately then adding them like new_df[""A""] = mean_a) Is there an efficient solution for this?",241515,,,,2012-07-02 13:07:24,Most efficient way to calculate the mean of a group of columns in a pandas DataFrame,<python><pandas>,2.0,,,
11391969,1,11397052.0,2012-07-09 09:04:33,2,508,"A Pandas DataFrame contains column named ""date"" that contains non-unique datetime values 
I can group the lines in this frame using:

datagroupby(data['date'])


However  this splits the data by the datetime values I would like to group these data by the year stored in the ""date"" column This page shows how to group by year in cases where the time stamp is used as an index  which is not true in my case

How do I achieve this grouping?",17523,,,,2012-12-02 09:31:27,How to group pandas DataFrame entries by date in a non-unique column,<python><pandas>,2.0,,,
10934323,1,,2012-06-07 14:55:58,2,602,"My code reads CSV file into pandas DataFrame - and processes it
The code relies on column names - uses dfix[ ] to get the columns
Recently some column names in the CSV file were changed (without notice)
But the code was not complaining and was silently producing wrong results
The ix[ ] construct doesn't check if column exists
If it doesn't - it simply creates it and populate with NaN
Here is the main idea of what was going on

df1=DataFrame({'a':[1 2 3] 'b':[4 5 6]})   # columns 'a' & 'b'
df2=df1ix[: ['a' 'c']]                    # trying to get 'a' & 'c'
print df2
       a   c
    0  1 NaN
    1  2 NaN
    2  3 NaN


So it doesn't produce an error or a warning

Is there an alternative way to select specific columns with extra check that columns exist?

My current workaround is to use my own small utility function  something like this:

import sys  inspect

def validate_cols_or_exit(df cols):
  """"""
    Exits with error message if pandas DataFrame object df 
    doesn't have all columns from the provided list of columns
    Example of usage:
      validate_cols_or_exit(mydf ['col1' 'col2'])
  """"""
  dfcols = list(dfcolumns)
  valid_flag = True
  for c in cols:
    if c not in dfcols:
       print ""Error  non-existent DataFrame column found - "" c
       valid_flag = False
  if not valid_flag:
    print ""Error  non-existent DataFrame column(s) found in function ""  inspectstack()[1][3]
    print ""valid column names are:""
    print ""\n""join(dfcolumns)
    sysexit(1)
",1442475,,1442475.0,2012-06-07 18:28:44,2012-06-12 21:20:16,DataFrame.ix() in pandas - is there an option to catch situations when requested columns do not exist?,<data.frame><pandas>,2.0,,,
10975690,1,10975782.0,2012-06-11 07:02:14,0,396,"Date Description 
0  6/09/2012      Amazon
1  6/09/2012      iTunes
2  6/08/2012      iTunes
3  6/08/2012    Building
4  6/08/2012   Slicehost


I have a DataFrame like the above I can pick out the day part of the above datestring using a function get_day() like this: 

def get_day(date_string):
    d = datetimestrptime(date_string  '%m/%d/%Y')
    return dday


Now how do I pass this function to the above DataFrame to get groupby going on the day rather than the datestring itself Couldn't figure it out from looking at the docs Any help appreciated ",1039553,,,,2012-06-11 08:42:29,How to do groupby in pandas with part of date string?,<python><group-by><pandas>,1.0,0.0,,
11037895,1,11038086.0,2012-06-14 16:58:42,5,373,"I'm using the excellent pandas package to deal with a large amount of varied meteorological diagnostic data and I'm quickly running out of dimensions as I stitch the data together  Looking at the documentation  it may be that using the MultiIndex may solve my problem  but I'm not sure how to apply it to my situation - the documentation shows examples of creating MultiIndexes with random data and DataFrames  but not Series with pre-existing timeseries data

Background

The basic data structure I'm using contains two main fields:

metadata  which is a dictionary consisting of key-value pairs describing what the numbers are
data  which is a pandas data structure containing the numbers themselves
The lowest common denominator is timeseries data  so the basic structure has a pandas Series object as the data entry  and the metadata field describes what those numbers actually are  (eg vector RMS error for 10-meter wind over the Eastern Pacific for a 24-hour forecast from experiment Test1)

I'm looking at taking that lowest-common-denominator and gluing the various timeseries together to make the results more useful and allow for easy combinations  For instance  I may want to look at all the different lead times - I have a filter routine that will take my timeseries that share the same metadata entries except for lead time (eg experiment  region  etc) and return a new object where the metadata field consists of only the common entries (ie Lead Time has been removed) and now the data field is a pandas DataFrame with the column labels given by the Lead Time value  I can extend this again and say I want to take the resulting frames and group them together with only another entry varying (eg the Experiment) to give me a pandas Panel for my entry where the item index is given by the Experiment metadata values from the constituent frames and the object's new metadata does not contain either Lead Time or Experiment

When I iterate over these composite objects  I have an iterseries routine for the frame and iterframes routine for the panel that reconstruct the appropriate metadata/data pairing as I drop one dimension (ie the series from the frame with lead time varying across the columns will have all the metadata of its parent plus the Lead Time field restored with the value taken from the column label)  This works great

Problem

I've run out of dimensions (up to 3-D with a Panel) and I'm also not able to use things like dropna to remove empty columns once everything is aligned in the Panel (this has led to several bugs when plotting summary statistics)  Reading about using pandas with higher-dimensional data has led to reading about the MultiIndex and its use  I've tried the examples given in the documentation  but I'm still a little unclear how to apply it to my situation  Any direction would be useful  I'd like to be able to:

Combine my Series-based data into a multi-indexed DataFrame along an arbitrary number of dimensions (this would be great - it would eliminate one call to create the frames from the series  and then another to create the panels from the frames)
Iterate over the resulting multi-indexed DataFrame  dropping a single dimension so I can reset the component metadata
Edit - Add Code Sample

Wes McKinney's answer below is almost exactly what I need - the issue is in the initial translation from the Series-backed storage objects I have to work with to my DataFrame-backed objects once I start grouping elements together  The Data-Frame-backed class has the following method that takes in a list of the series-based objects and the metadata field that will vary across the columns 

@classmethod
def from_list(cls  results_list  column_key):
    """"""
    Populate object from a list of results that all share the metadata except
    for the field `column_key`

    """"""
    # Need two copies of the input results - one for building the object
    # data and one for building the object metadata
    for_data  for_metadata = itertoolstee(results_list)

    self             = cls()
    selfcolumn_key  = column_key
    selfmetadata    = next(for_metadata)metadatacopy()
    if column_key in selfmetadata:
        del selfmetadata[column_key]
    selfdata = pandasDataFrame(dict(((transform(r[column_key])  rdata)
                                        for r in for_data)))
    return self


Once I have the frame given by this routine  I can easily apply the various operations suggested below - of particular utility is being able to use the names field when I
call concat - this eliminates the need to store the name of the column key internally
since it's stored in the MultiIndex as the name of that index dimension

I'd like to be able to implement the solution below and just take in the list of matching Series-backed classes and a list of keys and do the grouping sequentially  However  I don't know what the columns will be representing ahead of time  so:

it really doesn't make sense to me to store the Series data in a 1-D DataFrame
I don't see how to set the name of the index and the columns from the the initial Series -> Frame grouping
",24895,,24895.0,2012-06-14 20:41:29,2012-06-14 20:41:29,How can I generalize my pandas data grouping to more than 3 dimensions?,<python><pandas>,1.0,,2.0,
11040626,1,,2012-06-14 20:09:38,3,704,"how do I add 'd' to the index below without having to reset it first?

from pandas import DataFrame
df = DataFrame( {'a': range(6)  'b': range(6)  'c': range(6)} )
dfset_index(['a' 'b']  inplace=True)
df['d'] = range(6)

# how do I set index to 'a b d' without having to reset it first?
dfreset_index(['a' 'b' 'd']  inplace=True)
dfset_index(['a' 'b' 'd']  inplace=True)

df
",1441053,,,,2012-08-08 02:50:06,Pandas DataFrame Add column to index without resetting,<python><data.frame><pandas>,2.0,,,
11175213,1,,2012-06-24 05:33:45,3,418,"I am using pandas-08rc2 to read an input CSV with two columns of localized
datetime strings lacking UTC offset information  and need the dataframe series
properly converted to UTC

I've been trying workarounds to mitigate the fact that neither timestamp columns
represent the index  they are data tz_localize and tz_convert apparently work
on the index of a series/dataframe only  not a column I would very much like to
learn a better approach to do that  rather than the following code:

# testpy
import pandas

# inputcsv:
# starting ending measure
# 2012-06-21 00:00 2012-06-23 07:00 77
# 2012-06-23 07:00 2012-06-23 16:30 65
# 2012-06-23 16:30 2012-06-25 08:00 77
# 2012-06-25 08:00 2012-06-26 12:00 0
# 2012-06-26 12:00 2012-06-27 08:00 77

df = pandasread_csv('inputcsv'  parse_dates=[0 1])
print df

ser_starting = dfstarting
ser_startingindex = ser_startingvalues
ser_starting = ser_startingtz_localize('US/Eastern')
ser_starting = ser_startingtz_convert('UTC')

ser_ending = dfending
ser_endingindex = ser_endingvalues
ser_ending = ser_endingtz_localize('US/Eastern')
ser_ending = ser_endingtz_convert('UTC')

dfstarting = ser_startingindex
print df
dfending = ser_endingindex
print df


Second  the code is encountering some odd behavior It changes the timestamp
data of the second assignment back to the data frame  whether the order is
dfstarting or dfending:

$ python testpy 
              starting               ending  measure
0  2012-06-21 00:00:00  2012-06-23 07:00:00       77
1  2012-06-23 07:00:00  2012-06-23 16:30:00       65
2  2012-06-23 16:30:00  2012-06-25 08:00:00       77
3  2012-06-25 08:00:00  2012-06-26 12:00:00        0
4  2012-06-26 12:00:00  2012-06-27 08:00:00       77
             starting               ending  measure
0 2012-06-21 04:00:00  2012-06-23 07:00:00       77
1 2012-06-23 11:00:00  2012-06-23 16:30:00       65
2 2012-06-23 20:30:00  2012-06-25 08:00:00       77
3 2012-06-25 12:00:00  2012-06-26 12:00:00        0
4 2012-06-26 16:00:00  2012-06-27 08:00:00       77
Traceback (most recent call last):
  File ""testpy""  line 28  in 
    print df
  File ""/path/to/lib/python27/site-packages/pandas/core/framepy""  line 572  in __repr__
    if self_need_info_repr_():
  File ""/path/to/lib/python27/site-packages/pandas/core/framepy""  line 560  in _need_info_repr_
    selfto_string(buf=buf)
  File ""/path/to/lib/python27/site-packages/pandas/core/framepy""  line 1207  in to_string
    formatterto_string(force_unicode=force_unicode)
  File ""/path/to/lib/python27/site-packages/pandas/core/formatpy""  line 200  in to_string
    fmt_values = self_format_col(i)
  File ""/path/to/lib/python27/site-packages/pandas/core/formatpy""  line 242  in _format_col
    space=selfcol_space)
  File ""/path/to/lib/python27/site-packages/pandas/core/formatpy""  line 462  in format_array
    return fmt_objget_result()
  File ""/path/to/lib/python27/site-packages/pandas/core/formatpy""  line 589  in get_result
    fmt_values = [formatter(x) for x in selfvalues]
  File ""/path/to/lib/python27/site-packages/pandas/core/formatpy""  line 597  in _format_datetime64
    base = stampstrftime('%Y-%m-%d %H:%M:%S')
ValueError: year=1768 is before 1900; the datetime strftime() methods require year >= 1900


The print statements are just to demonstrate the problem The incorrect values
will carry through without exception if I avoid repr and other methods that call
strftime

The strange part is that if I keep calling the df{starting ending} assignments
at the repl  I usually end up with a correct dataframe  with timestamps:

In [151]: df
Out[151]: 
             starting              ending  measure
0 2012-06-21 04:00:00 2012-06-23 11:00:00  77
1 2012-06-23 11:00:00 2012-06-23 20:30:00  65
2 2012-06-23 20:30:00 2012-06-25 12:00:00  77
3 2012-06-25 12:00:00 2012-06-26 16:00:00   0
4 2012-06-26 16:00:00 2012-06-27 12:00:00  77


This is not repeatable  AFAICT  I can't describe the exact sequence of calls
that gets past the ValueError above  but it does 

I would appreciate any thoughts about whether if I'm up against a bug 
or if this is unsupported API usage

And as mentioned above  I'd rather just learn a better usage of the pandas API
to avoid doing it this way

Thanks",91567,,1301710.0,2012-06-24 19:31:29,2012-06-25 22:15:31,"pandas read_csv() input local datetime strings, tz_convert to UTC",<python><timezone><pandas>,1.0,,,
11418192,1,11475486.0,2012-07-10 16:56:37,1,705,"I would like to filter rows by a function of each row  eg

def f(row):
  return sin(row['velocity'])/npprod(['masses']) > 5

df = pandasDataFrame()
filtered = df[apply_to_all_rows(df  f)]


Or for another more complex  contrived example 

def g(row):
  if row['col1']method1() == 1:
    val = row['col1']method2() / row['col1']method3(row['col3']  row['col4'])
  else:
    val = row['col2']method5(row['col6'])
  return npsin(val)

df = pandasDataFrame()
filtered = df[apply_to_all_rows(df  g)]


How can I do so?",128580,,128580.0,2012-07-10 21:12:13,2012-07-13 17:33:12,pandas: complex filter on rows of DataFrame,<pandas>,2.0,,1.0,
11398688,1,11400509.0,2012-07-09 15:51:48,0,242,"I have multiple files containing dates and measured values Their setup is identical:

YYYY  MM  DD  val1
YYYY  MM  DD  val2
YYYY  MM  DD  val3


I use the following to read each of these files into a DataFrame

for cur_file in file_list:
    cur_df = paioparsersread_table(ospathjoin(data_path  result)
                                                  header=None
                                                  sep='\s*'
                                                  parse_dates=[[0 1  2]]
                                                  names=['day' 'month'  'hour'  cur_file[:-4]]
                                                  index_col=[0]
                                                )


The dates are not identical in all files There is sometimes some overlap  but not always

I could plot each of the cur_df individually via 

cur_dfplot()


in the loop

It seems like it would be a good idea to have all the cur_df in one ""big"" DataFrame Both for plotting and also for statistics later on How would this be done ideally  considering they have not the same dates? Is there a way to ""merge"" multiple DataFrames  but what is done at dates that occur only in one of the underlying DataFrames?

I guess I am looking for a data frame that looks like this:

YYYY MM DD  val1(from1)  NaN
YYYY MM DD  val2(from1)  val2(from2)
YYYY MM DD  NaN          val3(from2)


It would take the date stamp in the first line from the date of val1  in line two the dates of val1 and val2 are identical  and it would take the date in line 3 based on val2

I looked into
    cur_dfadd(cur_df2)
appends the two DataFrames I am not sure what
    cur_dfcombine(cur_df2  )
would do  especially since I am not sure what function should be used as second argument

Thanks for your help 
Cheers 
Claus",1510463,,,,2012-07-09 18:01:30,merging DataFrames with pandas,<io><pandas>,1.0,,,
11423690,1,11435635.0,2012-07-10 23:55:38,1,395,"I have a dataframe of species survey counts and need to aggregate the rows by multiple criteria The main problem is that I need to match seasonal samples across different years For example a spring 2005 sample will be matched to an autumn 2006 sample where the site sample method and replicates match Here is a simple example of the data:

# create the factors and dataframe
a = repeat('AAA' 4)
b = repeat('BBB' 2)
y1 = nparray([2005  2006])
y2 = nparray([2005  2007])
r = nparray([1  1  2  2  1   1])
d = {'site' : hstack((a b a b a b a b)) 
     'year' : hstack((y1  y1  y1  y2  y2  y2  y1  y1  y1  y2  y2  y2)) 
     'season' : hstack((repeat('AUTUMN'  6)  repeat('SPRING'  6)  repeat('AUTUMN'  6)  repeat('SPRING'  6))) 
     'method' : hstack((repeat('EDGE'  12)  repeat('RIFFLE'  12))) 
     'replicate' : hstack((r  r  r  r))}
df = DataFrame(d)

# now add some species
df['sp1'] = 1
df['sp2'] = 2
df['sp3'] = 3


Each row in the dataframe is a singe sample Currently I am creating a new merged 'id' column  iterating through each of the 'SPRING' samples searching for a matching autumn sample and updating this 'id' for both samples before grouping on the 'id' for example:

df['id'] = 'na' # new column for combined season id
grouped = dfgroupby('season') # split table by season 

for name  group in grouped:
    if name == 'AUTUMN':
        aut = group #autumn lookup list
    if name == 'SPRING':
        # for each spring sample
        for row_index  row in groupiterrows():
            # check for matching autumn sample
            n = aut[
                (aut['site'] == row['site']) &
                (aut['year'] == row['year'] + 1) &
                (aut['method'] == row['method']) &
                (aut['replicate'] == row['replicate'])]index
            if n:
                # create new combined season id
                new_id = row['site'] + \
                         str(row['year'])[-2:] + \
                         str(row['year'] + 1)[-2:] + \
                         row['method'][:1] + \
                         str(row['replicate'])
                # update id spring sample with matching autumn 
                dfidix[row_index] = new_id
                # get matching autumn table index
                dfidix[n] = new_id
df = df[df['id'] != 'na']
combined = dfgroupby(['method'  'id'  'site'])sum()
combined = combineddrop(['year'  'replicate']  axis=1)


This methods works quite well but I think it is a bit clunky and is not at all versatile Is there a vectorised way to aggregate data in this way? Sorry for the length of the post and please let me know if anything is unclear

thanks in advance",1498485,,,,2012-07-11 20:24:13,better method to aggregate pandas dataframe by non matching criteria,<python><aggregation><pandas>,1.0,,1.0,
11321243,1,11321950.0,2012-07-04 01:09:56,1,288,"I know that I can apply numpy methods by doing the following:

dataList is a list of DataFrames (same cols/rows)

testDF = (concat(dataList  axis=1  keys=range(len(dataList)))
        swaplevel(0  1  axis=1)
        sortlevel(axis=1)
        groupby(level=0  axis=1))

testDFaggregate(numpymean)
testDFaggregate(numpyvar)


and so on However  what if I want to compute the standard error of the mean (sem)?

I tried:

testDFaggregate(scipystatssem)


but it gave a confusing error Anyone know how to do this? What are the scipystats methods doing differently?

Here's some code that reproduces the error for me:

from scipy import stats as st
import pandas
import numpy as np
df_list = []
for ii in range(30):
    df_listappend(pandasDataFrame(nprandomrand(600  10)  
    columns = ['A'  'B'  'C'  'D'  'E'  'F'  'G'  'H'  'I'  'J']))

testDF = (pandasconcat(df_list  axis=1  keys=range(len(df_list)))
         swaplevel(0  1  axis=1)
         sortlevel(axis=1)
         groupby(level=0  axis=1))

testDFaggregate(stsem)


Here's the error message:

---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
 in ()
     12          groupby(level=0  axis=1))
     13 
---> 14 testDFaggregate(stsem)

/Library/Frameworks/EPD64framework/Versions/73/lib/python27/site-packages/pandas/core/groupbypy in aggregate(self  arg  *args  **kwargs)
   1177                 return self_python_agg_general(arg  *args  **kwargs)
   1178             else:
-> 1179                 result = self_aggregate_generic(arg  *args  **kwargs)
   1180 
   1181         if not selfas_index:

/Library/Frameworks/EPD64framework/Versions/73/lib/python27/site-packages/pandas/core/groupbypy in _aggregate_generic(self  func  *args  **kwargs)
   1248             else:
   1249                 result = DataFrame(result  index=objindex 
-> 1250                                    columns=result_index)
   1251         else:
   1252             result = DataFrame(result)

/Library/Frameworks/EPD64framework/Versions/73/lib/python27/site-packages/pandas/core/framepy in __init__(self  data  index  columns  dtype  copy)
    300             mgr = self_init_mgr(data  index  columns  dtype=dtype  copy=copy)
    301         elif isinstance(data  dict):
--> 302             mgr = self_init_dict(data  index  columns  dtype=dtype)
    303         elif isinstance(data  maMaskedArray):
    304             mask = magetmaskarray(data)

/Library/Frameworks/EPD64framework/Versions/73/lib/python27/site-packages/pandas/core/framepy in _init_dict(self  data  index  columns  dtype)
    389 
    390         # consolidate for now
--> 391         mgr = BlockManager(blocks  axes)
    392         return mgrconsolidate()
    393 

/Library/Frameworks/EPD64framework/Versions/73/lib/python27/site-packages/pandas/core/internalspy in __init__(self  blocks  axes  do_integrity_check)
    329 
    330         if do_integrity_check:
--> 331             self_verify_integrity()
    332 
    333     def __nonzero__(self):

/Library/Frameworks/EPD64framework/Versions/73/lib/python27/site-packages/pandas/core/internalspy in _verify_integrity(self)
    404         mgr_shape = selfshape
    405         for block in selfblocks:
--> 406             assert(blockvaluesshape[1:] == mgr_shape[1:])
    407         tot_items = sum(len(xitems) for x in selfblocks)
    408         assert(len(selfitems) == tot_items)

AssertionError:
",1383444,,1383444.0,2012-07-05 16:02:58,2012-07-05 18:40:59,grouped pandas DataFrames: how do I apply scipy.stats.sem to them?,<python><numpy><statistics><scipy><pandas>,1.0,4.0,,
11435668,1,11436825.0,2012-07-11 15:06:33,0,119,"Given a DataFrame that contains multiple columns (possible regressors)  how can I generate all possible combinations of columns to test them into different regressions? I'm trying to select the best regression model from all the possible combination of regressors

For example  I have this DataFrame:

            A   B
1/1/2011    1   4
1/2/2011    2   5
1/3/2011    3   6


and I want to generate the following ones:

            A   B
1/1/2011    1   4
1/2/2011    2   5
1/3/2011    3   6

            A
1/1/2011    1
1/2/2011    2
1/3/2011    3

            B
1/1/2011    4
1/2/2011    5
1/3/2011    6
",1518204,,,,2012-07-12 08:27:39,All possible combinations of columns of a DataFrame - pandas / python,<python><data.frame><combinations><pandas>,2.0,,,
11362943,1,,2012-07-06 13:16:37,0,330,"I am wondering if there is a better way to test if two variables are cointegrated than the following method:

import numpy as np
import statsmodelsapi as sm
import statsmodelstsastattools as ts

y = nprandomnormal(0 1  250)
x = nprandomnormal(0 1  250)

def cointegration_test(y  x):
    # Step 1: regress on variable on the other 
    ols_result = smOLS(y  x)fit() 
    # Step 2: obtain the residual (ols_resuldresid)
    # Step 3: apply Augmented Dickey-Fuller test to see whether 
    #        the residual is unit root    
    return tsadfuller(ols_resultresid)


The above method works; however  it is not very efficient When I run smOLS  a lot of things are calculated  not just the residuals  this of course increases the run time I could of course write my own code that calculates just the residuals  but I don't think this will be very efficient either 

I looking for either a build in test that just tests for cointegration directly I was thinking Pandas  but don't seem to be able to find anything Or maybe there is a clever to test for cointegration without running a regression  or some efficient method

I have to run a lot of cointegration tests  and it would nice to improve on my current method

Thank You in Advance",1078084,,953348.0,2012-07-06 13:39:38,2012-08-08 01:57:03,Efficient Cointegration Test in Python,<python><pandas><linear-regression>,1.0,,1.0,
11415701,1,11415882.0,2012-07-10 14:36:11,1,576,"I've inherited a data file saved in the Stata dta format I can load it in with scikitsstatsmodels genfromdta() function This puts my data into a 1-dimensional NumPy array  where each entry is a row of data  stored in a 24-tuple

In [2]: st_time = timetime(); initialload = smiolibgenfromdta(""/home/myfiledta""); ed_time = timetime(); print (ed_time - st_time)
666523324013

In [3]: type(initialload)
Out[3]: numpyndarray

In [4]: initialloadshape
Out[4]: (4809584 )

In [5]: initialload[0]
Out[5]: (199011300  2890  19900  120  199012310  180  403010000  'GB'  182420  -2368063  10  17783716290878204  4379355  6617669677734375  -9990  -9990  -060000002  -9990  -9990  -9990  -9990  -9990  02  3710)


I am curious if there's an efficient way to arrange this into a Pandas DataFrame From what I've read  building up a DataFrame row-by-row seems quite inefficient but what are my options?

I've written a pretty slow first-pass that just reads each tuple as a single-row DataFrame and appends it Just wondering if anything else is known to be better",567620,,567620.0,2012-07-10 14:43:31,2012-07-10 14:44:48,Efficiently construct Pandas DataFrame from large list of tuples/rows,<python><tuples><pandas><dta>,1.0,2.0,,
11495051,1,11495086.0,2012-07-15 19:44:50,11,458,"I'm not sure why I'm getting slightly different results for a simple OLS  depending on whether I go through panda's experimental rpy interface to do the regression in R or whether I use statsmodels in Python

import pandas
from rpy2robjects import r

from functools import partial

loadcsv = partial(pandasDataFramefrom_csv 
                  index_col=""seqn""  parse_dates=False)

demoq = loadcsv(""csv/DEMOcsv"")
rxq = loadcsv(""csv/quest/RXQ_RXcsv"")

num_rx = {}
for seqn  num in rxqrxd295iteritems():
    try:
        val = int(num)
    except ValueError:
        val = 0
    num_rx[seqn] = val

series = pandasSeries(num_rx  name=""num_rx"")
demoq = demoqjoin(series)

import pandasrpycommon as com
df = comconvert_to_r_dataframe(demoq)
rassign(""demoq""  df)
r('lmout |t|)    
(Intercept)    -01358216  00241399  -5626 189e-08 ***
demoq$ridageyr  00358161  00006232  57469  |t|    [950% Conf Int]
ridageyr     00331  0000   82787    0000        0032 0034


The install process is a a bit cumbersome But  there is an ipython notebook here  that can reproduce the inconsistency",177293,,1301710.0,2012-08-09 19:12:30,2012-08-09 19:12:30,Difference in Python statsmodels OLS and R's lm,<python><r><pandas><rpy2><statsmodels>,1.0,,3.0,
11423790,1,11434755.0,2012-07-11 00:09:49,1,115,"I have a Python function historical_data that pulls daily historical price and dividend data from Yahoo Finance and outputs it into a pandas DataFrame

>>> nlsn = yhistorical_data('NLSN')
>>> nlsn

DatetimeIndex: 366 entries  2012-07-10 00:00:00 to 2011-01-27 00:00:00
Data columns:
Open         366  non-null values
High         366  non-null values
Low          366  non-null values
Close        366  non-null values
Volume       366  non-null values
Adj Close    366  non-null values
Dividends    366  non-null values
dtypes: float64(6)  int64(1)
>>> nlsn['Adj Close']
Date
2012-07-10    2677
2012-07-09    2677
2012-07-06    2664
2012-07-05    2656
2012-07-03    2657

2011-02-01    2575
2011-01-31    2607
2011-01-28    2500
2011-01-27    2540
Name: Adj Close  Length: 366


I only want to store daily data persistently (vs having to store daily  monthly  weekly  etc) The following daily-to-monthly conversion doesn't seem to work  though:

>>> nlsn['Adj Close']asfreq('M'  method='bfill')
Traceback (most recent call last):
  File """"  line 1  in 
  File ""/home/michael/Projects/envs/fintools32/lib/python32/site-packages/pandas/core/genericpy""  line 156  in asfreq
    return asfreq(self  freq  method=method  how=how)
  File ""/home/michael/Projects/envs/fintools32/lib/python32/site-packages/pandas/tseries/resamplepy""  line 329  in asfreq
    return objreindex(dti  method=method)
  File ""/home/michael/Projects/envs/fintools32/lib/python32/site-packages/pandas/core/seriespy""  line 2053  in reindex
    level=level  limit=limit)
  File ""/home/michael/Projects/envs/fintools32/lib/python32/site-packages/pandas/core/indexpy""  line 791  in reindex
    limit=limit)
  File ""/home/michael/Projects/envs/fintools32/lib/python32/site-packages/pandas/core/indexpy""  line 719  in get_indexer
    assert(selfis_monotonic)
AssertionError


What's the right way for me to aggregate these stock prices into Monthly?

What I've tried

I tried all different method arguments (ffill  pad  bfill)  all of which seem to raise the same assertion error

I tried checking the source code indexpy  but there seems to be a Strategy pattern in effect where the class in question delegates is_monotonic to its _engine attribute  and I can't find where the _engine attribute is actually assigned",290443,,290443.0,2012-07-11 03:09:44,2012-11-26 10:52:12,Why am I getting an is_monotonic assertion error using pandas Series.asfreq,<python><pandas>,1.0,0.0,,
11511880,1,11512106.0,2012-07-16 20:17:12,4,338,"I am having trouble converting a pandas DataFrame in Python to an R object  for future use in R using rpy2

The new pandas release 080 (released a few weeks ago) has a function to convert pandas DataFrames to R DataFrames The problem is in converting the first column of my pandas DataFrame  which consists of python datetime objects (successively  in a time series) The conversion into an R dataframe returns an StrVector of the dates and times  rather than a vector of R datetime-type objects which I believe are called ""POSIXct"" objects

I know the command to convert a string of the type returned to a POSIXct  using the command ""asPOSIXct('yyyy-mm-dd hh:mm:ss')"" Unfortunately I have not been able to figure out the way to convert all these strings in the StrVector to POSIXct using python and rpy2 The dates need to be in the POSIXct format to be used with the TTR library in R Below is the relevant python code:

import pandas
from pandas import *
import pandasrpycommon as com
import rpy2robjects as robjects
r = robjectsr
rlibrary('TTR')        #library contains the function ADX  to be used later

dataframe = read_csv('file_name'  parse_dates = [0]  names  = ['Date' 'Col1' 'Col2' 'Col3']     #command makes 1st column into datetimedatetime object
r_dataframe = comconvert_to_r_dataframe(dataframe)

ADX = r['ADX']          #creating a name for an R function in python
adx = ADX(r_dataframe)    #will not work because the dates in r_dataframe are in a StrVector


Further I do not believe that the StrVector can be iterated through to convert each object to a POSIXct object individually  due to the definition of a StrVector Maybe there is a way to cast a StrVector to a generic one?

Any help/insight into this matter is greatly appreciated I am a novice programmer and have been working on this for a couple hours now to no avail

Thank you!",1529898,,,,2012-09-08 23:14:40,issue converting python pandas DataFrame to R dataframe for use with rpy2,<python><r><data.frame><pandas><rpy2>,2.0,0.0,,
11348183,1,11384667.0,2012-07-05 15:59:54,1,786,"I have a pandas DataFrame and I want to plot a bar chart that includes a legend

import pylab as pl
from pandas import *

x = DataFrame({""Alpha"": Series({1: 1  2: 3  3:25})  ""Beta"": Series({1: 2  2: 2  3:35})})


If I call plot directly  then it puts the legend above the plot:

xplot(kind=""bar"")


If I turn of the legend in the plot and try to add it later  then it doesn't retain the colors associated with the two columns in the DataFrame (see below):

xplot(kind=""bar""  legend=False)
l = pllegend(('Alpha' 'Beta')  loc='best')


What's the right way to include a legend in a matplotlib plot from a Pandas DataFrame?
",163053,,163053.0,2012-07-05 17:52:37,2012-07-08 16:16:39,Pandas bar plot with specific colors and legend location?,<python><legend><pandas>,1.0,3.0,1.0,
11350770,1,,2012-07-05 18:57:34,3,619,"I have a DataFrame with 4 columns of which 2 contain string values I was wondering if there was a way to select rows based on a partial string match against a particular column?

In other words  a function or lambda function that would do something like 

research(pattern  cell_in_question) 


returning a boolean I am familiar with the syntax of df[df['A'] == ""hello world""] but can't seem to find a way to do the same with a partial string match say 'hello'

Would someone be able to point me in the right direction?",1170342,,1301710.0,2012-07-05 23:52:25,2012-09-30 17:52:13,pandas + dataframe - select by partial string,<python><pandas>,2.0,,2.0,
11459106,1,11461644.0,2012-07-12 19:21:46,2,568,"I need to store Python decimal type values in a pandas TimeSeries/DataFrame object Pandas gives me an error when using the ""groupby"" and ""mean"" on the TimeSeries/DataFrame The following code based on floats works well:

[0]: by = lambda x: lambda y: getattr(y  x)

[1]: rng = date_range('1/1/2000'  periods=40  freq='4h')

[2]: rnd = nprandomrandn(len(rng))

[3]: ts = TimeSeries(rnd  index=rng)

[4]: tsgroupby([by('year')  by('month')  by('day')])mean()
2000  1  1    0512422
         2    0447235
         3    0290151
         4   -0227240
         5    0078815
         6    0396150
         7   -0507316


But i get an error if do the same using decimal values instead of floats:

[5]: rnd = [Decimal(x) for x in rnd]       

[6]: ts = TimeSeries(rnd  index=rng  dtype=Decimal)

[7]: tsgroupby([by('year')  by('month')  by('day')])mean()  #Crash!

Traceback (most recent call last):
File ""C:\Users\TM\Documents\Python\tmpy""  line 100  in 
print tsgroupby([by('year')  by('month')  by('day')])mean()
File ""C:\Python27\lib\site-packages\pandas\core\groupbypy""  line 293  in mean
return self_cython_agg_general('mean')
File ""C:\Python27\lib\site-packages\pandas\core\groupbypy""  line 365  in _cython_agg_general
raise GroupByError('No numeric types to aggregate')
pandascoregroupbyGroupByError: No numeric types to aggregate


The error message is ""GroupByError('No numeric types to aggregate')"" Is there any chance to use the standard aggregations like sum  mean  and quantileon on the TimeSeries or DataFrame containing Decimal values? 

Why doens't it work and is there a chance to have an equally fast alternative if it is not possible?

EDIT: I just realized that most of the other functions (min  max  median  etc) work fine but not the mean function that i desperately need :-(",1521724,,416224.0,2012-07-14 04:22:03,2012-07-14 04:22:03,How use the mean method on a pandas TimeSeries with Decimal type values?,<python><decimal><data.frame><pandas>,1.0,1.0,,
11538574,1,,2012-07-18 09:44:49,1,114,"I have a datafile in which i would like to do a sum by column

The data looks like :

Date;Heure;U12_min;U12_max;U12_moy
13/07/12-15:10:02;581072;41719;42058;41892
13/07/12-15:20:01;581088;41610;42108;41897
13/07/12-15:30:01;581105;41810;42048;41931
13/07/12-15:40:01;581122;41483;41988;41798

14/07/12-00:00:01;581955;42013;42377;42210
14/07/12-00:10:01;581972;41502;42117;41851
14/07/12-00:20:01;581988;41477;42349;42135
14/07/12-00:30:00;582005;42092;42517;42264
14/07/12-00:40:00;582022;41766;42653;42032 
14/07/12-00:50:01;582038;41836;42267;42069
14/07/12-01:00:01;582055;42056;42334;42214


Here is my code :

from pandas import *

fich = 'D://ENERGIE//test2csv'
df = read_csv(fich  delimiter="";""  index_col=""Date"")
dfsum[""Heure""]
print df


and i get the following message 

dfsum[""Heure""]
TypeError: 'instancemethod' object is not subscriptable
",1481983,,843822.0,2012-07-18 10:19:00,2012-07-18 10:19:00,Howto sum by col,<python><pandas>,3.0,,,
11553370,1,11563542.0,2012-07-19 03:06:07,0,219,"Following up on a previous question  is there a preferred efficient manner to get the type of each object within a column? This is specifically for the case where the dtype of the column is object to allow for heterogeneous types among the elements of the column (in particular  allowing for numeric NaN without changing the data type of the other elements to float)

I haven't done time benchmarking  but I am skeptical of the following immediately obvious way that comes to mind (and variants that might use map or filter) The use cases of interest need to quickly get info on the types of all elements  so generators and the like probably won't be an efficiency boon here 

# df is a pandas DataFrame with some column 'A'  such that
# df['A']dtype is 'object'

dfrm['A']apply(type) #Or npdtype  but this will fail for native types


Another thought was to use the NumPy vectorize function  but is this really going to be more efficient? For example  with the same setup as above  I could try:

import numpy as np
vtype = npvectorize(lambda x: type(x)) # Gives error without lambda

vtype(dfrm['A'])


Both ideas lead to workable output  but it's the efficiency I'm worried about

Added

I went ahead and did a tiny benchmark in IPython First is for vtype above  then for the apply route I repeated it a dozen or so times  and this example run is pretty typical on my machine

The apply() approach clearly wins  so is there a good reason to expect that I won't get more efficient than with apply()?

For vtype()

In [49]: for ii in [100 1000 10000 100000 1000000 10000000]:
   :     dfrm = pandasDataFrame({'A':nprandomrand(ii)})
   :     dfrm['A'] = dfrm['A']astype(object)
   :     dfrm['A'][0:-1:2] = None
   :     st_time = timetime()
   :     tmp = vtype(dfrm['A'])
   :     ed_time = timetime()
   :     print ""%s:\t\t %s""%(ii  ed_time-st_time)
   :     
100:         00351531505585
1000:        0000324010848999
10000:       000209212303162
100000:      00224051475525
1000000:     0211136102676
10000000:    22215731144


For apply()

In [50]: for ii in [100 1000 10000 100000 1000000 10000000]:
   :     dfrm = pandasDataFrame({'A':nprandomrand(ii)})
   :     dfrm['A'] = dfrm['A']astype(object)
   :     dfrm['A'][0:-1:2] = None
   :     st_time = timetime()
   :     tmp = dfrm['A']apply(type)
   :     ed_time = timetime()
   :     print ""%s:\t %s""%(ii  ed_time-st_time)
   :     
100:         0000900983810425
1000:        0000159025192261
10000:       000117015838623
100000:      00111050605774
1000000:     0103563070297
10000000:    103093600273
",567620,,567620.0,2012-07-19 03:41:14,2012-07-19 14:55:45,Efficient method for getting elementwise types in Python/Pandas,<python><performance><types><pandas>,1.0,5.0,,
11563435,1,,2012-07-19 14:49:41,0,70,"has set_index changed dramatically in the latest pandas release (08)? I'm having trouble getting it to work as expected:

My original attempt tried to set index on 'id'

ipdb> merged2['id']
16    130809
25    130687
32    130686
9      41736
22    131913
7     130691
33    129993
13    130680
28    134295
29    130708

ipdb> merged2set_index('id')
*** KeyError: 0

ipdb> [type(i) for i in merged2['id']]
[]


The current index is int:
    ipdb> merged2index
    Int64Index([16  25  32   9  22   7  33  13  28  29])

ipdb> [type(i) for i in merged2index]
[]


A workaround tried to construct a new index:

ndx=range(len(merged2))
[type(i) for i in ndx]
[]


ipdb> merged2set_index(ndx)
*** KeyError: 'no item named 0'


Finally  mapping my index as int works:

merged2['id']=map(lambda x: int(x)  merged2['id']
merged2set_index('id')


Thoughts on what I'm doing wrong?",1037869,,,,2012-07-19 16:32:03,Float vs Int behavior in df.set_index(),<python><pandas>,1.0,2.0,,
11400181,1,,2012-07-09 17:36:31,1,257,"I'm having a little trouble with pivoting in pandas The dataframe (dates  location  data) I'm working on looks like:

dates    location    data
date1       A         X
date2       A         Y
date3       A         Z
date1       B         XX
date2       B         YY


Basically  I'm trying to pivot on location to end up with a dataframe like:

dates   A    B    C
date1   X    XX   etc
date2   Y    YY
date3   Z    ZZ 


Unfortunately when I pivot  the index  which is equivalent to the original dates column  does not change and I get:

dates  A   B   C
date1  X   NA  etc
date2  Y   NA
date3  Z   NA
date1  NA  XX
date2  NA  YY


Does anyone know how I can fix this issue to get the dataframe formate I'm looking for?

I'm current calling Pivot as such:

dfpivot(index=""dates""  columns=""location"")


because I have a # of data columns I want to pivot (don't want to list each one as an argument) I believe by default pivot pivots the rest of the columns in the dataframe
Thanks",1170240,,1170240.0,2012-07-10 13:32:12,2012-07-10 17:28:46,"pandas pivoting a dataframe, duplicate rows",<python><pivot><pivot-table><pandas>,3.0,1.0,2.0,
11414596,1,11414827.0,2012-07-10 13:40:36,1,369,"I'm trying to drop the last row in a dataframe created by pandas in python and seem to be having trouble 

index = DateRange('1/1/2000'  periods=8)
df = DataFrame(randn(8  3)  index=index  columns=['A'  'B'  'C'])


I tried the drop method like this:

dfdrop([shape(df)[0]-1]  axis = 0)


but it keeps saying label not contained in the axis 

I also tried to drop by index name and it still doesn't seem to be working

Any advice would be appreciated Thanks!!!",1504276,,1252759.0,2012-07-10 13:49:53,2012-07-10 13:51:12,"dropping a row in pandas with dates indexes, python",<python><pandas>,1.0,,,
11416692,1,11419721.0,2012-07-10 15:26:15,1,103,"I am trying to create a plot waith an x range of eg 500 milliseconds

rng = date_range(s periods=500 freq=""U"")
df = DataFrame(randn(500) index=rng columns=[""A""])

to plot column A:

df[""A""]plot()


The whole plot will be squeezed into a single spike because the x range is defined from Jan-2011 until Jul-2014 

Is there a way to change this?",1515250,,1012284.0,2012-07-10 15:27:56,2012-07-10 18:37:44,Plotting millisecond range in pandas,<plot><pandas>,1.0,,,
11470105,1,11495984.0,2012-07-13 12:00:42,0,493,"in a Pandas (v080) DataFrame I want to overwrite one slice of columns with another

The below code throws the listed error  

What would be an efficient alternative method for achieving this?  Thanks

df = DataFrame({'a' : range(0 7) 
'b' : nprandomrandn(7) 
'c' : nprandomrandn(7) 
'd' : nprandomrandn(7) 
'e' : nprandomrandn(7) 
'f' : nprandomrandn(7) 
'g' : nprandomrandn(7)})

# overwrite cols
dfix[: 'b':'d'] = dfix[:  'e':'g']

Traceback (most recent call last):
File ""C:\Python27\lib\site-packages\pandas\core\indexingpy""  line 68  in __setitem__
self_setitem_with_indexer(indexer  value)
File ""C:\Python27\lib\site-packages\pandas\core\indexingpy""  line 98  in   _setitem_with_indexer
raise ValueError('Setting mixed-type DataFrames with '
ValueError: Setting mixed-type DataFrames with array/DataFrame pieces not yet supported


--

Edited

And as a permutation  how could I also specify a subset of the rows to set

dfix[df['a'] ",1258833,,1258833.0,2012-07-17 09:31:01,2012-07-17 12:45:57,Pandas advanced indexing assignment,<python><pandas>,1.0,,,
11586989,1,,2012-07-20 21:11:28,3,358,"I am using matplotlib's imshow() function to show a pandasDataFrame 

I would like the labels and ticks for both x and y axes to be drawn from the DataFrameindex and DataFramecolumns lists  but I can't figure out how to do it

Assuming that data is a pandasDataFrame:

>>> print data

Index: 201 entries   1901 to  2101
Data columns:
jan    201  non-null values
feb    201  non-null values
mar    201  non-null values
apr    201  non-null values
may    201  non-null values
jun    201  non-null values
jul    201  non-null values
aug    201  non-null values
sep    201  non-null values
oct    201  non-null values
nov    201  non-null values
dec    201  non-null values


When I do this:

ax1 = figadd_subplot(131  xticklabels=datacolumns  yticklabels=dataindex)
ax1set_title(""A"")
ax1tick_params(axis='both'  direction='out')
im1 = ax1imshow(data  
                 interpolation='nearest'  
                 aspect='auto' 
                 cmap=cmap )


I end up with nicely spaced tick labels on the y axis of the image  but the labels are 1901-1906 instead of 1901 thru 2101 Likewise  the x axis tick labels are feb-jul instead of jan-dec

If I use 

ax1 = figadd_subplot(131) # without specifying tick labels


Then I end up with the axis tick labels simply being the underlying ndarray index values (ie 0-201 and 0-12) I don't need to modify the spacing or quantity of ticks and labels  I just want the label text to come from the DataFrame index and column lists Not sure if I am missing something easy or not?

Thanks in advance",877192,,,,2012-08-01 16:47:05,"pandas, matplotlib, use dataframe index as axis tick labels",<python><matplotlib><pandas>,1.0,,1.0,
11298097,1,,2012-07-02 17:06:58,1,195,I am using pandas in python  I have several Series indexed by dates that I would like to concat into a single DataFrame  but the Series are of different lengths because of missing dates etc  I would like the dates that do match up to match up  but where there is missing data for it to be interpolated or just use the previous date or something like that  What is the easiest way to do this?,1496672,,,,2012-07-03 14:11:33,concatenating TimeSeries of different lengths using Pandas,<python><data.frame><concat><pandas>,1.0,1.0,,
11329611,1,,2012-07-04 12:57:23,0,189,"I have a dataset that I read in like so:

pm10 = pdsread_csv('pm10csv'  index_col = [0 1]  parse_dates=True)
panel_exog = pm10to_panel()['pass_ind']


but when I do the conversion  the returned panel has large integer numbers where there are supposed to be zeros (or 1s) I had to add ""0"" to all rows of that binary variable to make it work the way I wanted it to",1501612,,1501612.0,2012-07-05 10:37:26,2012-07-08 16:45:17,Python Pandas: to_panel() from dataframe returns weird numbers for binary variable,<python><data><panel><fixed><pandas>,2.0,,,
11362376,1,,2012-07-06 12:39:18,1,78,"Is it possible to customize Serie (in a simple way  and DataFrame by the way :p) from pandas to append extras informations on the display and in the plots? A great thing will be to have the possibility to append informations like ""unit""  ""origin"" or anything relevant for the user that will not be lost during computations  like the ""name"" parameter",1486079,,1253219.0,2012-09-03 00:55:12,2012-09-03 00:55:12,Append extras informations to Serie in Pandas,<pandas>,1.0,,,
11422552,1,11435721.0,2012-07-10 21:48:20,3,123,"Sometimes I am dealing with DataFrames that have a numerical index  but I would like to bypass it to reference rows according to their order 

In [49]: df = pandasDataFrame(nprandomrandn(3  5))

In [50]: df
Out[50]: 
          0         1         2         3         4
0 -2426211  0670384  0545880 -1435168  0675598
1  0507128  0478832 -0159536 -0696284 -1112171
2  0938019 -1673491 -0567462  0381804 -1280602

In [51]: df[1:2]
Out[51]: 
          0         1         2         3         4
1  0507128  0478832 -0159536 -0696284 -1112171

In [52]: df2 = dfix[1:2]

In [53]: df2
Out[53]: 
          0         1         2         3         4
1  0507128  0478832 -0159536 -0696284 -1112171
2  0938019 -1673491 -0567462  0381804 -1280602

In [55]: df2ix[0]
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)


In [56]: df2rename(index={1: ""one""  2:""two""})
Out[56]: 
            0         1         2         3         4
one  0507128  0478832 -0159536 -0696284 -1112171
two  0938019 -1673491 -0567462  0381804 -1280602

In [57]: df3 = df2rename(index={1: ""one""  2:""two""})

In [58]: df3ix[0]
Out[58]: 
0    0507128
1    0478832
2   -0159536
3   -0696284
4   -1112171
Name: one


How can I bypass pandas' indices to access the underlying matrix indices?",128580,,,,2012-07-11 15:09:16,pandas: bypassing numerical index,<python><pandas>,1.0,,,
11613800,1,,2012-07-23 13:45:07,0,183,"I am using pydev plugin in Eclipse Juno for my python programming in windows 7 and i am using python 32  it works fine while running python application which using standard python packages For my one of my project i have to use pandas library  for that i download and install numpy and pandas Windows installer for python 3 But while running even a small program it shows error message i am a beginner in python So anyone have any idea about how to install and test pandas in Windows 7 by using eclipse  just pass it to me 

The error message is like this:

Traceback (most recent call last):
import numpy
File ""C:\Python32\lib\site-packages\numpy\__init__py""  line 137  in 
from  import add_newdocs
File ""C:\Python32\lib\site-packages\numpy\add_newdocspy""  line 9  in 
from numpylib import add_newdoc
File ""C:\Python32\lib\site-packages\numpy\lib\__init__py""  line 4  in 
from type_check import *
File ""C:\Python32\lib\site-packages\numpy\lib\type_checkpy""  line 8  in 
import numpycorenumeric as _nx
File ""C:\Python32\lib\site-packages\numpy\core\__init__py""  line 40  in 
from numpytesting import Tester
File ""C:\Python32\lib\site-packages\numpy\testing\__init__py""  line 8  in 
from unittest import TestCase
File ""C:\Python32\lib\unittest\__init__py""  line 59  in 
from case import (TestCase  FunctionTestCase  SkipTest  skip  skipIf 
File ""C:\Python32\lib\unittest\casepy""  line 6  in 
import pprint
EOFError: EOF read where not expected


Thanks in advance for your time",750810,,,,2012-07-23 13:45:07,How to install and run pandas python library in Eclipse Juno?,<numpy><python-3.x><pandas>,,,,
11587782,1,11589000.0,2012-07-20 22:33:29,1,233,"I'm trying to create a series of dummy variables from a categorical variable using pandas in python I've come across the get_dummies function  but whenever I try to call it I receive an error that the name is not defined 

Any thoughts or other ways to create the dummy variables would be appreciated",1074057,,,,2012-07-21 02:29:57,Creating dummy variables in pandas for python,<python><pandas>,1.0,,1.0,
11621165,1,11622565.0,2012-07-23 21:57:01,1,498,"I can't seem to find an elegant way to index a pandasDataFrame by an integer index
In the following example I want to get the value 'a' from the first element of the 'A' column 

import pandas
df = pandasDataFrame(
    {'A':['a' 'b'  'c']  'B':['f'  'g'  'h']}  
    index=[10 20 30]
    )


I would expect df['A']ix[0] and df['A'][10] both  to return 'a' 
The df['A'][10] does return 'a'  but df['A']ix[0] throws a KeyError: 0 The only way I could think of to get the value 'a' based on the index 0 is to use the following approach 

df['A'][df['A']index[0]]


Is there a shorter way to get 'a' out of the dataframe  using the 0 index?",386327,,,,2012-07-24 04:21:16,Indexing a pandas dataframe by integer,<python><pandas>,2.0,,1.0,
11637384,1,11639358.0,2012-07-24 18:50:11,3,1555,"I am having issues with joins in pandas and I am trying to figure out what is wrong 
   Say I have a dataframe x:


DatetimeIndex: 1941 entries  2004-10-19 00:00:00 to 2012-07-23 00:00:00
Data columns:
close    1941  non-null values
high     1941  non-null values
low      1941  non-null values
open     1941  non-null values
dtypes: float64(4)


should I be able to join it with y on index with a simple join command where y = x except colnames have +2  

 
 DatetimeIndex: 1941 entries  2004-10-19 00:00:00 to 2012-07-23 00:00:00
 Data columns:
 close2    1941  non-null values
 high2     1941  non-null values
 low2      1941  non-null values
 open2     1941  non-null values
 dtypes: float64(4)

 yjoin(x) or pandasDataFramejoin(y x):
 
 DatetimeIndex: 34879 entries  2004-12-16 00:00:00 to 2012-07-12 00:00:00
 Data columns:
 close2    34879  non-null values
 high2     34879  non-null values
 low2      34879  non-null values
 open2     34879  non-null values
 close     34879  non-null values
 high      34879  non-null values
 low       34879  non-null values
 open      34879  non-null values
 dtypes: float64(8)


I expect the final to have 1941 non-values for both I tried merge as well but I have the same issue

I had thought the right answer was pandasconcat([x y])  but this does not do what I intend either

In [83]: pandasconcat([x y]) 
Out[83]:  
DatetimeIndex: 3882 entries  2004-10-19 00:00:00 to 2012-07-23 00:00:00 
Data columns: 
close2 3882 non-null values 
high2 3882 non-null values 
low2 3882 non-null values 
open2 3882 non-null values 
dtypes: float64(4) 


edit: 
If you are having issues with join  read Wes's answer below I had one time stamp that was duplicated",1064197,,1064197.0,2012-07-25 14:06:48,2012-07-25 14:06:48,Pandas join/merge/concat two dataframes,<python><pandas>,2.0,,1.0,
11672403,1,,2012-07-26 15:09:43,5,201,"I am retrieving some web data  parsing it  and storing the output as a Pandas DataFrame into an HDF5 file Right before I write the DataFrame into the H5 file  I add my own description string to annotate some metadata about where the data came from and whether anything went wrong while parsing it

In [1]: my_data_framedesc = ""Some string about the data""

In [2]: my_data_framedesc

Out[1]: ""Some string about the data""

In [3]: print type(my_data_frame)



However  after loading the same data with pandasiopytablesHDFStore()  my added desc attribute is missing and I get the error: AttributeError: 'DataFrame' object has no attribute 'desc' as if I had never added this new attribute

How can I get my metadata descriptions to persist as an extra attribute of the DataFrame object? (Or is there some existing  recognized attribute of a DataFrame that I can hijack for my metadata purposes?)",567620,,,,2012-07-26 15:28:46,Adding my own description attribute to a Pandas DataFrame,<python><metadata><data.frame><pandas>,1.0,,,
11640243,1,11643893.0,2012-07-24 22:30:16,1,320,"I know pandas supports a secondary Y axis  but Im curious if anyone knows a way to put a tertiary Y axis on plots currently I am achieving this with numpy+pyplot  but it is slow with large data sets

this is to plot different measurements with distinct units on the same graph for easy comparison (eg Relative Humidity/Temperature/ and Electrical Conductivity)

so really just curious if anyone knows if this is possible in PANDAs without too much work

[Edit] I doubt that there is a way to do this(without too much overhead) however I hope to be proven wrong   this may be a limitation of matplotlib",541038,,541038.0,2012-07-24 22:49:43,2012-07-25 06:28:37,PANDAS plot multiple Y axes,<python><pandas>,1.0,,1.0,
11668446,1,11672789.0,2012-07-26 11:38:30,0,610,"I'm trying to cast the type of a DataFrame using the function astype in the same way it is used in NumPy

First in NumPy:

In [175]: x = nprecarray([('a' '1') ('b' '2')] names='col1 col2')

In [176]: x
Out[176]: 
recarray([('a'  '1')  ('b'  '2')]  
    dtype=[('col1'  '|S1')  ('col2'  '|S1')])

In [177]: dt=[('col1'  '|S1')  ('col2'  'i8')]

In [178]: xastype(dt)
Out[178]: 
recarray([('a'  1)  ('b'  2)]  
    dtype=[('col1'  '|S1')  ('col2'  '",172865,,,,2012-07-26 15:30:56,pandas DataFrame type cast,<python><numpy><pandas>,1.0,,,
11686272,1,11686295.0,2012-07-27 10:45:55,0,289,"I have timeseries data from three different sensors over the period of a year  the sensors produce a data point roughly every 3 minutes  the sensors are not synchronized so they produce a datapoint output at different times relative to each other

This data is in an sqlite db in one table of approximately half a million records I intend to display this data using the javascript chart library dygraph  I have already produced timeseries charts for each of these sensors individually by doing an sql query by sensor name and save to csv I wish to have one chart which displays all the data points  with a line representing each sensor

I have created a numpy 2d array of type string called 'minutes_array' with the first column as unix timestamps rounded to the nearest minute covering every minute from the start of the sensor timeseries to the end with three empty columns to be filled with data from each of the 3 sensors where available

minutes_array

[['1316275620' '' '' '']
 ['1316275680' '' '' '']
 ['1316275740' '' '' '']
   
 ['1343206920' '' '' '']
 ['1343206980' '' '' '']
 ['1343207040' '' '' '']]


The sensor timeseries data is then also rounded to the nearest minute and I use numpyin1d and take the timestamps from the above 'minutes_array' and the 'sensor_data' array and create a mask for the records relating to that sensor

sensor_data

[['1316275680' '2152']
 ['1316275860' '2270']
 ['1316276280' '2122']
   
 ['1343206380' '1877']
 ['1343206620' '1894']
 ['1343206980' '1929']]

 mask = npin1d(minutes_array[: 0]  sensor_data[: 0])

 [False  True False   False  True False]


I then wish to modify the records in minutes_array which are true for that mask and place the sensor_data value into the first column following the timestamp in minutes_array From my attempts it does not seem possible to alter the original 'minutes_array' when a mask is applied to it  is there a way to achieve this outcome in numpy without using for loops and matching timestamps individually?

Solved

Based on the answer below from @eumiro I used a solution from the Pandas Docs and the 'sensor_data' numpy array described above 

sensors = ['s1' 's2' 's3']    
sensor_results = {}
for sensor in sensors:
    sensor_data = get_array(db_cursor  sensor)
    sensor_results[sensor] = pdSeries(sensor_data[: 1]  \
                                   index=sensor_data[: 0])
df = pdDataFrame(buoy_results)
dfto_csv(""outputcsv"")
",1135883,,1135883.0,2012-07-27 14:40:42,2012-07-27 14:40:42,Combining multiple timeseries data to one 2d numpy array,<python><numpy><time-series><pandas>,1.0,,1.0,
11673453,1,,2012-07-26 16:06:06,0,519,"I have some CSV data like

20111208 22:45 133434 133465 133415 133419 265
20111208 23:00 133419 133542 133419 133472 391
20111208 23:15 133470 133483 133383 133411 420
20111208 23:30 133413 133451 133389 133400 285


coming from Metatrader 4 in a file named EURUSD15csv

I would like to import this file with Python using Pandas library and read_csv function

So I did this :

#!/usr/bin/env python
from pandas import *
df = read_csv('data/EURUSD15csv'  header=None)
dfcolumns = ['Date'  'Time'  'Open'  'High'  'Low'  'Close'  'Volume']
print(df)


I would like now to have date/time parsed

so I changed

df = read_csv('data/EURUSD15csv'  header=None)


to

df = read_csv('data/EURUSD15csv'  header=None  parse_dates=[[1  2]])


But I get this error message

Exception: Length mismatch (7 vs 6)


How can I parse date and time columns and have the 2 columns considered as 1 ""datetime"" column",1555275,,,,2012-07-27 08:29:34,Import historical Metatrader CSV data to Python with Pandas library (date/time parsing),<python><parsing><csv><pandas><metatrader4>,2.0,,,
11689668,1,11691757.0,2012-07-27 14:16:50,0,105,"I have got some problems with plotting a sliced DataFrame with entire columns filled with NaN's

How come:

pandasDataFrame(
    dict(
        A=pandasSeries([npNaN]*32) 
        B=pdSeries(range(-1 32))
    )
)plot()


differs from:

#Ugly fix
pandasDataFrame(
    dict(
        A=pandasSeries( [0] + [numpyNaN]*32) 
        B=pdSeries(range(-1 32))
    )
)plot()


by plotting a 0-line as if the column is filled with zeros
Shouldn't the first code work just as:

pylabplot(
    range(0 33) 
    range(-1 32) 
    range(0 32) 
    [numpyNaN]*32
)


And also plotting just a Series filled with NaN works fine:

pandasSeries([numpyNaN]*32)plot()


What am I missing? Is there a right way to plot a column with all NaN's or is it a bug?",905596,,,,2012-07-27 20:16:25,NaN-columns is plotted as a all 0 column in pandas,<pandas>,1.0,,,
11707805,1,,2012-07-29 06:11:30,5,1246,"How do I use pandas and scikit-learn to train a model a large csv data (~75mb) without running into memory problems?

I'm using IPython notebook as the programming environment  and pandas+sklearn packages to analyze data from kaggle's digit recognizer tutorial

The data is available on the webpage   link to my code   and here is the error message: 

KNeighborsClassifier is used for the prediction

Problem:

""MemoryError"" occurs when loading large dataset using read_csv function To bypass this problem temporarily  I have to restart the kernel  which then read_csv function successfully loads the file  but the same error occurs when I run the same cell again

Anyway  when the read_csv function loads the file successfully  after making changes to the dataframe  I can pass the features and labels to the KNeighborsClassifier's fit() function At this point  similar memory error occurs

I tried the following:

Iterate through the CSV file in chunks  and fit the data accordingly  but the problem is that the predictive model is overwritten every time for a chunk of data

What do you think I can do to successfully train my model without running into memory problems?

Note: I don't really have to use pandas  as long as I can load the large data and train my model in a memory efficient way

Thanks!",1253437,ji.,1253437.0,2012-07-29 20:56:50,2012-07-30 12:59:14,Scikit and Pandas: Fitting Large Data,<machine-learning><pandas><scikit-learn>,1.0,3.0,1.0,
11586068,1,,2012-07-20 19:49:20,1,233,"I used pivot to reshape my data and now have a column multiindex  I want the resulting columns to be the X variables in a simple OLS regression  The Y's are another series with the same row index  

When I try running

model1 = ols(y = gdp0  x = MIDAS_small)


I get

TypeError: can only call with other hierarchical index objects


I can imagine two solutions but can't figure out either one:

Collapse the multiindex  Rather than having columns of the form ('before'  'var1') and ('after'  'var1')  I would just have a bunch of 'beforevar1'  'aftervar1'  etc  Then I could use ols to produce a nice and sufficiently legible table
Is there some way to run a regression with a multiindex?  It seems like it was designed in part for this sort of thing  especially panel regressions  but I couldn't find any relevant examples or documentation
Well  I found an inelegant solution to #1:
I can create a new dataframe  loop over both column indexes  and insert new columns into the new dataframe with the same name  but with names as strings instead of tuples  There must be a more elegant  single command  right?",1476254,,1476254.0,2012-07-20 21:11:56,2012-07-20 21:11:56,Collapse a Pandas multiindex or run OLS regression on a multiindexed dataframe,<python><hierarchical-data><regression><pandas><multi-index>,,1.0,1.0,
11615504,1,11617682.0,2012-07-23 15:20:43,3,476,"I'm starting using Python and I have a simple question related with csv files and parsing datetime

I have a csv file that look like this:

YYYYMMDD  HH     X
20110101   1    10
20110101   2    20
20110101   3    30


I would like to read it using pandas (read_csv) and have it in a dataframe indexed by the datetime So far I've tried to implement the following:

import pandas as pnd
pndread_csv(""\\filecsv""   parse_dates = True  index_col = [0 1])


and the result I get is:

                         X
YYYYMMDD    HH            
2011-01-01 2012-07-01   10
           2012-07-02   20
           2012-07-03   30


As you see the parse_dates in converting the HH into a different date

Is there a simple and efficient way to combine properly the column ""YYYYMMDD"" with the column ""HH"" in order to have something like this? :

                      X
Datetime              
2011-01-01 01:00:00  10
2011-01-01 02:00:00  20
2011-01-01 03:00:00  30


Thanks in advance for the help",1520997,,9453.0,2012-07-23 15:24:18,2012-07-23 17:45:03,Parse dates when YYYYMMDD and HH are in separate columns using pandas in Python,<python><pandas>,1.0,,2.0,
11622652,1,11622769.0,2012-07-24 00:50:49,3,800,"I am exploring switching to python and pandas as a long-time SAS user  However  when running some tests today  I was surprised that python ran out of memory when trying to pandasread_csv() a 128mb csv file  It had about 200 000 rows and 200 columns of mostly numeric data

With SAS  I can import a csv file into a SAS dataset and it can be as large as my hard drive Is there something analogous in pandas?  I regularly work with large files and do not have access to a distributed computing network

Thanks",919872,,,,2012-09-09 02:46:50,"Large, persistent DataFrame in pandas",<python><sas><pandas>,2.0,1.0,3.0,
11728836,1,,2012-07-30 20:08:57,2,135,"I often need to apply a function to the groups of a very large DataFrame (of mixed data types) and would like to take advantage of multiple cores

I can create an iterator from the groups and use the multiprocessing module  but it is not efficient because every group and the results of the function must be pickled for messaging between processes

Is there any way to avoid the pickling or even avoid the copying of the DataFrame completely? It looks like the shared memory functions of the multiprocessing modules are limited to Numpy arrays Are there any other options?",1429196,,,,2012-08-15 16:15:15,Efficiently applying a function to a grouped pandas DataFrame in parallel,<multiprocessing><shared-memory><pandas>,2.0,,1.0,
11492215,1,11492771.0,2012-07-15 13:14:38,1,90,"I am hitting on a corner case in pandas I am trying to use the agg fn but without doing a groupby Say I want an aggregation on the entire dataframe  ie

from pandas import *
DF = DataFrame( randn(5 3)  index = list( ""ABCDE"")  columns = list(""abc"") )
DFgroupby([])agg({'a' : npsum  'b' : npmean } ) # ",1477436,,298479.0,2012-07-15 13:20:56,2012-07-15 15:20:22,How to do a groupby on an empty set of columns in Pandas?,<python><group-by><pandas>,2.0,,,
11515290,1,,2012-07-17 02:35:00,2,163,"With pandas 080 

import pandas
import pandastseriesoffsets
h = pandastseriesoffsetsHour()
times = pandasdate_range(start='2010-1-1 1:00:05'  periods=3  freq='3H')
times



  
  [2010-01-01 01:00:05    2010-01-01 07:00:05]
  Length: 3  Freq: 3H  Timezone: None


timessnap(h)



  
  [2010-01-01 01:00:05    2010-01-01 07:00:05]
  Length: 3  Freq: H  Timezone: None


This is because:

honOffset(times[0])



  True


I'd guess that this functionality is pretty new  it doesn't seem to be documented much


  The rollforward and rollback methods do exactly what you would expect:


My larger goal here is using 2 frequencies (eg 4 hours and 1 day) and bucketing a series of timestamps based on the first frequency modulo the second (eg 7:05:33 -> 1  19:59:59 -> 4  21:44:00 -> 5)",1497828,,,,2012-07-19 16:58:35,Snapping to hours in pandas,<python><time-series><pandas><snap>,1.0,1.0,,
11607387,1,11610785.0,2012-07-23 06:24:19,2,147,"I'm extracting mass data from a legacy backend system using C/C++ and move it to Python using distutils After obtaining the data in Python i put it into a pandas DataFrame object for data analysis Now i want to go faster and would like to avoid the second step 

Is there a C/C++ API for pandas to create a DataFrame in C/C++  add my C/C++ data and pass it to Python? I'm thinking of something that is similar to numpy C API

I already thougth of creating numpy array objects in C as a workaround but i'm heavily using timeseries data and would love to have the TimeSeries and date_range objects as well

Thanks for any help
Thomas",1521724,,,,2012-07-23 10:31:10,Is there a C/C++ API for python pandas?,<python><c><api><pandas>,1.0,,,
11720334,1,,2012-07-30 11:13:37,2,355,"I have a DataFrame with MultiIndex  for example:

In [1]: arrays = [['one 'one' 'one' 'two' 'two' 'two'] [1 2 3 1 2 3]]
In [2]: df = DataFrame(randn(6 2) index=MultiIndexfrom_tuples(zip(*arrays)) columns=['A' 'B'])
In [3]: df
Out [3]:
          A         B
one 1 -2028736 -0466668
    2 -1877478  0179211
    3  0886038  0679528
two 1  1101735  0169177
    2  0756676 -1043739
    3  1189944  1342415


Now I want to compute the means of elements 2 and 3 (index level 1) for each row (index level 0) and each column So I need a DataFrame which would look like 

                                 A                            B
one 1 mean(df['A']ix['one'][1:3])  mean(df['B']ix['one'][1:3])
two 1 mean(df['A']ix['two'][1:3])  mean(df['B']ix['two'][1:3])


How do I do that without using loops over rows (index level 0) of the original data frame? What if I want to do the same for a Panel? There must be a simple solution with groupby  but I'm still learning it and can't think of an answer

Thank you!",1557300,,,,2012-11-07 19:34:40,selecting data from pandas panel with MultiIndex,<pandas><multi-index>,2.0,,1.0,
11754334,1,,2012-08-01 07:18:34,0,87,"DataFrameto_records gives an error when type(index) is DatetimeIndex

python version is 273  pandas is 081

The following is an simple example code that make the error in IPython shell

How can I get the right answer  an array of records from to_records method

Python 273 (default  Apr 20 2012  22:39:59)  
Type ""copyright""  ""credits"" or ""license"" for more information

IPython 0121 -- An enhanced Interactive Python  
?         -> Introduction and overview of IPython's features  
%quickref -> Quick reference  
help      -> Python's own help system  
object?   -> Details about 'object'  use 'object??' for extra details  

In [1]: import os

In [2]: ossystem('pip search pandas')
pandas                    - Powerful data structures for data analysis  time
                            series and statistics
  INSTALLED: 081 (latest)
Out[2]: 0

In [3]: from pandas import *

In [4]: from numpyrandom import randn

In [5]: data = randn(5 3)

In [6]: index = date_range('2012-08-01'  periods=5)

In [7]: columns = ['a'  'b'  'c']

In [8]: df = DataFrame(data  index=index  columns=columns)

In [9]: df  
Out[9]: 
                   a         b         c  
2012-08-01  2355928 -2465061  0240094  
2012-08-02 -0952323  0746623 -0384021  
2012-08-03  1460156  0292560 -0494793  
2012-08-04 -0989584 -1630384  1373587  
2012-08-05  0014760 -0789603 -0622780  

In [10]: dfto_records()
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)  
/home/ideabong/ in ()  
----> 1 dfto_records()  

/usr/local/lib/python27/dist-packages/pandas/core/framepyc in to_records(self  index)  
    889             names = list(map(str  selfcolumns))  
    890   
--> 891         return nprecfromarrays(arrays  names=names)  
    892   
    893     @classmethod  

/usr/local/lib/python27/dist-packages/numpy/core/recordspyc in fromarrays(arrayList  dtype  shape  formats  names  titles  aligned  byteorder)  
    546     # Determine shape from data-type  

    547     if len(descr) != len(arrayList):  
--> 548         raise ValueError  ""mismatch between the number of fields ""\  
    549               ""and the number of arrays""  
    550   

ValueError: mismatch between the number of fields and the number of arrays  
",1567727,,,,2012-08-02 01:28:03,DataFrame.to_records gives an error when type(index) is DatetimeIndex,<pandas>,1.0,,,
11548005,1,11548224.0,2012-07-18 18:30:02,4,596,"Is there a preferred way to keep the data type of a NumPy array fixed as int (or int64 or whatever)  while still having an element inside listed as numpyNaN?

In particular  I am converting an in-house data structure to a Pandas DataFrame In our structure  we have integer-type columns that still have NaN's (but the dtype of the column is int) It seems to recast everything as a float if we make this a DataFrame  but we'd really like to be int

Thoughts?

Things tried:

I tried using the from_records() function under pandasDataFrame  with coerce_float=False and this did not help I also tried using NumPy masked arrays  with NaN fill_value  which also did not work All of these caused the column data type to become a float",567620,,567620.0,2012-07-18 18:43:52,2012-07-18 18:43:52,NumPy or Pandas: Keeping array type as integer while having a NaN value,<python><numpy><int><pandas><data-type-conversion>,1.0,3.0,,
11620721,1,11621042.0,2012-07-23 21:23:50,1,250,"I have a 200 000 x 500 dataframe loaded into Pandas Is there a function that can automatically tell me which columns are missing data? Or do I have to iterate over each column and check element by element?

Once I've found a missing element  how do I define a custom function (based on both the column name and some other data in the same row) to do automatic replacements I see the fillna() method  but I don't think it takes a (lambda) function as an input

Thanks!",190894,,,,2012-07-23 21:51:56,handling missing data in Pandas,<pandas>,1.0,,1.0,
11651048,1,11651153.0,2012-07-25 13:38:37,1,139,"Apologies in advance for the super-newbie question

I'm learning to use pandas  and have this simple operation that I can't figure out how to perform:

I have the following data frame:

print df 
Out[19]: 
USERNAME  REQUEST_TYPE   STATUS  LATENCY
0      foo             1  SUCCESS        7
1      foo             2  SUCCESS       17
2      bar             1  SUCCESS       10
3      bar             2  FAILURE       12


I would like to have one row for each USERNAME  which is the concatenation
of the STATUS and LATENCY columns per REQUEST_TYPE The output should look like this:

USERNAME    STATUS_1  LATENCY_1     STATUS_2    LATENCY_2
0      foo  SUCCESS     7           SUCCESS        17
1      bar  SUCCESS     10          FAILURE        12


I thought of something starting with pandasgroupby(df ['USERNAME'  'REQUEST_TYPE'])  but I am not sure how to concatenate the rows back  and whether there is any method which would create new column names

Thanks!",1032006,,,,2012-07-25 13:43:57,Pandas concatenate rows,<python><pandas>,1.0,,1.0,
11685618,1,,2012-07-27 10:04:27,1,233,"Suppose I have an HDF5 file (myHDFh5) with a hierarchy of groups  something like:

/root/groupA
     /groupB


Now I want to add a DataFrame (myFrame) to the groupA (along with some other objects such as dictionaries) How do I do that? If I open my HDFh5 with pandasioHDFStore: 

store = pandasioHDFStore('myHDFh5')


and then try:

store['groupA']['myFrame'] = myFrame


I get:

AttributeError: Attribute 'pandas_type' does not exist in node: '/groupA'


What is the proper way to do this? ",1557300,,250259.0,2012-07-29 20:38:44,2012-12-29 23:15:50,how do I add a pandas object (e.g. DataFrame) to a group within an HDF file?,<pandas><hdf5>,2.0,,,
11761884,1,,2012-08-01 14:52:59,1,120,"It looks pandasconcat is doing 'left outer' join instead of just union the indexes Seems a bug to me but maybe I'm missing something obvious

    import pandas
    import pandasutiltesting as put
    ts1 = putmakeTimeSeries()
    ts2 = putmakeTimeSeries()[::2]
    ts3 = putmakeTimeSeries()[::3]
    ts4 = putmakeTimeSeries()[::4]

    ## to join with union
    ## these two are of different length!
    pandasconcat([ts1 ts2]  join='outer'  axis = 1) 
    pandasconcat([ts2 ts1]  join='outer'  axis = 1)


Any idea how can I get the full union (as they do claim by using join='outer' on the pandas document)

Thanks",1568919,,,,2012-08-02 01:25:38,pandas concat('outer') not doing union?,<python><pandas>,1.0,,,
11619144,1,11624916.0,2012-07-23 19:26:32,0,143,"The following

from pandas import *
import datetime
DataFrame([1 1]  index =  [ datetimedatetime(2012 1 1)  datetimedatetime(2012 9 1) ] )plot()


gives a plot where the xaxis is not readable I think the reason is that in tools\plottingpy the condition (which decides to autofmt) is false

condition = (not self_use_dynamic_x
             and dfindexis_all_dates
             and not selfsubplots
             or (selfsubplots and selfsharex))


Should the first line not be self_use_dynamic_x() instead?",1477436,,,,2012-07-24 05:53:10,Pandas xaxis auto-format issue,<python><matplotlib><pandas>,2.0,,,
11679716,1,11684081.0,2012-07-26 23:57:40,0,76,"Is there a way to compare the size of two DateOffset objects?

>>> from pandascoredatetools import *
>>> Hour(24) > Minute(5)
False


This works with timedelta  so I assumed that pandas would inherit that behavior - or is the time system made from scratch?",1325447,,,,2012-07-27 08:22:32,Comparing DateOffsets in pandas,<python><datetime><pandas>,1.0,,,
11707586,1,11711637.0,2012-07-29 07:44:51,5,986,"Is there a way to widen the display of output in either interactive or script-execution mode?

Specifically  I am using the describe() function on a Pandas dataframe  When the dataframe is 5 columns (labels) wide  I get the descriptive statistics that I want  However  if the dataframe has any more columns  the statistics are suppressed and something like this is returned:

>Index: 8 entries  count to max  
>Data columns:  
>x1          8  non-null values  
>x2          8  non-null values  
>x3          8  non-null values  
>x4          8  non-null values  
>x5          8  non-null values  
>x6          8  non-null values  
>x7          8  non-null values  


The ""8"" value is given whether there are 6 or 7 columns  What does the ""8"" refer to?

I have already tried dragging the IDLE window larger  as well as increasing the ""Configure IDLE"" width options  to no avail

My purpose in using Pandas and describe() is to avoid using a second program like STATA to do basic data manipulation and investigation

Thanks

Python/IDLE 273
Pandas 081
Notepad++ 614 (UNICODE)
Windows Vista SP2  ",1560238,,,,2013-01-10 10:06:59,"Python pandas, widen output display?",<python><pandas>,3.0,,1.0,
11762815,1,,2012-08-01 15:41:19,2,593,"I'm having a pandas Series object filled with decimal numbers of dtype Decimal I'd like to use the new pandas 08 function to resample the decimal time series like this:

resampled = tsresample('D'  how = 'mean')


When trying this i get an ""GroupByError: No numeric types to aggregate"" error I assume the problem is that npmean is used internaly to resample the values and npmean expects floats instead of Decimals 

Thanks to the help of this forum i managed to solve a similar question using groupBy and the apply function but i would love to also use the cool resample function

How use the mean method on a pandas TimeSeries with Decimal type values?

Any idea how to solve this?

Here is the complete ipython session creating the error:

In [37]: from decimal import Decimal

In [38]: from pandas import *

In [39]: rng = date_range('112012' periods=48  freq='H')

In [40]: rnd = nprandomrandn(len(rng))

In [41]: rnd_dec = [Decimal(x) for x in rnd]

In [42]: ts = Series(rnd_dec  index=rng)

In [43]: ts[0:3]

Out[43]:
2012-01-01 00:00:00    -01020591335576267189022559023214853368699550628
2012-01-01 01:00:00    099245713975437366283216533702216111123561859130
2012-01-01 02:00:00    180080710727195758558139004890108481049537658691
Freq: H

In [44]: type(ts[0])
Out[44]: decimalDecimal

In [45]: tsresample('D'  how = 'mean')
---------------------------------------------------------------------------
GroupByError                              Traceback (most recent call last)
C:\Users\THM\Documents\Python\ in ()
----> 1 tsresample('D'  how = 'mean')

C:\Python27\lib\site-packages\pandas\core\genericpyc in resample(self  rule  how      axis  fill_method  closed  label  convention  kind  loffset  l
imit  base)
    187                               fill_method=fill_method  convention=convention 
    188                               limit=limit  base=base)
--> 189         return samplerresample(self)
    190
    191     def first(self  offset):

C:\Python27\lib\site-packages\pandas\tseries\resamplepyc in resample(self  obj)
     65
     66         if isinstance(axis  DatetimeIndex):
---> 67             rs = self_resample_timestamps(obj)
     68         elif isinstance(axis  PeriodIndex):
     69             offset = to_offset(selffreq)

C:\Python27\lib\site-packages\pandas\tseries\resamplepyc in _resample_timestamps(self  obj)
    184             if len(grouperbinlabels)  186                 result = groupedaggregate(self_agg_method)
    187             else:
    188                 # upsampling shortcut


C:\Python27\lib\site-packages\pandas\core\groupbypyc in aggregate(self  func_or_funcs     *args  **kwargs)
   1215         """"""
   1216         if isinstance(func_or_funcs  basestring):
-> 1217             return getattr(self  func_or_funcs)(*args  **kwargs)
   1218
   1219         if hasattr(func_or_funcs '__iter__'):

C:\Python27\lib\site-packages\pandas\core\groupbypyc in mean(self)
    290         """"""
    291         try:
--> 292             return self_cython_agg_general('mean')
    293         except GroupByError:
    294             raise

C:\Python27\lib\site-packages\pandas\core\groupbypyc in _cython_agg_general(self  how)
    376
    377         if len(output) == 0:
--> 378             raise GroupByError('No numeric types to aggregate')
    379
    380         return self_wrap_aggregated_output(output  names)

GroupByError: No numeric types to aggregate


Any help is appreciated
Thanks 
Thomas",1521724,,,,2012-08-01 21:43:55,How to resample a python pandas TimeSeries containing dytpe Decimal values?,<python><decimal><pandas><series>,1.0,,,
11773862,1,11774271.0,2012-08-02 08:40:01,2,176,"I try to upsample daily TimeSeries values using the pandas resample function When i'm upsampling a single day (2012-01-01) i expect the result to be the mean of the day considered for upsampling The result should look like this:

2012-01-01   -0 285344
Freq: D


However  pandas returns two days like this:

2012-01-01   -0412417
2012-01-02    0127073
Freq: D


Is this a bug or a feature? If it is a feature how can i set the resample arguments to achieve my goal?

Thanks 
Thomas

Here is the full example:

In [66]: rng = date_range('1/1/2012'  periods=24  freq='H')

In [67]: ts = Series(nprandomrandn(len(rng))  index=rng)

In [68]: ts
Out[68]:
2012-01-01 00:00:00   -0412417
2012-01-01 01:00:00    0442482
2012-01-01 02:00:00    1321009
2012-01-01 03:00:00    0104408
2012-01-01 04:00:00    1124611
2012-01-01 05:00:00   -1041293
2012-01-01 06:00:00    1194104
2012-01-01 07:00:00   -0249706
2012-01-01 08:00:00    1927320
2012-01-01 09:00:00   -0828365
2012-01-01 10:00:00    0163760
2012-01-01 11:00:00   -0736053
2012-01-01 12:00:00   -0323408
2012-01-01 13:00:00    1478162
2012-01-01 14:00:00    1449437
2012-01-01 15:00:00   -1114443
2012-01-01 16:00:00   -0003780
2012-01-01 17:00:00    0554562
2012-01-01 18:00:00   -2019614
2012-01-01 19:00:00    0463484
2012-01-01 20:00:00    0862818
2012-01-01 21:00:00   -1280048
2012-01-01 22:00:00   -0137987
2012-01-01 23:00:00   -0428777
Freq: H

In [69]: tsresample('D')
Out[69]:
2012-01-01   -0412417
2012-01-02    0127073
Freq: D
",1521724,,,,2012-08-02 09:05:01,Unexpected result when upsampling hourly values using the pandas resample function,<python><time-series><pandas>,1.0,1.0,,
11697887,1,,2012-07-28 02:59:02,10,374,"I am convert a Django QuerySet to a pandas DataFrame as follows:

qs = SomeModelobjectsselect_related()filter(date__year=2012)
q = qsvalues('date'  'OtherField')
df = pdDataFramefrom_records(q)


It works  but is there a more efficient way?",1392323,,1240268.0,2012-12-10 18:00:55,2012-12-10 18:00:55,Converting Django QuerySet to pandas DataFrame,<python><django><pandas>,1.0,6.0,4.0,
11714768,1,,2012-07-30 02:26:41,4,497,"I have two largish (snippets provided) pandas DateFrames with unequal dates as indexes that I wish to concat into one:

           NABAX                                  CBAAX
            Close    Volume                         Close    Volume
Date                                    Date
2009-06-05  3651   4962900             2009-06-08  2195         0
2009-06-04  3679   5528800             2009-06-05  2195   8917000
2009-06-03  3680   5116500             2009-06-04  2221  18723600
2009-06-02  3633   5303700             2009-06-03  2311  11643800
2009-06-01  3616   5625500             2009-06-02  2280  14249900
2009-05-29  3514  13038600   --AND--   2009-06-01  2252  11687200
2009-05-28  3395   7917600             2009-05-29  2202  22350700
2009-05-27  3513   4701100             2009-05-28  2163   9679800
2009-05-26  3545   4572700             2009-05-27  2174   9338200
2009-05-25  3480   3652500             2009-05-26  2164   8502900


Problem is  if I run this: 

keys = ['CBAAX' 'NABAX']
mv = pandasconcat([data['CBAAX'][650:660] data['NABAX'][650:660]]  axis=1  keys=stocks ) 


the following DateFrame is produced:

                                 CBAAX          NABAX        
                              Close  Volume   Close  Volume
Date                                                      
2200-08-16 04:24:21460041     NaN     NaN     NaN     NaN
2203-05-13 04:24:21460041     NaN     NaN     NaN     NaN
2206-02-06 04:24:21460041     NaN     NaN     NaN     NaN
2208-11-02 04:24:21460041     NaN     NaN     NaN     NaN
2211-07-30 04:24:21460041     NaN     NaN     NaN     NaN
2219-10-16 04:24:21460041     NaN     NaN     NaN     NaN
2222-07-12 04:24:21460041     NaN     NaN     NaN     NaN
2225-04-07 04:24:21460041     NaN     NaN     NaN     NaN
2228-01-02 04:24:21460041     NaN     NaN     NaN     NaN
2230-09-28 04:24:21460041     NaN     NaN     NaN     NaN
2238-12-15 04:24:21460041     NaN     NaN     NaN     NaN


Does anybody have any idea why this might be the case?

On another point: is there any python libraries around that pull data from yahoo and normalise it?

Cheers

EDIT: For reference: 

data = {
'CBAAX': 
    DatetimeIndex: 2313 entries  2011-12-29 00:00:00 to 2003-01-01 00:00:00
    Data columns:
        Close     2313  non-null values
        Volume    2313  non-null values
    dtypes: float64(1)  int64(1) 

 'NABAX': 
    DatetimeIndex: 2329 entries  2011-12-29 00:00:00 to 2003-01-01 00:00:00
    Data columns:
        Close     2329  non-null values
        Volume    2329  non-null values
    dtypes: float64(1)  int64(1)
}
",515311,,515311.0,2012-07-30 02:48:09,2012-11-27 21:57:44,concat pandas DataFrame along timeseries indexes,<python><numpy><scipy><pandas><yahoo-finance>,2.0,4.0,1.0,
11824341,1,11826408.0,2012-08-06 07:47:34,0,384,"I'm getting an error using pandas' date_range function I've given the trace below  and can provide more context  but it seems like something I'm really going to have to dig into myself to solve 

So what I'd like to know is if there's a way to get at the source for the files mentioned in the trace  specifically datetimepyx  or wherever the Timestamp class is defined - I can't seem to find it I'm pretty green with Python and don't know my way around packages and sources very well  but I'm hoping that if I can dig a little deeper I'll be able to shed some light on this

>>> d=process_files(args  options  False)
Processing file K2csv
Traceback (most recent call last):
  File """"  line 1  in 
  File ""/tmp/py7041Jtppy""  line 158  in process_files
  File ""/tmp/py7041Jtppy""  line 81  in process_csv
  File ""/usr/local/lib/python27/dist-packages/pandas-081-py27-linux-x86_64egg/pandas/tseries/indexpy""  line 1317  in date_range
    freq=freq  tz=tz  normalize=normalize  name=name)
  File ""/usr/local/lib/python27/dist-packages/pandas-081-py27-linux-x86_64egg/pandas/tseries/indexpy""  line 176  in __new__
    tz=tz  normalize=normalize)
  File ""/usr/local/lib/python27/dist-packages/pandas-081-py27-linux-x86_64egg/pandas/tseries/indexpy""  line 254  in _generate
    start = Timestamp(start)
  File ""datetimepyx""  line 87  in pandaslibTimestamp__new__ (pandas/src/tseriesc:26892)
  File ""datetimepyx""  line 511  in pandaslibconvert_to_tsobject (pandas/src/tseriesc:32019)
  File ""datetimepyx""  line 640  in pandaslib_string_to_dts (pandas/src/tseriesc:33452)
",1325447,,,,2012-08-07 18:40:35,Pandas: date_range error,<python><date><pandas>,5.0,2.0,,
11795992,1,,2012-08-03 12:44:23,2,203,"I have a Pandas dataframe with columns as such:

event_id  obj_0_type  obj_0_foo  obj_0_bar  obj_1_type  obj_1_foo  obj_1_bar  obj_n_type  obj_n_foo  obj_n_bar  

For example:

col_idx = ['event_id']
[col_idxextend(('obj_%d_id' %d  'obj_%d_foo' %d  'obj_%d_bar' %d)) for d in range(5)]
event_id = nparray(range(0 5))
data = nprandomrand(15 5)
data = npvstack((event_id  data))
df = DataFrame(dataT  index = range(5)  columns = col_idx)


I would like to split each individual row of the dataframe so that I'd have a single entry per object  as such:

event_id  obj_type  obj_foo  obj_bar

Where event_id would be shared among all the objects of a given event

There are lots of very slow ways of doing it (iterating over the dataframe rows and creating new series objects) but those are atrociously slow and obviously unpythonic  Is there a simpler way I am missing?",1574073,,,,2012-08-08 02:28:44,Pandas: Efficiently splitting entries,<python><numpy><pandas>,1.0,,,
11858472,1,,2012-08-08 05:57:33,3,333,"I have a following DataFrame:

from pandas import *
df = DataFrame({'foo':['a' 'b' 'c']  'bar':[1  2  3]})


It looks like this:

    bar foo
0    1   a
1    2   b
2    3   c


Now I want to have something like:

     bar
0    1 is a
1    2 is b
2    3 is c


How can I achieve this?
I tried the following:

df['foo'] = '%s is %s' % (df['bar']  df['foo'])


but it gives me a wrong result:

>>>print dfix[0]

bar                                                    a
foo    0    a
1    b
2    c
Name: bar is 0    1
1    2
2
Name: 0


Sorry for a dumb question  but this one pandas: combine two columns in a DataFrame wasn't helpful for me",1583620,,,,2012-08-08 23:15:47,Pandas: Combine string and int columns,<python><numpy><data.frame><pandas>,2.0,,2.0,
11870058,1,,2012-08-08 17:36:35,0,128,"With a DataFrame  you can output Series when using DataFrameapply to generate a new DataFrame with new columns

          a         b         c
0 -0119342  0286710  0266750
1 -1514301  0556106 -2743888
2 -0156469 -0352915 -1963398
3  1165479  1364303  0648178
4  1541738  0714239 -1468896

def f(x):
    return pandasSeries([ x['a']+x['b']  x['b'] + x['c']  x['a'] + x['c'] ]  index=['ab'  'bc'  'ac'])

In [52]: dfapply(f  axis=1)
Out[52]: 
         ab        bc        ac
0  0167368  0553460  0147408
1 -0958195 -2187782 -4258188
2 -0509384 -2316313 -2119867
3  2529782  2012481  1813658
4  2255977 -0754657  0072842


Attempting to output new DataFrame objects instead of Series objects results in stacking objects rather than creating a cohesive DataFrame

In [53]: def f(x):
    return pandasDataFrame([[ x['a']+x['b']  x['b'] + x['c']  x['a'] + x['c'] ]]  columns=['ab'  'bc'  'ac'])
   : 

In [54]: dfapply(f  axis=1)
Exception ValueError: ValueError('Cannot call bool() on DataFrame' ) in 'util_checknull' ignored
Exception ValueError: ValueError('Cannot call bool() on DataFrame' ) in 'util_checknull' ignored
Exception ValueError: ValueError('Cannot call bool() on DataFrame' ) in 'util_checknull' ignored
Exception ValueError: ValueError('Cannot call bool() on DataFrame' ) in 'util_checknull' ignored
Exception ValueError: ValueError('Cannot call bool() on DataFrame' ) in 'util_checknull' ignored
Exception ValueError: ValueError('Cannot call bool() on DataFrame' ) in 'util_checknull' ignored
Exception ValueError: ValueError('Cannot call bool() on DataFrame' ) in 'util_checknull' ignored
Exception ValueError: ValueError('Cannot call bool() on DataFrame' ) in 'util_checknull' ignored
Exception ValueError: ValueError('Cannot call bool() on DataFrame' ) in 'util_checknull' ignored
Exception ValueError: ValueError('Cannot call bool() on DataFrame' ) in 'util_checknull' ignored
Out[54]: 
0             ab       bc        ac
0  0167368  0553
1             ab        bc        ac
0 -0958195 -218
2             ab        bc        ac
0 -0509384 -231
3             ab        bc        ac
0  2529782  201
4             ab        bc        ac
0  2255977 -075


Is there a way to output DataFrames (or multiple Series) that can be stacked the way outputting single Series objects are?",128580,,,,2012-08-09 12:51:28,pandas: stacking DataFrames generated by apply,<pandas>,1.0,,,
11794584,1,,2012-08-03 11:11:12,0,62,"I have a pandas Series (DivFactor) I would like to calculate using cumprod It depends upon two other given series: AdjClose and Div

Here's my current loop:

DivFactor[0] = 1
for t in range(1  periods):
    DivFactor[t] = DivFactor[t-1] - (Div[t-1] * DivFactor[t-1]) /
                                     (AdjClose[t] / DivFactor[t-1] + Div[t-1]))


The issue is that the formula seems to be hard to express in terms of a cumulative product

Is there a way to recast DivFactor so that I can use pandas cumprod (or another cumulative function) and avoid using a Python loop?",290443,,,,2012-08-08 02:35:09,pandas - cumprod when prior index is in denominator and numerator,<python><pandas>,1.0,,,
11871152,1,11876415.0,2012-08-08 18:49:06,-1,231,"This question is somehow a continuation of this one I've been able to correctly takes what I'm interested in a downloadable csv file as follow

import time
import urllib2
import csv
import sys
import pandas
response=urllib2urlopen('http://wwweuribor-ebfeu/assets/modules/rateisblue/processed_files/hist_EURIBOR_2012csv')
localFile = open('filecsv'  'w')
localFilewrite(responseread())
localFileclose()
df2=pandasioparsersread_csv('filecsv' index_col = 0  parse_dates = True  dayfirst = True)[:15]transpose()[:200] ## transpose in order to be compatible with pandas dataframe
df2 = df2dropna() ## drop the values which are not-a-number
eur3m = df2['3m']


Now eur3m is a Series in Pandas and I would like to have information on a given time period I know I can generate daterange with DateRange What I would basically like to do is the have for example statics over 1 month period (let's say mean and std in the period from 1 of July 2012 and 31 of July 2012) For some reasons  although I read the csv file trying to parse the date considering that these dates are in european format (DD/MM/YYYY) I'm not able to follow this example Let's say trying something like

day=eur3mindex
i = ((day >= '01/07/2012') & (day ",462901,,,,2012-08-09 03:34:22,Handle with European date format in python pandas,<python><numpy><pandas>,1.0,,,
11888599,1,11888784.0,2012-08-09 17:15:40,1,95,"I would like to partially ""collapse"" a matrix and keep the  structure intact by just summing the condensed values  For example  I have this:

CHROM     POS     GENE     DESC     JOE      FRED   BILLY    SUSAN    TONY
10        1442    LOXL4    bad      1        0      0        1        0
10        335     LOXL4    bad      1        0      0        0        0
10        3438    LOXL4    good     0        0      1        0        0
10        4819    PYROXD2  bad      0        1      0        0        0
10        4829    PYROXD2  bad      0        1      0        1        0
10        9851    HPS1     good     1        0      0        0        0


The first 4 columns are descriptors  and the last 4 columns are people/observations  The end goal is to count how many total ""good"" and ""bad"" observations per GENE per person  Thus  I want this:

GENE     DESC     JOE      FRED   BILLY    SUSAN    TONY
LOXL4    bad      2        0      0        1        0
LOXL4    good     0        0      1        0        0
PYROXD2  bad      0        2      0        1        0
HPS1     good     1        0      0        0        0


The following code collapses all the individual observations (Joe  Fred  etc)  how can I keep them separate?  I would also like to be flexible enough to accommodate a more individuals in the future (keeping the same 4 descriptor columns)

mytablegroupby(['GENE' 'DESC'])size()
",1062520,,,,2012-08-14 05:39:22,Python pandas to partially collapse 2d matrix,<python><grouping><pivot-table><pandas><crosstab>,1.0,,,
11706826,1,,2012-07-29 05:04:18,0,119,"I downloaded statsmodels and ran the exe file to install the module Then I did import statsmodelsapi as sm and I got an error saying I needed pandas  so I got this module and downloaded it Now when I run import statsmodelsapi as sm I get an error saying: 

import statsmodelstoolsdata as data_util

""Atribute error: 'module' object has no attribute 'data'


Now the weird thing is that I can run import statsmodels and import statsmodelstoolsdata and it works  but i cannot do import statsmodelsapi",1560517,,1301710.0,2012-08-09 19:24:10,2012-08-09 19:24:10,Trouble Importing Statsmodels.api (python),<python><module><pandas><statsmodels>,,1.0,,
11848578,1,11904292.0,2012-08-07 14:50:42,1,273,"I'm creating a data frame in Pandas

df_data = dict()

for x in data:
    series = pandasSeries(x['value']['values']  index=x['value']['timestamps'])

    df_data[x['_id']] = series

df = pandasDataFrame(df_data)


data is a list of dicts in the format

{u'_id': u'770000000049' 
 u'value': {u'timestamps': [datetimedatetime(2012  7  25  10  16  1  270000) 
                            datetimedatetime(2012  7  25  10  18  29  745000) 
                            datetimedatetime(2012  7  25  10  21  54  931000) 
                            datetimedatetime(2012  7  25  10  23  18  896000)] 
            u'values': [2040  16788  1392  116004]}}


Printing an example series gives me

>>> print df_data['770000000049']

>>> 2012-07-25 10:16:01270000    204000
2012-07-25 10:18:29745000     16788
2012-07-25 10:21:54931000    139200
2012-07-25 10:23:18896000    116004


As expected However  printing the resulting data frame gives me

>>> print df['770000000049']

>>> 1992-06-05 15:50:11527680   NaN
2181-10-17 22:55:34850625   NaN
2215-08-27 21:41:15306049   NaN
1936-05-22 00:55:45848401   NaN
1783-06-08 06:38:26257076   NaN
2017-03-12 18:30:17469108   NaN
2209-08-06 03:45:09779652   NaN
1768-02-06 12:00:22653272   NaN
1916-07-20 06:51:31628376   NaN
2086-01-25 18:30:58261336   NaN
1940-08-26 15:13:33790568   NaN
1712-12-17 22:48:01743241   NaN
1803-06-16 16:32:58309017   NaN
1981-11-05 04:38:27140059   NaN
2246-05-25 09:09:27875035   NaN



WTF! The data is all wrong Both keys and values are completely wrong

What am I doing wrong? 

Edit: Printing df gives me

DatetimeIndex: 386 entries  1992-06-05 15:50:11527680 to 1774-08-13 02:00:15237103
Data columns:
770000000006    0  non-null values
770000000009    0  non-null values
770000000010    0  non-null values
770000000011    0  non-null values
770000000012    0  non-null values
770000000013    0  non-null values
770000000018    0  non-null values
770000000020    0  non-null values
770000000021    0  non-null values
770000000022    0  non-null values
770000000024    0  non-null values
770000000029    0  non-null values
770000000030    0  non-null values
770000000032    0  non-null values
770000000034    0  non-null values
770000000049    0  non-null values
dtypes: float64(16)


Completely wrong

Edit 2:

I've written a module that reproduces the bug for me",1569050,,1569050.0,2012-08-07 17:37:53,2012-08-10 15:30:34,Pandas messing up data frame,<python><numpy><pandas>,2.0,3.0,,
11855203,1,,2012-08-07 22:46:22,0,109,"I'm using pandas in a Django application running on Ubuntu 1204 and after upgrading from pandas 061 to 081 I'm getting some errors that completely prevent the application from running

If I run Django's development server (managepy runserver)  everything works fine  but if instead I run uWSGI I see the following 2 tracebacks in my uwsgi log:

Traceback (most recent call last):
  File ""/usr/local/lib/python27/dist-packages/django/core/handlers/basepy""  line 101  in get_response
    requestpath_info)
  File ""/usr/local/lib/python27/dist-packages/django/core/urlresolverspy""  line 298  in resolve
    for pattern in selfurl_patterns:
  File ""/usr/local/lib/python27/dist-packages/django/core/urlresolverspy""  line 328  in url_patterns
    patterns = getattr(selfurlconf_module  ""urlpatterns""  selfurlconf_module)
  File ""/usr/local/lib/python27/dist-packages/django/core/urlresolverspy""  line 323  in urlconf_module
    self_urlconf_module = import_module(selfurlconf_name)
  File ""/usr/local/lib/python27/dist-packages/django/utils/importlibpy""  line 35  in import_module
    __import__(name)
  File ""/sites/ycharts/urlspy""  line 5  in 
    from appscompaniessitemaps import CompanySitemap
  File ""/sites/ycharts/apps/companies/sitemapspy""  line 6  in 
    from appscompaniesmodels import Company
  File ""/sites/ycharts/apps/companies/modelspy""  line 19  in 
    from appsmainutils import googlesearch  wikipedia  date_utils  data_utils  \
  File ""/sites/ycharts/apps/main/utils/data_utilspy""  line 3  in 
    import pandas
  File ""/usr/local/lib/python27/dist-packages/pandas/__init__py""  line 14  in 
    import pandaslib as lib
AttributeError: 'module' object has no attribute 'lib'


and 

Traceback (most recent call last):
  File ""/usr/local/lib/python27/dist-packages/django/core/handlers/basepy""  line 101  in get_response
    requestpath_info)
  File ""/usr/local/lib/python27/dist-packages/django/core/urlresolverspy""  line 298  in resolve
    for pattern in selfurl_patterns:
  File ""/usr/local/lib/python27/dist-packages/django/core/urlresolverspy""  line 328  in url_patterns
    patterns = getattr(selfurlconf_module  ""urlpatterns""  selfurlconf_module)
  File ""/usr/local/lib/python27/dist-packages/django/core/urlresolverspy""  line 323  in urlconf_module
    self_urlconf_module = import_module(selfurlconf_name)
  File ""/usr/local/lib/python27/dist-packages/django/utils/importlibpy""  line 35  in import_module
    __import__(name)
  File ""/sites/ycharts/urlspy""  line 5  in 
    from appscompaniessitemaps import CompanySitemap
  File ""/sites/ycharts/apps/companies/sitemapspy""  line 6  in 
    from appscompaniesmodels import Company
  File ""/sites/ycharts/apps/companies/modelspy""  line 19  in 
    from appsmainutils import googlesearch  wikipedia  date_utils  data_utils  \
  File ""/sites/ycharts/apps/main/utils/data_utilspy""  line 3  in 
    import pandas
  File ""/usr/local/lib/python27/dist-packages/pandas/__init__py""  line 28  in 
    from pandascoreapi import *
  File ""/usr/local/lib/python27/dist-packages/pandas/core/apipy""  line 10  in 
    from pandascoreformat import (set_printoptions  reset_printoptions 
  File ""/usr/local/lib/python27/dist-packages/pandas/core/formatpy""  line 147  in 
    class DataFrameFormatter(object):
  File ""/usr/local/lib/python27/dist-packages/pandas/core/formatpy""  line 156  in DataFrameFormatter
    __doc__ += docstring_to_string
TypeError: unsupported operand type(s) for +=: 'NoneType' and 'str


After reading in this question  I thought it may be a path issue so I tried overwriting syspath in my uwsgi configuration file so that it's the exact same path that the dev server sets up for me  but that didn't help

Any tips / debugging ideas / help on the above would be greatly appreciated

Thank you!

ps I opened up an issue for this on github as well: https://githubcom/pydata/pandas/issues/1741",931775,,,,2012-08-07 22:46:22,Pandas 0.8.1. error when run in Django + uWSGI but everything works fine with Django's dev server,<python><django><pandas><uwsgi>,,,,
11889474,1,11889488.0,2012-08-09 18:12:21,3,428,"I have a DataFrame with a mix of 0's and other numbers  I would like to convert the 0's to missing  

For example  I am looking for the command that would convert

In [618]: a=DataFrame(data=[[1 2] [0 1] [1 2] [0 0]])

In [619]: a
Out[619]: 
   0  1
0  1  2
1  0  1
2  1  2
3  0  0


to

In [619]: a
Out[619]: 
   0   1
0  1   2
1  NaN 1
2  1   2
3  NaN NaN


I tried pandasreplace(0  NaN)  but I get an error that NaN is not defined  And I don't see anywhere to import NaN from",670525,,,,2012-08-09 18:55:51,Converting data to missing in pandas,<python><numpy><pandas>,1.0,,,
11787883,1,11788556.0,2012-08-03 00:55:14,3,140,"I have a DataFrame that represents stock returns To split adjust the closing price  I have the following method:

def returns(ticker  start=None  end=None):
    p = historical_prices(ticker  start  end  data='d'  convert=True)
    d = historical_prices(ticker  start  end  data='v'  convert=True)

    p['Dividends'] = d['Dividends']
    p['Dividends']fillna(value=0  inplace=True)
    p['DivFactor'] = 1
    p['SAClose'] = p['Close']

    records  fields = pshape
    for t in range(1  records):
        p['SAClose'][t] = p['Adj Close'][t] / p['DivFactor'][t-1] + \
                          p['Dividends'][t-1]
        p['DivFactor'][t] = p['DivFactor'][t-1] * \
                            (1 - p['Dividends'][t-1] / p['SAClose'][t])

    p['Lagged SAClose'] = p['SAClose']shift(periods=-1)
    p['Cash Return'] = p['Dividends'] / p['Lagged SAClose']
    p['Price Return'] = p['SAClose'] / p['Lagged SAClose'] - 1
    return psort_index()


Note how SAClose (ie Split Adjusted Close) depends upon lagged DivFactor values In turn  DivFactor depends on both lagged DivFactor values as well as the current SAClose value

The method above works  but it is incredibly slow in the loop section Is there a more efficient way for me to do this in pandas? Given the ""circular"" dependency (not really circular given the lags)  I'm not sure how I could do either regular series math or use normal shift operations (eg as I do with Cash Return)",290443,,,,2012-08-03 02:47:01,pandas - More efficient way to calculate two Series with circular dependencies,<python><pandas>,1.0,,1.0,
11811392,1,11811425.0,2012-08-04 19:25:34,2,401,"I have a pandas dataframe object that looks like this:

   one  two  three  four  five
0    1    2      3     4     5
1    1    1      1     1     1


I'd like to generate a list of lists objects where the first item is the column label and the remaining list values are the column data values:

nested_list = [['one'  1  1]
               ['two'  2  1]
               ['three'  3  1]
               ['four'  4  1]
               ['five'  5  1]]


How can I do this? Thanks for the help",1255817,,,,2012-08-04 23:02:21,How to generate a list from a pandas Data Frame with the column name and column values?,<python><pandas>,2.0,,,
11918342,1,,2012-08-11 23:01:25,1,290,"I have stock ticker data in the following format:

4028965972
4028966319
4028966667


and Excel is able to magically convert them to:

4/22/14 3:50 PM
4/22/14 3:55 PM
4/22/14 4:00 PM


via ""Format Cells""

How do I do the same conversion in Pandas?",190894,,1070354.0,2012-08-11 23:10:40,2012-08-12 02:14:37,Converting date/time in Pandas,<python><pandas>,1.0,1.0,1.0,
11920721,1,11921678.0,2012-08-12 08:12:11,4,496,"I have a dataframe that looks like the following:

In [74]: data2

Out[74]: 
            a  b  c

2012-06-12  0  1  1
2012-06-13  1  1  0
2012-06-14  1  0  1
2012-06-15  1  0  1
2012-06-16  1  1  0
2012-06-17  1  0  1


Is there a way to make the values = the column heading where the value = 1?

Result df:

            a  b  c

2012-06-12  0  b  c
2012-06-13  a  b  0
2012-06-14  a  0  c
2012-06-15  a  0  c
2012-06-16  a  b  0
2012-06-17  a  0  c


And then remove the values that = 0 such that the df reduces to 2 columns:
(column heading is not relevant at this point)

Result df:

            1  2  
2012-06-12  c  b  
2012-06-13  a  b  
2012-06-14  a  c  
2012-06-15  a  c  
2012-06-16  a  b  
2012-06-17  a  c  
",1593147,,1301710.0,2012-08-12 10:04:15,2012-08-16 02:18:09,pandas dataframe - change values based on column heading,<python><pandas>,2.0,0.0,,
11940420,1,11950214.0,2012-08-13 18:47:16,3,117,"I have a DataFrame of financial data:

              open    high     low   close     volume
date                                                 
2012-02-13  03476  03592  03449  03530  105990679
2012-02-14  03470  03528  03409  03429  131799968
2012-02-15  03453  03513  03365  03393  119421442
2012-02-16  03358  03438  03271  03438  123189697
2012-02-17  03488  03588  03464  03546  167932148
2012-02-20  03590  03682  03577  03634  127657258
2012-02-21  03630  03675  03524  03575  137016196


which I am currently grouping as:

agg = {'open': lambda s: s[0] 
       'high': lambda s: smax() 
       'low': lambda s: smin() 
       'close': lambda s: s[-1] 
       'volume': lambda s: ssum()}


through

dfgroupby(lambda d: disocalendar()[:2])agg(agg)


This works well except for the fact that my data is now indexed by tuples of (year  week)  I wish for the data to be indexed by the date of the earliest member of the group  My current hack is along the lines of:

agg['date'] = lambda s: s[0]
df2 = dfcopy()
df2['date'] = df2index
df2groupby(lambda d: disocalendar()[:2])agg(agg)set_index('date')


which seems to work however I am wondering if there is a means of separating grouping and indexing so that the group keys do not automatically become the frame index",315974,,,,2012-08-15 13:01:19,Pandas: Separating grouping and indexing,<python><pandas>,1.0,,1.0,
11887504,1,,2012-08-09 16:09:31,3,198,"this is my first post here :) ! I am trying to add seconds to python datetime excluding weekends using pandas The code below works  but I would like to know if there is a simpler way to achieve this

import datetime
from pandas import *
from pandastseriesoffsets import *

def add_seconds(start_date  offset_in_seconds):
# get input date in datetime
d = datetimestrptime(start_date  '%Y-%m-%d %H:%M:%S')

# get days  hours  mins  secs
    no_of_days  remainder = divmod(offset_in_seconds  86400)
    hours  minutes = divmod(remainder  3600)
    minutes  seconds = divmod(minutes  60)

# increment the input date  to the appropriate business day
    end_date_pre = d + no_of_days*BDay() 

# dial back to previous evening if hour is under 24
    if 16 + hours = 24:
    end_date = end_date_pre + 1*BDay()
    new_end_date = datetime(end_dateyear  end_datemonth  end_dateday 9  0  0)
    return start_date  end_date  new_end_datestrftime('%Y-%m-%d %H:%M:%S')
    else:
    return start_date  end_date  end_datestrftime('%Y-%m-%d %H:%M:%S')
",1588032,,,,2012-08-09 18:12:29,Add seconds to python datetime excluding weekends,<python><datetime><pandas>,1.0,1.0,1.0,
11941492,1,11942697.0,2012-08-13 20:07:07,5,1022,"I'm suspicious that this is trivial  but I yet to discover the incantation that will let me select rows from a Pandas dataframe based on the values of a hierarchical key So  for example  imagine we have the following dataframe:

import pandas
df = pandasDataFrame({'group1': ['a' 'a' 'a' 'b' 'b' 'b'] 
                       'group2': ['c' 'c' 'd' 'd' 'd' 'e'] 
                       'value1': [11 2 3 4 5 6] 
                       'value2': [71 8 9 10 11 12]
})
df = dfset_index(['group1'  'group2'])


df looks as we would expect: 



If df were not indexed on group1 I could do the following:

df['group1' == 'a']


But that fails on this dataframe with an index So maybe I should think of this like a Pandas series with a hierarchical index:

df['a' 'c']


Nope That fails as well 

So how do I select out all the rows where:

group1 == 'a'
group1 == 'a' & group2 == 'c'
group2 == 'c'
group1 in ['a' 'b' 'c']
",37751,,37751.0,2012-08-13 20:40:50,2012-08-13 21:37:08,Selecting rows from a Pandas dataframe with a compound (hierarchical) index,<python><ipython><pandas>,2.0,,2.0,
11945897,1,12188163.0,2012-08-14 04:43:43,0,234,"My error relates to datetime conversion on constructing a Pandas DataFrame after I upgraded to pandas 081  Specifically  the *from_records()* method passing a Django Queryset with timezone aware datetimes throws a datetime64 conversion error:

type(data)
> djangodbmodelsqueryQuerySet

dv = datavalues('begin_time'  'datum')
dv[0]
> {'begin_time': datetimedatetime(2006  4  27  12  40  tzinfo=) 
   'datum': Decimal('133097500')}

df = pdDataFramefrom_records( dv  index='begin_time' )
> ValueError: Tz-aware datetimedatetime cannot be converted to datetime64
  unless utc=True


The Django datetime is timezone aware and looks to me that it is set to UTC  so I find the error message a little cryptic  but that is probably my failing   I'm relatively knew to Python

So  I'm thinking that either I need to:
1 Somewhere set utc=true as the error says but I don't know where to exactly or
2 Switch off conversion to datetime64 in from_records  but I also don't know where

Thanks for your help!",1555255,,497043.0,2012-08-30 00:50:49,2012-08-30 00:50:49,DataFrame.from_records convert (Django) datetime to datetime64: Value Error,<python><django><datetime><pandas>,2.0,,,
11925827,1,12090798.0,2012-08-12 20:57:10,3,678,"I have a simple use case for dfto_excel() that I'm struggling with I want to write to a specific worksheet tab (let's call it ""Data"") of an existing XLSX workbook  which could be referenced by formulas and pivots on other tabs 

I've tried to modify ExcelWriter in two ways but both produce errors from openpyxl

Read an existing sheet using get_sheet_by_name (This errors: ""NotImplementedError: use 'iter_rows()' instead"")
Create a new sheet using create_sheet (This errors:""ReadOnlyWorkbookException: Cannot create new sheet in a read-only workbook"")

df=DataFrame()
from openpyxlreaderexcel import load_workbook
book = load_workbook('my_excel_filexlsx'  use_iterators=True) # Assume my_excel_filexlsx contains a sheet called 'Data'
class temp_excel_writer(ExcelWriter): # I need this to inherit the other methods of ExcelWriter in io/parserspy
def __init__(self  path  book):
    selfbook=book
    test_sheet=selfbookcreate_sheet(title='Test') # This errors: ReadOnlyWorkbookException
    selfuse_xlsx = True
    selfsheet_names=selfbookget_sheet_names()
    selfactual_sheets=selfbookworksheets
    selfsheets={}
    for i j in enumerate(selfsheet_names):
      selfsheets[j] = (selfactual_sheets[i] 1)
    selfcur_sheet = None
    selfpath = save
my_temp_writer=temp_excel_writer('my_excel_filexlsx'  book)
dfto_excel(my_temp_writer  sheet_name='Data')

Any thoughts? Am I missing something obvious? I'm still in pandas 72",1037869,,,,2012-08-23 11:44:36,pandas: Writing to an existing excel file (xlsx) using to_excel,<python><pandas><openpyxl>,1.0,,,
11970820,1,11971139.0,2012-08-15 14:09:13,3,290,"I have a relatively simple python multiprocessing script that sets up a pool of workers that append output to a pandas dataframe by way of a custom manager What I am finding is when I call close()/join() on the pool  not all the tasks submitted by apply_async are being completed

Here's a simplified example that submits 1000 jobs but only half complete causing an assertion error Have I overlooked something very simple or is this perhaps a bug?

from pandas import DataFrame
from multiprocessingmanagers import BaseManager  Pool

class DataFrameResults:
    def __init__(self):
        selfresults = DataFrame(columns=(""A""  ""B"")) 

    def get_count(self):
        return selfresults[""A""]count()

    def register_result(self  a  b):
        selfresults = selfresultsappend([{""A"": a  ""B"": b}]  ignore_index=True)

class MyManager(BaseManager): pass

MyManagerregister('DataFrameResults'  DataFrameResults)

def f1(results  a  b):
    resultsregister_result(a  b)

def main():
    manager = MyManager()
    managerstart()
    results = managerDataFrameResults()

    pool = Pool(processes=4)

    for (i) in range(0  1000):
        poolapply_async(f1  [results  i  i*i])
    poolclose()
    pooljoin()

    print resultsget_count()
    assert resultsget_count() == 1000

if __name__ == ""__main__"":
    main()
",370696,,,,2012-08-16 09:02:26,"python multiprocessing, pool workers do not complete all tasks",<python><multiprocessing><data.frame><manager><pandas>,1.0,,,
11973741,1,,2012-08-15 17:04:17,1,255,"I'm struggling with hierarchical indexes in the Python pandas package  Specifically I don't understand how to filter and compare data in rows after it has been pivoted  

Here is the example table from the documentation:  

import pandas as pd
import numpy as np

In [1027]: df = pdDataFrame({'A' : ['one'  'one'  'two'  'three'] * 6 
                             'B' : ['A'  'B'  'C'] * 8 
                             'C' : ['foo'  'foo'  'foo'  'bar'  'bar'  'bar'] * 4 
                             'D' : nprandomrandn(24) 
                             'E' : nprandomrandn(24)})

In [1029]: pdpivot_table(df  values='D'  rows=['A'  'B']  cols=['C'])
Out[1029]: 
    C             bar       foo
    A     B                    
    one   A -1154627 -0243234
          B -1320253 -0633158
          C  1188862  0377300
    three A -1327977       NaN
          B       NaN -0079051
          C -0832506       NaN
    two   A       NaN -0128534
          B  0835120       NaN
          C       NaN  0838040


I would like to analyze as follows:

1) Filter this table on column attributes  for example selecting rows with negative 'foo':

    C             bar       foo
    A     B                    
    one   A -1154627 -0243234
          B -1320253 -0633158
    three B       NaN -0079051
    two   A       NaN -0128534


2) Compare the remaining B series values between the distinct A series groups?  I'm not sure how to access this information: {'one':['A' 'B']  'two':['A']  'three':['B']} and determine which series B values are unique to each key  or seen in multiple key groups  etc

Is there a way to do this directly within the pivot table structure  or do I need to convert this back in to a panda data frame?

Thank you

edit:  I think this code is a step in the right direction  It at least lets me access individual values within this table  but I am still hard-coding the series vales:

table = pivot_table(df  values='D'  rows=['A'  'B']  cols=['C'])
tableix['one'  'A']
",1062520,,1062520.0,2012-08-15 20:02:47,2012-09-09 21:15:28,Filtering and selecting from pivot tables made with python pandas,<python><indexing><pivot><pivot-table><pandas>,1.0,,,
11971381,1,11972498.0,2012-08-15 14:39:44,1,184,"In a very general sense  the problem I am looking to solve is changing one component of a multi-level index into columns  That is  I have a Series that contains a multilevel index and I want the lowest level of the index changed into columns in a dataframe  Here is the actual example problem I'm trying to solve 

Here we can generate some sample data:

foo_choices = [""saul""  ""walter""  ""jessee""]
bar_choices = [""alpha""  ""beta""  ""foxtrot""  ""gamma""  ""hotel""  ""yankee""]

df = DataFrame([{""foo"":randomchoice(foo_choices)  
                 ""bar"":randomchoice(bar_choices)} for _ in range(20)])
dfhead()


which gives us 

     bar     foo
0    beta    jessee
1    gamma   jessee
2    hotel   saul
3    yankee  walter
4    yankee  jessee



Now  I can groupby bar and get value_counts of the foo field 

dfgb = dfgroupby('foo')
dfgb['bar']value_counts()


and it outputs 

foo            
jessee  hotel      4
        gamma      2
        yankee     1
saul    foxtrot    3
        hotel      2
        gamma      1
        alpha      1
walter  hotel      2
        gamma      2
        foxtrot    1
        beta       1


But what I want is something like 

          hotel    beta    foxtrot    alpha    gamma    yankee
foo                        
jessee     1       1       5          4        1        1
saul       0       3       0          0        1        0
walter     1       0       0          1        1        0


My solution was to write the following bit:

for v in df['bar']unique():
    if v is npnan: continue
    df[v] = npnan
    dfix[df['bar'] == v  v] = 1

dfgb = dfgroupby('foo')
dfgbcount()[df['bar']unique()]
",154508,,,,2012-08-15 15:40:50,MultiLevel index to columns : getting value_counts as columns in pandas,<python><ipython><pandas>,1.0,,2.0,
11979194,1,11982602.0,2012-08-16 00:39:34,2,123,"I am trying to create a subclass of a Panda data structure to substitute  in my code  a subclass of a dict with a subclass of a Series  I don't understand why this example code doesn't work

from pandas import Series    

class Support(Series):
    def supportMethod1(self):
        print 'I am support method 1'       
    def supportMethod2(self):
        print 'I am support method 2'

class Compute(object):
    supp=None        
    def test(self):
        selfsupp()  

class Config(object):
    supp=None        
    @classmethod
    def initializeConfig(cls):
        clssupp=Support()
    @classmethod
    def setConfig1(cls):
        Computesupp=clssuppsupportMethod1
    @classmethod
    def setConfig2(cls):
        Computesupp=clssuppsupportMethod2            

ConfiginitializeConfig()

ConfigsetConfig1()    
c1=Compute()
c1test()

ConfigsetConfig2()    
c1test()


Probably it is not the best method to change the configuration of some objects  anyway I found this usefull in my code and most of all I want to understand why with dict instead of series it works as I expect

Thanks a lot!",1601780,,,,2012-08-16 07:37:02,subclasses of pandas' object work differently from subclass of other object?,<python><subclass><pandas><series>,2.0,,3.0,
12001043,1,,2012-08-17 06:51:01,0,164,"I have a working script on python 27 with pywin32 version 217 installed When I build using PyInstaller (in debug mode)  i get the following warnings:


WARNING: library python%s%s required via ctypes not found

WARNING: pythoncom is changing its name to pythoncom27
WARNING: pywintypes is changing its name to pywintypes27
WARNING: library gdiplusdll required via ctypes not found


When i run the executable (built in debug mode)  I get the following error(s):

C:\pyinstaller-20\TestModel\dist\TestModel>TestModelexe
_MEIPASS2 is NULL
archivename is C:\pyinstaller-20\TestModel\dist\TestModel\TestModelexe
No need to extract files to run; setting extractionpath to homepath
Already in the child - running!
manifestpath: C:\pyinstaller-20\TestModel\dist\TestModel\TestModelexemanifest
Activation context created
Activation context activated
C:\pyinstaller-20\TestModel\dist\TestModel\python27dll
Manipulating evironment
PYTHONPATH=C:/pyinstaller-20/TestModel/dist/TestModel
PYTHONHOME=C:/pyinstaller-20/TestModel/dist/TestModel/
importing modules from CArchive
extracted iu
extracted struct
extracted archive
Installing import hooks
out00-PYZpyz
Running scripts
Traceback (most recent call last):
  File """"  line 65  in 
  File ""C:\pyinstaller-20\PyInstaller\loader\iupy""  line 386  in importHook
    mod = _self_doimport(nm  ctx  fqname)
  File ""C:\pyinstaller-20\PyInstaller\loader\iupy""  line 480  in doimport
    exec co in mod__dict__
  File ""C:\pyinstaller-20\TestModel\build\pyiwin32\TestModel\out00-PYZpyz\win32com""  line 6  in 
  File ""C:\pyinstaller-20\PyInstaller\loader\iupy""  line 409  in importHook
    raise ImportError(""No module named %s"" % fqname)
ImportError: No module named pythoncom
RC: -1 from pyi_rth_win32comgenpy
OK
Deactivating activation context
Releasing activation context
Done


I have read that using version 212 might help  but it was not built for python 27  can someone help me with the error? I would appreciate the help a lot

I have realized that importing pandas was causing the error  if that helps someone to help me out here",1201501,,1201501.0,2012-08-18 02:31:18,2012-08-18 02:31:18,Import Error with pythoncom after building with PyInstaller,<pandas><importerror><pyinstaller><pythoncom>,,,,
11740587,1,,2012-07-31 12:54:59,0,86,I have two DataFrames  one of which is larger than the other (A) The labels on B are all contained within A I want to take the difference (A-B) for the row/column values that correspond Does anyone have any suggestions? ,1504276,,,,2012-07-31 15:15:06,pandas: taking corresponding differences,<python><pandas>,1.0,,1.0,
11794935,1,11819672.0,2012-08-03 11:35:21,1,503,"I have a Pandas DataFrame with four columns  A  B  C  D  It turns out that  sometimes  the values of B and C can be 0  I therefore wish to obtain the following:

B[i] = B[i] if B[i] else min(A[i]  D[i])
C[i] = C[i] if C[i] else max(A[i]  D[i])


where I have used i to indicate a run over all rows of the frame  With Pandas it is easy to find the rows which contain zero columns:

df[dfB == 0] and df[dfC == 0]


however I have no idea how to easily perform the above transformation  I can think of various inefficient and inelegant methods (for loops over the entire frame) but nothing simple",315974,,,,2012-08-05 20:07:32,Pandas DataFrame Apply,<python><pandas>,2.0,,2.0,
11836286,1,11836385.0,2012-08-06 21:33:08,1,119,"I'm trying to access the names of variables from the results generated by statsmodels I'll elaborate more after the example code 

import scikitsstatsmodelsapi as sm
import pandas as pd
data = smdatasetslongleyload()
df = pdDataFrame(dataexog  columns=dataexog_name)
y = dataendog
df['intercept'] = 1
olsresult = smOLS(y  df)fit()


This summary output includes the variable names When you call something like olsresultparams it returns the following:

In [21]: olsresultparams
Out[21]: 
GNPDEFL           15061872
GNP               -0035819
UNEMP             -2020230
ARMED             -1033227
POP               -0051104
YEAR            1829151465
intercept   -3482258634596


Now what I'm curious about doing is creating something like a dictionary with the variable name as a key and the parameter value as the value So  something like {'GNPDELF':150618  'GNP':-0035819} and so on If it's impossible to do this  is there any other way to access the variable name and value individually?",1074057,,1301710.0,2012-08-09 19:12:01,2012-08-09 19:12:01,Pulling variable names when using pandas and statsmodels,<python><pandas><statsmodels>,2.0,,,
11881165,1,11882354.0,2012-08-09 10:15:27,4,552,"I am working with survey data loaded from an h5-file as hdf = pandasHDFStore('Surveyh5') through the pandas package Within this DataFrame  all rows are the results of a single survey  whereas the columns are the answers for all questions within a single survey 

I am aiming to reduce this dataset to a smaller DataFrame including only the rows with a certain depicted answer on a certain question  ie with all the same value in this column I am able to determine the index values of all rows with this condition  but I can't find how to delete this rows or make a new df with these rows only",698207,,1315131.0,2012-08-16 13:01:47,2012-08-16 13:01:47,Slice Pandas DataFrame by Row,<python><pandas><slice>,1.0,,3.0,
12007406,1,12007574.0,2012-08-17 14:08:18,2,288,"I am absolutely new to Python and Panda and even though I have checked the documentation  I don't seem to understand the right way to index a Pandas DataFrame I would like to divide a DataFrame full of stock prices by their respective initial values in order to index the different stocks to 100 I want to use this to compare their performance The DataFrame looks like this:

>>> IndexPrices

DatetimeIndex: 157 entries  1999-12-31 00:00:00 to 2012-12-31 00:00:00
Freq: M
Data columns:
MSCI WORLD :G U$                        148  non-null values
S&P 500 COMPOSITE                       148  non-null values
DAX 30 PERFORMANCE                      148  non-null values
RUSSELL 2000                            148  non-null values
FTSE 100                                148  non-null values
US Treasury Bond Yields 30 Year Bond    148  non-null values
dtypes: float64(6)


So far I have messed around with stuff like this  but it's not getting me anywhere

IndexPricesdivide(IndexPrices[0:1])


Thanks for your help guys!",1190648,,,,2012-08-17 14:20:05,Python: Pandas Divide DataFrame by first row,<python><pandas>,1.0,,1.0,
12021730,1,12022047.0,2012-08-18 19:45:34,2,129,"I have a textfile where columns are separated by variable amounts of whitespace Is it possible to load this file directly as a pandas dataframe without pre-processing the file? In the pandas documentation the delimiter section says that I can use a 's*' construct but I couldn't get this to work 

## sample data
head sampletxt

#                                                                            --- full sequence --- -------------- this domain -------------   hmm coord   ali coord   env coord
# target name        accession   tlen query name           accession   qlen   E-value  score  bias   #  of  c-Evalue  i-Evalue  score  bias  from    to  from    to  from    to  acc description of target
#------------------- ---------- ----- -------------------- ---------- ----- --------- ------ ----- --- --- --------- --------- ------ ----- ----- ----- ----- ----- ----- ----- ---- ---------------------
ABC_membrane         PF0066418   275 AAF674942_AF170880  -            615     8e-29  1007  114   1   1     3e-32     1e-28  1004   79     3   273    42   313    40   315 095 ABC transporter transmembrane region
ABC_tran             PF0000522   118 AAF674942_AF170880  -            615   26e-20   728   00   1   1   19e-23   64e-20   715   00     1   118   402   527   402   527 093 ABC transporter
SMC_N                PF0246314   220 AAF674942_AF170880  -            615   38e-08   327   02   1   2    00036        12    49   00    27    40   391   404   383   408 086 RecF/RecN/SMC N terminal domain
SMC_N                PF0246314   220 AAF674942_AF170880  -            615   38e-08   327   02   2   2   18e-09   61e-06   254   00   116   210   461   568   428   575 085 RecF/RecN/SMC N terminal domain
AAA_16               PF131911    166 AAF674942_AF170880  -            615   31e-06   275   03   1   1     2e-09     7e-06   264   02    20   158   386   544   376   556 072 AAA ATPase domain
YceG                 PF0261811   297 AAF674951_AF170880  -            284   34e-64  2166   00   1   1   29e-68     4e-64  2163   00    68   296    53   274    29   275 085 YceG-like family
Pyr_redox_3          PF137381    203 AAF674962_AF170880  -            352   29e-28   991   00   1   2   28e-30   48e-27   952   00     1   201     4   198     4   200 085 Pyridine nucleotide-disulphide oxidoreductase

#load data
from pandas import *
data = read_table('sampletxt'  skiprows=3  header=None  sep="" "")

ValueError: Expecting 83 columns  got 91 in row 4

#load data part 2
data = read_table('sampletxt'  skiprows=3  header=None  sep=""'s*' "")
#this mushes some of the columns into the first column and drops the rest
    X1
1    ABC_tran PF0000522 118 AAF674942_
2    SMC_N PF0246314 220 AAF674942_
3    SMC_N PF0246314 220 AAF674942_
4    AAA_16 PF131911 166 AAF674942_
5    YceG PF0261811 297 AAF674951_
6    Pyr_redox_3 PF137381 203 AAF674962_
7    Pyr_redox_3 PF137381 203 AAF674962_
8    FMO-like PF0074314 532 AAF674962_
9    FMO-like PF0074314 532 AAF674962_


While I can preprocess the files to change the whitespace to commas/tabs it would be nice to load them directly

thanks 
zach cp

(FYI this is the *hmmdomtblout output from the hmmscan program)",983191,,983191.0,2012-08-20 01:42:52,2012-08-20 01:42:52,can pandas handle variable-length whitespace as column delimeters,<python><pandas>,1.0,6.0,,
12030398,1,12030465.0,2012-08-19 22:06:01,1,451,"As a follow up to this post  I would like to concatenate a number of columns based on their index but I am encountering some problems In this example I get an Attribute error related to the map function  Help around this error would be appreciated as would code that does the equivalent concatenation of columns

    #data
    df = DataFrame({'A':['a' 'b' 'c']  'B':['d' 'e' 'f']  'C':['concat' 'me' 'yo']  'D':['me' 'too' 'tambien']})

    #row function to concat rows with index greater than 2
    def cnc(row):
        temp = []
        for x in range(2 (len(row))):
            if row[x] != None:
                tempappend(row[x])
        return map(concat  temp)

    #apply function per row
    new = dfapply(cnc axis=1)

    #Expected Output
    new

    concat me
    me too
    yo tambien


thanks 
zach cp",983191,,983191.0,2012-08-20 16:03:56,2013-01-09 12:39:10,concatenate multiple columns based on index in pandas,<python><pandas>,1.0,2.0,,
11763204,1,11763490.0,2012-08-01 16:04:15,3,206,"I'm using read_csv to read CSV files into pandas data frames My CSV files contain large numbers of decimals/floats The numbers are encoded using the european decimal notation:

1234456 78


This means that the '' is used as the thousand seperator and the ' ' is the decimal mark

pandas 08 provides a read_csv argument called 'thousands' to set the thousand seperator Is there an additional argument to provide the decimal mark as well? If no  what is the most effcient way to parse a europen style decimal number?

Currently i'm using string replace which i consider to be a significant perfomance penalty The coding i'm using is this:

# Convert to float data type and change decimal point from ' ' to ''
f = lambda x: stringreplace(x  u' '  u'')
df['MyColumn'] = df['MyColumn']map(f)


Any help is appreciated

Thanks 
Thomas",1521724,,,,2012-08-01 16:21:23,How to efficiently handle european decimal separators using the pandas read_csv function?,<python><csv><decimal><pandas>,1.0,1.0,,
11869910,1,11872393.0,2012-08-08 17:25:37,3,757,"Most operations in pandas can be accomplished with operator chaining (groupby  aggregate  apply  etc)  but the only way I've found to filter rows is via normal bracket indexing

df_filtered = df[df['column'] == value]


This is unappealing as it requires I assign df to a variable before being able to filter on its values  Is there something more like the following?

df_filtered = dfmask(lambda x: x['column'] == value)
",128580,,,,2012-08-09 23:20:59,pandas: filter rows of DataFrame with operator chaining,<pandas>,2.0,,1.0,
11885916,1,11886434.0,2012-08-09 14:41:05,4,151,"I'm trying to wrap my brain around Pandas data structures and trying to use them in anger a bit I've figured out that groupby operations result in a pandas series object But I can't quite figure out how to use the resulting series In particular  I want to do two thing:

1) ""join"" the results back to the initial DataFrame

2) select a specific value from the resulting series based on the hierarchical index 

Here's a toy example to work with:

import pandas
df = pandasDataFrame({'group1': ['a' 'a' 'a' 'b' 'b' 'b'] 
                       'group2': ['c' 'c' 'd' 'd' 'd' 'e'] 
                       'value1': [11 2 3 4 5 6] 
                       'value2': [71 8 9 10 11 12]
})
dfGrouped = dfgroupby( [""group1""  ""group2""]   sort=True)

## toy function  obviously not my real function
def fun(x): return mean(x**2)

results = dfGroupedapply(lambda x: fun(xvalue1))


so the resulting series (results) looks like this:

group1  group2
a       c          2605
        d          9000
b       d         20500
        e         36000


That makes sense But how do I:

1) join this back to the original DataFrame df

2) Select a single value where  say  group1=='b' & group2=='d'",37751,,37751.0,2012-08-09 15:09:59,2012-08-09 15:16:32,Joining a Pandas series with a hierarchical index back to the source DataFrame,<ipython><pandas>,2.0,,2.0,
12022823,1,,2012-08-18 22:50:26,1,207,"I need to add columns iteratively to a DataFrame object This is a simplified version:

>>> x=DataFrame()
>>> for i in 'ps':
  x = xappend(DataFrame({i:[3 4]}))
 
>>> x
    p   s
0   3 NaN
1   4 NaN
0 NaN   3
1 NaN   4


What should I do to get:

    p   s
 0  3   3
 1  4   4


?",1583620,,,,2012-08-18 23:12:28,appending columns produces NaN in pandas DataFrame,<python><data-mining><pandas>,2.0,,1.0,
12031532,1,12034043.0,2012-08-20 02:04:05,0,165,"In a previous post  I found out that pandas read_table() function can handle variable-lenth whitespace as a delimiter if you use the read_table('datafile'  sep=r'\s*') construction While this works great for many of my files  it does not work for others despite being highly similar 

EDIT:
I had posted examples that could not replicate the problem when other tried So I am posting links to the original files for AY907538 and AY942707 as well as leaving the error message that I cannot manage to solve

## filename:AY942707
# this will load with no problem
data = read_table('AY942707hmmdomtblout'  header=None  skiprows=3  sep=r'\s*')

## filename: AY907538
data = read_table('AY907538hmmdomtblout'  header=None  skiprows=3  sep=r'\s*')


which will generate the following error:

---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
 in ()
      2 
      3 #temp = get_dataset('AY907538hmmdomtblout')
----> 4 data = read_table('AY907538hmmdomtblout'  header=None  skiprows=3  sep=r'\s*')
      5 #data = read_table('AY942707hmmdomtblout'  header=None  skiprows=3  sep=r'\s*')

/opt/local/Library/Frameworks/Pythonframework/Versions/27/lib/python27/site-packages/pandas/io/parserspyc in read_table(filepath_or_buffer  sep  dialect  header  index_col  names  skiprows  na_values  thousands  comment  parse_dates  keep_date_col  dayfirst  date_parser  nrows  iterator  chunksize  skip_footer  converters  verbose  delimiter  encoding  squeeze)
    282     kwds['encoding'] = None
    283 
--> 284     return _read(TextParser  filepath_or_buffer  kwds)
    285 
    286 @Appender(_read_fwf_doc)

/opt/local/Library/Frameworks/Pythonframework/Versions/27/lib/python27/site-packages/pandas/io/parserspyc in _read(cls  filepath_or_buffer  kwds)
    189         return parser
    190 
--> 191     return parserget_chunk()
    192 
    193 @Appender(_read_csv_doc)

/opt/local/Library/Frameworks/Pythonframework/Versions/27/lib/python27/site-packages/pandas/io/parserspyc in get_chunk(self  rows)
    779             msg = ('Expecting %d columns  got %d in row %d' %
    780                    (col_len  zip_len  row_num))
--> 781             raise ValueError(msg)
    782 
    783         data = dict((k  v) for k  v in izip(selfcolumns  zipped_content))

ValueError: Expecting 26 columns  got 28 in row 6
",983191,,983191.0,2012-08-20 02:58:48,2012-08-20 07:55:49,possible inconsistency in text handling of pandas read_table() function,<python><pandas>,1.0,8.0,,
12034143,1,,2012-08-20 08:04:25,0,160,"I am using pywin32 217  python 272 and pandas developer version:
My script contains only one line:

import pandas as pd


When I run the compiled executable created using pyinstaller in debug mode  i get the following error:

_MEIPASS2 is NULL
archivename is C:\pyinstaller-devel\TestImport\dist\TestImport\TestImportexe
No need to extract files to run; setting extractionpath to homepath
Already in the child - running!
manifestpath: C:\pyinstaller-devel\TestImport\dist\TestImport\TestImportexemanifest
Activation context created
Activation context activated
C:\pyinstaller-devel\TestImport\dist\TestImport\python27dll
Manipulating evironment
PYTHONPATH=C:/pyinstaller-devel/TestImport/dist/TestImport
PYTHONHOME=C:/pyinstaller-devel/TestImport/dist/TestImport/
importing modules from CArchive
extracted iu
extracted struct
extracted archive
Installing import hooks
out00-PYZpyz
Running scripts
Traceback (most recent call last):
File """"  line 65  in
File ""E:\Software\Python\PyInstaller\pyinstaller-pyinstaller-2d4aecc\PyInstaller\loader\iupy""  line 386  in importHook
mod = self_doimport(nm  ctx  fqname)
File ""E:\Software\Python\PyInstaller\pyinstaller-pyinstaller-2d4aecc\PyInstaller\loader\iupy""  line 480  in doimport
exec co in moddict_
File ""C:\pyinstaller-devel\TestImport\build\pyiwin32\TestImport\out00-PYZpyz\win32com""  line 6  in
File ""E:\Software\Python\PyInstaller\pyinstaller-pyinstaller-2d4aecc\PyInstaller\loader\iupy""  line 409  in importHook
raise ImportError(""No module named %s"" % fqname)
ImportError: No module named pythoncom
RC: -1 from pyi_rth_win32comgenpy
OK
Deactivating activation context
Releasing activation context
Done


As can be seen above I get import error saying no module named pythoncom:

ImportError: No module named pythoncom

To avoid this   i have also tried to explicitly import pythoncom in my script  the script runs fine but the error with executable still remains
I have tried with earlier versions of pandas as well as pywin32 (i could go back to pywin 213 only)
I noticed in issue #1136 that the problem was resolved with developer version of pandas  but it doesn't work for me
Any quick pointers will be really appreciated a lot",1201501,,,,2012-08-20 08:04:25,"import error with pyinstaller, related to pythoncom; pywin32",<pandas><pywin32><pyinstaller><pythoncom>,,,,
12047418,1,12054685.0,2012-08-21 01:41:25,1,402,"Having read the docs one the ix method of DataFrames  I'm a bit confused by the following behavior with my MultiIndexed DataFrame (specifying select columns of the index)

In [57]: metals
Out[57]: 

MultiIndex: 24245 entries  (u'BI'  u'Arsenic  Dissolved'  -20837685760  10) 
                        to (u'WC'  u'Zinc  Total'          16611831040  1140)
Data columns:
Inflow_val      20648  non-null values
Outflow_val     20590  non-null values
Inflow_qual     20648  non-null values
Outflow_qual    20590  non-null values
dtypes: float64(2)  object(2)

In [58]: metalsix['BI']shape  # first column in the index  ok
Out[58]: (3368  4)

In [59]: metalsix['BI'  :  :  :]shape  # first + other columns  ok
Out[59]: (3368  4)

In [60]: metalsix['BI'  'Arsenic  Dissolved']shape # first two cols
Out[60]: (225  4)

In [61]: metalsix['BI'  'Arsenic  Dissolved'  :  :]shape # first two + all others
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
 in ()
----> 1 metalsix['BI'  'Arsenic  Dissolved'  :  :]shape                              
# traceback spaghetti snipped
KeyError: 'no item named Arsenic  Dissolved'

In [62]: metalsix['BI'  'Arsenic  Dissolved'  :  10]shape # also fails


It took me a long time to realize that what I had been trying to achieve with In [61] was possible with In [60] Why does the ix method behave like this? What I'm really trying to get at is the scenario at In [62] 

My guess is that I need to redefine the index hierarchy  but I'm curious if there's an easier way

Thanks",1552748,,1552748.0,2012-08-21 16:57:12,2012-08-21 16:57:12,How do I really use the `ix` method of a pandas DataFrame?,<python><pandas>,2.0,,,
11927715,1,11927922.0,2012-08-13 03:02:57,5,677,"I just started using pandas/matplotlib as a replacement for Excel to generate stacked bar charts  I am running into an issue  

(1) there are only 5 colors in the default colormap  so if I have more than 5 categories then the colors repeat  How can I specify more colors?  Ideally  a gradient with a start color and an end color  and a way to dynamically generate n colors in between?

(2) the colors are not very visually pleasing  How do I specify a custom set of n colors?  Or  a gradient would also work

An example which illustrates both of the above points is below:

  4 from matplotlib import pyplot
  5 from pandas import *
  6 import random
  7 
  8 x = [{i:randomrandint(1 5)} for i in range(10)]
  9 df = DataFrame(x)
 10 
 11 dfplot(kind='bar'  stacked=True)


And the output is this:



Sorry  I literally just started using matplotlib today  and couldn't figure out from the docs how to specify custom colors",1058521,,,,2012-12-20 13:17:50,How to give a pandas/matplotlib bar graph custom colors,<python><matplotlib><pandas>,1.0,,1.0,
12047193,1,12060886.0,2012-08-21 01:02:02,2,596,"Any help on this problem will be greatly appreciated So basically I want to run a query to my SQL database and store the returned data as Pandas data structure I have attached code for query I am reading the documentation on Pandas  but I have problem to identify the return type of my query I tried to print the query result  but it doesn't give any useful information 
    Thanks!!!! 

from sqlalchemy import create_engine


engine2 = create_engine('mysql://THE DATABASE I AM ACCESSING')
connection2 = engine2connect()
dataid = 1022
resoverall = connection2execute(""SELECT sum(BLABLA) AS BLA  sum(BLABLABLA2) AS BLABLABLA2  sum(SOME_INT) AS SOME_INT  sum(SOME_INT2) AS SOME_INT2  100*sum(SOME_INT2)/sum(SOME_INT) AS ctr  sum(SOME_INT2)/sum(SOME_INT) AS cpc FROM daily_report_cooked WHERE campaign_id = '%s'""%dataid)


So I sort of want to understand what's the format/datatype of my variable ""resoverall"" and how to put it with PANDAS data structure",1613017,,,,2013-01-23 19:38:02,How to convert SQL Query result to PANDAS Data Structure?,<python><mysql><data-structures><pandas>,4.0,3.0,1.0,
12052067,1,12184679.0,2012-08-21 09:37:29,3,217,Now that pandas provides a data frame structure  is there any need for structured/record arrays in numpy? There are some modifications I need to make to an existing code which requires this structured array type framework  but I am considering using pandas in its place from this point forward Will I at any point find that I need some functionality of structured/record arrays that pandas does not provide?,143476,,,,2012-08-29 18:53:08,"If I use python pandas, is there any need for structured arrays?",<numpy><scipy><pandas>,3.0,,,
12066550,1,12068217.0,2012-08-22 05:01:39,1,524,"I have a csv file that I read into a dataframe using the pandas API 
I intend to set my own header instead of the default first row (I also get rid of some of the rows) How do I best achieve this?

I tried the following but this didn't work as expected:

header_row=['col1' 'col2' 'col3' 'col4'  'col1'  'col2'] # note the header has duplicate column values
df = pandasread_csv(csv_file  skiprows=[0 1 2 3 4 5]  names=header_row)


This gives following error -

File ""third_party/py/pandas/io/parserspy""  line 187  in read_csv
File ""third_party/py/pandas/io/parserspy""  line 160  in _read
File ""third_party/py/pandas/io/parserspy""  line 628  in get_chunk
File ""third_party/py/pandas/core/framepy""  line 302  in __init__
File ""third_party/py/pandas/core/framepy""  line 388  in _init_dict
File ""third_party/py/pandas/core/internalspy""  line 1008  in form_blocks
File ""third_party/py/pandas/core/internalspy""  line 1036  in _simple_blockify
File ""third_party/py/pandas/core/internalspy""  line 1068  in _stack_dict
IndexError: index out of bounds


I then tried settings the columns via 

dfcolumns = header_row


But this error-ed out probably because of duplicate column values

File ""enginespyx""  line 101  in pandas_enginesDictIndexEngineget_loc    
(third_party/py/pandas/src/enginesc:2498)
File ""enginespyx""  line 107  in pandas_enginesDictIndexEngineget_loc 
(third_party/py/pandas/src/enginesc:2447)
Exception: ('Index values are not unique'  'occurred at index entity')


I am using pandas 073 version
From the documentation - 

names : array-like
    List of column names

I am sure I am missing something simple here Thanks for any help here",593237,,593237.0,2012-08-22 05:17:50,2012-08-22 07:27:59,Set the headers using pandas.read_csv,<python><pandas>,1.0,,0.0,
12034727,1,12035069.0,2012-08-20 08:53:05,0,80,"I'm trying to unstack a dataframe perform operations on it (over time only) and then stack it back together like this:

import pandas as pd
import numpy as np
from itertools import *

time = pddate_range(pddatetime(2007 1 1) pddatetime(2007 1 2))
slot = map(lambda n:""s-""+str(n) reversed(range(2)))
obj  = map(lambda n:""o-""+str(n) reversed(range(2)))
idx  = pdMultiIndexfrom_tuples(list(product(slot  obj  time))  names=['Ananas' 'Bananas' 'time']) #list() needed to get a length  should this really be needed?
data = nprandomrand(len(idx) 4)

df = pdDataFrame(data=data index=idx  columns=['a' 'b' 'c' 'd']) #why is idxsize==0?

print dfto_string()
print ""=====""
unstacked = dfunstack(level=[0 1])
print unstackedto_string()
print ""=====""
stacked = unstackedstack(level=[2 1])
print stackedto_string()


The problem is that multiindex is getting reversed after the operation  is there any easy way to make this work? Perhaps I'm misusing the stack from the start?",905596,,,,2012-08-20 09:19:56,How to revert an unstacked dataframe in pandas,<pandas>,1.0,,,
12065885,1,12065904.0,2012-08-22 03:16:56,2,618,"i have a dataframe 'rpt' of python pandas :

rpt

MultiIndex: 47518 entries  ('000002'  '20120331') to ('603366'  '20091231')
Data columns:
STK_ID                    47518  non-null values
STK_Name                  47518  non-null values
RPT_Date                  47518  non-null values
sales                     47518  non-null values


I can filter the rows whose stock id is '600809' like this : rpt[rpt['STK_ID']=='600809']


MultiIndex: 25 entries  ('600809'  '20120331') to ('600809'  '20060331')
Data columns:
STK_ID                    25  non-null values
STK_Name                  25  non-null values
RPT_Date                  25  non-null values
sales                     25  non-null values


and I want to get all the rows of some stocks together  such as ['600809' '600141' '600329']  that means I want a syntax like this : 

stk_list = ['600809' '600141' '600329']

rst = rpt[rpt['STK_ID'] in stk_list] ### this does not works in pandas 


Since pandas not accept above command  how to achieve the target ? ",1072888,,384985.0,2012-08-22 03:27:38,2012-08-22 03:27:38,"how to filter the dataframe rows of pandas by ""within""/""in""?",<python><pandas>,1.0,,,
12096252,1,12098586.0,2012-08-23 16:31:12,1,468,"Possible Duplicate:how to filter the dataframe rows of pandas by within/in?  




Lets say I have the following pandas dataframe:

df = DataFrame({'A' : [5 6 3 4]  'B' : [1 2 3  5]})
df

     A   B
0    5   1
1    6   2
2    3   3
3    4   5


I can subset based on a specific value:

x = df[df['A'] == 3]
x

     A   B
2    3   3


But how can I subset based on a list of values? - something like this:

list_of_values = [3 6]

y = df[df['A'] in list_of_values]
",983191,,,,2012-08-23 19:20:12,use a list of values to select rows from a pandas dataframe,<python><pandas>,1.0,1.0,,2012-08-24 10:29:42
12101113,1,12117333.0,2012-08-23 22:54:04,4,550,"I have a #-separated file with three columns: the first is integer  the second looks like a float  but isn't  and the third is a string  I attempt to load this directly into python with pandasread_csv

In [149]: d = pandasread_csv('resources/names/fos_namescsv'   sep='#'  header=None  names=['int_field'  'floatlike_field'  'str_field'])

In [150]: d
Out[150]: 

Int64Index: 1673 entries  0 to 1672
Data columns:
int_field          1673  non-null values
floatlike_field    1673  non-null values
str_field          1673  non-null values
dtypes: float64(1)  int64(1)  object(1)


pandas tries to be smart and automatically convert fields to a useful type  The issue is that I don't actually want it to do so (if I did  I'd used the converters argument)  How can I prevent pandas from converting types automatically?",128580,,128580.0,2012-08-23 23:44:34,2012-10-20 20:12:00,Prevent pandas from automatically infering type in read_csv,<python><pandas>,2.0,5.0,2.0,
12041519,1,,2012-08-20 16:32:26,0,149,"I started learning two weeks ago and now I'm kind of stuck I have 2 TimeSeries which look like this one: 

2011-01-09 00:00:00+00:00    7430126
2011-01-09 01:00:00+00:00    6793855
2011-01-09 02:00:00+00:00    6675949
2011-01-09 03:00:00+00:00    6756636
2011-01-09 04:00:00+00:00    6875174
2011-01-09 05:00:00+00:00    5432611
2011-01-09 06:00:00+00:00    6059197
2011-01-09 21:00:00+00:00    5338928
2011-01-09 22:00:00+00:00    5259672
2011-01-09 23:00:00+00:00    5247196
2011-01-10 00:00:00+00:00    5889274
2011-01-10 01:00:00+00:00    6133871
2011-01-10 02:00:00+00:00    6111958
2011-01-10 03:00:00+00:00    5873732
2011-01-10 04:00:00+00:00    5627684
2011-01-10 05:00:00+00:00    5265644
2011-01-10 06:00:00+00:00    5505559
2011-01-10 21:00:00+00:00    3835050
2011-01-10 22:00:00+00:00    3879653
2011-01-10 23:00:00+00:00    4034543
2011-01-11 00:00:00+00:00    4844272
2011-01-11 01:00:00+00:00    4670967
2011-01-11 02:00:00+00:00    4584164
2011-01-11 03:00:00+00:00    4786821


This is a data of a measurement of wind speed and I want to compare it with model data More specifically  I want to compare the wind speeds at night (2100 - 600) So I defined a function: 

def func(model  measure):
    return (model-measure)mean()


In addition  I created a loop over the data:

mean_night = []
start = 7
for a in night:
    mean_nightappend(func(model  measure[start:(start+10)]))
    start = start+11
    if start>5378:
            break


The problem is that I lose my time index and that some data is missing (for example 1 day or 1 week)  so I get in trouble reindexing it with a DateRange In the end  it should look like this: 

date    difference_means
2011-01-09    diff_1
2011-01-09    diff_2


and so on I use pandas 071 Thanks for support! (and sorry for my bad English :P)",1612198,,1612198.0,2012-08-20 18:03:49,2012-08-22 15:08:07,How to get the mean of a specific span of time,<python><time><span><pandas><mean>,3.0,3.0,,
12053003,1,12187475.0,2012-08-21 10:30:56,0,364,"I have been testing out pandas and pytables for some large financial data sets  and have run in to a real stumbling block:

When storing in a pytables file  pandas appears to be storing multidimensional data in massively long rows  not columns

try this:

from pandas import *
df = DataFrame({'col1':randn(100000000) 'col2':randn(100000000)})
store = HDFStore('testh5')
store['data'] = df    #should be a warning here about exceeding the maximum recommended rowsize
storehandle


output:

File(filename=test7h5  title=''  mode='a'  rootUEP='/'  filters=Filters(complevel=0  shuffle=False  fletcher32=False))
/ (RootGroup) ''
/data (Group) ''
/data/axis0 (Array(2 )) ''
  atom := StringAtom(itemsize=4  shape=()  dflt='')
  maindim := 0
  flavor := 'numpy'
  byteorder := 'irrelevant'
  chunkshape := None
/data/axis1 (Array(100000000 )) ''
  atom := Int64Atom(shape=()  dflt=0)
  maindim := 0
  flavor := 'numpy'
  byteorder := 'little'
  chunkshape := None
/data/block0_items (Array(2 )) ''
  atom := StringAtom(itemsize=4  shape=()  dflt='')
  maindim := 0
  flavor := 'numpy'
  byteorder := 'irrelevant'
  chunkshape := None
/data/block0_values (Array(2  100000000)) ''
  atom := Float64Atom(shape=()  dflt=00)
  maindim := 0
  flavor := 'numpy'
  byteorder := 'little'
  chunkshape := None


I'm not totally sure  but i reckon that combined with the error message  the Array(2 100000000) means a 2D array with 2 rows and 100 000 000 columns This is also the way it's shown in HDFView

I've been experiencing extremely poor performance (10 seconds for data['ticks']head() in some cases)  is this what's to blame?",1019856,,,,2012-08-29 22:38:41,Pandas Pytables warnings and slow performance,<python><performance><pandas><hdf5><pytables>,1.0,2.0,1.0,
12107053,1,,2012-08-24 09:52:54,1,169,"I have a Pandas plot that looks like this:

ts = pdSeries(randn(1000)  index=date_range('1/1/2000'  periods=1000))
ts = tscumsum()
tsplot()


And I would like to show it in a GUI  which I designed with Qt Designer As I am using Python(x y)  there is a MatplotlibWidget  which I placed on my Gui However  I am not sure how to make this widget display my earlier created plot  or if this is possible at all? I am able to make the widget display basic plots like:

x = nplinspace(-10  10)
selfwidgetaxesplot(x  x**2)
selfwidgetdraw()


Is there any way to make the widget display my Pandas plot  or do I have to create a new class? How would I go about this? As I am using Qt Designer  could I add this new plot class as a widget? How?

Thanks for your help!",1190648,,,,2012-08-24 09:52:54,include Pandas plot in MatplotlibWidget,<python><matplotlib><pyqt4><pandas>,,3.0,,
12100396,1,12100941.0,2012-08-23 21:40:56,2,80,"I have a dataFrame from a large questionnaire  I'm generating summaries by aggregating the data on different axis by doing:

dfgroupby(group_name)agg([npmean  npstd  npcount_nonzero])


This generates a column with mean  std  and count per question in my questionnaire The names of each column in the grouped dataFrame are a tuple (original_column_name  function_applied) 

The problem is that when I output to CSV (using to_csv()) the column names are outputted as a tuple ie ('gender'  'mean')  ('gender'  'std') where ideally I would like something like gender_mean & gender_std

How can I process these column names before output to CSV? ",240068,,939986.0,2012-08-24 04:23:40,2012-08-28 06:31:41,column names on grouped DataFrame output to CSV,<python><csv><pandas>,1.0,,,
12140417,1,12140833.0,2012-08-27 10:40:17,3,167,"I have input data in a flattened file I want to normalize this data  by splitting it into tables Can I do that neatly with pandas - that is  by reading the flattened data into a DataFrame instance  and then applying some functions to obtain the resulting DataFrame instances?

Example:

Data is given to me on disk in the form of a CSV file like this:

ItemId   ClientId   PriceQuoted  ItemDescription
1        1          10           scroll of Sneak
1        2          12           scroll of Sneak
1        3          13           scroll of Sneak
2        2          2500         scroll of Invisible
2        4          2200         scroll of Invisible


I want to create two DataFrames:

ItemId   ItemDescription
1        scroll of Sneak
2        scroll of Invisibile


and

ItemId   ClientId   PriceQuoted
1        1          10
1        2          12
1        3          13
2        2          2500
2        4          2200


If pandas only has a good solution for the simplest case (normalization results in 2 tables with many-to-one relationship - just like in the above example)  it might be enough for my current needs I may need a more general solution in the future  however",336527,,,,2012-08-27 11:07:30,pandas: normalizing a DataFrame,<python><pandas><database-normalization>,1.0,,1.0,
12147392,1,12157914.0,2012-08-27 18:22:33,0,97,"I need to read in to a dataframe from a fixed width flat file This is a somewhat performance sensitive operation

I would like all blank whitespace to be stripped from column value After that whitespace is stripped  I would like blank strings to be converted to NaN or None values Here are the two ideas I had:

pdread_fwf(path  colspecs=markers  names=columns 
            converters=create_convert_dict(columns))

def create_convert_dict(columns):
    convert_dict = {}
    for col in columns:
        convert_dict[col] = null_convert
        return convert_dict

def null_convert(value):
    value = valuestrip()
    if value == """":
        return None
    else:
        return value


or:

pdread_fwf(path  colspecs=markers  names=columns  na_values='' 
            converters=create_convert_dict(columns))

def create_convert_dict(columns):
    convert_dict = {}
    for col in columns:
        convert_dict[col] = col_strip
    return convert_dict

def col_strip(value):
    return valuestrip()


Obviously the second option depends on the converter (which strips whitespace) be evaluated before na_values

I was wondering if the second one would work The reason I am curious is because it seems better to retain NaN has the Null value opposed to None

I am also open to any other suggestions for how I might perform this operation (stripping whitespace and then converting blank strings to NaN)

I do not have access to a computer with pandas installed at the moment  which is why I cannot test this myself

Thanks",1234686,,,,2012-08-28 11:04:06,Will na_values in the read_fwf()/read_csv()/read_table() be converted after converter functions are performed?,<python><pandas>,1.0,,,
12082568,1,12083600.0,2012-08-22 23:03:38,0,138,"In python-pandas boxplots with default settings  the red bar is the mean  and the box signifies the 25th and 75th quartiles  but what exactly do the whiskers mean in this case?  Where is the documentation to figure out the exact definition (couldn't find it)?  

Example code:

dfboxplot()


Example result:

",1058521,,,,2012-08-23 01:34:04,What exactly do the whiskers in pandas' boxplots specify?,<documentation><pandas><boxplot>,1.0,,,
12085393,1,,2012-08-23 05:42:14,0,197,"Have been trying to load in a large-ish file (~480MB  5 250 000 records  stock price daily data -dt o h l c v val adj fv sym code - for about 4 500 instruments) into pandas using read_csv It runs fine  and creates the DataFrame I found  however  that on conversion to a Panel  the values for several stocks are way off  and nowhere close to the values in the original csv file

I then attempted to use the chunksize parameter in read_csv  and used a for loop to:

reader = read_csv(""bigfilecsv"" index_col=[0 9] parse_dates=True names=['n1' 'n2'  'nn']  chunksize=100000)

new_df = DataFrame(readerget_chunk(1))

for chunk in reader:
    new_df = concat(new_df  chunk)


This reads in the data  but:

I get the same erroneous values (edit:) when converting to a Panel 
It takes ages longer than the plain read_csv (no iterator)
Any ideas how to get around this?

Edit:
Changed the question to reflect the problem - the dataframe is fine  conversion to a Panel is the problem Found the error appearing even after splitting the input csv file  merging and then converting to a panel If i maintain a multi-index DataFrame  there is no problem and the values are represented correctly",878571,,878571.0,2012-08-29 06:02:03,2012-11-28 04:16:54,Problems converting a large file to a panel using read_csv into python pandas,<python><large-files><pandas>,2.0,2.0,,
12110812,1,,2012-08-24 13:54:16,0,152,"I have a questionnaire dataset in which one of the columns (a question) has multiple possible answers The data for that column is a sting of a list  with multiple possible values from none up to five ie '[1]' or '[1  2  3  5]' 

I am trying to process that column to access the values independently as follows:

def f(x):
        if notnull(x):
            p = recompile( '[\[\]\'\s]' )
            places = psub( ''  x )split( ' ' )
            place_tally = {'1':0  '2':0  '3':0  '4':0  '5':0}
            for place in places:
                place_tally[place] += 1
            return place_tally

df['places'] = dfwhere_buymap(f)


This creates a new column in my dataframe ""places"" with a dict from the values ie: {'1': 1  '3': 0  '2': 0  '5': 0  '4': 0} or {'1': 1  '3': 1  '2': 1  '5': 1  '4': 0}

Now what is the most efficient/succinct way to extract that data form the new column? I've tried iterating through the DataFrame with no good results ie 

    for row_index  row in dfiterrows():
         r = row['places']
         if r is not None:
             dfix[row_index]['large_super'] = r['1']
             dfix[row_index]['small_super'] = r['2']


This does not seem to be working

Thanks",240068,,,,2012-09-05 10:44:34,Processing multiple values string in Pandas DataFrame column,<python><pandas>,1.0,1.0,,
12178808,1,12184743.0,2012-08-29 13:12:38,0,235,"I have a pandas dataframe consisting of 23 series with a default sequential index (0 1 2 ) obtained by importing an ndarray

Two of the series in the dataframe contain record time information  One series ('SECONDS') contains the number of seconds since the start of the year 1900  The other series  ('NANOSECONDS') contains the number of nanoseconds into the corresponding second

In python the conversion can be accomplished (on a single record) as:

import datetime as dt
Mydt = dtdatetime(1990 1 1 0 0 0) + dttimedelta(seconds = 706500000)
print Mydt


Does there exist in pandas methods to perform a similar array calculation to obtain a datetime(64) date/time stamp with which I can replace the current sequential dataframe index?",1633204,,,,2012-08-29 18:57:05,Pandas datetime index from seconds series,<python><numpy><pandas>,1.0,,,
11970614,1,11975948.0,2012-08-15 13:57:39,1,69,"I have some number in a DataFrame that are currently being displayed as: 0000000 When I try:

set_printoptions(precision = 12)


I will get something like: 000000049100

Is there a way to display all/some of the numbers in this columns as exponents?",1600653,,,,2012-08-15 19:32:35,Force exponent display of floats in pandas,<pandas>,1.0,,,
12021754,1,12022003.0,2012-08-18 19:49:41,2,182,"I have a Pandas Data Frame object that has 1000 rows and 10 columns I would simply like to slice the Data Frame and take the first 10 rows How can I do this? I've been trying to use this:

>>> dfshape
(1000 10)
>>> my_slice = dfix[10 :]
>>> my_sliceshape
(10 )


Shouldn't my_slice be the first ten rows  ie a 10 x 10 Data Frame? How can I get the first ten rows  such that my_slice is a 10x10 Data Frame object? Thanks",1255817,,,,2012-09-09 19:21:23,How to slice a Pandas Data Frame by position?,<pandas>,3.0,,,
12091967,1,,2012-08-23 12:48:48,0,95,"I have a data frame 'df' which has a multilevel index ('STK_ID' 'RPT_Date'):

    sales        cogs    net_pft
STK_ID RPT_Date                                   
600809 20120331  2214010000   509940000  492532000
       20111231  4488150000  1077190000  780547000
       20110930  3563660000   850789000  707537000
       20110630  2894820000   703883000  658625000


Some code:

>>> dfindexnames
['STK_ID'  'RPT_Date']


now I want to get the RPT_Date column's series value (20120331 20111231 20110930 20110630) by: 

>>> df['RPT_Date'] # not work


How do I get that data?",1072888,,748858.0,2012-08-23 12:49:48,2012-12-17 10:38:41,How to get one series within a multilevel index in python pandas,<python><pandas>,2.0,2.0,,
12115421,1,,2012-08-24 19:18:31,2,128,In Python there is the library Pandas for time series I have a txt file and in python program a simple counter for most frequent words in the document and also days Can I use Pandas library to make simple time line for most frequent words?,1617766,,401828.0,2012-09-09 00:10:12,2012-09-09 00:10:12,Panda Time Series from text,<python><time-series><pandas>,1.0,,,
12138126,1,12140128.0,2012-08-27 07:53:22,0,168,"This code:

import pandas as pd
from StringIO import StringIO

data = ""date c1\n2012-07-31 02:00 11\n2012-07-31 02:15 22\n2012-07-31 02:30 33\n""

df1 = pdread_csv(StringIO(data) parse_dates=True)set_index(('date'))
df2 = pdread_csv(StringIO(data) parse_dates=[0] )set_index(('date'))

print ""df1:\n{index}""format(index=df1index)
print ""df2:\n{index}""format(index=df2index)


returns:

df1:
array([2012-07-31 02:00  2012-07-31 02:15  2012-07-31 02:30]  dtype=object)
df2:

[2012-07-31 02:00:00    2012-07-31 02:30:00]
Length: 3  Freq: None  Timezone: None


Is this difference between df1 and df2 a bug feature  or have I misunderstood something? ",905596,,,,2012-08-27 12:02:14,Difference between parse_date=[0] and parse_date=True in pandas.read_csv,<pandas>,2.0,,,
12167324,1,,2012-08-28 20:49:57,1,103,"I have two pandas arrays  A and B  that result from groupby operations A has a 2-level multi-index consisting of both quantile and date B just has an index for date

Between the two of them  the date indices match up (within each quantile index for A)

Is there a standard Pandas function or idiom to ""broadcast"" B such that it will have an extra level to its multi-index that matches the first multi-index level of A?",567620,,497043.0,2012-08-30 00:50:29,2012-08-30 00:50:29,How to broadcast to a multiindex,<python><arrays><pandas><multi-index>,1.0,,,
12190874,1,12192021.0,2012-08-30 06:12:46,2,418,"I'm trying to read a fairly large CSV file with Pandas and split it up into two random chunks  one of which being 10% of the data and the other being 90%

Here's my current attempt:

rows = dataindex
row_count = len(rows)
randomshuffle(list(rows))

datareindex(rows)

training_data = data[row_count // 10:]
testing_data = data[:row_count // 10]


For some reason  sklearn throws this error when I try to use one of these resulting DataFrame objects inside of a SVM classifier:

IndexError: each subindex must be either a slice  an integer  Ellipsis  or newaxis


I think I'm doing it wrong Is there a better way to do this?",464744,,,,2012-08-30 07:36:18,Pandas: Sampling a DataFrame,<python><partitioning><pandas>,2.0,,,
11976503,1,11982843.0,2012-08-15 20:10:04,4,209,"I would like to merge two data frames  and keep the index from the first frame as the index on the merged dataset  However  when I do the merge  the resulting DataFrame has integer index  How can I specify that I want to keep the index from the left data frame?

In [441]: a=DataFrame(data={""col1"": [1 2 3]  'to_merge_on' : [1 3 4]}  index=[""a"" ""b"" ""c""])

In [442]: b=DataFrame(data={""col2"": [1 2 3]  'to_merge_on' : [1 3 5]})
In [443]: a
Out[443]: 
   col1  to_merge_on
a     1            1
b     2            3
c     3            4

In [444]: b
Out[444]: 
   col2  to_merge_on
0     1            1
1     2            3
2     3            5


In [445]: amerge(b  how=""left"")
Out[445]: 
   col1  to_merge_on  col2
0     1            1     1
1     2            3     2
2     3            4   NaN

In [446]: _index
Out[447]: Int64Index([0  1  2])


EDIT: Switched to example code that can be easily reproduced",670525,,670525.0,2012-08-15 20:51:06,2012-08-16 07:53:01,How to keep index when using pandas merge,<python><pandas>,1.0,,2.0,
11994765,1,12001086.0,2012-08-16 20:11:51,3,206,"When I try to use to_string to output a column from a dataframe  it truncates the output of the column

print gtf_dfix[:1][['transcript_id' 'attributes']]to_string(header=False index=False)

Out: ' CUFF11  gene_id ""CUFF1""; transcript_id ""CUFF11""; FPKM '

print gtf_dfix[:1]['attributes'][0]

Out: 'gene_id ""CUFF1""; transcript_id ""CUFF11""; FPKM ""16703038168650887""; frac ""1000000""; conf_lo ""0000000""; conf_hi ""5010911450595""; cov ""9658694354"";'


Any ideas as to how to resolve this problem?
Thanks!   ",1604269,,,,2012-08-17 06:54:29,pandas DataFrame.to_string() truncating strings from columns,<python><pandas>,1.0,2.0,,
12100497,1,,2012-08-23 21:50:36,4,372,"pandas offers the ability to look up by lists of row and column indices 

In [49]: index = ['a'  'b'  'c'  'd']

In [50]: columns = ['one'  'two'  'three'  'four']

In [51]: M = pandasDataFrame(nprandomrandn(4 4)  index=index  columns=columns)

In [52]: M
Out[52]: 
        one       two     three      four
a -0785841 -0538572  0376594  1316647
b  0530288 -0975547  1063946 -1049940
c -0794447 -0886721  1794326 -0714834
d -0158371  0069357 -1003039 -0807431

In [53]: Mlookup(index  columns) # diagonal entries
Out[53]: array([-078584142  -097554698   179432641  -08074308 ])


I would like to use this same method of indexing to set M's elements  How can I do this?",128580,,,,2012-12-30 18:18:41,"pandas: set values with (row, col) indices",<python><pandas>,1.0,,,
12184558,1,12184631.0,2012-08-29 18:47:18,2,421,"I have installed easy_install with Portable Python 2-7 on Windows XP (through the distribute module) But when I type a DOS command such as 

easy_install pandas


I get the following: 


And I don't find any ""parse"" module on internet (and ""easy_install parse"" yields the same output)",1100107,,1267329.0,2012-08-29 18:53:57,2012-08-29 18:53:57,easy-install with Portable Python: no module named parse,<python><pandas><easy-install><portable-python>,1.0,1.0,,
12200693,1,12201723.0,2012-08-30 15:45:26,4,642,"I have the following data frame in IPython  where each row is a single stock:

In [261]: bdata
Out[261]:

Int64Index: 21210 entries  0 to 21209
Data columns:
BloombergTicker      21206  non-null values
Company              21210  non-null values
Country              21210  non-null values
MarketCap            21210  non-null values
PriceReturn          21210  non-null values
SEDOL                21210  non-null values
yearmonth            21210  non-null values
dtypes: float64(2)  int64(1)  object(4)


I want to apply a groupby operation that computes cap-weighted average return across everything  per each date in the ""yearmonth"" column

This works as expected:

In [262]: bdatagroupby(""yearmonth"")apply(lambda x: (x[""PriceReturn""]*x[""MarketCap""]/x[""MarketCap""]sum())sum())
Out[262]:
yearmonth
201204      -0109444
201205      -0290546


But then I want to sort of ""broadcast"" these values back to the indices in the original data frame  and save them as constant columns where the dates match

In [263]: dateGrps = bdatagroupby(""yearmonth"")

In [264]: dateGrps[""MarketReturn""] = dateGrpsapply(lambda x: (x[""PriceReturn""]*x[""MarketCap""]/x[""MarketCap""]sum())sum())
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
/mnt/bos-devrnd04/usr6/home/espears/ws/Research/Projects/python-util/src/util/ in ()
----> 1 dateGrps[""MarketReturn""] = dateGrpsapply(lambda x: (x[""PriceReturn""]*x[""MarketCap""]/x[""MarketCap""]sum())sum())

TypeError: 'DataFrameGroupBy' object does not support item assignment


I realize this naive assignment should not work But what is the ""right"" Pandas idiom for assigning the result of a groupby operation into a new column on the parent dataframe?

In the end  I want a column called ""MarketReturn"" than will be a repeated constant value for all indices that have matching date with the output of the groupby operation

One hack to achieve this would be the following:

marketRetsByDate  = dateGrpsapply(lambda x: (x[""PriceReturn""]*x[""MarketCap""]/x[""MarketCap""]sum())sum())

bdata[""MarketReturn""] = nprepeat(npNaN  len(bdata))

for elem in marketRetsByDateindexvalues:
    bdata[""MarketReturn""][bdata[""yearmonth""]==elem] = marketRetsByDateix[elem]


But this is slow  bad  and unPythonic",567620,,567620.0,2012-08-30 16:22:59,2012-09-08 22:35:52,Python Pandas How to assign groupby operation results back to columns in parent dataframe?,<python><group-by><data.frame><pandas>,4.0,3.0,2.0,
12187928,1,12192362.0,2012-08-29 23:32:03,0,319,"Hi I am trying to use the bootstrap_plot() function on a data sample  but run into the following error I have used the same data arrays to plot histograms using matplotlibs hist() function with no problem

Traceback (most recent call last):
 File ""/home/Astrophysics/BootStrappingpy""  line 19  in 
    bootstrap_plot(C  size=17517  samples=1000  color='grey')
  File ""/usr/local/lib/python27/dist-packages/pandas-082dev_eec8a83-py27- linux-    i686egg/pandas/tools/plottingpy""  line 301  in bootstrap_plot
    data = seriesvalues
AttributeError: 'numpyndarray' object has no attribute 'values'


My Code

import matplotlibpyplot as plt
import numpy as np
import scipy
import matplotlibmlab as mlab
import statsmodelsapi as sm
import pyfits
from pandastoolsplotting import bootstrap_plot

F = '/home/khary/Astrophysics/outfiles/outmag20_2dr_9_658txt'
#data_cube  header_data_cube = pyfitsgetdata(""/home/Astrophysics/SDSS_Counts    /countdr72bright03dlz01r01c060fits"" 1 header=True)


#load arrays with data 
data=nploadtxt(F)
C=data[: 3]
#obs_count = data_cubefield(3)

#Bootstrap plot
bootstrap_plot(C  size=17517  samples=1000  color='grey')
pltshow()
",908924,,,,2012-08-30 07:57:13,AttributeError: 'numpy.ndarray' object has no attribute 'values' using PANDAS,<pandas>,1.0,,,
12203901,1,12204428.0,2012-08-30 19:15:56,1,83,"Very weird bug here: I'm using pandas to merge several dataframes  As part of the merge  I have to call reset_index several times  But when I do  it crashes unexpectedly on the second or third use of reset_index

Here's minimal code to reproduce the error:

import pandas
A = pandasDataFrame({
    'val' :  ['aaaaa'  'acaca'  'ddddd'  'zzzzz'] 
    'extra' : range(10 14) 
})
A = Areset_index()
A = Areset_index()
A = Areset_index()


Here's the relevant part of the traceback:


    A = Areset_index()
  File ""/usr/local/lib/python27/dist-packages/pandas/core/framepy""  line 2393  in reset_index
    new_objinsert(0  name  _maybe_cast(selfindexvalues))
  File ""/usr/local/lib/python27/dist-packages/pandas/core/framepy""  line 1787  in insert
    self_datainsert(loc  column  value)
  File ""/usr/local/lib/python27/dist-packages/pandas/core/internalspy""  line 893  in insert
    raise Exception('cannot insert %s  already exists' % item)
Exception: cannot insert level_0  already exists


Any idea what's going wrong here?  How do I work around it?",660664,,,,2012-08-30 19:52:44,pandas crashes on repeated DataFrame.reset_index(),<python><pandas>,1.0,,,
12224778,1,12228299.0,2012-09-01 03:57:04,2,170,"I have a 'df' which have a multilevel index (STK_ID RPT_Date)

                       sales         cogs     net_pft
STK_ID RPT_Date                                      
000876 20060331          NaN          NaN         NaN
       20060630    857483000    729541000    67157200
       20060930   1063590000    925140000    50807000
       20061231    853960000    737660000    51574000
       20070331  -2695245000  -2305078000  -167642500
       20070630   1146245000   1050808000   113468500
       20070930   1327970000   1204800000    84337000
       20071231   1439140000   1331870000    53398000
       20080331  -3135240000  -2798090000  -248054300
       20080630   1932470000   1777010000   133756300
       20080930   1873240000   1733660000    92099000
002254 20061231 -16169620000 -15332705000  -508333200
       20070331   -763844000   -703460000    -1538000
       20070630    501221000    289167000   118012200
       20070930    460483000    274026000    95967000


How to write a command to filter the rows whose 'RPT_Date' contains '0630' (which is the Q2 report) ? the result should be :

                       sales         cogs     net_pft
STK_ID RPT_Date                                      
000876 20060630    857483000    729541000    67157200
       20070630   1146245000   1050808000   113468500
       20080630   1932470000   1777010000   133756300
002254 20070630    501221000    289167000   118012200


I am trying to use df[df['RPT_Date']strcontains('0630')]  but Pandas refuses to work as 'RPT_Date' is not a column but a sub_level index

Thanks for your tips ",1072888,,,,2012-09-03 19:10:06,How to filter by sub-level index in Pandas,<python><pandas>,1.0,0.0,2.0,
12133075,1,12133235.0,2012-08-26 19:10:01,2,195,"I am trying to figure out how to sort a results of pandasDataFramegroupbymean() in a smart way

I generate an aggregation of my DataFrame like this:

means = dftestColumngroupby(dftestCategory)mean()


I now try to sort this by value  but get an error:

meanssort()

-> Exception: This Series is a view of some other array  to sort in-place you must create a copy


I then try creating a copy:

meansCopy = Series(means)
meansCopysort()
-> Exception: This Series is a view of some other array  to sort in-place you must create a copy


How can I get this sort working?",1058521,,,,2012-08-26 19:31:37,pandas - how to sort the result of DataFrame.groupby.mean()?,<python><pandas>,1.0,,,
12186994,1,12187770.0,2012-08-29 21:47:00,1,266,"I can't find any reference on funcionality to perform Johansen cointegration test in any Python module dealing eith statistics and time series analysis (pandas and statsmodel) Does anybpdy know if there's some code around that can perform such a test for cointegration among time series?
Thanks for your help 

Maruizio",1529852,,,,2012-09-11 01:12:19,Johansen cointegration test in python,<python><statistics><pandas><statsmodels>,2.0,,1.0,
12201099,1,12201215.0,2012-08-30 16:07:19,0,481,"I'm using python pandas for data analysis and I want to change the name of a series in a dataframe

This works  but it seems very inefficient:

AA = pandasDataFrame( A )
for series in A:
    AA[A_prefix+series] = A[series]
    del A[series]


Is there a way to change the series name in place?",660664,,,,2012-08-30 16:14:27,python pandas: Rename a series within a dataframe?,<python><dictionary><pandas><series>,1.0,,,
12227483,1,12228008.0,2012-09-01 11:54:46,2,258,"I have a largeish DataFrame  loaded from a csv file (about 300MB)

From this  I'm extracting a few dozen features to use in a RandomForestClassifier: some of the features are simply derived from columns in the data  for example:

 feature1 = data[""SomeColumn""]apply(len)
 feature2 = data[""AnotherColumn""]


And others are created as new DataFrames from numpy arrays  using the index on the original dataframe:

feature3 = pandasDataFrame(count_array  index=dataindex)


All these features are then joined into one DataFrame:

features = feature1join(feature2) # etc


And I train a random forest classifier:

classifier = RandomForestClassifier(
    n_estimators=100 
    max_features=None 
    verbose=2 
    compute_importances=True 
    n_jobs=n_jobs 
    random_state=0 
)
classifierfit(features  data[""TargetColumn""])


The RandomForestClassifier works fine with these features  building a tree takes O(hundreds of megabytes of memory) However: if after loading my data  I take a small subset of it:

data_slice = data[data['somecolumn'] > value]


Then building a tree for my random forest suddenly takes many gigabytes of memory - even though the size of the features DataFrame is now O(10%) of the original

I can believe that this might be because a sliced view on the data doesn't permit further slices to be done efficiently (though I don't see how I this could propagate into the features array)  so I've tried:

data = pandasDataFrame(data_slice  copy=True)


but this doesn't help

Why would taking a subset of the data massively increase memory use?
Is there some other way to compact / rearrange a DataFrame which might make things more efficient again?
",233201,,,,2012-09-01 13:05:59,Pandas & Scikit: memory usage when slicing DataFrame,<python><pandas><scikit-learn>,1.0,2.0,3.0,
12152716,1,12152759.0,2012-08-28 04:18:11,1,286,"In R  there is a rather useful replace function
Essentially  it does conditional re-assignment in a given column of a data frame
It can be used as so:
replace(df$column  df$column==1 'Type 1');

What is a good way to achieve the same in pandas?

Should I use a lambda with apply? (If so  how do I get a reference to the given column  as opposed to a whole row)

Should I use npwhere on data_framevalues?
It seems like I am missing a very obvious thing here

Any suggestions are appreciated",1414230,,,,2012-08-28 04:45:23,Python pandas equivalent for replace,<python><pandas><equivalent>,1.0,,,
12168648,1,12168857.0,2012-08-28 22:50:21,1,1012,"The index that I have in the dataframe (with 30 rows) is of the form:

Int64Index([171  174 173  172  199
        175  200])


The index is not strictly increasing because the data frame is the output of a sort()
I want to have add a column which is the series:

[1  2  3  4  5  30]


How should I go about doing that?

Thanks",968643,,,,2012-08-29 03:13:24,Pandas (python): How to add column to dataframe for index?,<python><index><data.frame><pandas>,2.0,,0.0,
12219771,1,,2012-08-31 17:11:59,0,110,"I am trying to explore Pandas library and stopped by an example that I frequently face and I think pandas had the solution for it Given the folloing code:

In [63]: d1 = nprandomrand(3 3)
In [63]: d2 = nprandomrand(3 3)

In [64]:s1 = pandasSeries(d1 index = [['a1']*d1shape[0] 
                             [4]*d1shape[0] 
                             range(d1shape[0])])

Out[64]:a1  4  0    [ 000881133  071344668  003611378]
               1    [ 037328776  063195947  023000941]
               2    [ 068466443  085891677  031740809]

In [65]: s2 = pandasSeries(d2 index = [['a2']*d2shape[0] 
                             [5]*d2shape[0] 
                             range(d2shape[0])])
Out[65]:a2  5  0    [ 000881133  071344668  003611378]
               1    [ 037328776  063195947  023000941]
               2    [ 068466443  085891677  031740809]

s = s1append(s2)

a1  4  0    [ 000881133  071344668  003611378]
       1    [ 037328776  063195947  023000941]
       2    [ 068466443  085891677  031740809]
    5  0    [ 000881133  071344668  003611378]
       1    [ 037328776  063195947  023000941]
       2    [ 068466443  085891677  031740809]


How to obtain a list of all the data matrices alone without their labels?",308849,,,,2012-08-31 19:07:29,Aggregating data in pandas,<python><numpy><pandas>,2.0,,,
12227974,1,,2012-09-01 13:01:35,0,134,"I have to include the pandas library in my package to run my software on a cluster server without pandas library installed but with all the right dependencies

I tried to build on my pc the pandas library from source (ver 081): 

python setuppy build_ext --inplace


and if I move the builded pandas folder anywhere in some other folder in my pc I can import it in my software (ex /mySoftwareFolder/pandas)

But when I move my software with pandas libray on the cluster server it raise me the error:

File ""testPandaspy""  line 9  in 
    import pandas
File ""/home/TEST/pandas/__init__py""  line 15  in 

raise ImportError('C extensions not built: if you installed already '
ImportError: C extensions not built: if you installed already verify that you are not importing from the source directory


like if it is not compiled

What is wrong in my way to include library?

thanks a lot!

update:
the directory that I copy to cluster server contains:

-bash-42$ ll -a pandas
totale 11476
drwxr-xr-x 14 francesco dottor    4096  1 set 1337 
drwxr-xr-x 10 francesco dottor    8192  1 set 1336 
-rwxr-xr-x  1 francesco dottor 2648299  1 set 1336 _algosso
drwxr-xr-x  2 francesco dottor    4096  1 set 1336 compat
drwxr-xr-x  2 francesco dottor    4096  1 set 1336 core
-rw-r--r--  1 francesco dottor     394  1 set 1336 infopy
-rw-r--r--  1 francesco dottor     557  1 set 1336 infopyc
-rw-r--r--  1 francesco dottor    1269  1 set 1336 __init__py
-rw-r--r--  1 francesco dottor    1643  1 set 1337 __init__pyc
drwxr-xr-x  3 francesco dottor    4096  1 set 1336 io
-rwxr-xr-x  1 francesco dottor 7437108  1 set 1336 libso
-rwxr-xr-x  1 francesco dottor  474199  1 set 1336 _periodso
drwxr-xr-x  2 francesco dottor    4096  1 set 1336 rpy
drwxr-xr-x  3 francesco dottor    4096  1 set 1336 sandbox
-rw-r--r--  1 francesco dottor     844  1 set 1336 setuppy
drwxr-xr-x  3 francesco dottor    4096  1 set 1336 sparse
-rwxr-xr-x  1 francesco dottor 1065313  1 set 1336 _sparseso
drwxr-xr-x  3 francesco dottor    4096  1 set 1336 src
drwxr-xr-x  3 francesco dottor    4096  1 set 1336 stats
drwxr-xr-x  3 francesco dottor    4096  1 set 1336 tests
drwxr-xr-x  3 francesco dottor    4096  1 set 1336 tools
drwxr-xr-x  3 francesco dottor    4096  1 set 1336 tseries
drwxr-xr-x  2 francesco dottor    4096  1 set 1336 util
-rw-r--r--  1 francesco dottor      42  1 set 1336 versionpy
-rw-r--r--  1 francesco dottor     204  1 set 1336 versionpyc
",1601780,,1601780.0,2012-09-02 17:49:09,2012-09-02 17:49:09,how include pandas library on my python package?,<import><module><pandas>,1.0,3.0,,
11987270,1,,2012-08-16 12:36:11,0,89,"I'm trying to run sortlevel(0 0) on a DataFrame with a MultiIndex (3 levels) and a size of about 900'000x4

>>>dataas_matrix()shape
(899262  4)
>>>datasortlevel(0 0) #",905596,,905596.0,2012-08-16 16:06:58,2012-11-26 16:07:45,Getting MemoryError from DataFrame.sortlevel in pandas,<pandas>,3.0,6.0,,
11991627,1,11994944.0,2012-08-16 16:27:44,1,1760,"I have a large Pandas DataFrame


DatetimeIndex: 3425100 entries  2011-12-01 00:00:00 to 2011-12-31 23:59:59
Data columns:
sig_qual    3425100  non-null values
heave       3425100  non-null values
north       3425099  non-null values
west        3425097  non-null values
dtypes: float64(4)


I select a subset of that DataFrame using ix[start_datetime:end_datetime] and I pass this to a peakdetect function which returns the index and value of the local maxima and minima in two seperate lists I extract the index position of the maxima and using DataFrameindex I get a list of pandas TimeStamps

I then attempt to extract the relevant subset of the large DataFrame by passing the list of TimeStamps to ix[] but it always seems to return an empty DataFrame I can loop over the list of TimeStamps and get the relevant rows from the DataFrame but this is a lengthy process and I thought that ix[] should accept a list of values according to the docs? 
(Although I see that the example for Pandas 07 uses a numpyndarray of numpydatetime64)

Update:
A small 8 second subset of the DataFrame is selected below  # lines show some of the values:

y = raw_disp['heave']ix[datetime(2011 12 30 0 0 0):datetime(2011 12 30 0 0 8)]
#csv representation of y time-series 
2011-12-30 00:00:00 -3100
2011-12-30 00:00:01 -2380
2011-12-30 00:00:01500000 -1140
2011-12-30 00:00:02500000 600
2011-12-30 00:00:03 1850
2011-12-30 00:00:04 2590
2011-12-30 00:00:04500000 2310
2011-12-30 00:00:05500000 1390
2011-12-30 00:00:06500000 550
2011-12-30 00:00:07 -490
2011-12-30 00:00:08 -1440

index = yindex

[2011-12-30 00:00:00    2011-12-30 00:00:08]
Length: 11  Freq: None  Timezone: None

#_max returned from the peakdetect function  one local maxima for this 8 seconds period
_max = [[5  2590]]

indexes = [x[0] for x in _max]
#[5]

timestamps = [index[z] for z in indexes]
#[]

print raw_dispix[timestamps]
#Empty DataFrame
#Columns: array([sig_qual  heave  north  west  extrema]  dtype=object)
#Index: 
#Length: 0  Freq: None  Timezone: None

for timestamp in timestamps:
    print raw_dispix[timestamp]
#sig_qual      0
#heave       259
#north        27
#west        132
#extrema       0
#Name: 2011-12-30 00:00:04


Update 2:
I created a gist  which actually works because when the data is loaded in from csv the index columns of timestamps are stored as numpy array of objects which appear to be strings Unlike in my own code where the index is of type  and each element is of type   I thought passing a list of pandaslibTimestamp would work the same as passing individual timestamps  would this be considered a bug?

If I create the original DataFrame with the index as a list of strings  querying with a list of strings works fine It does increase the byte size of the DataFrame significantly though

Update 3:
The error only appears to occur with very large DataFrames  I reran the code on varying sizes of DataFrame ( some detail in a comment below ) and it appears to occur on a DataFrame above 27 million records Using strings as opposed to TimeStamps resolves the issue but increases memory usage

Fixed
In latest github master (18/09/2012)  see comment from Wes at bottom of page",1135883,,1135883.0,2012-09-19 14:42:37,2012-09-19 14:42:37,Selecting a subset of a Pandas DataFrame indexed by DatetimeIndex with a list of TimeStamps,<python><time-series><pandas>,1.0,,,
12127315,1,,2012-08-26 03:02:41,0,109,"How do I tell pandas' DataFrameplot(subplots=True)  not to use an offset on the tick labels?

Normally for a single plot I can issue:

matplotlibpyplotticklabel_format(axis='y'  useOffset=False)

but it doesn't seem to work for the subplot option",1625344,,,,2012-08-27 07:09:19,pandas subplot useoffset=False,<pandas>,1.0,,,
12169170,1,12169357.0,2012-08-28 23:58:47,0,83,"I have a dataframe with columns A B I need to create a column C such that for every record / row:

C = max(A  B)

How should I go about doing this?

Thanks",968643,,,,2012-08-29 00:27:33,How should I take the max of 2 columns in a dataframe and make it another column?,<python><data.frame><pandas>,1.0,0.0,,
12182744,1,12183507.0,2012-08-29 16:46:39,5,339,"I want to apply a function with arguments to a series in python pandas:

x = my_seriesapply(my_function  more_arguments_1)
y = my_seriesapply(my_function  more_arguments_2)



The documentation describes support for an apply method  but it doesn't accept any arguments  Is there a different method that accepts arguments?  Alternatively  am I missing a simple workaround?",660664,,,,2012-08-29 17:36:10,python pandas: apply a function with arguments to a series,<python><pandas><apply>,1.0,1.0,,
12223689,1,12254295.0,2012-08-31 23:41:48,0,48,"I have a dataframe that holds data at a particular level of aggregation - let's call it regional

I also have a dict that explains how these regions are formed Something like this:

map = {'Alabama': 'region_1'  'Arizona': 'region_1'  'Arkansas': 'region_2'  }


And a set of weights for each state within its region  stored as a series:

Alabama    25
Arizona    75
Arkansas   33



Is there an efficient way to apply this disaggregation map to get a new dataframe at a State level?

Aggregation is easy:

df_regional = df_statesgroupby(map)sum()


But how can I do disaggregation?",1639821,,1639821.0,2012-09-01 02:22:49,2012-09-03 21:39:36,"How can I efficiently disaggregate data in a Dataframe (given a set of weights, mapping, etc.)?",<pandas>,2.0,,,
12250475,1,12252786.0,2012-09-03 15:16:55,1,365,"I'd like to create a pandas DataFrame from the following csv file:


EUR MS 3M;20111025;7d;11510;
EUR MS 3M;20111024;7d;11530;
EUR MS 3M;20111025;1m;11580;
EUR MS 3M;20111024;1m;11590;



The DataFrame ideally would have a name given as the first column value (""EUR MS 3M"")  an index composed by values in the second column (""20111025"")  and where the column names for the DataFrame would be taken from the third column (""7d""  ""1m"" etc)  with correspondent values given in the last csv column (""1150"" etc)
I have tried with different methods  but couldn't sort this thing out the proper way I think the first thing I should do should be to 'unstack' the values in the csv  in order to have an aligned index first  and then create a DataFrame  but really don't how
Anyone more expert than me have any clue? I'm started learning pandas only few weeks ago
Thanks for your kind help!

(I'm editing the question to make thing a bit clearer:
I'd like to get a dataframe named EUR MS 3M containing columns such as: 

index    7d     1m 
20111024 11530 11590 
20111025 11510 11580 


hope it is a bit clearer now Thanks)",1529852,,1529852.0,2012-09-03 16:48:34,2012-09-03 18:52:56,Create a pandas DataFrame from a csv stacked file,<python><csv><data.frame><pandas>,1.0,2.0,,
12193253,1,12193483.0,2012-08-30 08:54:00,0,335,"I start out with a timeseries and use a loop to produce new timeseries I would like to merge the existing series with the new ones subsequently in every loop  while preserving their (different) indices I tried concat  but somehow I cannot add another series after the first one

orig = pdSeries(data  index=index)
for i in list:
    new = pdSeries()
    orig = pdconcat([orig  new]  axis=1)


Thanks for your help!",1190648,,,,2012-08-30 14:09:19,"pandas merge timeseries, concat/append/... ?",<python><pandas>,2.0,1.0,,
12207326,1,12207352.0,2012-08-31 00:10:12,3,245,"One last newbie pandas question for the day:  How do I generate a table for a single Series?

For example:

my_series = pandasSeries([1 2 2 3 3 3])
pandasmagical_frequency_function( my_series )

>> {
     1 : 1 
     2 : 2  
     3 : 3
   }


Lots of googling has led me to Seriesdescribe() and pandascrosstabs  but neither of these does quite what I need: one variable  counts by categories  Oh  and it'd be nice if it worked for different data types: strings  ints  etc",660664,,,,2012-09-26 20:23:30,pandas: Frequency table for a single variable,<python><pandas>,1.0,,,
12250024,1,12250416.0,2012-09-03 14:44:51,3,243,"I'm currently using pandas to read an excel file and present its sheet names to the user  so he can select which sheet he would like to use The problem is that the files are really big (70 columns x 65k rows)  taking up to 14s to load on a notebook (the same data in a CSV file is taking 3s)

My code in panda goes like this:

xls = pandasExcelFile(path)
sheets = xlssheet_names


I tried xlrd before  but obtained similars results This was my code with xlrd:

xls = xlrdopen_workbook(path)
sheets = xlssheet_names


So  can anybody sugest me a faster way to retrieve the sheet names from an excel file than reading the whole file?",1643926,,,,2012-09-03 15:12:35,How to obtain sheet names from XLS files without loading the whole file in Python?,<python><excel><pandas><xlrd><sheet>,1.0,2.0,,
12269158,1,12273693.0,2012-09-04 18:14:35,0,377,"Suppose I have the example TimeSeries below:

ts = pandasTimeSeries({'a':[1 2 3 4 5]  'b':[6 7 8 9 10]})


The best method that I could think of for converting this to a 5-column DataFrame is as follows:

tsDataFrame = pandasDataFrame(
                               [tuple(elem) for elem in tsvalues]  
                               index=tsindexvalues
                              )


Is this the best-practices idiom for making this happen  or is there any sort of constructor or built-in that sort of ""flattens"" a column whose values are arrays into a set of columns?",567620,,,,2012-09-05 02:21:45,Python Pandas idiom for converting TimeSeries with lists as values into a DataFrame,<python><data.frame><time-series><pandas>,1.0,,,
12278347,1,,2012-09-05 09:29:34,3,804,"I think the title covers the issue  but to elucidate:

The pandas python package has a DataFrame data type for holding table data in python It also has a convenient interface to the hdf5 file format  so pandas DataFrames (and other data) can be saved using a simple dict-like interface (assuming you have pytables installed)

import pandas 
import numpy
d = pandasHDFStore('datah5')
d['testdata'] = pandasDataFrame({'N': numpyrandomrandn(5)})
dclose()


So far so good However  if I then try to load that same hdf5 into R I see things aren't so simple:

> library(hdf5)
> hdf5load('datah5')
NULL
> testdata
$block0_values
         [ 1]      [ 2]      [ 3]       [ 4]      [ 5]
[1 ] 1498147 08843877 -1081656 008717049 -1302641
attr( ""CLASS"")
[1] ""ARRAY""
attr( ""VERSION"")
[1] ""23""
attr( ""TITLE"")
[1] """"
attr( ""FLAVOR"")
[1] ""numpy""

$block0_items
[1] ""N""
attr( ""CLASS"")
[1] ""ARRAY""
attr( ""VERSION"")
[1] ""23""
attr( ""TITLE"")
[1] """"
attr( ""FLAVOR"")
[1] ""numpy""
attr( ""kind"")
[1] ""string""
attr( ""name"")
[1] ""N""

$axis1
[1] 0 1 2 3 4
attr( ""CLASS"")
[1] ""ARRAY""
attr( ""VERSION"")
[1] ""23""
attr( ""TITLE"")
[1] """"
attr( ""FLAVOR"")
[1] ""numpy""
attr( ""kind"")
[1] ""integer""
attr( ""name"")
[1] ""N""

$axis0
[1] ""N""
attr( ""CLASS"")
[1] ""ARRAY""
attr( ""VERSION"")
[1] ""23""
attr( ""TITLE"")
[1] """"
attr( ""FLAVOR"")
[1] ""numpy""
attr( ""kind"")
[1] ""string""
attr( ""name"")
[1] ""N""

attr( ""TITLE"")
[1] """"
attr( ""CLASS"")
[1] ""GROUP""
attr( ""VERSION"")
[1] ""10""
attr( ""ndim"")
[1] 2
attr( ""axis0_variety"")
[1] ""regular""
attr( ""axis1_variety"")
[1] ""regular""
attr( ""nblocks"")
[1] 1
attr( ""block0_items_variety"")
[1] ""regular""
attr( ""pandas_type"")
[1] ""frame""


Which brings me to my question: ideally I would be able to save back and forth from R to pandas I can obviously write a wrapper from pandas to R (I think though I think if I use a pandas MultiIndex that might become trickier)  but I don't think I can easily then use that data back in pandas Any suggestions?

Bonus: what I really want to do is use the datatable package in R with a pandas dataframe (the keying approach is suspiciously similar in both packages) Any help on that one greatly appreciated",678486,,,,2013-01-18 20:45:51,How can I efficiently save a python pandas dataframe in hdf5 and open it as a dataframe in R?,<python><r><pandas><data.table><hdf5>,3.0,3.0,2.0,
12260124,1,12262719.0,2012-09-04 09:05:41,2,114,"I'm plotting a timeseries in pandas using matplotlib and I'm trying to color a plot look like this



I have the times for the A-F points I've tried to get the position of them in the plot using 

gcf()canvasmpl_connect('button_press_event'  debug_print_onclick_event)


and ended up with x positions being around 22'395'850 (not even close to unixtime :S)

My code basically looks like this:

plot = dataplot(legend=False) #where data is the timeseries (pandasDataFrame)
plotadd_patch(
    pltRectangle(
        (0 22395760)  
        60  
        45 
        facecolor='green' 
        edgecolor='green'
    )
)
pltdraw()
pltshow()


But nothings of the patch shows up
Also tested to use time directly  it actually ran but no patch was rendered

pltRectangle(
    (0 datetime_D)  
    60  
    4*pandasdatetoolsMinutes(15) 
    facecolor='green' 
    edgecolor='green'
)


What is the underlying type? How should I position things in time in matplotlib? Any uglyhack working is appreciated ",905596,,,,2012-09-04 11:39:03,How to plot a rectangle behind a function over time,<matplotlib><pandas>,1.0,5.0,,
12269528,1,12273646.0,2012-09-04 18:43:55,2,700,"I have several CSV files with the format:

Year Day Hour Min Sec P1'S1
 2003   1  0  0 1222  0541
 2003   1  1  0 2069  0708
 2003   1  2  0  495  0520
 2003   1  3  0 1342  0539



(where day  is the day of the year) and I'm trying to read them using the pandas library (seems a fantastic lib so far)

There is a built-in function to read CSV in pandas  and even better  that function supposedly checks the columns for a date type and automatically uses that as an index (which would be exactly perfect for what I'm doing)

The thing is  I cannot get it to work with date data in this format

I tried:

data = pdread_csv(""csvFilecsv""  index_col=[0  1]     index_col=[0  1  2  3  4] parse_dates=True)


but it only gets the year correctly:

In [36]: dataindex
Out[36]: 
MultiIndex
[(  1  0  0  1222)
 (  1  1  0  2069)
 (  1  2  0  495)  
 (  365  21  0  377)
 (  365  22  0  146)
 (  365  23  0  1336)]


From the documentation  I see that you can specify the ""date_parser"" attribute in the read_csv function of pandas But the documentation doesn't show how and I'm not being able to figure it out
Anyone with experience in the subject that can give a hand

Cheers 
Bruno",865662,,865662.0,2012-09-04 18:48:55,2012-09-05 02:14:39,"Using python pandas to parse CSV with date in format Year, Day, Hour, Min, Sec",<python><pandas><dateutil>,1.0,,2.0,
12297759,1,12298872.0,2012-09-06 10:12:12,0,161,"I have a pandasSeries of time-stamped data - basically a sequence of events:

0      2012-09-05 19:28:52
1      2012-09-05 19:28:52
2      2012-09-05 19:44:37
3      2012-09-05 19:44:37
4      2012-09-05 20:04:53
5      2012-09-05 20:04:53
6      2012-09-05 20:12:59
7      2012-09-05 20:13:15
8      2012-09-05 20:13:15
9      2012-09-05 20:13:15


I'd like to create a pandasTimeSeries over a specific pandasdate_range (eg 15min interval; pandasdate_range(start  end  freq='15T')) which holds the count of events for each period How can this be accomplished?

thanks  
 Peter",177614,,,,2012-09-06 11:16:56,Reindex time-stamped data with date_range,<python><time-series><pandas><data-analysis>,1.0,,,
12307099,1,12307162.0,2012-09-06 19:32:25,1,244,"Assume I have a pandas DataFrame with two columns  A and B I'd like to modify this DataFrame (or create a copy) so that B is always NaN whenever A is 0 How would I achieve that?

I tried the following

df['A'==0]['B'] = npnan


and

df['A'==0]['B']valuesfill(npnan)


without success",645212,,,,2012-09-26 17:14:39,Modifying a subset of rows in a pandas dataframe,<python><pandas>,2.0,,,
12167634,1,,2012-08-28 21:14:58,1,137,"I have a Python dataframe with 1408 lines of data My goal is to compare the largest number and smallest number associated with a given weekday during one week to the next week's number on the same day of the week which the prior largest/smallest occurred Essentially  I want to look at quintiles (since there are 5 days in a business week) rank 1 and 5 and see how they change from week to week Build a cdf of numbers associated to each weekday

To clean the data  I need to remove 18 weeks in total from it That is  every week in the dataframe associated with holidays plus the entire week following week after the holiday occurred 
After this  I think I should insert a column in the dataframe that labels all my data with Monday through Friday-- for all the dates in the file (there are 6 years of data) The reason for labeling M-F is so that I can sort each number associated to the day of the week in ascending order And query on the day of the week
Methodological suggestions on either 1 or 2 or both would be immensely appreciated

Thank you!",1374969,,,,2012-09-05 02:19:23,data cleaning a python dataframe,<python><data.frame><pandas>,1.0,,,
12190716,1,12192290.0,2012-08-30 06:00:25,2,180,"I have a dataframe 'RPT' indexed by (STK_ID RPT_Date)  contains the accumulated sales of stocks for each qurter:

                       sales
STK_ID  RPT_Date
000876  20060331      798627000
        20060630     1656110000
        20060930     2719700000
        20061231     3573660000
        20070331      878415000
        20070630     2024660000
        20070930     3352630000
        20071231     4791770000
600141  20060331      270912000
        20060630      658981000
        20060930     1010270000
        20061231     1591500000
        20070331      319602000
        20070630      790670000
        20070930     1250530000
        20071231     1711240000


I want to calculate the single qurterly sales using 'groupby' by STK_ID & RPT_Yr  such as : RPTgroupby('STK_ID' 'RPT_Yr')['sales']transform(lambda x: x-xshift(1))   how to do that ?

suppose I can get the year by lambda x : datetimestrptime(x  '%Y%m%d')year  ",1072888,,1072888.0,2012-08-30 06:08:21,2012-08-30 10:50:54,How to do a 'groupby' by multilevel index in Pandas,<python><pandas>,1.0,,1.0,
12324162,1,,2012-09-07 19:21:24,0,248,"I've tick by tick data for Forex pairs 

Here is a sample of EURUSD/EURUSD-2012-06csv

EUR/USD 20120601 00:00:00207 123618 12363
EUR/USD 20120601 00:00:00209 123618 123631
EUR/USD 20120601 00:00:00210 123618 123631
EUR/USD 20120601 00:00:00211 123623 123631
EUR/USD 20120601 00:00:00240 123623 123627
EUR/USD 20120601 00:00:00423 123622 123627
EUR/USD 20120601 00:00:00457 12362 123626
EUR/USD 20120601 00:00:01537 12362 123625
EUR/USD 20120601 00:00:03010 12362 123624
EUR/USD 20120601 00:00:03012 12362 123625


Full tick data can be downloaded here
http://dlfreefr/k4vVF7aOD

Columns are :

Symbol Datetime Bid Ask


I would like to convert this tick by tick data to Renko chart
http://wwwinvestopediacom/terms/r/renkochartasp

A parameter of the chart is candles height (close-open) : let's call it candle_height

I would like to use Python and Pandas library to achieve this task

I've done a little part of the job reading the tick by tick data file

I want to get data like

Id Symbol open_price close_price


to be able to draw Renko

Id is the number of the candle

Price on candle will be based on Bid column

Here is the code

#!/usr/bin/env python

import pandas as pd
import matplotlibpyplot as plt
import numpy as np
from matplotlibfinance import candlestick

def candle_price_ref(price  candle_height):
  #return(int(price/candle_height)*candle_height)
  return(round(price/candle_height)*candle_height)

print(""Reading CSV (please wait)"")
df = pdread_csv('test_EURUSD/EURUSD-2012-07csv'  names=['Symbol'  'Date_Time'  'Bid'  'Ask']  index_col=1)
print(""End of reading"")

df['Bid'] = df['Bid']
#candle_height = 00015
#candle_height = 00010
#candle_height = 00005
candle_height = 00001
#candle_height = 0000001

price = dfix[0]['Bid']
price_ref = candle_price_ref(price  candle_height)

ID = 0
#print(""ID={0} price_ref={1}""format(ID  price_ref))

candle_price_open = []
candle_price_close = []

candle_price_openappend(price) # price ou price_ref
candle_price_closeappend(price)

for i in range(len(df)):
  price = dfix[i]['Bid']
  candle_price_close[ID] = price

  new_price_ref = candle_price_ref(price  candle_height)


  if new_price_ref!=price_ref:
    candle_price_close[ID]=new_price_ref
    price_ref = new_price_ref
    ID += 1
    candle_price_openappend(price_ref)
    candle_price_closeappend(price_ref)

IDs=range(ID+1)
volume=npzeros(ID+1)

a_price_open=nparray(candle_price_open)
a_price_close=nparray(candle_price_close)
b_green_candle = a_price_open ",1555275,,1555275.0,2012-09-08 19:46:39,2012-09-08 19:46:39,From tick by tick data to Renko chart,<python><numpy><scipy><finance><pandas>,1.0,2.0,,
12326641,1,12327090.0,2012-09-07 23:27:20,2,123,"I am very new to Pandas but familiar with Numpy and Python

Supposing I have a `PandasDataFrame' of X Y points (float64) indexed by time (datetime)  how can I pythonically calculate speeds from that  providing I already know how to calculate euclidean distances between points?

EDIT: I have just read the help on pandasSeriesdiff()  but still I'd like to ""replace"" the subtraction used on diff by another function  say `euclidean_distance()' Is there a way to do that?

DataFrame looks like (index in first column  positions in second):

2009-08-07 16:16:44    [37800185  -122426361]
2009-08-07 16:16:48    [37800214  -122426153]
2009-08-07 16:16:49    [37800222  -122426118]
2009-08-07 16:16:52    [37800197  -122426072]
2009-08-07 16:17:32    [37800214  -122425903]
2009-08-07 16:17:34    [37800236  -122425826]
2009-08-07 16:17:40    [37800282  -122425534]
2009-08-07 16:17:44    [37800307  -122425315]
2009-08-07 16:17:46    [37800324  -122425207]
2009-08-07 16:17:47    [37800331  -122425153]
2009-08-07 16:17:49    [37800343  -122425047]
2009-08-07 16:17:50    [37800355  -122424994]
2009-08-07 16:17:51    [37800362  -122424942]
2009-08-07 16:17:54    [37800378  -122424796]
2009-08-07 16:17:56    [37800357  -122424764]


What I want is some way to get speeds from that  providing the speed of first data sample will always be zero by definition (no known timedelta from a previous sample)

Thanks a lot!",401828,,401828.0,2012-09-07 23:43:26,2012-09-08 00:55:08,Calculate speed from timestamped positions in Pandas.DataFrame,<python><time-series><pandas>,1.0,,,
12322289,1,12326113.0,2012-09-07 16:49:21,4,303,"kdb+ has an aj function that is usually used to join tables along time columns

Here is an example where I have trade and quote tables and I get the prevailing quote for every trade

q)5# t
time         sym  price size 
-----------------------------
09:30:00439 NVDA 1342 60511
09:30:00439 NVDA 1342 60511
09:30:02332 NVDA 1342 100  
09:30:02332 NVDA 1342 100  
09:30:02333 NVDA 1341 100  

q)5# q
time         sym  bid   ask   bsize asize
-----------------------------------------
09:30:00026 NVDA 1334 1344 3     16   
09:30:00043 NVDA 1334 1344 3     17   
09:30:00121 NVDA 1336 1365 1     10   
09:30:00386 NVDA 1336 1352 21    1    
09:30:00440 NVDA 134  1344 15    17

q)5# aj[`time; t; q]
time         sym  price size  bid   ask   bsize asize
-----------------------------------------------------
09:30:00439 NVDA 1342 60511 1336 1352 21    1    
09:30:00439 NVDA 1342 60511 1336 1352 21    1    
09:30:02332 NVDA 1342 100   1334 1361 1     1    
09:30:02332 NVDA 1342 100   1334 1361 1     1    
09:30:02333 NVDA 1341 100   1334 1351 1     1  


How can I do the same operation using pandas? I am working with trade and quote dataframes where the index is datetime64

In [55]: quoteshead()
Out[55]: 
                              bid    ask  bsize  asize
2012-09-06 09:30:00026000  1334  1344      3     16
2012-09-06 09:30:00043000  1334  1344      3     17
2012-09-06 09:30:00121000  1336  1365      1     10
2012-09-06 09:30:00386000  1336  1352     21      1
2012-09-06 09:30:00440000  1340  1344     15     17

In [56]: tradeshead()
Out[56]: 
                            price   size
2012-09-06 09:30:00439000  1342  60511
2012-09-06 09:30:00439000  1342  60511
2012-09-06 09:30:02332000  1342    100
2012-09-06 09:30:02332000  1342    100
2012-09-06 09:30:02333000  1341    100


I see that pandas has an asof function but that is not defined on the DataFrame  only on the Series object I guess one could loop through each of the Series and align them one by one  but I am wondering if there is a better way?",220120,,220120.0,2012-09-07 17:01:30,2012-09-09 02:22:04,KDB+ like asof join for timeseries data in pandas?,<python><join><time-series><pandas><kdb>,2.0,,2.0,
12353359,1,12361644.0,2012-09-10 14:04:02,2,150,"I have a Pandas dataframe 'dt = myfunc()'   and copy the screen output from IDLE as below:

>>> from __future__ import division
>>> dt = __get_stk_data__(['*']  frq='CQQ'  from_db=False) # my function
>>> dt = dt[dt['ebt']==0][['tax' 'ebt']]
>>> type(dt)

>>> dt
                tax ebt
STK_ID RPT_Date        
000719 20100331   0   0
       20100630   0   0
       20100930   0   0
       20110331   0   0
002164 20080331   0   0
300155 20120331   0   0
600094 20090331   0   0
       20090630   0   0
       20090930   0   0
600180 20090331   0   0
600757 20110331   0   0
>>> dt['tax_rate'] = dttax/dtebt
Traceback (most recent call last):
  File """"  line 1  in 
  File ""D:\Python\Lib\site-packages\pandas\core\seriespy""  line 72  in wrapper
    return Series(na_op(selfvalues  othervalues) 
  File ""D:\Python\Lib\site-packages\pandas\core\seriespy""  line 53  in na_op
    result = op(x  y)
ZeroDivisionError: float division
>>> 


It costs me a lot time to figure out why Pandas raises the 'ZeroDivisionError: float division'   while Pandas works very well for below sample code: 

tuples = [('000719' '20100331') ('000719' '20100930') ('002164' '20080331')]
index = MultiIndexfrom_tuples(tuples  names=['STK_ID'  'RPT_Date'])
dt =DataFrame({'tax':[0 0 0] 'ebt':[0 0 0]} index=index)
dt['tax_rate'] = dttax/dtebt

>>> dt
                 ebt  tax  tax_rate
STK_ID RPT_Date                    
000719 20100331    0    0       NaN
       20100930    0    0       NaN
002164 20080331    0    0       NaN
>>> 


I expect Pandas offer 'NaN' for both cases  why 'ZeroDivisionError' happens in first case ? How to fix it ? 

below codes & screen output is attached to provide further information to debug

def __by_Q__(df):
    ''' this function transforms the input financial report data (which
        is accumulative) to qurterly data
    '''
    df_q1=df[dfindexmap(lambda x: x[1]endswith(""0331""))]

    print 'before diff:\n'
    print dfdtypes
    df_delta = dfdiff()
    print '\nafter diff: \n'
    print df_deltadtypes


    q1_mask = df_deltaindexmap(lambda x: x[1]endswith(""0331""));
    df_q234 = df_delta[~q1_mask]

    rst = concat([df_q1 df_q234])

    rst=rstsort_index()
    return rst


screen output:

before diff:

sales                      float64
discount                    object
net_sales                  float64
cogs                       float64
ebt                        float64
tax                        float64

after diff: 

sales                      object
discount                   object
net_sales                  object
cogs                       object
ebt                        object
tax                        object
",1072888,,1072888.0,2012-09-11 13:18:43,2012-09-11 13:18:43,Why Pandas cause 'ZeroDivisionError' in one case but not in the other?,<python><pandas>,2.0,2.0,,
12368180,1,12369472.0,2012-09-11 10:58:50,1,127,"I'm trying to create a new dataframe that derives from pivoting this one:

                 dataframe name      date tenor mat strike      capvol
      0   EUR CapFloor Volat_3m  20120903    3m  1y   025  152202160
      1   EUR CapFloor Volat_3m  20120903    3m  1y   050  151969370
      2   EUR CapFloor Volat_3m  20120903    3m  1y      1  149266970
      3   EUR CapFloor Volat_3m  20120903    3m  1y   150  152940750
      4   EUR CapFloor Volat_3m  20120903    3m  1y      2  157229350
      5   EUR CapFloor Volat_3m  20120903    3m  1y   225  159325890


My goal is to have data grouped by date  mat and strike (I can drop the '3m' and 'dataframe name' columns since they're common to all data)
I tried with the command:

      df = framepivot('date' 'mat' 'strike')


but get this error:

      'Index contains duplicate entries  cannot reshape'


altough I checked with my data and contains no duplpicates on rows

Can anyone help me with this issue  or propose an alternative approach to the pivot function?

Thanks for your help",1529852,,,,2012-09-11 12:13:43,Duplicate entries for index in pandas pivot function,<python><data.frame><pivot><pandas>,1.0,,1.0,
12372899,1,12373535.0,2012-09-11 15:14:09,1,98,"I've been trying to fill a DataFrame and Series using fillna with the value and limit keywords The limit is respected when not including value  but as soon as including value the limits are no longer respected Here's an example using a DataFrame:

import pandas as pd
import numpy as np

df = pdDataFrame(nprandomrandn(5  3)  index=['a'  'c'  'e'  'f'  'h'] columns=['one'  'two' 'three'])
df2 = dfreindex(['a'  'b'  'c'  'd'  'e'  'f'  'g' 'h' 'i'  'j' 'k'])

In [7]: df2
Out[7]:
        one       two     three
a -0942695  0465658 -0966754
b       NaN       NaN       NaN
c -1208036  0287274 -1116466
d       NaN       NaN       NaN
e  0041212  0065966 -1895570
f  0869104 -3481962 -0280699
g       NaN       NaN       NaN
h -1151732 -0310296 -1701202
i       NaN       NaN       NaN
j       NaN       NaN       NaN
k       NaN       NaN       NaN

In [8]: df2fillna(method='pad'  limit=1)
Out[8]:
        one       two     three
a -0942695  0465658 -0966754
b -0942695  0465658 -0966754
c -1208036  0287274 -1116466
d -1208036  0287274 -1116466
e  0041212  0065966 -1895570
f  0869104 -3481962 -0280699
g  0869104 -3481962 -0280699
h -1151732 -0310296 -1701202
i -1151732 -0310296 -1701202
j       NaN       NaN       NaN
k       NaN       NaN       NaN

In [9]: df2fillna(value=999 method='pad'  limit=1)
Out[9]:
          one         two       three
a   -0942695    0465658   -0966754
b  999000000  999000000  999000000
c   -1208036    0287274   -1116466
d  999000000  999000000  999000000
e    0041212    0065966   -1895570
f    0869104   -3481962   -0280699
g  999000000  999000000  999000000
h   -1151732   -0310296   -1701202
i  999000000  999000000  999000000
j  999000000  999000000  999000000
k  999000000  999000000  999000000


Am I missing something here  or is this a bug?

Cheers

Edit: using pandas 081 on python 27 with numpy 161",752092,,,,2012-09-11 15:50:04,Bug in pandas.Series/DataFrame.fillna limit?,<python><pandas>,1.0,,1.0,
12389898,1,,2012-09-12 13:58:12,0,705,"I want to mark some quantiles in my data  and for each row of the DataFrame  I would like the entry in a new column called eg ""xtile"" to hold this value

For example  suppose I create a data frame like this:

import pandas  numpy as np
dfrm = pandasDataFrame(
                        {
                         'A':nprandomrand(100)  
                         'B':(50+nprandomrandn(100))  
                         'C':nprandomrandint(low=0  high=3  size=(100 ))
                        }
                       )


And let's say I write my own function to compute the quintile of each element in an array I have my own function for this  but for example just refer to scipystatsmstatsmquantile

import scipystats as st
def mark_quintiles(x  breakpoints):
    # Assume this is filled in  using stmstatsmquantiles
    # This returns an array the same shape as x  with an integer for which
    # breakpoint-bucket that entry of x falls into


Now  the real question is how to use transform to add a new column to the data Something like this:

def transformXtiles(
                    dataFrame  
                    inputColumnName  
                    newColumnName 
                    breaks
                   ):
    dataFrame[newColumnName] = mark_quintiles(
                                              dataFrame[inputColumnName]values  
                                              breaks
                                             )
    return dataFrame


And then:

dfrmgroupby(""C"")transform(lambda x: transformXtiles(x  ""A""  ""A_xtile""  [02  04  06  08  10]))


The problem is that the above code will not add the new column ""A_xtile"" It just returns my data frame unchanged If I first add a column full of dummy values  like NaN  called ""A_xtile""  then it does successfully over-write this column to include the correct quintile markings

But it is extremely inconvenient to have to first write in the column for anything like this that I may want to add on the fly

Note that a simple apply will not work here  since it won't know how to make sense of the possibly differently-sized result arrays for each group",567620,,,,2012-09-12 18:19:35,Python Pandas: how to add a totally new column to a data frame inside of a groupby/transform operation,<python><group-by><transform><data.frame><pandas>,1.0,2.0,,
12183432,1,12183460.0,2012-08-29 17:30:24,0,84,"I have a dataframe with columns A  B I need to add a column C which is basically the division of entries in A by the entries in B

I tried this:

df['C'] = df['A'] / df['B']


But I need to convert to double or float before I do this How should I type-cast the dtype of the columns?

Thanks",968643,,,,2012-08-29 19:43:58,Typecasting before division (or any other mathematical operator) of columns in dataframes,<python><type-conversion><data.frame><pandas>,1.0,,,
12217960,1,12218559.0,2012-08-31 15:00:12,0,196,"Suppose I have two data frame 'df_a' & 'df_b'   both have the same index structure and columns  but some of the inside data elements are different:

>>> df_a
           sales cogs
STK_ID QT           
000876 1   100  100
       2   100  100
       3   100  100
       4   100  100
       5   100  100
       6   100  100
       7   100  100

>>> df_b
           sales cogs
STK_ID QT           
000876 5    50   50
       6    50   50
       7    50   50
       8    50   50
       9    50   50
       10   50   50


And now I want to replace the element of df_a by element of df_b which have the same (index  column) coordinate  and attach df_b's elements whose (index  column) coordinate beyond the scope of df_a  Just like add a patch 'df_b' to 'df_a' :

>>> df_c = patch(df_a df_b)
           sales cogs
STK_ID QT           
000876 1   100  100
       2   100  100
       3   100  100
       4   100  100
       5    50   50
       6    50   50
       7    50   50
       8    50   50
       9    50   50
       10   50   50


How to write the 'patch(df_a df_b)' function ?",1072888,,,,2012-09-03 19:49:30,How to replace&add the dataframe element by another dataframe in Python Pandas?,<python><pandas>,3.0,2.0,,
12329853,1,,2012-09-08 10:16:04,1,136,">>> df =DataFrame({'a':[1 2 3 4] 'b':[2 4 6 8]})
>>> df['x']=dfa + dfb
>>> df['y']=dfa - dfb
>>> df
   a  b   x  y
0  1  2   3 -1
1  2  4   6 -2
2  3  6   9 -3
3  4  8  12 -4


Now I want to rearrange the column sequence  which makes 'x' 'y' column to be the first & second columns by :

>>> df = df[['x' 'y' 'a' 'b']]
>>> df
    x  y  a  b
0   3 -1  1  2
1   6 -2  2  4
2   9 -3  3  6
3  12 -4  4  8


But if I have a long coulmns 'a' 'b' 'c' 'd'  and I don't want to explictly list the columns How can I do that ?

Or Does Pandas provide a function like set_column_sequence(dataframe col_name  seq) so that I can do  :  set_column_sequence(df 'x' 0) and set_column_sequence(df 'y' 1) ?",1072888,,,,2012-09-11 08:44:21,How to rearrange Pandas column sequence?,<python><pandas>,3.0,,,
12391758,1,12393132.0,2012-09-12 15:36:01,0,160,"I import into a panda DataFrame a directory of |-delimiteddat files The following code works  but I eventually run out of RAM with a MemoryError:

import pandas as pd
import glob

temp = []
dataDir = 'C:/users/richard/research/data/edgar/masterfiles'
for dataFile in globglob(dataDir + '/master_*dat'):
    print dataFile
    tempappend(pdread_table(dataFile  delimiter='|'  header=0))

masterAll = pdconcat(temp)


Is there a more memory efficient approach? Or should I go whole hog to a database? (I will move to a database eventually  but I am baby stepping my move to pandas) Thanks!

FWIW  here is the head of an example dat file:

cik|cname|ftype|date|fileloc
1000032|BINCH JAMES G|4|2011-03-08|edgar/data/1000032/0001181431-11-016512txt
1000045|NICHOLAS FINANCIAL INC|10-Q|2011-02-11|edgar/data/1000045/0001193125-11-031933txt
1000045|NICHOLAS FINANCIAL INC|8-K|2011-01-11|edgar/data/1000045/0001193125-11-005531txt
1000045|NICHOLAS FINANCIAL INC|8-K|2011-01-27|edgar/data/1000045/0001193125-11-015631txt
1000045|NICHOLAS FINANCIAL INC|SC 13G/A|2011-02-14|edgar/data/1000045/0000929638-11-00151txt
",334755,,,,2012-11-13 17:01:47,Memory efficient import many data files into panda DataFrame in Python,<python><pandas>,1.0,,,
12251483,1,12252958.0,2012-09-03 16:37:25,2,263,"I have a csv file with a time column representing POSIX timestamps in milliseconds When I read it in pandas  it correctly reads it as Int64 but I would like to convert it to a DatetimeIndex Right now I first convert it to datetime object and then cast it to a DatetimeIndex

In [20]: dftimehead()

Out[20]: 
0    1283346000062
1    1283346000062
2    1283346000062
3    1283346000062
4    1283346000300
Name: time

In [21]: map(datetimefromtimestamp  dftimehead()/1000)
Out[21]: 
[datetimedatetime(2010  9  1  9  0  0  62000) 
 datetimedatetime(2010  9  1  9  0  0  62000) 
 datetimedatetime(2010  9  1  9  0  0  62000) 
 datetimedatetime(2010  9  1  9  0  0  62000) 
 datetimedatetime(2010  9  1  9  0  0  300000)]

In [22]: pandasDatetimeIndex(map(datetimefromtimestamp  dftimehead()/1000))
Out[22]: 

[2010-09-01 09:00:00062000    2010-09-01 09:00:00300000]
Length: 5  Freq: None  Timezone: None


Is there an idiomatic way of doing this? And more importantly is this the recommended way of storing non-unique timestmaps in pandas?",220120,,,,2012-09-04 19:37:20,Idiomatic way to parse POSIX timestamps in pandas?,<python><numpy><pandas>,2.0,,,
12286607,1,12286958.0,2012-09-05 17:18:21,3,162,"I have a dataframe generated from Python's Pandas package How can I generate  heatmap using DataFrame from pandas package 

import numpy as np 
from pandas import *

Index= ['aaa' 'bbb' 'ccc' 'ddd' 'eee']
Cols = ['A'  'B'  'C' 'D']
df = DataFrame(abs(nprandomrandn(5  4))  index= Index  columns=Cols)

>>> df
          A         B         C         D
aaa  2431645  1248688  0267648  0613826
bbb  0809296  1671020  1564420  0347662
ccc  1501939  1126518  0702019  1596048
ddd  0137160  0147368  1504663  0202822
eee  0134540  3708104  0309097  1641090
>>> 
",1649335,,,,2012-09-05 18:39:45,python Making heatmap from DataFrame,<python><pandas><heatmap>,1.0,1.0,3.0,
12290844,1,,2012-09-05 22:49:52,2,89,"I have a 2D numpy array with 4 columns and a lot of rows (>10000  this number is not fixed)

I need to create n subarrays by the value of one of the columns; the closest question I found was How slice Numpy array by column value; nevertheless  I dont know the exact values in the field (they're floats and they change in every file I need)  but I know they are no more than 20

I guess I could read line by line  record the different values and then make the split  but I figure there is a more efficient way to do this

Thank you",1621048,,243238.0,2012-09-06 05:17:40,2012-09-06 08:00:27,How can I slice a numpy array by the value of the ith field?,<arrays><numpy><split><pandas>,2.0,,1.0,
12382197,1,12383860.0,2012-09-12 06:16:15,1,84,"It's hard to explain what I'm trying to do with words so here's an example

Let's say we have the following inputs:

In [76]: x
Out[76]: 
0    a
1    a
2    c
3    a
4    b

In [77]: z
Out[77]: ['a'  'b'  'c'  'd'  'e']


I want to get:

In [78]: ii
Out[78]: 
array([[1  0  0  0  0] 
       [1  0  0  0  0] 
       [0  0  1  0  0] 
       [1  0  0  0  0] 
       [0  1  0  0  0]])


ii is an array of boolean masks which can be applied to z to get back the original x

My current solution is to write a function which converts z to a list and uses the index method to get the index of the element in z and then generate a row of zeroes except for the index where there is a one This function gets applied to each row of x to get the desired result",243238,,,,2012-10-24 16:23:56,Generating a boolean mask indexing one array into another array,<numpy><pandas>,3.0,4.0,,
12393387,1,12393965.0,2012-09-12 17:23:17,1,163,"I have a column of data that contains strings  and I want to create a new column that takes only the first two characters from the corresponding data string

It seems logical to use the apply function for this  but it doesn't work like expected It does not even seem to be consistent with other uses of apply See below

In [205]: dfrm_test = pandasDataFrame({""A"":nprepeat(""the""  10)})

In [206]: dfrm_test
Out[206]:
     A
0  the
1  the
2  the
3  the
4  the
5  the
6  the
7  the
8  the
9  the

In [207]: dfrm_test[""A""]apply(lambda x: x+"" cat"")
Out[207]:
0    the cat
1    the cat
2    the cat
3    the cat
4    the cat
5    the cat
6    the cat
7    the cat
8    the cat
9    the cat
Name: A

In [208]: dfrm_test[""A""]apply(lambda x: x[0:2])
Out[208]:
0    the
1    the
Name: A


Based on this  it appears that apply does nothing but perform the NumPy equivalent of whatever is called inside That is  apply seems to execute the same thing as arr + "" cat"" in the first example And if NumPy happens to broadcast that  then it will work If not  then it won't

But this seems to break from what apply promises in the docs Below is the quotation for what pandasSeriesapply should expect:


  Invoke function on values of Series Can be ufunc or Python function expecting only single values (link)


It says explicitly that it can accept Python functions expecting only single values And the function that's not working (lambda x: x[0:2]) definitely satisfies that It doesn't say that the single argument must be an array And given that things like numpysqrt are commonly used for single inputs (so not exclusively arrays)  it seems natural to expect Pandas to work with any such function

Is there some way of using apply that I am missing here?

Note: I did write my own extra function below:

def ix2(arr):
    return npasarray([x[0:2] for x in arr])


and I verified that this version does work with Pandas apply But this is beside the point It would be easier to write something that operated externally on top of a Series object than to have to constantly write wrappers that use list comprehensions to effectively loop over the contents of the Series Isn't this specifically what apply is supposed to abstract away from the user?

I am using Pandas version 073  and it is on a workplace shared network  so there's no way to upgrade to the recent release

Added:

I was able to confirm that this behavior changes from version 073 to version 081 In 081 it works as expected with no NumPy ufunc wrapper

My guess is that in the code  someone was trying to use numpyvectorize or numpyfrompyfunc within a try-except statement Perhaps it did not work correctly with the particular lambda function I am using  and so in the except part of the code  it defaulted to just relying on generic NumPy broadcasting

It would be great to get some confirmation on this from a Pandas developer  if possible But in the meantime  the ufunc workaround should suffice",567620,,567620.0,2012-09-12 19:22:11,2012-10-24 16:19:17,Python Pandas: How to broadcast an operation using apply without writing a secondary function,<python><numpy><pandas><apply>,3.0,4.0,,
12406162,1,12449785.0,2012-09-13 12:23:23,0,260,"I get a KeyError when I try to plot a slice of a pandas DataFrame column with datetimes in it Does anybody know what could cause this?

I managed to reproduce the error in a little self contained example (which you can also view here: http://nbvieweripythonorg/3714142/):

import numpy as np
from pandas import DataFrame
import datetime
from pylab import *

test = DataFrame({'x' : [datetimedatetime(2012 9 10) + datetimetimedelta(n) for n in range(10)]  
                  'y' : range(10)})


Now if I plot:

plot(test['x'][0:5])


there is not problem  but when I plot:

plot(test['x'][5:10])


I get the KeyError below (and the error message is not very helpfull to me) This only happens with datetime columns  not with other columns (as far as I experienced) Eg plot(test['y'][5:10]) is not a problem

Ther error message:

---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
 in ()
----> 1 plot(test['x'][5:10])

C:\Python27\lib\site-packages\matplotlib\pyplotpyc in plot(*args  **kwargs)
   2456         axhold(hold)
   2457     try:
-> 2458         ret = axplot(*args  **kwargs)
   2459         draw_if_interactive()
   2460     finally:

C:\Python27\lib\site-packages\matplotlib\axespyc in plot(self  *args  **kwargs)
   3846         lines = []
   3847 
-> 3848         for line in self_get_lines(*args  **kwargs):
   3849             selfadd_line(line)
   3850             linesappend(line)

C:\Python27\lib\site-packages\matplotlib\axespyc in _grab_next_args(self  *args  **kwargs)
    321                 return
    322             if len(remaining)  323                 for seg in self_plot_args(remaining  kwargs):
    324                     yield seg
    325                 return

C:\Python27\lib\site-packages\matplotlib\axespyc in _plot_args(self  tup  kwargs)
    298             x = nparange(yshape[0]  dtype=float)
    299 
--> 300         x  y = self_xy_from_xy(x  y)
    301 
    302         if selfcommand == 'plot':

C:\Python27\lib\site-packages\matplotlib\axespyc in _xy_from_xy(self  x  y)
    215         if selfaxesxaxis is not None and selfaxesyaxis is not None:
    216             bx = selfaxesxaxisupdate_units(x)
--> 217             by = selfaxesyaxisupdate_units(y)
    218 
    219             if selfcommand!='plot':

C:\Python27\lib\site-packages\matplotlib\axispyc in update_units(self  data)
   1277         neednew = selfconverter!=converter
   1278         selfconverter = converter
-> 1279         default = selfconverterdefault_units(data  self)
   1280         #print 'update units: default=%s  units=%s'%(default  selfunits)
   1281         if default is not None and selfunits is None:

C:\Python27\lib\site-packages\matplotlib\datespyc in default_units(x  axis)
   1153         'Return the tzinfo instance of *x* or of its first element  or None'
   1154         try:
-> 1155             x = x[0]
   1156         except (TypeError  IndexError):
   1157             pass

C:\Python27\lib\site-packages\pandas\core\seriespyc in __getitem__(self  key)
    374     def __getitem__(self  key):
    375         try:
--> 376             return selfindexget_value(self  key)
    377         except InvalidIndexError:
    378             pass

C:\Python27\lib\site-packages\pandas\core\indexpyc in get_value(self  series  key)
    529         """"""
    530         try:
--> 531             return self_engineget_value(series  key)
    532         except KeyError  e1:
    533             if len(self) > 0 and selfinferred_type == 'integer':

C:\Python27\lib\site-packages\pandas\_enginespyd in pandas_enginesIndexEngineget_value (pandas\src\enginesc:1479)()

C:\Python27\lib\site-packages\pandas\_enginespyd in pandas_enginesIndexEngineget_value (pandas\src\enginesc:1374)()

C:\Python27\lib\site-packages\pandas\_enginespyd in pandas_enginesDictIndexEngineget_loc (pandas\src\enginesc:2498)()

C:\Python27\lib\site-packages\pandas\_enginespyd in pandas_enginesDictIndexEngineget_loc (pandas\src\enginesc:2460)()

KeyError: 0
",653364,,653364.0,2012-09-14 08:09:11,2012-09-16 19:03:26,KeyError when plotting a sliced pandas dataframe with datetimes,<numpy><matplotlib><pandas>,2.0,,,
12408826,1,12409459.0,2012-09-13 14:40:11,0,47,"I'm currently working on a dataframe that has a subscription date for every members I would like to stats subscriptions per months but default behavior would counts each dates of every month separately

I found a way of doing it modifying the date with slices and setting every dates day on 01 but i would rather use something that's made by pandas

Any suggestion on where i should head to ?",1518056,,,,2012-09-13 15:13:17,Simplify a date in a Dataframe,<python><django><pandas>,2.0,4.0,,
12410438,1,12411852.0,2012-09-13 16:05:55,0,337,"I very often want to create a new DataFrame by combining multiple columns of a grouped DataFrame The apply() function allows me to do that  but it requires that I create an unneeded index:

 In [359]: df = pandasDataFrame({'x': 3 * ['a'] + 2 * ['b']  'y': nprandomnormal(size=5)  'z': nprandomnormal(size=5)})

 In [360]: df
 Out[360]: 
    x         y         z
 0  a  0201980 -0470388
 1  a  0190846 -2089032
 2  a -1131010  0227859
 3  b -0263865 -1906575
 4  b -1335956 -0722087

 In [361]: dfgroupby('x')apply(lambda x: pandasDataFrame({'r': (xy + xz)sum() / xzsum()  's': (xy + xz ** 2)sum() / xzsum()}))
 ---------------------------------------------------------------------------
 ValueError                                Traceback (most recent call last)
 /home/emarkley/work/src/partner_analysis2/mainpy in ()
 ----> 1 dfgroupby('x')apply(lambda x: pandasDataFrame({'r': (xy + xz)sum() / xzsum()  's': (xy + xz ** 2)sum() / xzsum()}))

 /usr/local/lib/python32/site-packages/pandas-082dev-py32-linux-x86_64egg/pandas/core/groupbypy in apply(self  func  *args  **kwargs)
     267         applied : type depending on grouped object and function
     268         """"""
 --> 269         return self_python_apply_general(func  *args  **kwargs)
     270 
     271     def aggregate(self  func  *args  **kwargs):

 /usr/local/lib/python32/site-packages/pandas-082dev-py32-linux-x86_64egg/pandas/core/groupbypy in _python_apply_general(self  func  *args  **kwargs)
     417             group_axes = _get_axes(group)
     418 
 --> 419             res = func(group  *args  **kwargs)
     420 
     421             if not _is_indexed_like(res  group_axes):

 /home/emarkley/work/src/partner_analysis2/mainpy in (x)
 ----> 1 dfgroupby('x')apply(lambda x: pandasDataFrame({'r': (xy + xz)sum() / xzsum()  's': (xy + xz ** 2)sum() / xzsum()}))

 /usr/local/lib/python32/site-packages/pandas-082dev-py32-linux-x86_64egg/pandas/core/framepy in __init__(self  data  index  columns  dtype  copy)
     371             mgr = self_init_mgr(data  index  columns  dtype=dtype  copy=copy)
     372         elif isinstance(data  dict):
 --> 373             mgr = self_init_dict(data  index  columns  dtype=dtype)
     374         elif isinstance(data  maMaskedArray):
     375             mask = magetmaskarray(data)

 /usr/local/lib/python32/site-packages/pandas-082dev-py32-linux-x86_64egg/pandas/core/framepy in _init_dict(self  data  index  columns  dtype)
     454         # figure out the index  if necessary
     455         if index is None:
 --> 456             index = extract_index(data)
     457         else:
     458             index = _ensure_index(index)

 /usr/local/lib/python32/site-packages/pandas-082dev-py32-linux-x86_64egg/pandas/core/framepy in extract_index(data)
    4719 
    4720         if not indexes and not raw_lengths:
 -> 4721             raise ValueError('If use all scalar values  must pass index')
    4722 
    4723         if have_series or have_dicts:

 ValueError: If use all scalar values  must pass index

 In [362]: dfgroupby('x')apply(lambda x: pandasDataFrame({'r': (xy + xz)sum() / xzsum()  's': (xy + xz ** 2)sum() / xzsum()}  index=[0]))
 Out[362]: 
             r         s
 x                      
 a 0  1316605 -1672293
 b 0  1608606 -0972593


Is there any way to use apply() or some other function to get the same results without the extra index of zeros?",1429196,,,,2012-09-13 17:41:05,How to use Pandas groupby apply() without adding an extra index,<python><pandas><apply>,1.0,,,
12315810,1,,2012-09-07 09:54:46,1,79,"Suppose I have a pandas DF with 'A' 'B' 'C' as column name

A    B   C 
a1  b11 c11
a1  b12 c12
a2  b21 c21
a2  b22 c22


I can group by 'A'  but can I get

A  B  C
a1 [b11 b12]  [c11 c12]
a2 [b21 b22]  [c21 c22]


without any aggregation? Hopefully the order (b11 before b12) is kept as occured in the original table",1568919,,488657.0,2012-09-07 09:58:06,2012-09-08 23:50:09,Output pandas grouped dataframe without aggregation,<python><data.frame><pandas>,2.0,,,
12356501,1,12356541.0,2012-09-10 17:17:38,4,754,"I am working with the pandas library and I want to add two new columns to a dataframe df with n columns (n > 0)
These new columns result from the application of a function to one of the columns in the dataframe

The function to apply is like:

def calculate(x):
    operate
    return z  y


One method for creating a new column for a function returning only a value is:

df['new_col']) = df['column_A']map(a_function)


So  what I want  and tried unsuccesfully (*)  is something like:

(df['new_col_zetas']  df['new_col_ys']) = df['column_A']map(calculate)


What the best way to accomplish this could be ? I scanned the documentation with no clue 

*df['column_A']map(calculate) returns a panda Series each item consisting of a tuple z  y And trying to assign this to two dataframe columns produces a ValueError ",308903,,,,2012-09-10 17:20:49,Pandas: create two new columns in a dataframe with values calculated from a pre-existing column,<python><pandas>,1.0,,3.0,
12358360,1,,2012-09-10 19:39:24,4,297,"How do I order columns according to the values of the last row? In the example below  my final df will have columns in the following order: 'ddd' 'aaa' 'ppp' 'fff'

>>> df = DataFrame(nprandomrandn(10  4)  columns=['ddd'  'fff'  'aaa'  'ppp'])
>>> df
        ddd       fff       aaa       ppp
0 -0177438  0102561 -1318710  1321252
1  0980348  0786721  0374506 -1411019
2  0405112  0514216  1761983 -0529482
3  1659710 -1017048 -0737615 -0388145
4 -0472223  1407655 -0129119 -0912974
5  1221324 -0656599  0563152 -0900710
6 -1816420 -2898094 -0232047 -0648904
7  2793261  0568760 -0850100  0654704
8 -2180891  2054178 -1050897 -1461458
9 -1123756  1245987 -0239863  0359759
",1649335,,,,2012-09-10 20:23:12,Python pandas order column according to the values in a row,<python><sorting><pandas>,1.0,,3.0,
12373840,1,,2012-09-11 16:08:58,0,170,"I'd like to find the elapsed time since the first time an event was observed For this I saved each observation in a CSV file Each event is identified by a unique hash

Right now I'm doing the following:

from pandas import *
from bz2 import BZ2File
events = DataFramefrom_csv(BZ2File('eventscsvbz2'  'r')  sep='\t'  header=0  index_col=None)
m = eventsgroupby('hash')['timestamp']min()


at this point I have a Series indexed by the hash and the timestamp of the first observation
How would I use this to get the time offset for each row in the events DataFrame (simply timestamp - min(timestamp))?",49561,,938949.0,2012-09-12 19:36:36,2012-09-12 19:36:36,How to subtract the minimum from a column in a pandas DataFrame,<python><pandas>,1.0,,,
12417129,1,12417434.0,2012-09-14 01:50:48,0,241,"I'm trying to read this tab-delimited file into pandas with one caveat:  the last column (mean)  must be converted from a string representing a value in scientific notation to a numpyfloat64

So far  I've tried

df = pdDataFrame(pdioparsersread_table(fle  converters={'mean': lambda x: npfloat64(x)}))


but all I get in df['mean'] is 0 and -0

I've also tried importing without the converters kwarg  and later casting the column by doing df['mean']astype(npfloat64)  with similar results

What gives?",1156707,,,,2012-10-13 08:39:12,How do I convert a column from a pandas DataFrame from str (scientific notation) to numpy.float64?,<python><pandas>,2.0,3.0,,
12322779,1,12323599.0,2012-09-07 17:30:38,1,170,I have a DataFrame that has duplicated rows I'd like to get a DataFrame with a unique index and no duplicates It's ok to discard the duplicated values Is this possible? Would it be a done by groupby?,656188,,,,2012-09-07 20:17:54,pandas unique dataframe,<pandas>,2.0,,,
12322869,1,,2012-09-07 17:38:32,2,559,"I've tick by tick data for Forex pairs 

Here is a sample of EURUSD/EURUSD-2012-06csv

EUR/USD 20120601 00:00:00207 123618 12363
EUR/USD 20120601 00:00:00209 123618 123631
EUR/USD 20120601 00:00:00210 123618 123631
EUR/USD 20120601 00:00:00211 123623 123631
EUR/USD 20120601 00:00:00240 123623 123627
EUR/USD 20120601 00:00:00423 123622 123627
EUR/USD 20120601 00:00:00457 12362 123626
EUR/USD 20120601 00:00:01537 12362 123625
EUR/USD 20120601 00:00:03010 12362 123624
EUR/USD 20120601 00:00:03012 12362 123625


Full tick data can be downloaded here
http://dlfreefr/k4vVF7aOD

Columns are :

Symbol Datetime Bid Ask


I would like to convert this tick by tick data to candlestick data
(also called OHLC Open High Low Close)
I will say that I want to get a M15 timeframe (15 minutes) as an example

I would like to use Python and Pandas library to achieve this task

I've done a little part of the job reading the tick by tick data file

Here is the code

#!/usr/bin/env python

import pandas as pd
import matplotlibpyplot as plt
import numpy as np
from matplotlibfinance import candlestick
from datetime import *

def conv_str_to_datetime(x):
    return(datetimestrptime(x  '%Y%m%d %H:%M:%S%f'))

df = pdread_csv('test_EURUSD/EURUSD-2012-07csv'  names=['Symbol'  'Date_Time'  'Bid'  'Ask']  converters={'Date_Time': conv_str_to_datetime})

PipPosition = 4
df['Spread'] = (df['Ask'] - df['Bid']) * 10**PipPosition

print(df)

print(""=""*10)

print(dfix[0])


but now I don't know how to start rest of the job

I want to get data like

Symbol Datetime_open_candle open_price high_price low_price close_price


Price on candle will be based on Bid column

The first part of the problem is in my mind to get the first Datetime_open_candle (compatible with the desired timeframe  lets say that the name of the variable is dt1) and the last Datetime_open_candle (let's say that the name of this variable is dt2)

After I will probably need to get data from dt1 to dt2 (and not data before dt1 and after dt2)

Knowing dt1 and dt2 and desired timeframe I can know the number of candles I will have

I've ""just to"" know  for each candle  what is open/high/low/close price

I'm looking for a quite fast algorithm  if possible a vectorized one (if it's possible) as tick data can be very big",1555275,,,,2012-09-07 18:49:06,From tick by tick data to candlestick,<python><numpy><scipy><finance><pandas>,1.0,3.0,2.0,
12420598,1,,2012-09-14 08:19:12,0,36,"I write a lot of tests (nose based) involving DataFrame
Those tests should be readable by end-users DataFrame constructors are not very friendly
to read compared to a plain text table representation

What about using a text representation like reStructured to construct/assert DataFrame ?

=========== =========== ========= ========= ========================
id1         id2         net       nnet      desc
(int64)     (int64)     (float64) (float64) (object)
----------- ----------- --------- --------- ------------------------
1001        1002             100       00 Closed part of queue
1002                          00       30 Opened part of queue
=========== =========== ========= ========= ========================


The (dtype) line is useful to enforce the columns type to not fail on assert (could be optional)

I need community feedback before coding this reST DataFrame construct/assert feature
I also think about using ipython notebooks as test cases

What is your preferred DataFrame representation when readability counts ?",31335,,,,2012-09-14 11:27:40,Which DataFrame representation for readable tests,<python><pandas><restructuredtext>,1.0,1.0,,
12255179,1,12262022.0,2012-09-03 23:49:01,1,226,"I have a data that arrives in this format:

[
  (1  ""000010101001010101011101010101110101""  ""aaa""   ) 
  (0  ""111101010100101010101110101010111010""  ""bb""   ) 
  (0  ""100010110100010101001010101011101010""  ""ccc""   ) 
  (1  ""000010101001010101011101010101110101""  ""ddd""   ) 
  (1  ""110100010101001010101011101010111101""  ""eeee""   ) 
  
]


In tuple format  it looks like this:

(Y  X  other_info   )


At the end of the day  I need to train a classifier (eg sklearnlinear_modellogisticLogisticRegression) using Y and X

What's the most straightforward way to turn the string of ones and zeros into something like a nparray  so that I can run it through the classifier?  Seems like there should be an easy answer here  but I haven't been able to think of/google one

A few notes:

I'm already using numpy/pandas/sklearn  so anything in those libraries is fair game
For a lot of what I'm doing  it's convenient to have the other_info columns together in a DataFrame
The strings are is pretty long (~20 000 columns)  but the total data frame is not very tall (~500 rows)
",660664,,,,2012-09-04 10:55:12,numpy/pandas: How to convert a series of strings of zeros and ones into a matrix,<python><numpy><pandas><scikit-learn>,2.0,,,
12370349,1,12379527.0,2012-09-11 13:02:00,1,112,"I am doing SPC analysis using numpy/pandas 

Part of this is checking data series against the Nelson rules and the Western Electric rules

For instance (rule 2 from the Nelson rules): Check if nine (or more) points in a row are on the same side of the mean

Now I could simply implement checking a rule like this by iterating over the array 

But before I do that  I'm checking here on SO if numpy/pandas has a way to do this without iteration? 
In any case: What is the ""numpy-ic"" way to implement a check like the one described above?
",3571,,,,2012-09-12 00:06:26,Reasoning about consecutive data points without using iteration,<python><numpy><pandas><spc>,4.0,2.0,0.0,
12383436,1,12384507.0,2012-09-12 07:47:20,2,146,"Is it possible to access Series item via dot notation instead of bracket notation ?

s = pandasSeries(dict(a=4  b=4))
print s['a']  # works
print sa     # fails


As we can do with DataFrame :

df = pandasDataFrame([dict(a=4  b=4)  dict(a=4  b=4)])
print df['a']  # works
print dfa     # works
",31335,,,,2012-09-12 08:55:04,Access Pandas Series item with dot notation like DataFrame,<python><pandas>,2.0,,,
12390336,1,13297472.0,2012-09-12 14:21:11,2,185,"I have a Pandas dataframe 'df' like this :

         X   Y  
IX1 IX2
A   A1  20  30
    A2  20  30
    A5  20  30
B   B2  20  30
    B4  20  30


It lost some rows  and I want to fill in the gap in the middle like this:

         X   Y  
IX1 IX2
A   A1  20  30
    A2  20  30
    A3  NaN NaN
    A4  NaN NaN
    A5  20  30
B   B2  20  30
    B3  NaN NaN
    B4  20  30


Is there a pythonic way to do this ? ",1072888,,,,2012-11-08 20:42:12,How to fill the missing record of Pandas dataframe in pythonic way?,<python><pandas>,2.0,3.0,1.0,
12429279,1,12452587.0,2012-09-14 17:28:29,0,123,"Being a regular user of numpy  I am falling in love with Pandas for its syntax and concepts  and started to read about PyTables  which seems very robust and performance-oriented

My main doubts are:

If I were to use a home-made framework composed by Pandas + PyTables  which layer of the data-processing pipeline (data importing  converting  analyzing  storing  retrieving) should be delegated to each one?
What's the natural talent of Pandas and how it is related to the natural talent of PyTables  and how their talents relate?
Are they orthogonal/complimentary/independent  or do they overlap functionality somehow?
How do they diferentiate from one another?
Thanks for any light!",401828,,,,2012-09-17 02:45:53,Design strategy for managing and processing datasets with Pandas + PyTables,<design><dataset><pandas><pytables>,1.0,,,
12436895,1,12436963.0,2012-09-15 11:15:03,0,84,"I have a data series 'rpt_date' :

>>> rpt_date
STK_ID
000002    [u'20060331'  u'20060630']
000005    [u'20061231'  u'20070331'  u'20070630']
>>> type(rpt_date)

>>> 


And how to create a multiIndex object (pandascoreindexMultiIndex) by:

'my_index = gen_index_by_series (rpt_date)'


'my_index' looks like :

>>> my_index
MultiIndex
[('000002'  '20060331') ('000002'  '20060630') ('000005'  '20061231')
 ('000005'  '20070331') ('000005'  '20070630')]
>>> type(my_index)

>>> 


So how to write 'gen_index_by_series(series)' ? ",1072888,,1072888.0,2012-09-15 12:53:35,2012-09-15 12:53:35,How to create a multiIndex object from series?,<python><pandas>,1.0,,,
12436979,1,,2012-09-15 11:30:35,4,2066,"I would like to install Python Pandas library (081) on Mac OS X 1068 This library needs Numpy>=16

I tried this

$ sudo easy_install pandas
Searching for pandas
Reading http://pypipythonorg/simple/pandas/
Reading http://pandaspydataorg
Reading http://pandassourceforgenet
Best match: pandas 081
Downloading http://pypipythonorg/packages/source/p/pandas/pandas-081zip#md5=d2c5c5bea971cd760b0ae6f6850fcb74
Processing pandas-081zip
Running pandas-081/setuppy -q bdist_egg --dist-dir /tmp/easy_install-ckAMym/pandas-081/egg-dist-tmp-0mlL7t
error: Setup script exited with pandas requires NumPy >= 16 due to datetime64 dependency


So I tried to install Numpy

$ sudo easy_install numpy
Searching for numpy
Best match: numpy 162
Adding numpy 162 to easy-installpth file

Using /Library/Python/26/site-packages
Processing dependencies for numpy
Finished processing dependencies for numpy


So I tried again

$ sudo easy_install pandas


But the problem is still the same !

error: Setup script exited with pandas requires NumPy >= 16 due to datetime64 dependency


I run Python 

$ python
Python 261 (r261:67515  Jun 24 2010  21:47:49) 
[GCC 421 (Apple Inc build 5646)] on darwin
Type ""help""  ""copyright""  ""credits"" or ""license"" for more information
>>> import numpy as np
>>> np__version__
'121'


So Numpy 16 doesn't seems to be installed correctly !

I tried to install Numpy 16 with pip (instead of easy_install)

$ sudo pip install numpy
Requirement already satisfied (use --upgrade to upgrade): numpy in /Library/Python/26/site-packages
Cleaning up


I added --upgrade flag

$ sudo pip install numpy --upgrade
Requirement already up-to-date: numpy in /Library/Python/26/site-packages
Cleaning up

$ sudo pip install pandas
Downloading/unpacking pandas
  Downloading pandas-081zip (19MB): 19MB downloaded
  Running setuppy egg_info for package pandas
    pandas requires NumPy >= 16 due to datetime64 dependency
    Complete output from command python setuppy egg_info:
    pandas requires NumPy >= 16 due to datetime64 dependency

----------------------------------------
Command python setuppy egg_info failed with error code 1 in /tmp/pip-build/pandas
Storing complete log in /Users/MyUsername/Library/Logs/piplog


I also tried to install binary version of Numpy http://sourceforgenet/projects/numpy/files/
numpy-162-py26-pythonorg-macosx103dmg but it fails !!! (installer said me that numpy 162 can't be install on this disk Numpy requires pythonorg Python 26 to install",1609077,,1609077.0,2012-09-15 12:24:36,2013-01-02 22:41:52,How to fix Python Numpy/Pandas installation?,<python><numpy><pip><pandas><easy-install>,6.0,6.0,2.0,
12369546,1,12369616.0,2012-09-11 12:18:27,0,183,"I'm currently exploring the python library pandas with some hands-on data  where one of the columns contains a datetime object However  when a table is parsed using the DataFrame method  the datetime objects in the date column are parsed to an initial value eg 1970-01-16 14:12:28

If I for instance have a nparray with following content:

nparray(result  dtype=my_dtype) =
array([ (datetimedatetime(2012  9  9  0  0)  datetimedatetime(2012  9  8  15  10)) 
dtype=[('Date'  ('",256664,,,,2012-10-20 20:19:59,Pandas - Parsing datetime column from sqlite database,<python><datetime><pandas>,2.0,,,
12376863,1,12377083.0,2012-09-11 19:48:28,4,516,"I have an OHLC price data set  that I have parsed from CSV into a Pandas dataframe and resampled to 15 min bars:


DatetimeIndex: 500047 entries  1998-05-04 04:45:00 to 2012-08-07 00:15:00
Freq: 15T
Data columns:
Close    363152  non-null values
High     363152  non-null values
Low      363152  non-null values
Open     363152  non-null values
dtypes: float64(4)


I would like to add various calculated columns  starting with simple ones such as period Range (H-L) and then booleans to indicate the occurrence of price patterns that I will define - eg a hammer candle pattern  for which a sample definition:

def closed_in_top_half_of_range(h l c):
    return c > l + (h-1)/2

def lower_wick(o l c):
    return min(o c)-l

def real_body(o c):
    return abs(c-o)

def lower_wick_at_least_twice_real_body(o l c):
    return lower_wick(o l c) >= 2 * real_body(o c)

def is_hammer(row):
    return lower_wick_at_least_twice_real_body(row[""Open""] row[""Low""] row[""Close""]) \
    and closed_in_top_half_of_range(row[""High""] row[""Low""] row[""Close""])


Basic problem: how do I map the function to the column  specifically where I would like to reference more than one other column or the whole row or whatever? 

This post deals with adding two calculated columns off of a single source column  which is close  but not quite it

And slightly more advanced: for price patterns that are determined with reference to more than a single bar (T)  how can I reference different rows (eg T-1  T-2 etc) from within the function definition?

Many thanks in advance",1583083,,,,2012-09-11 20:56:40,Adding calculated column(s) to a dataframe in pandas,<python><pandas>,2.0,,2.0,
12416932,1,12417924.0,2012-09-14 01:17:06,3,99,"I have some data that has dates in it such as:

1979-02-15
1979-02-15
1979-02-17
1979-02-17


I would like to group the data both by year  month  and date so that the data looks like

1979
02
15

1979-02-15
1979-02-15

1979
02
17

1979-02-17
1979-02-17


I have found the function 

grouped = dfgroupby(lambda x: xyear)


but this only allows for grouping by the year So  my question is how do I do multi-level grouping by date in pandas?",1074057,,,,2012-09-14 03:59:01,Grouping data by multiple dates in pandas,<python><pandas>,1.0,,1.0,
12450924,1,12451149.0,2012-09-16 21:36:07,1,237,I have a dataframe where a few indexes are string values  How can I delete the rows of data where the index is a string value or a non integer?  Thanks for your help,1236977,,,,2012-09-16 22:07:47,Pandas Delete Row of Data where Index is String,<python><pandas>,1.0,,,
12411649,1,12411730.0,2012-09-13 17:27:27,2,227,"I have a Pandas Data Frame where I would like to filter out all columns which only contain zeros For example  in the Data Frame below  I'd like to remove column 2:

        0      1      2      3      4
0   0381  0794  0000  0964  0304
1   0538  0029  0000  0327  0928
2   0041  0312  0000  0208  0284
3   0406  0786  0000  0334  0118
4   0511  0166  0000  0181  0980


How can I do this? I've been trying something like this:

dffilter(lambda x: x == 0)
",1255817,,,,2012-09-13 17:32:12,Filter columns of only zeros from a Pandas data frame,<python><pandas>,1.0,,,
12432803,1,,2012-09-14 22:31:56,0,106,"I don't know if any of you have ever had such a behaviour  but after debugging my code I noticed a really weird thing happening to my code
Basically  I am filling a numpy array ('vols') with values contained in a pandas dataframe (with 2 hierarchical columns) at a certain index ('date')  but if I try to execute this piece of code in a script:

   for i in range(capEndDates_size):
      for j in range(strike_size):         
       vols[i j] = float(df[capEndDates[i]][strikes_list[j]]ix[date])


I cannot properly fill all values of variable 'vols' with those included in the dataframe at index 'date'  instead getting some 'nan' where I would expect values
The funny thing is that if I execute the piece of code in the interpreter  this goes the right way (ie by pressing f9 on the editor of Spyder  which is the IDE I'm using)! I found a workaround at this bug by repeating those lines twice (that is  forcing the script to execute the lines two times) 
My solution is like this:

   for i in range(capEndDates_size):
      for j in range(strike_size):         
       vols[i j] = float(df[capEndDates[i]][strikes_list[j]]ix[date])

   for i in range(capEndDates_size):
      for j in range(strike_size):         
       vols[i j] = float(df[capEndDates[i]][strikes_list[j]]ix[date])


Which is really unacceptable
Does any one have an idea of why this is happening?",1529852,,,,2012-09-30 11:57:04,weird behaviour when executing python script,<python><data.frame><pandas><bugs>,2.0,6.0,,
12490657,1,12491287.0,2012-09-19 08:00:10,1,210,"I am running the following snippet of code in the ipython notebook  using the pandas data analysis library along with matplotlibpyplot

titles = {'gradStat_p3': ""P3:  Gradiometers""  'magStat_p3': ""P3:  Magnetometers"" 
          'gradStat_mmn': ""MMN:  Gradiometers""  'magStat_mmn': ""MMN:  Magnetometers""}

scales = {'gradStat': (-20 * 1e-22  35 * 1e-22)  'magStat': (-16 * 1e-25  45 * 1e-25)}

fig  axes = pltsubplots(nrows=2  ncols=2  figsize=(8  5))
figtight_layout()
for c  component in enumerate(('p3'  'mmn')):
    for s  sensor in enumerate(('gradStat'  'magStat')):
        key = sensor + '_' + component
        axes[c  s]set_ylim(scales[sensor])
        agg = aggregated[key]
        # Plot
        aggplot(ax=axes[c  s]  kind='bar'  legend=False  title=titles[key])
        axes[c  s]set_xticklabels(aggindexformat(names=False))
        if not c:  # hide the labels
            axes[c  s]xaxisset_visible(False)

        saveFile = '/tmp/ERF_comparative_barplotpdf'
        figsavefig(saveFile)


When the above code is executed  the following (correct) plot is produced in the ipython notebook's inline graphical output:



Note that the x-lables are correctly displayed

When the image is saved  however  the x-labels are cropped as such:



I have tried calling figsavefig(savefile  bbox_inches=0  but to no avail  How can I avoid this cropping?

NOTE: For your convenience  I have pickled the aggregated variable here  This is a dictionary of pandas DataFrame objects and it should be all you need to run the above code and reproduce the bug (assuming you have pandas v081 installed)

Thanks very much in advance!",1156707,,,,2012-09-19 08:49:25,How can I save this matplotlib figure such that the x-axis labels are not cropped out?,<python><matplotlib><pandas>,1.0,,1.0,
12490762,1,12491917.0,2012-09-19 08:09:24,1,142,"I have multilevel dataframe 'df' like this :

             col1 col2
first second
a        0    5    5
         1    5    5
         2    5    5
b        0    5    5
         1    5    5


And I want to apply a function func (exp: 'lambda x: x*10') to second  somewhat like : 

dfgroupby(level='first')secondapply(func)


and result will lokk like:

             col1 col2
first second
a        0    5    5
         10   5    5
         20   5    5
b        0    5    5
         10   5    5


The above command not work for second is not a column  so second is not accepted by Pandas  

I don't want to do that by dfreset_index()   blablabla  then finally dfset_index() I prefer to do it in one command  How to do ?",1072888,,1072888.0,2012-09-19 08:16:58,2012-09-19 09:47:43,How to groupby the first level index and apply function to the second index in Pandas,<python><pandas>,1.0,,,
12498809,1,,2012-09-19 16:17:54,2,154,"I have a large tab delimited data file  and I want to read it in python using pandas ""read_csv or 'read_table' function When I am reading this large file it is showing me the following error  even after turning off the ""index_col"" value

>>> read_csv(""test_datatxt""  sep = ""\t""  header=0  index_col=None)
Traceback (most recent call last):
  File """"  line 1  in 
  File ""/Library/Frameworks/EPD64framework/Versions/73/lib/python27/site-packages/pandas/io/parserspy""  line 187  in read_csv
    return _read(TextParser  filepath_or_buffer  kwds)
  File ""/Library/Frameworks/EPD64framework/Versions/73/lib/python27/site-packages/pandas/io/parserspy""  line 160  in _read
    return parserget_chunk()
  File ""/Library/Frameworks/EPD64framework/Versions/73/lib/python27/site-packages/pandas/io/parserspy""  line 613  in get_chunk
    raise Exception(err_msg)
Exception: Implicit index (columns 0) have duplicate values [372  1325  1497  1636  2486  2679  3032  3125  4261  4669  5215  5416  5569  5783  5821  6053  6597  6835  7485  7629  7684  7827  8590  9361  10194  11199  11707  11782  12397  15134  15299  15457  15637  16147  17448  17659  18146  18153  18398  18469  19128  19433  19702  19830  19940  20284  21724  22764  23514  25095  25195  25258  25336  27011  28059  28418  28637  30213  30221  30574  30611  30871  31471  


I thought I might have duplicate values in my data and thus used grep to redirect some of these values into a file 

 grep ""9996744\|9965107\|740645\|9999752"" test_datatxt > delnowtxt


Now  when I read this file  it is read correctly as you can see below 

>>> read_table(""delnowtxt""  sep = ""\t""  header=0  index_col=None)

Int64Index: 20 entries  0 to 19
Data columns:
0740645                                                                 20  non-null values
M                                                                       20  non-null values
BLACK/CAPE VERDEAN                                                      20  non-null values


What is going on here? I am struggling for a solution but to no avail 

I also tried 'uniq' command in unix to see if duplicate lines exist but could not find any 

Does it has to do something with chunk-size? 

I am using the following version of pandas 

>>> pandas__version__
'073'
>>> 
",1649335,,1649335.0,2012-09-19 17:30:24,2012-09-19 17:30:24,Python pandas duplicate values error,<python><data.frame><pandas>,1.0,3.0,,
12514590,1,12514711.0,2012-09-20 14:16:45,1,174,"I try to read txt with missing values using pandasread_csv My data is of the format:

10/08/2012 12:10:10 name1 081 402 50;185701400N 4;077693770E 792 1050 00106 430 00301
10/08/2012 12:10:11 name2     1087 140 00099 970 00686


with thousands of samples with same name of the point  gps position  and other readings 
I use a code:

myData = read_csv('~/datatxt'  sep=' '  na_values='')


The code is wrong as na_values does not gives NaN or other indicator Columns should have the same size but I finish with different length

I don't know what exactly should be typed in after na_values (did try all different things)
Thanks ",1661173,,1661173.0,2012-09-20 15:32:51,2012-09-20 15:41:55,reading file with missing values in python pandas,<python><pandas>,2.0,3.0,1.0,
12427056,1,12449440.0,2012-09-14 14:59:21,1,182,"I've managed to make the following bar plot using matplotlibpyplot



The plot comes from an aggregated PANDAS DataFrame  printed below  Note that each bar in the bar-plot corresponds to a value in the mean colun  Also note that the values are not zero  but that PANDAS outputs 0 and -0 when floats are very small

Group Local Global Attn  mean
ASub  LD    GD     Attn    -0
                   Dist    -0
            GS     Attn    -0
                   Dist    -0
      LS    GD     Attn    -0
                   Dist    -0
            GS     Attn    -0
                   Dist    -0
DSub  LD    GD     Attn    -0
                   Dist     0
            GS     Attn    -0
                   Dist    -0
      LS    GD     Attn    -0
                   Dist    -0
            GS     Attn    -0
                   Dist    -0


I would like to label the x-axis of my bar plot hierarchically  in a manner corresponding to the labels above  In other words  the left half of the x-axis corresponds to the ASub group  The left half of the ASub group corresponds to the LD level of the  Local factor  and so on

Can this be done?

EDIT:

I think I should probably clarify exactly what I want  I'd like for there to be several labels  progressing from most general (Group) to most specific (Attn)  similar to the organization on the left 4 columns of the DataFrame above",1156707,,1156707.0,2012-09-14 20:07:54,2012-10-20 20:03:02,Is it possible to hierarchically label a matplotlib (pyplot) bar plot?,<python><matplotlib><pandas>,1.0,1.0,,
12483121,1,,2012-09-18 18:48:17,0,276,"I am using the pandas function read_csv to read a CSV with no index column 

read_csv(""filecsv""  header=1)


I was expecting that PANDAS would generate an index for each row based on the documentation


  index_col: column number  column name  or list of column numbers/names  to use as the index (row labels) of the resulting DataFrame By default  it will number the rows without using any column  unless there is one more data column than there are headers  in which case the first column is taken as the index


However  while loading the file it throws 

Exception: Reindexing only valid with uniquely valued Index objects


And I cannot figure out why this would be the case  What causes this exception?

I have also tried passing skiprows and nrows and the same exception occurs",154508,,,,2012-09-18 19:05:32,Pandas read_csv index exception,<python><csv><data><pandas>,1.0,4.0,,
12497402,1,13059751.0,2012-09-19 15:01:32,0,374,"I have a dataframe with repeat values in column A  I want to drop duplicates  keeping the row with the highest value in column B

So this:

A B
1 10
1 20
2 30
2 40
3 10


Should turn into this:

A B
1 20
2 40
3 10


Wes has added some nice functionality to drop duplicates: http://wesmckinneycom/blog/?p=340  But AFAICT  it's designed for exact duplicates  so there's no mention of criteria for selecting which rows get kept

I'm guessing there's probably an easy way to do this---maybe as easy as sorting the dataframe before dropping duplicates---but I don't know groupby's internal logic well enough to figure it out  Any suggestions?",660664,,,,2012-10-25 00:10:02,"python pandas: Remove duplicates by columns A, keeping the row with the highest value in column B",<python><duplicates><pandas>,3.0,,,
12519103,1,,2012-09-20 18:58:25,0,120,"My code requires the use of 'set_index' 

I import:

import sys

import io

import csv

import math

from pandas import *

import numpy as np


I assumed that this would suffice It doesn't  since I receive a :

NameError: name set_index is not defined 

I am using iPython EPDFree and pandas from PyPI (Riverbank computing) Is there a module I'm overlooking?",1374969,,1427416.0,2012-09-20 19:04:55,2012-09-20 19:04:55,set_index in DataFrame,<python><pandas>,1.0,1.0,,
12411897,1,12413347.0,2012-09-13 17:44:31,0,77,"I had a similar question some days ago that was solved  but now  some of my files have a very similar file  but where the header has a space before the name  or a """" in the end and it just doesn't work

So  I have this data1:

 Year Day Hour Min Sec P1S1
 2003   1  0  1 3009  0295E+04
 2003   1  1  0 1184  0297E+04
 2003   1  2  0  826  0338E+04
 2003   1  3  0  469  0291E+04
 2003   1  4  0  111  0337E+04


And I can read it with (notice the need of a space before Year in ' Year'  that is needed to read the file!):

import pandas as pd

def parse(yr  doy  hr  min  sec):
    yr  doy  hr  min = [int(x) for x in [yr  doy  hr  min]]
    sec = float(sec)
    mu_sec = int((sec - int(sec)) * 1e6)
    sec = int(sec)
    dt = datetime(yr - 1  12  31)
    delta = timedelta(days=doy  hours=hr  minutes=min  seconds=sec  microseconds=mu_sec)

    return dt + delta

# notice the need of a space before Year in ' Year'  that is needed to read the file!
pdread_csv(data1  parse_dates=[[' Year' 'Day' 'Hour' 'Min' 'Sec']]  date_parser=parse  index_col=0)


Now  if I try the same with  data2 (notice that now there is a '' after Min that didn't exist in data1):

 Year Day Hour Min Sec P1S1
 2003   1  0  0  000  0261E+04
 2003   1  0  5  000  0281E+04
 2003   1  0 10  000  0268E+04
 2003   1  0 15  000  0305E+04


When I do:

pdread_csv(data2  parse_dates=[[' Year' 'Day' 'Hour' 'Min' 'Sec']]  date_parser=parse  index_col=0)


I get an error because Python/Pandas is not expecting that '' after 'Min'  or the same when I have a file without the space before 'Year' Or any other slight difference in those first 5 header field names

So  my question is  is there any way to make this more robust? I know the first 5 fields are always in this format  it's just their name in the header that changes",865662,,229044.0,2012-09-13 17:54:07,2012-09-13 19:23:13,Can I make fuzzy matches for header field names in python pandas csv loading?,<python><csv><pandas>,1.0,,,
12425602,1,12426686.0,2012-09-14 13:29:30,0,173,"I want to build an analytics engine on top of an article publishing platform More specifically  I want to track the users' reading behaviour (eg number of views of an article  time spent with the article open  rating  etc)  as well as statistics on the articles themselves (eg number of paragraphs  author  etc)

This will have two purposes:

Present insights about users and articles
Provide recommendations to users
For the data analysis part I've been looking at cubes  pandas and pytables There is a lot of data  and it is stored in MySQL tables; I'm not sure which of these packages would better handle such a backend

For the recommendation part  I'm simply thinking about feeding data from the data analysis engine to a clustering model

Any recommendations about how to put all this together  as well as cool python projects out there that can help me out?
Please let me know if I should give more information

Thank you",1491915,,,,2012-09-14 14:37:24,Python package recommendation for data analysis and learning,<python><olap><pandas><pytables><cubes>,1.0,3.0,1.0,
12520537,1,,2012-09-20 20:36:23,1,103,"I am trying to find dendrogram a dataframe created using PANDAS package in python An example data is shown below 

import numpy as np
from pandas import *
import matplotlibpyplot as plt
from hcluster import pdist  linkage  dendrogram
from numpyrandom import rand

Index= ['aaa' 'bbb' 'ccc' 'ddd' 'eee']
Cols = ['A'  'B'  'C' 'D']
df = DataFrame(abs(nprandomrandn(5  4))  index= Index  columns=Cols)


>>> df
            A         B         C         D
aaa  0987415  0192240  0709559  0317106
bbb  0856932  0252441  1183127  0712855
ccc  1687198  0462673  1046469  0159287
ddd  0977152  2657582  0491975  0027280
eee  0120464  0945034  0142658  0537024
>>> 

X = dfTvalues #Transpose values 
Y = pdist(X)
Z = linkage(Y)
dendrogram(Z)


The above code generate the dendrogram but misses the column names How can I keep track of the same ",1649335,,1649335.0,2012-09-20 21:42:42,2012-09-21 13:00:17,python How to make a dendrogram from a dataframe,<python><pandas><dendrogram><ndarray>,1.0,2.0,2.0,
12504493,1,12504527.0,2012-09-20 00:02:13,0,141,"I'm trying to do what I think is a straight froward operation in pandas but I can't seem to make it work

I have two pandas Series with different numbers of indices  I would like to add values together if they share an index  otherwise I would just like to pass the values that don't have corresponding indices along

For example

Sr1 = pdSeries([1 2 3 4]  index = ['A'  'B'  'C'  'D'])
Sr2 = pdSeries([5 6]  index = ['A'  'C'])
Sr1        Sr2
A     1    A     5
B     2    C     6
C     3
D     4


Sr1 + Sr2 or Sr1add(Sr2) give

A     6
B   NaN
C     9
D   NaN


But what I want is

A     6
B     2
C     9
D     4


where the B and D values for Sr1 are just passed along

Any suggestions?",1473531,,,,2012-09-20 00:09:18,Adding pandas Series with different indices without getting NaNs,<python><pandas>,1.0,,,
12520727,1,13831248.0,2012-09-20 20:50:22,1,65,"I have a list of dates and times in columns of a csv I'm trying to perform a function on every unique date - for each time associated to that date The function should start at the first time (930am) of every new date Each of the dates are repeated 42 times There are 62 035 rows I say: 

My question:  Will a loop like the 'for' loop below logistically achieve that end? IE  ""do something"" on each new date for all times in that date only Then move to the next date

data=read_csv(file)  
idf= dataset_index(['date'  'time'])  
for ((date  time)  data) in idf:
# (I also tried - for (i in idf):)

 --do something here--


Right now  I get the IndexError: 'list index out of range' 
Any ideas why this might be occuring?",1374969,,,,2012-12-12 01:16:38,Restricting function to unique dates - Multi-Index,<python><data.frame><pandas>,1.0,2.0,,
12544361,1,12549298.0,2012-09-22 13:54:10,2,90,"Suppose I have a DataFrame which has a subindex structure like the following  with 'date'  'tenor' 'mat' and 'strike' and where the fields to be observed are stored in the column 'vol':      

date     tenor   mat strike    vol                                      
20120903 3m      1y  025      52
                     050      51
                     100      49
20120903 3m      5y  025      32
                     050      55
                     100      23
20120903 3m      10y 025      65
                     050      55
                     100      19
20120904 3m      1y  025      32
                     050      57
                     100      44
20120904 3m      5y  025      54
                     050      50
                     100      69
20120904 3m      10y 025      42
                     050      81
                     100      99


Say I want to reorganize this data by getting a new dataframe with subindexes 'date' + 'tenor' and with 'values' given by a 3d array composed by 'mat'  'strike' and 'vol' from the original dataframe in a manner like this:

date     tenor   values                                                       
20120903 3m      [[1y 5y 10y] [025  050  100]  [52  51  49] 
                                                  [32  55  23] 
                                                  [65  55  19]]
20120904 3m      [[1y 5y 10y] [025  050  100]  [32  57  44] 
                                                  [54  50  69] 
                                                  [42  81  99]]


I tried with various attempts of 'unstack'  'groupby' and 'pivot' but with no success I could only reach my objective byusing a lot of python vector manipulation  but this was a slow and inefficient procedure Is there any specific  more efficient pandas procedure in order to get the same result? I'm getting lost at this
Thanks for your help 
Maurizio",1529852,,,,2012-09-23 02:38:01,How to organize structured data in pandas dataframe,<python><pandas><pivot-table>,1.0,,,
12421279,1,,2012-09-14 09:03:27,0,54,"I'm currently working on a page that load a lot of data and calculations To simplify the called views i decided to create a task that will do the maths times to times and save the results in the cache

Should I rather store the direct results of calculations or store a Dataframe containing enough data to make a simple request ? 

If i get a dataframe from the cache in a view  will it copy a new dataframe in ram and is there a way to prevent this from happening ?",1518056,,,,2012-09-14 09:03:27,Django caching system,<python><django><caching><pandas>,,,1.0,
12576151,1,,2012-09-25 04:18:30,0,72,"I'm trying to create a wrapper that blocks the execution of some methods The classic solution is to use this pattern:

class RestrictingWrapper(object):
    def __init__(self  w  block):
        self_w = w
        self_block = block
    def __getattr__(self  n):
        if n in self_block:
            raise AttributeError  n
        return getattr(self_w  n)


The problem with this solution is the overhead that introduces in every call  so I am trying to use a MetaClass to accomplish the same task Here is my solution:

class RestrictingMetaWrapper(type):
    def __new__(cls  name  bases  dic):
        wrapped = dic['_w']
        block = dicget('_block'  [])

        new_class_dict = {}
        new_class_dictupdate(wrapped__dict__)
        for attr_to_block in block:
            del new_class_dict[attr_to_block]
        new_class_dictupdate(dic)

        return type__new__(cls  name  bases  new_class_dict)


Works perfectly with simple classes:

class A(object):
    def __init__(self  i):
        selfi = i
    def blocked(self):
        return 'BAD: executed'
    def no_blocked(self):
        return 'OK: executed'
class B(object):
    __metaclass__ = RestrictingMetaWrapper
    _w = A
    _block = ['blocked']

b= B('something')
bno_blocked  # 'OK: executed'
bblocked     # OK: AttributeError: 'B' object has no attribute 'blocked'


The problem comes with 'more complex' classes like ndarray from numpy:

class NArray(object):
    __metaclass__ = RestrictingMetaWrapper
    _w = npndarray
    _block = ['max']

na = NArray()        # OK
namax()             # OK: AttributeError: 'NArray' object has no attribute 'max'
na = NArray([3 3])   # TypeError: object__new__() takes no parameters
namin()             # TypeError: descriptor 'min' for 'numpyndarray' objects doesn't apply to 'NArray' object


I assume that my metaclass is not well defined because other classes (ex: pandasSeries) suffer weird errors  like not blocking the indicated methods

Could you find where the error is? Any other idea to solve this problem?  

UPDATE:
The nneonneo's solution works great  but seems like wrapped classes can break the blocker with some black magic inside the class definition

Using the nneonneo's solution:

import pandas

@restrict_methods('max')
class Row(pandasSeries):
    pass

r = Row([1 2 3])
rmax()        # BAD: 3      AttributeError expected  
",282061,,282061.0,2012-09-25 15:18:59,2012-09-25 15:18:59,Python pattern's RestrictingWrapper with metaclass,<python><design-patterns><numpy><pandas><metaclass>,1.0,4.0,,
12576313,1,12577614.0,2012-09-25 04:44:02,4,362,"I've been given a reasonably large Excel file (5k rows)  also as a CSV  that I would like to make into a pandas multilevel DataFame  The file is structured like this:

SampleID    OtherInfo    Measurements    Error    Notes
sample1     stuff                                 more stuff
                         36              6
                         26              7
                         37              8
sample2     newstuff                              lots of stuff
                         25              6
                         27              7


where the number of measurements is variable (and sometimes zero)  There is no full blank row in between any of the information  and the 'Measurements' and 'Error' columns are empty on rows that have the other (string) data; this might make it harder to parse(?)  Is there an easy way to automate this conversion?  My initial idea is to parse the file with Python first and then feed stuff into DataFrame slots in a loop  but I don't know exactly how to implement it  or if it is even the best course of action

Thanks in advance!",1696130,,,,2012-09-25 06:52:35,Convert excel or csv file to pandas multilevel dataframe,<python><excel><csv><data.frame><pandas>,2.0,,,
12548349,1,12549057.0,2012-09-22 22:56:42,0,1033,"I have a large txt with data in bad formats I would like to remove some rows and convert rest of data to float numbers I would like to remove rows with 'X' or 'XX'  The rest I should convert to float  number like 4;001 should be converted to 4001 The file looks like this sample:

0 1 10/09/2012 3:01 4;091 5 6 7 8 9 10 11
1 -0581586 11/09/2012 -1:93 0;203 739705  0892921 5  6 7
2 XX 10/09/2012 3:04 4;760 0183095 -0057214 -0504856 NaN 0183095 12
3 -0256051 10/09/2012 9:65 1;549 483293 0504967 0074442 -1716287 7 0504967 0504967
4 -0728092 11/09/2012 0:78 1;534 232247 4556 0328062 1382914 NaN 4556 4
5 4 11/09/2012 NaN NaN 60008 NaN NaN NaN 6000800 6000000 6000800
6 X 11/09/2012 X X 5 X 8 2 1 17000000 33000000
7  11/09/2012      6000000 5000000 2000000 2000000
8 4 11/09/2012 7:98 3;045 5 6 3 7000000 3000000 3000000 2
9 6 11/09/2012 2:21 4;672 5 2 2 7 3 8000000 4000000


I read it to DataFrame and choose rows 

from pandas import *
from csv import *
fileName = '~/datatxt'
colName = ['a'  'b'  'c'  'd'  'e'  'f'  'g'  'h'  'i'  'j'  'k'  'l']
df = DataFrame(read_csv(fileName  names=colName))
print df[df['b']isin(['X' 'XX' None 'NaN'])]to_string()


An output from last last line gives me only:

>>> print df[df['b']isin(['X' 'XX' None 'NaN'])]to_string()
    b           c     d       e         f          g         h   i         j   k   l
a                                                                                   
2  XX  10/09/2012  3:04  4;760  0183095  -0057214 -0504856 NaN  0183095  12 NaN
6   X  11/09/2012     X       X  5000000          X  8000000   2  1000000  17  33


Does not pick up row 7  and I would like to go through all df not only one column (original file is very large)

At the moment for conversion I use as below  but need remove unwanted rows first to apply it to all df

convert1 = lambda x : xreplace(''  '')
convert2 = lambda x : float(xreplace(';'  ''))
newNumber = convert2(convert1(df['e'][0])) 


After choosing rows I would like to remove them from df  I try dfpop() but it works only for column not for rows I try to name rows but don't luck In this particular txt I should finish with a new df from rows [0 3 8 9] with column 'c' as a date format  'd' as a time format and the rest as the float I try to figure it out for quite a while now  but do not know where to move  is it possible in pandas (probably should be) or do I need to change to ndarray or anything else? Thanks for your advise",1661173,,,,2012-09-23 01:33:09,removing particular rows from DataFrame in python pandas,<python><pandas>,1.0,1.0,1.0,
12569730,1,12570410.0,2012-09-24 17:11:53,1,155,"I have a dataframe in python pandas with several columns taken from a CSV file

For instance  data =:

Day P1S1 P1S2 P1S3 P2S1 P2S2 P2S3
1   1    2    2    3    1    2
2   2    2    3    5    4    2


And what I need is to get the sum of all columns which name starts with P1 something like P1* with a wildcard

Something like the following which gives an error:


  P1Sum = data[""P1*""]


Is there any why to do this with pandas?",865662,,865662.0,2012-09-24 17:17:45,2012-09-24 18:05:26,Sum all columns with a wildcard name search using Python Pandas,<python><wildcard><pandas>,1.0,4.0,,
12597926,1,12598379.0,2012-09-26 08:50:48,2,110,"I have a dataframe:

Form nr Element Type    Text    Options
   1    Name1   select  text1   op1
   1    Name1   select  text    op2
   1    Name1   select  text    op3
   1    Name2   input   text2   NaN
   2    Name1   input   text2   NaN


Is there a way to greate a ""nested"" hierarchical index like this:

Form nr Element Type    Text    Options
   1    Name1   select  text1   op1
                                op2
                                op3
        Name2   input   text2   NaN
   2    Name1   input   text2   NaN
",1199589,,,,2012-09-26 09:16:28,Pandas hierarchical dataframe,<python><data.frame><pandas>,1.0,,1.0,
12579150,1,12580527.0,2012-09-25 08:38:49,2,104,"I want to resample a TimeSeries in daily (exactly 24 hours) frequence starting at a certain hour

Like:

index = date_range(datetime(2012 1 1 17)  freq='H'  periods=60)

ts = Series(data=[1]*60  index=index)

tsresample(rule='D'  how='sum'  closed='left'  label='left')


Result i get:

2012-01-01  7
2012-01-02 24
2012-01-03 24
2012-01-04  5
Freq: D


Result i wish:

2012-01-01 17:00:00 24
2012-01-02 17:00:00 24
2012-01-03 17:00:00 12
Freq: D


Some weeks ago you could pass '24H' to the freq argument and it worked totally fine
But now it combines '24H' to '1D'

Was I using a bug with '24H' which is fixed now?
And how can i get the wished result in a efficient and pythonic (or pandas) way back?

versions:

python 273
pandas 090rc1 (but doesn't work in 081  too)
numpy 161
",1622231,,1301710.0,2012-09-25 14:24:38,2012-09-25 14:24:38,Resample hourly TimeSeries with certain starting hour,<python><pandas>,1.0,,,
12588031,1,12593896.0,2012-09-25 17:24:35,0,194,"Suppose I have the DataFrame below:

>>> dfrm = pandasDataFrame({
                             ""A"":[1 2 3]  
                             ""id1"":[True  True  False]  
                             ""id2"":[False  True  False]
                            })

>>> dfrm
   A    id1    id2
0  1   True  False
1  2   True   True
2  3  False  False


How can I flatten the two Boolean columns into a new column (that possibly will cause rows of the DataFrame to need to be repeated)  such that in the new column  the entries for all of the True occurrences appear

Specifically  in the example above  I would want the output to look like this:

index A   id1    id2   all_ids
    0 1  True  False       id1
    1 2  True   True       id1
    1 2  True   True       id2
    2 3 False  False       NaN


(preferably not multi-indexed on all_ids but I would take that if it was the only way to do it)

I've commonly seen this as ""wide to long"" and the inverse (going from one column to a bunch of Booleans) as ""long to wide""

Is there any built-in support for this in Pandas?",567620,,567620.0,2012-09-25 17:46:42,2012-09-26 13:43:21,Python Pandas: converting several boolean columns into a (possibly repeated) column made up of the boolean column names,<python><boolean><data.frame><pandas><multiple-columns>,1.0,,1.0,
12590131,1,12593202.0,2012-09-25 19:54:51,1,132,"I have a DataFrame object with 16 rows and 14671872 columns I cannot for the life of me figure out how to slice this array in any reasonable amount of time on a quad core Dell T410 with 24GB of RAM  

I would just use the transpose of the array because that's much faster  but then I would have a MultiIndex on the columns  and I haven't yet found any documentation in Pandas showing how to use MultiIndexs as columns 

I thought about opening up an issue on the Github tracker  but I wanted to post here before I did that just in case I missed something totally obvious",564538,,,,2012-09-26 01:03:36,How to slice multindex columns in pandas DataFrames?,<python><pandas>,1.0,3.0,0.0,
12598520,1,,2012-09-26 09:24:41,1,75,"How to index a DataFrame on 2 columns when one column is empty ?

df = DataFrame([
    dict(a=1  p=0)  
    dict(a=2  m=10)  
    dict(a=3  m=11  p=20)  
    dict(a=4  m=12  p=21)
]  columns=('a'  'm'  'p'  'x'))

# two-columns index including an empty column fails !!
# IndexError: index out of range for array
dfset_index(['a'  'x'])


# single column index on an empty column works
dfset_index(['x'])

# two-columns index on non-empty columns works
dfset_index(['a'  'm'])
dfset_index(['a'  'p'])
dfset_index(['m'  'p'])


Pandas version is 081",31335,,,,2012-09-26 09:24:41,set_index on multiple columns with one empty column,<python><pandas>,,5.0,,2012-12-19 02:32:10
12461470,1,12463255.0,2012-09-17 14:29:46,1,54,"I have a hierarchically indexed data frame:

>>> import pandas as pd
>>> df = pdDataFrame(nparange(4) 
                      index=[['John'  'John'  'Vicki'  'Vicki']  
                             ['a' 'b'  'a' 'b']] 
                      columns=['score'])

         score
John  a      0
      b      1
Vicki a      2
      b      3


and a series with index identical to the first index level in the above data frame:

>>> series = pdSeries([100  200]  index=['John'  'Vicki'])

John     100
Vicki    200


Now I want to merge the data frame with the series  such that the values from the series are broadcasted along the second level index The resulting data frame should look like this:

         score  salary
John  a      0     100
      b      1     100
Vicki a      2     200
      b      3     200


How can I achieve that in pandas?",74342,,,,2012-09-17 16:16:28,How to broadcast on a single index in hierarchically indexed DataFrame?,<python><pandas>,1.0,,1.0,
12521909,1,12523037.0,2012-09-20 22:35:57,1,155,"I'm trying to multiply (add/divide/etc) two dataframes that have different column labels

I'm sure this is possible  but what's the best way to do it? I've tried using rename to change the columns on one df first  but (1) I'd rather not do that and (2) my real data has a multiindex on the columns (where only one layer of the multiindex is differently labeled)  and rename seems tricky for that case

So to try and generalize my question  how can I get df1 * df2 using map to define the columns to multiply together?

df1 = pdDataFrame([1 2 3]  index=['1'  '2'  '3']  columns=['a'  'b'  'c'])
df2 = pdDataFrame([4 5 6]  index=['1'  '2'  '3']  columns=['d'  'e'  'f'])
map = {'a': 'e'  'b': 'd'  'c': 'f'}

df1 * df2 = ?
",1639821,,1639821.0,2012-09-21 03:26:25,2012-09-21 03:26:25,How can I multiply two dataframes with different column labels in pandas?,<python><pandas>,1.0,2.0,,
12555323,1,12555510.0,2012-09-23 19:00:01,2,720,"I have a DataFrame with named columns and rows indexed with not continuous numbers like from the code:

df1 = DataFrame(nprandomrandn(10  4)  columns=['a'  'b'  'c'  'd'])
mask = df1applymap(lambda x: x ",1661173,,1661173.0,2012-09-23 19:30:56,2012-12-12 16:26:41,Adding new column to existing DataFrame in python pandas,<python><pandas>,3.0,,,
12641606,1,12648275.0,2012-09-28 14:06:39,0,193,"I'am trying to sort a dataframe by column using dfsort_index Such strings column  the second  is composed by numbers within text After operation I've got:

15 rs1820451 32681212 0441 0493 05358 989 29 0 0441 T:A 
14 rs1820450 32680556 0441 0493 05358 989 29 0 0441 G:C 
38 rs1820447 32693541 0421 0332 00915 944 26 0 0211 G:A 
37 rs1820446 32693440 0483 0499 09633 1000 30 0 0475 G:T 
7 rs1808502 32660555 0517 046 0543 1000 30 0 0358 C:G 
24 rs17817908 32687035 0407 0362 06159 989 29 0 0237 C:T 
22 rs17817896 32686160 0407 0362 06159 989 29 0 0237 T:A 
66 rs17236946 32717247 0492 0453 07762 989 29 0 0347 T:C


Which isn't exactly what I want The last three lines should be in the beginning
Is there any other dataframe method or an overcome to this?",1289107,,1289107.0,2012-09-28 20:02:16,2012-09-28 22:41:05,Pandas: sorting numbers within text by column,<python><sorting><data.frame><pandas>,4.0,,,
12604909,1,,2012-09-26 15:12:25,0,275,"I have a data frame with a column called ""Date"" and want all the values from this column to have the same value (the year only) Example:

City     Date

Paris    01/04/2004

Lisbon   01/09/2004

Madrid   2004

Pekin    31/2004


What I want is:

City     Date

Paris    2004

Lisbon   2004

Madrid   2004

Pekin    2004


Thanks in advance!

More precisions: Here is my code:

fr61_70xls = pdExcelFile('AMADEUS FRANCE 1961-1970xlsx')

#Here we import the individual sheets and clean the sheets

years=(['1961' '1962' '1963' '1964' '1965' '1966' '1967' '1968' '1969' '1970'])

fr={}


header=(['City' 'Country' 'NACE' 'Cons' 'Last_year' 'Op_Rev_EUR_Last_avail_yr' 'BvD_Indep_Indic' 'GUO_Name' 'Legal_status' 'Date_of_incorporation' 'Legal_status_date'])


for year in years:
  fr[year]=fr61_70xlsparse(year header=0 parse_cols=10) #save every sheet in variable fr['1961']  fr['1962'] and so on
  fr[year]columns=header
  fr[year]=fr[year]drop(['Legal_status_date' 'Date_of_incorporation'] axis=1) #drop the entire Legal status date column
  fr[year]=fr[year]dropna(axis=0 how='all' subset=[['GUO_Name']]) #drop every row where GUO Name is empty
  fr[year]=fr[year]set_index(['GUO_Name' 'Date_of_incorporation'])


It happens that in my data frames  called for example fr['1961'] the values of Date_of_incorporation can be anything (strings  integer  and so on) So maybe it would be best to completely erase this column and then attach another column with only the year to the data frames?",1298051,,1298051.0,2012-09-26 15:44:10,2012-09-26 15:44:10,Pandas: how to change all the values of a column?,<python><database><pandas>,1.0,2.0,,
12609631,1,,2012-09-26 20:13:51,0,92,"Possible Duplicate:pandas: Frequency table for a single variable  




I am looking for a function to count number of different strings in an array (It is in the format of pandas series) I have this loop but need something more efficient

from pandas import *
ser = Series(['a'  'a'  'b'  'b'  'b'  'c'  'c'])
def occuranceCount(ser):
    labels = []
    x = ''
    for i in range(len(ser)):
        if ser[i] != x:            
            labelsappend(ser[i])
            x = ser[i]
    return labels

print len(occuranceCount(ser)) 


The main question is to count how many different labels I've got  but also need to know how many time particular label occur in the array Thanks",1661173,,,,2012-09-26 20:13:51,Python- count number of strings in pandas series,<python><pandas>,,3.0,,2012-12-10 20:14:28
12619215,1,12619626.0,2012-09-27 10:25:01,0,116,"I want to create panel with different dataframes  I try to write a function to return this panel I use some already written function createNewDf(i) which returns dataFrame from existing data  depending on given numeric value

def panelCreation():
    dp1 = Panel({})
    for i in range(1 3):
        name = 'X' + str(i)
        name = createNewDf(i)
    dp1update(name)       # This does not work
    return dp1


I can't find a name of the method to put in place of dp1update(name)",1661173,,,,2012-09-27 10:47:00,adding dataFrame to panel in python pandas,<python><pandas>,1.0,,,
12625650,1,12627465.0,2012-09-27 16:10:31,3,95,"Is there a grep like built-in function in Pandas to drop a row if it has some string or value?
Thanks in advance",1289107,,,,2012-09-27 18:03:33,Pandas: grep like function,<grep><row><pandas>,1.0,,1.0,
12651618,1,12652419.0,2012-09-29 09:37:40,3,151,"Using DataFrame (pandas as  pd  numpy as np):

test = pdDataFrame({'A' : [10 11 12 13 15 25 43 70]   
                     'B' : [1 2 3 4 5 6 7 8]   
                     'C' : [1 1 1 1 2 2 2 2]})


In [39]: test
Out[39]: 
    A  B  C
0  10  1  1
1  11  2  1
2  12  3  1
3  13  4  1
4  15  5  2
5  25  6  2
6  43  7  2
7  70  8  2


Grouping DF by 'C' and aggregating with npmean (also sum  min  max) produces column-wise aggregation within groups:

In [40]: test_g = testgroupby('C')

In [41]: test_gaggregate(npmean)
Out[41]: 
       A    B
C            
1  1150  25
2  3825  65


However  it looks like aggregating using npmedian produces DataFrame-wise aggregation within groups:

In [42]: test_gaggregate(npmedian)
Out[42]: 
      A     B
C            
1   70   70
2  115  115


(using groupbymedian method seems to produce expected column-wise results though)

I would appreciate addressing following issues:

What is the reason/mechanism of such an outcome?
If this behaviour is confirmed  how does it affect recommended ""best practices"" of aggregating groupings? Could other aggregation functions work this way?
",1707495,,667301.0,2012-09-29 12:12:25,2012-09-30 00:42:15,Inconsistency in results of aggregating pandas groupby object using numpy.median vs other functions,<python><numpy><aggregate><pandas>,3.0,,,
12608718,1,12608947.0,2012-09-26 19:08:06,0,80,"I am trying to use dfapply() function in pandas but getting the following error The function is trying to convert every entry into 0 if it is less than 'threshold' 

from pandas import * 
import numpy as np
def discardValueLessThan(x  threshold):
    if x >> df
          A         B         C
0 -1389871  1362458  1531723
1 -1200067 -1114360 -0020958
2 -0064653  0426051  1856164
3  1103067  0194196  0077709
4  2675069 -0848347  0152521
5 -0773200 -0712175 -0022908
6 -0796237  0016256  0390068
7 -0413894  0190118 -0521194

dfapply(discardValueLessThan  01)

>>> dfapply(discardValueLessThan  01)
Traceback (most recent call last):
  File """"  line 1  in 
  File ""/Library/Frameworks/EPD64framework/Versions/73/lib/python27/site-packages/pandas-081-py27-macosx-105-x86_64egg/pandas/core/framepy""  line 3576  in apply
    return self_apply_standard(f  axis)
  File ""/Library/Frameworks/EPD64framework/Versions/73/lib/python27/site-packages/pandas-081-py27-macosx-105-x86_64egg/pandas/core/framepy""  line 3637  in _apply_standard
    eargs = eargs + ('occurred at index %s' % str(k) )
UnboundLocalError: local variable 'k' referenced before assignment
",1649335,,,,2012-09-26 19:24:56,python pandas unbound local error while calling a function 'df.apply',<python><pandas><apply>,2.0,,,
12696612,1,,2012-10-02 19:05:00,1,88,"I'm currently maintaining timeseries data and background information for the timeseries in a ZODB The background data is quite interconnected and contains lots of constraints between each other  which I'm currently enforcing on object level with descriptors Heritages  time sequences of events (not before  not after between different attributes)  a bunch of other restrictions  So this won't be a good fit for pandas I guess and I'm quite sure to keep that in ZODB as is Nevertheless  the timelines itself could be cut out quite easily just keeping an integer id to the internal objects

So far I was using only plain lists  composed to some homebrew classes (see this question) For analysis I have some automated export to CSVs So I could analyze it with matlab later  while I used the advantages and flexibility of an object oriented database like ZODB  to let the data continuously grow from different sources

Lately I'm reading lots of good things about pandas and wonder  if it would be worth  to replace my homebrew classes with pandas dataframes Therefore I have some questions:

Do dataframes cause any problems with persistence? Anyone tried them out already together with ZODB  shelve or something like that?
Can I grow dataframes dynamically? How is the overhead for that? As neglectible as with lists or do I better stay with lists and convert them to dataframes later?
Can pandas dataframes hold data in the size of several gigabytes in one frame? My data is copped into small handy parts by nature  but for analysis all has to come together in the end again So I wonder if it makes sense to squeeze everything into one dataframe outside of ZODB persisted somehow differently  or better keep small chunks as is inside ZODB  and merge frames as needed? For persisting smaller objects would be favourable afaik  as every single write would grow the DB enormously
How was the backwards compatibility for the dataframe so far? In general  the stability of the datatype? Are there still regular changes  or is this basically stable  and I'll still be able to use it minimum next year with the newest pandas version without conversions?
Thanks for answers in advance  partial ones also welcome!",715042,,715042.0,2012-10-04 11:28:56,2012-10-04 11:28:56,Managing / combining pandas dataframes with ZODB,<python><persistence><time-series><pandas><zodb>,,1.0,,2012-12-11 23:30:10
12504951,1,12505031.0,2012-09-20 01:20:19,1,169,Would be useful save the session variables which could be loaded easily into memory at a later stage,968643,,,,2012-09-20 01:34:06,Is it possible to save session in ipython like in MATLAB?,<python><ipython><pandas>,1.0,2.0,1.0,
12504976,1,12505089.0,2012-09-20 01:24:57,0,103,"I have a column in a pandas DataFrame that I would like to split on a single space The splitting is simple enough with DataFramestrsplit(' ')  but I can't make a new column from the last entry When I strsplit() the column I get a list of arrays and I don't know how to manipulate this to get a new column for my DataFrame

Here is an example Each entry in the column contains 'symbol data price' and I would like to split off the price (and eventually remove the ""p"" or ""c"" in half the cases)

import pandas as pd
temp = pdDataFrame({'ticker' : ['spx 5/25/2001 p500'  'spx 5/25/2001 p600'  'spx 5/25/2001 p700']})
temp2 = temptickerstrsplit(' ')


which yields

0    ['spx'  '5/25/2001'  'p500']
1    ['spx'  '5/25/2001'  'p600']
2    ['spx'  '5/25/2001'  'p700']


But temp2[0] just gives one list entry's array and temp2[:][-1] fails How can I convert the last entry in each array to a new column? Thanks!",334755,,,,2012-10-24 16:13:48,"Get last ""column"" after .str.split() operation on column in pandas DataFrame",<string><split><pandas>,2.0,,,
12522963,1,12523035.0,2012-09-21 00:56:50,0,142,"I have a txt data where columns 6 and 7 are GPS position in the form:

50;185701400N 4;077693770E


When I read it by read_csv I try to convert it to cartesian coordinates by using converters I wrote the function for converter

convertFunc = lambda x : float((x[0:5]+x[6:12])replace(';' ''))
convert = {6:convertFunc 7:convertFunc}


when I use it on single value it works how I would like:

convertFunc(myDataLat[1])
Out [159]:  55187110250000003


when I try to use it in read_csv it does not work

myData = DataFrame(read_csv('~/datatxt'  sep=' ' names=['A'  'B'  'C'  'D'  'E'  'Lat'  'Long'] converters=convert))


I have an error:


convertFunc = lambda x : float((x[0:5] + x[6:12])replace(';'  ''))
ValueError: invalid literal for float(): DGPS ongitu


I don't know where do it wrong or what I misunderstand in converters?
Or maybe anyone knows good way (package) to work with GPS data in that form?

(I think it can be some problem with lambda When I want to apply my function to the column  I have an error: TypeError: only length-1 arrays can be converted to Python scalars)",1661173,,1661173.0,2012-09-21 01:15:09,2012-09-21 07:06:06,converters for python pandas,<python><pandas>,2.0,3.0,,
12525722,1,12525836.0,2012-09-21 07:04:23,2,317,"Suppose I have a pandas data frame df: 

I want to calculate the column wise mean of a data frame  

This is easy: 

dfapply(average) 


then the column wise range max(col) - min (col) this is easy again: 

dfapply(max) - dfapply(min)


Now for each element I want to subtract its columns mean and divide by its columns range 
I am not sure how to do that

Any help/pointers much appreciated 

thanks ",1039553,,,,2012-09-21 07:14:15,Normalize data in pandas,<python><map><numpy><pandas><apply>,1.0,,,
12539627,1,12540009.0,2012-09-22 00:13:26,2,74,"I need to edit rows in a pandas dataframe by dividing each value by the rowmax()

what is the recommended way to do this?

I tried 

dfxs('rowlabel') /= dfxs('rowlabel')max() 


as I'd do on a numpy array  but it didn't work",34747,,,,2012-09-22 01:40:16,Assigning to a Row in Pandas,<python><pandas>,1.0,,,
12702423,1,,2012-10-03 05:33:24,2,95,"Encountered a strange problem while trying to generate a date range of the second Thursday of each quarter start  beginning in March

import pandas as pd
import numpy as np
from datetime import *

start = datetime(2010  1  1)
end = datetime(2012  12  31)

startrng = pdbdate_range(start end freq='BQS-MAR')
startrng = startrng + pddatetoolsWeekOfMonth(week=1 weekday=3)

ts = pdSeries(nprandomrandn(len(startrng)) index=startrng)


I get the following results:

2010-03-11   -0585671
2010-06-10   -1201917
2010-09-09   -0063487
2010-12-09    0221270
2011-03-10   -0239836
2011-06-09    0256052
2011-09-08    1074758
2011-12-08    0962388
2012-03-08   -1601438
2012-06-14    0854736
2012-09-13    1532970
2012-12-13   -0084027


While most of the date offset works  2012-06-14 is actually the 3rd Thursday of the month  with 2012-06-01 starting on a Friday I have to generate a date range going back a couple of years and it is not productive to manually check for all the dates generated

Is there another method to produce the correct set of results instead?",1711722,,1711722.0,2012-10-04 04:00:24,2012-10-09 03:40:31,DateOffset: Week of Month,<python><datetime><pandas>,1.0,5.0,1.0,
12662223,1,,2012-09-30 15:01:23,0,106,"I wonder why 


  
    
      pandasTimestamp(dtdatetime(2009 1 6))week
    
  


and 


  
    
      datetimedatetime(2009 1 6)isocalendar()[1]
    
  


don't give the same result",1709800,,1709800.0,2012-09-30 19:35:30,2012-09-30 19:35:30,Timestamp.week vs. datetime.isocalendar,<python><pandas>,1.0,2.0,,
12680754,1,12681217.0,2012-10-01 20:42:16,0,254,"I have a pandas dataframe in which one column of text strings contains comma-separated values I want to split each CSV field and create a new row per entry (assume that CSV are clean and need only be split on ' ') For example  a should become b:

In [7]: a
Out[7]: 
    var1  var2
0  a b c     1
1  d e f     2

In [8]: b
Out[8]: 
  var1  var2
0    a     1
1    b     1
2    c     1
3    d     2
4    e     2
5    f     2


So far  I have tried various simple functions  but the apply method seems to only accept one row as return value when it is used on an axis  and I can't get transform to work Any suggestions would be much appreciated!

Example data: 

from pandas import DataFrame
import numpy as np
a = DataFrame([{'var1': 'a b c'  'var2': 1} 
               {'var1': 'd e f'  'var2': 2}])
b = DataFrame([{'var1': 'a'  'var2': 1} 
               {'var1': 'b'  'var2': 1} 
               {'var1': 'c'  'var2': 1} 
               {'var1': 'd'  'var2': 2} 
               {'var1': 'e'  'var2': 2} 
               {'var1': 'f'  'var2': 2}])


I know this won't work because we lose DataFrame meta-data by going through numpy  but it should give you a sense of what I tried to do: 

def fun(row):
    letters = row['var1']
    letters = letterssplit(' ')
    out = nparray([row] * len(letters))
    out['var1'] = letters
a['idx'] = range(ashape[0])
z = agroupby('idx')
ztransform(fun)
",342331,,,,2012-10-01 21:15:03,Split pandas dataframe string entry to separate rows,<python><numpy><pandas>,1.0,,,
12711211,1,12711422.0,2012-10-03 14:58:52,0,39,"I have several lists that I'm trying to enter into a data frame:

    mydates = [0  6  15  21  30  37  45  53]
    prices = [304  325  317  312  327  341  358  378]
    mylist = [6907894736842111  -2461538461538464  -15772870662460567 4807692307692319 
    42813455657492305  4985337243401747  55865921787709505  -39682539682539684]


I enter

    df = DataFrame(prices  mylist  index=mydates  columns=['Prices'  'Percentage
    s']) 


And receive this error:

    Traceback (most recent call last):
      File """"  line 1  in 
    TypeError: __init__() got multiple values for keyword argument 'index'


While if I enter this everything works fine

    df = DataFrame(prices  index=mydates  columns=['Prices'])


Could someone please tell me how to fix what's wrong? Thanks",1709173,,,,2012-10-03 15:29:50,Receiving 'error multiple values for index' in a simple data frame,<python><data><frame><pandas>,1.0,,,
12725417,1,12726468.0,2012-10-04 10:36:50,4,191,"In my application I load text files that are structured as follows:

First non numeric column (ID)
A number of non-numeric columns (strings)
A number of numeric columns (floats)
The number of the non-numeric columns is variable Currently I load the data into a DataFrame like this:

source = pandasread_table(inputfile  index_col=0)


I would like to drop all non-numeric columns in one fell swoop  without knowing their names or indices  since this could be doable reading their dtype Is this possible with pandas or do I have to cook up something on my own?",241515,,,,2012-10-04 11:41:00,Drop non-numeric columns from a pandas DataFrame,<python><pandas>,1.0,,1.0,
12772498,1,,2012-10-07 20:15:34,0,133,"This post includes my question but is actually about another question: Pandas DataFrame serialization

Is there serialization routine like numpysavez for pandas dataframes? I see I can use hdf5 but I was hoping to avoid this since hdf5 is an extra install and I haven't been able to get h5py up and running on all the platforms I need ",287238,,,,2012-10-08 14:18:01,serialize pandas (python) dataframe to binary format,<python><data.frame><pandas>,2.0,,,
12810801,1,12811078.0,2012-10-10 01:43:11,1,98,"I am attempting to use pandas to perform data analysis on a flat source of data Specifically  what I'm attempting to accomplish is the equivalent of a Union All query in SQL

I am using the read_csv() method to input the data and the output has unique integer indices and approximately 30+ columns

Of these columns  several contain identifying information  whilst others contain data

In total  the first 6 columns contain identifying informations which uniquely identifies an entry Following these 6 columns there are a range of columns (A B etc) which reference the value Some of these columns are linked together in sets  for example (A B C) belong together  as do (D E F)

However  (D E F) are also related to (A B C) as follows ((A D) (B E) (C F))
What I am attempting to do is take my data set which has as follows:

(id1 id2 id3 id4 id5 id6 A B C D E F) 


and return the following

((id1 id2 id3 id4 id5 id6 A B C) 
 (id1 id2 id3 id4 id5 id6 D E F))


Here  as A and D are linked they are contained within the same column

(Note  this is a simplification  there are approximately 12 million unique combinations in the total dataset)

I have been attempting to use the merge  concat and join functions to no avail I feel like I am missing something crucial as in an SQL database I can simply perform a union all query (which is quite slow admittedly) to solve this issue

I have no working sample code at this stage

Another way of writing this problem based upon some of the pandas docs

left = key lval
right = key rval
merge(left  right  on=key) = key  lval  rval


Instead I want:

left = kev  lval
right = key  lval
union(left  right) = key  lval
                     key  rval


I'm not sure if a new indexing key value would need to be created for this",1307298,,,,2012-10-10 02:21:24,Union all type query with python pandas,<python><union><pandas>,1.0,,,
12818103,1,12818681.0,2012-10-10 11:20:08,-1,228,"I am using Pandas 081 to fetch Yahoo stock price

from datetime import datetime
from pandasiodata import DataReader

stk_price = DataReader('600809ss'  'yahoo'  datetime(2006 1 1)  datetime(2012 12 31))reset_index()

>>> stk_priceDate
0    2010-01-04 00:00:00
1    2010-01-05 00:00:00
2    2010-01-06 00:00:00
3    2010-01-07 00:00:00
4    2010-01-08 00:00:00


And I want to convert the Date to string by:

>>>stk_priceDateastype('|S10')
0     1970-01-15
1     1970-01-15
2     1970-01-15
3     1970-01-15
4     1970-01-15
5     1970-01-15


Why it shows ""1970-01-15"" instead of ""2010-01-04"" etc ? How to fix it?

And if I have a 

DATE_LIST = [
 u'20090331'  u'20090630'  u'20090930'  u'20091231'  \
 u'20100331'  u'20100630'  u'20100930'  u'20101231'  \
 u'20110331'  u'20110630'  u'20110930'  u'20111231'  \
 u'20120331'  u'20120630'  u'20120930'  u'20121231'
 ]


I just try to filter the rows of stk_price whose Date column is in DATE_LIST by below: 

from datetime import datetime
from pandasiodata import DataReader
import numpy as np

DATE_LIST = [
 u'20090331'  u'20090630'  u'20090930'  u'20091231'  \
 u'20100331'  u'20100630'  u'20100930'  u'20101231'  \
 u'20110331'  u'20110630'  u'20110930'  u'20111231'  \
 u'20120331'  u'20120630'  u'20120930'  u'20121231'
 ]

DATE_ARRAY = nparray(DATE_LIST dtype='datetime64[us]')
stk_price = DataReader('600809ss'  'yahoo'  datetime(2006 1 1)  datetime(2012 12 31))reset_index()
rst = stk_price[stk_priceDateisin(DATE_ARRAY)]Close


but the rst is empty   

How to fix it or any Pandas function can filter the result ?",1072888,,,,2012-10-10 15:41:46,Pandas datetime convert & filter issue,<python><pandas>,2.0,1.0,,
12647811,1,12680776.0,2012-09-28 21:46:38,0,113,"I'm struggling to swap values from 2 columns of a dataframe as follows:  

rs649071 rs640249 0265 049 
rs647621 rs640249 0227 034 
rs644339 rs640249 0116 008 
rs641563 rs640249 10 3396 
rs640249 rs11073074 0248 077 
rs640249 rs11637397 0194 068 


The idea is to test if each cell of column 2 is rs640249 and if not  change to the corresponding string from column 1 and vice-versa This way the final results would be something like:    

rs649071 rs640249 0265 049 
rs647621 rs640249 0227 034 
rs644339 rs640249 0116 008 
rs641563 rs640249 10 3396 
rs11073074 rs640249 0248 077 
rs11637397 rs640249 0194 068 


I was trying to iterate over tuples  however  tuples does not support item assignment

rscode='rs640249'
for inf in LDfiles:
    df = read_csv(inf  sep='\t'  skiprows=1  names=['A'  'B'  'C'])
    for tup in dfitertuples():
        if tup[2] != rscode:
            tup[1]  tup[2] = tup[2]  tup[1]
        print(tup)


Any help is appreciated",1289107,,1289107.0,2012-10-01 20:45:47,2012-10-20 19:42:29,pyPandas: swap cells strings from 2 dataframe columns [SOLVED],<python><data.frame><pandas><compare-and-swap>,3.0,1.0,,
12720159,1,,2012-10-04 04:10:11,-1,63,"I have some code that works well with excel spredsheets via PyWorkbooks  but I would like to do the same job on pandas This is the original code:

from PyWorkbooksExWorkbook import ExWorkbook
B = ExWorkbook()
Bchange_workbook(""Somelistxlsx"")
Bchange_sheet(""Tab1"")

#column 6 = 1 if row customer likes cucumber  0 otherwise
#column 7 = 1 if row customer likes carrot  0 otherwise
#column 8 = 1 if row customer likes spinach  0 otherwise
string1 = ""Likes %s""
string2 = ""Likes %s and %s""
string3 = ""Likes %s  %s  and %s""

def findveg(row):
    veggies = []
    if B[row 6] == 1:
        veggiesappend('cucumber')
    if B[row 7] == 1:
        veggiesappend('carrot')
    if B[row 8] == 1:
        veggiesappend('spinach')
    return tuple(veggies)

for i in range(1 100):
    if len(veggies) == 1:
        veggies= findveg(i)
        B[i 9] = string1 % veggies
    if len(veggies) == 2:
        veggies= findveg(i)
        B[i 9] = string2 % veggies
    if len(veggies) == 3:
        veggies= findveg(i)
        B[i 9] = string3 % veggies
",1479269,,1479269.0,2012-10-04 04:39:03,2012-10-04 05:07:57,pandas alternative to string creation loop,<python><string><for-loop><pandas>,1.0,2.0,,
12747417,1,,2012-10-05 13:34:16,1,106,"If read a file with default column names  how do call them after?
df[1] seems to work almost all of the time However  it complains about types when writing conditions like:

In [60]: cond = ((df[1] != node) & (df[2] != deco))
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
/home/ferreirafm/work/colab/SNP/rawdata/ in ()
----> 1 cond = ((df[1] != node) & (df[2] != deco))

/usr/lib64/python27/site-packages/pandas/core/seriespyc in wrapper(self  other)
140             if npisscalar(res):
141                 raise TypeError('Could not compare %s type with Series'
--> 142                                 % type(other))
143             return Series(na_op(values  other) 
144                           index=selfindex  name=selfname)

TypeError: Could not compare  type with Series


Treat dataframe columns by default names are more appropriate for my applications ",1289107,,1301710.0,2012-10-07 12:24:35,2012-10-07 20:12:51,pyPandas: default columns names,<python><data.frame><pandas>,3.0,,,
12834568,1,12834717.0,2012-10-11 07:41:36,0,124,"I define a DatetimeIndex as below

>>> date_rng = pandasdate_range('20060101' '20121231' freq='D')

>>> type(date_rng)

>>> date_rng[0]



And each element in the 'date_rng' is 'Timestamp'  How can I convert it to a string series like below ?

>>> pandasSeries(['2006-01-01' '2006-01-02' '2006-01-03'])
0    2006-01-01
1    2006-01-02
2    2006-01-03
",1072888,,,,2012-10-11 07:58:58,How to convert a Pandas DatetimeIndex to string accordingly,<python><pandas>,1.0,,,
12589481,1,,2012-09-25 19:05:26,5,190,"Given the following (totally overkill) data frame example

df = pandasDataFrame({
                       ""date"":[datetimedate(2012 x 1) for x in range(1 11)]  
                       ""returns"":005*nprandomrandn(10)  
                       ""dummy"":nprepeat(1 10) 
                      })


is there an existing built-in way to apply two different aggregating functions to the same column  without having to call agg multiple times? 

The syntactically wrong  but intuitively right  way to do it would be:

# Assume `function1` and `function2` are defined for aggregating
dfgroupby(""dummy"")agg({""returns"":function1  ""returns"":function2})


Obviously  Python doesn't allow duplicate keys Is there any other manner for expressing the input to agg? Perhaps a list of tuples [(column  function)] would work better  to allow multiple functions applied to the same column? But it seems like it only accepts a dictionary

Is there a workaround for this besides defining an auxiliary function that just applies both of the functions inside of it? (How would this work with aggregation anyway?)",567620,,,,2012-11-27 20:57:33,Python Pandas: Multiple aggregations of the same column,<python><aggregate><pandas>,2.0,,2.0,
12687742,1,12688408.0,2012-10-02 09:26:49,2,173,"I have two timeseries with datetime columns as indexes:

2012-08-10 11       2012-08-10 11
2012-08-11 12       2012-08-11 12
2012-08-12 18       2012-08-13 11
2012-08-14 14       2012-08-15 13
2012-08-15 17       2012-08-16 11
2012-08-17 16       2012-08-17 12
2012-08-18 11       2012-08-18 11


How to compare them and get two timeseries with dates  which are present in both of them:

2012-08-10 11       2012-08-10 11
2012-08-11 12       2012-08-11 12
2012-08-15 17       2012-08-15 13
2012-08-17 16       2012-08-17 12
2012-08-18 11       2012-08-18 11
",1701030,,1701030.0,2012-10-02 09:53:55,2012-10-02 10:13:08,comparison of two pandas-timeseries,<python><time-series><pandas>,2.0,,,
12698764,1,12700373.0,2012-10-02 21:39:16,1,43,"I'm reading a CSV  and I'd like to make one of the columns a Categorical  with my own ordering  How do I do that?  The three labels are 'read'  'write' and 'mixed'  Here are some things that don't work:

Categorical(my_csvrw  ['read'  'write'  'mixed'])
    ValueError: invalid literal for long() with base 10: 'mixed'

Categorical(my_csvrw  Index(['read'  'write'  'mixed']))
    ValueError: invalid literal for long() with base 10: 'mixed'

Categorical(['read'  'mixed'  'write']  Index(['read'  'write'  'mixed']))
    ValueError: invalid literal for long() with base 10: 'mixed'

Categoricalfrom_array(['read'  'mixed'  'write']) # Levels in wrong order


So  how should I do it?",3917,,,,2012-10-23 21:17:53,How do I create a Categorial with my own levels in Pandas?,<python><pandas>,2.0,,,
12829428,1,12847586.0,2012-10-10 22:29:44,1,125,"Suppose I have a DataFrame with 100k rows and a column name I would like to split this name into first and last name as efficiently as possibly My current method is 

def splitName(name):
  return pandasSeries(namesplit()[0:2])

df[['first'  'last']] = dfapply(lambda x: splitName(x['name'])  axis=1)


Unfortunately  DataFrameapply is really  really slow Is there anything I can do to make this string operation nearly as fast as a numpy operation?

Thanks!",128580,,,,2012-10-11 20:03:34,Quickly applying string operations in a pandas DataFrame,<python><pandas>,1.0,1.0,,
12841827,1,12842390.0,2012-10-11 14:24:51,0,135,"I have the following pandas Dataframe:

from pandas import DataFrame  MultiIndex
index = MultiIndexfrom_tuples(zip([21 22 23] [45 45 46])  names=['A'  'B'])
df = DataFrame({'values': [067  087  023]}  index=index)

Out[10]:
         values
A  B         
21 45    067
22 45    087
23 46    023


What is the correct  way to access the value for the element (22 45)? I have tried all the obvious alternatives but any of them seems to work:

df[22 45]
df[(22 45)]
dfix[22 45]
dfix[(22 45)]


I am using pandas 090dev-1e68fd9",1330293,,,,2012-10-11 14:50:59,Accessing pandas Multiindex Dataframe using integer indexes,<python><pandas>,1.0,2.0,,
12593759,1,12594030.0,2012-09-26 02:30:35,0,65,"I need to reconcile two separate dataframes  Each row within the two dataframes has a unique id that I am using to match the two dataframes  Without using a loop  how can I reconcile one dataframe against another and vice-versa?  

I tried merging the two dataframes on an index (unique id) but the problem I run into when I do this is when there are duplicate rows of data  Is there a way to identify duplicate rows of data and put that data into an array or export it to a CSV?

Your help is much appreciated  Thanks",1236977,,,,2012-09-26 03:07:11,Pandas Data Reconcilation,<python><pandas>,1.0,,,
12645216,1,12658729.0,2012-09-28 18:11:46,0,74,"I think a reverse/negative dataframedrop functionality would be a very useful tool
Has anybody have a overcome to this?
Best    ",1289107,,,,2012-09-30 04:59:55,pyPandas functionality request: reverse/negative df.drop,<python><drop><pandas>,2.0,2.0,,
12734320,1,,2012-10-04 19:18:13,0,82,"I have two dataframes: one with 12 cols and the other with 9  both of them have 624 rows I would like to join them side by side resulting in a 21 cols dataframe with the same 624 number of rows I want to preserve the rows order as is Observe that both dataframes are aligned in descending order of the column 'Name' and the column 'L1' I have tried several things join them by axis=1 ignoring index or not All that I have is a dataframe with rows doubled and a bunch of NANs I also tried concat and append  but with no success
Any help is appreciated
Best        

n        Name  Position  ObsHET  PredHET  HWpval  %Geno  FamTrio  MendErr    MAF Alleles Rating
48  rs17818182  32945574   0153    0141  10000   989       29        0  0076     G:T    NaN
45  rs17818176  32944041   0033    0033  10000  1000       30        0  0017     G:T    NaN
133  rs17818104  32879319   0136    0126  10000   989       29        0  0068     T:C    NaN
105  rs17818087  32863970   0241    0307  02037   967       29        1  0190     T:C    NaN
165  rs17818021  32794604   0302    0329  07637   856       20        0  0208     A:C    NaN

           L1        L2      D   LOD     r2  CIlow  CIhi   Dist T-int
31331  rs17818182  rs640249  0423  027  0012   004  080  66596     -
31328  rs17818176  rs640249  1000  021  0014   005  097  65063     -
29083  rs17818104  rs640249  1000  301  0092   051  100    341     -
27571  rs17818087  rs640249  0143  014  0006   001  044  15008     -
14857  rs17818021  rs640249  0311  068  0033   006  057  84374     -
",1289107,,,,2012-10-04 23:13:55,pyPandas: mess with join/append/concat two dataframes,<python><join><data.frame><pandas><concat>,1.0,,,
12741092,1,12741168.0,2012-10-05 06:55:44,3,462,"I can use map(func) on any column in a df  like:

df=DataFrame({'a':[1 2 3 4 5 6] 'b':[2 3 4 5 6 7]})

df['a']=df['a']map(lambda x: x > 1)


I could also:

df['a'] df['b']=df['a']map(lambda x: x > 1) df['b']map(lambda x: x > 1)


Is there a more pythonic way to apply a function to all columns or the entire frame (without a loop)?",1199589,,1491200.0,2012-10-05 08:17:32,2012-10-05 08:17:32,Pandas DataFrame: apply function to all columns,<python><data.frame><pandas>,1.0,3.0,1.0,
12844900,1,12846006.0,2012-10-11 17:08:57,1,135,"I'm running into problems when taking lower-frequency time-series in pandas  such as monthly or quarterly data  and upsampling it to a weekly frequency For example 

data = nparange(3  dtype=npfloat64)
s = Series(data  index=date_range('2012-01-01'  periods=len(data)  freq='M'))
sresample('W-SUN')


results in a series filled with NaN everywhere Basically the same thing happens if I do:

sreindex(DatetimeIndex(start=sindex[0]replace(day=1)  end=sindex[-1]  freq='W-SUN'))


If s were indexed with a PeriodIndex instead I would get an error: ValueError: Frequency M cannot be resampled to 

I can understand why this might happen  as the weekly dates don't exactly align with the monthly dates  and weeks can overlap months However  I would like to implement some simple rules to handle this anyway In particular  (1) set the last week ending in the month to the monthly value  (2) set the first week ending in the month to the monthly value  or (3) set all the weeks ending in the month to the monthly value What might be an approach to accomplish that? I can imagine wanting to extend this to bi-weekly data as well

EDIT: An example of what I would ideally like the output of case (1) to be would be:

2012-01-01   NaN
2012-01-08   NaN
2012-01-15   NaN
2012-01-22   NaN
2012-01-29   0
2012-02-05   NaN
2012-02-12   NaN
2012-02-19   NaN
2012-02-26   1
2012-03-04   NaN
2012-03-11   NaN
2012-03-18   NaN
2012-03-25   2
",233446,,233446.0,2012-10-11 17:26:32,2012-10-11 18:15:47,Upsampling to weekly data in pandas,<python><pandas>,1.0,1.0,,
12664590,1,12664893.0,2012-09-30 19:58:30,4,215,"I have a Pandas dataframe with columns like

Order     Balance     Profit cum (%)


I'm doing a linear regression

model_profit_tr = pdols(y=df_closed['Profit cum (%)']  x=df_closed['Order'])


The problem with this is that standard model is like (equation of a line that does not pass through the origin)

y = a * x + b


There is 2 degrees of freedom (a and b)

slope (a):

a=model_profit_trbeta['x']


and intercept (b):

b=model_profit_trbeta['intercept']


I'd like to reduce degree of freedom for my model (from 2 to 1) and I 'd like to have a model like

y = a * x
",1555275,,1301710.0,2012-10-01 19:45:33,2012-10-01 19:45:33,Linear regression - reduce degrees of freedom,<python><numpy><statistics><pandas><curve-fitting>,2.0,,,
12704305,1,12707465.0,2012-10-03 08:05:48,3,111,"from pandas import DataFrame
import pyodbc

cnxn = pyodbcconnect(databasez)
cursorexecute(""""""SELECT ID  NAME AS Nickname  ADDRESS AS Residence FROM tablez"""""")
DF = DataFrame(cursorfetchall())


This is fine to populate my pandas DataFrame But how do I get

DFcolumns = ['ID'  'Nickname'  'Residence']


straight from cursor? Is that information stored in cursor at all?",1479269,,,,2012-10-03 23:38:18,return column names from pyodbc execute() statement,<pandas><pyodbc>,2.0,,1.0,
12711551,1,12714937.0,2012-10-03 15:16:58,0,132,"I would like to feed a empty dataframe appending several files of the same type and structure However  I can't see what's wrong here:   

def files2df(colnames  ext):
    df = DataFrame(columns = colnames)
    for inf in sorted(globglob(ext)):
        dfin = read_csv(inf  sep='\t'  skiprows=1)
        print(dfinhead()  '\n')
        dfappend(dfin  ignore_index=True)
    return df


The resulting dataframe is empty Could someone give me a hand?

    10  1659  0597  087  101   3282 10008
 0  0953  1452  0561  080   099   4355      -
 1  1000  3159  1000  094   100   6322      -
 2  1000   609  0237  071   100  10568      -
 3  1000  3129  1000  094   100  14363      -
 4  1000  3159  1000  094   100  19797      - 

      10   669  0199  074  101   186 1316
 0      1   088  0020  013   099   394     -
 1      1   075  0017  011   099  1052     -
 2      1   334  0097  057   100  1178     -
 3      1   150  0035  026   100  1211     -
 4      1  2059  0940  088   100  1583     - 

      10  012  00030  004  097   2285 262
 0     1  125   0135  018  099   2480    -
 1     1  003   0001  004  097   7440    -
 2     1  012   0003  004  097   8199    -
 3     1  110   0092  016  099  11174    -
 4     1  027   0007  006  098  11310    - 

   0244  007  00030  002  076  41314 132
 0  0181  064   0028  003  036  41755    -
 1  0161  018   0008  001  045  42420    -
 2  0161  018   0008  001  045  42461    -
 3  0237  025   0011  002  056  43060    -
 4  0267  103   0047  007  046  43321    - 

 0163  012  00060  001   05  103384 127
 0  0243  027   0014  002  056  104693    -
 1  0215  066   0029  004  041  105192    -
 2  0190  010   0005  001  059  105758    -
 3  0161  012   0006  001  050  109783    -
 4  0144  016   0007  001  042  110067    - 

Empty DataFrame
Columns: array([D  LOD  r2  CIlow  CIhi  Dist  T-int]  dtype=object)
Index: array([]  dtype=object)
",1289107,,,,2012-10-03 19:34:40,feed empty pandas.dataframe with several files,<python><data.frame><pandas>,2.0,,,
12877189,1,,2012-10-13 21:31:34,0,202,"I'm reading a CSV with float numbers like this:

Bob 0085
Alice 0005


And import into a dataframe  and write this dataframe to a new place

df = pdread_csv(orig)
dfto_csv(pandasfile)


Now this pandasfile has:

Bob 0085000000000000006
Alice 00050000000000000001


What happen? maybe I have to cast to a different type like float32 or something? 

Im using pandas 090 and numpy 162",472866,,904365.0,2012-10-15 10:14:28,2012-11-03 17:48:23,float64 with pandas to_csv,<python><numpy><pandas>,2.0,2.0,1.0,
12700251,1,,2012-10-03 00:13:49,0,109,"I'm using pandas 081

Creating a non-empty DataFrame:

import pandas
blah = pandasDataFrame(numpyempty([1  1])  columns=['A']  index=pandasDatetimeIndex(['2008-01-01']))
blah[numpyarray([true]  bool)]

            A
2008-01-01  0

blah[numpyarray([True]  bool)] = 00


That's all fine  but in the empty case:

blah = pandasDataFrame(numpyempty([0  1])  columns=['A']  index=pandasDatetimeIndex([]))
blah[numpyarray([]  bool)]

Empty DataFrame


blah[numpyarray([]  bool)] = 00

Traceback (most recent call last):

IndexError: invalid index


That's not right  is it?",1497828,,,,2012-10-20 20:24:04,Pandas: Edge case on boolean index into empty DataFrame,<python><data.frame><pandas><bugs>,0.0,2.0,,
12726432,1,,2012-10-04 11:38:28,7,690,"I am working on time series in python The libraries which I found useful and promising are 

pandas     
statsmodel (for ARIMA)
Also for visualization: matplotlib

Does anyone know a library for exponential smoothing?",1719510,,1719510.0,2012-11-27 18:14:56,2012-11-27 18:14:56,Using Python for Time Series Analysis and Forecasting,<python><time-series><pandas><statsmodels>,3.0,1.0,2.0,
12797352,1,12798599.0,2012-10-09 09:53:11,0,116,"How to transpose a DataFrame returned by concat() ?

df = DataFrame([
            dict(a=1  b=10  c=41) 
            dict(a=1  b=20  c=42) 
            ])

concat([df  df])T


I get :

AttributeError: 'DataFrame' object has no attribute 'dtypes' !


If I try :

concat([df  df])Tto_dict()


I get :

RuntimeError: maximum recursion depth exceeded in cmp


I think this is related to the duplicates introduced by concat() in the index but didn't find a workaround",31335,,,,2012-10-09 10:58:45,Pandas transpose concat(),<python><pandas>,1.0,3.0,,
12898266,1,,2012-10-15 14:54:25,5,169,"I'm new to pandas and I'm trying to read a strange formated file into a DataFrame
The original file looks like this:

; No   Time   Date  MoistAve  MatTemp  TDRConduct  TDRAve  DeltaCount  tpAve  Moist1  Moist2  Moist3  Moist4  TDR1  TDR2  TDR3  TDR4
1  11:38:17   11072012  1137  4820  515  8887  15  34450  1184  1135  1159  1525  890  890  890  880
2  11:38:18   11072012  1144  4820  513  8888  2  34622  1208  1183  -100  -100  890  890  -10  -10
3  11:38:19   11072012  1110  4820  496  8900  3  33784  1183  1159  1062  -100  890  890  890  -10
4  11:38:19   11072012  1182  4820  554  8860  3  35592  1110  1354  1232  -100  890  880  880  -10


I managed to get an equally structured DataFrame with:

In [42]: date_spec = {'FetchTime': [1  2]}

In [43]: df = pdread_csv('MeasureCK32450-20120711114050mck'  header=7  sep='\s\s+' 
                          parse_dates=date_spec  na_values=['-10'  '-100'])

In [44]: df
Out[52]: 
               FetchTime  ; No  MoistAve  MatTemp  TDRConduct  TDRAve  DeltaCount   tpAve  Moist1  Moist2  Moist3  Moist4  TDR1  TDR2  TDR3  TDR4
0    2012-11-07 11:38:17     1     1137     482        515   8887          15  34450   1184   1135   1159   1525    89    89    89    88
1    2012-11-07 11:38:18     2     1144     482        513   8888           2  34622   1208   1183     NaN     NaN    89    89   NaN   NaN
2    2012-11-07 11:38:19     3     1110     482        496   8900           3  33784   1183   1159   1062     NaN    89    89    89   NaN
3    2012-11-07 11:38:19     4     1182     482        554   8860           3  35592   1110   1354   1232     NaN    89    88    88   NaN


But now I have to expand each line of this DataFrame

   Moist1  Moist2  Moist3  Moist4  TDR1  TDR2  TDR3  TDR4
1   1184   1135   1159   1525    89    89    89    88
2   1208   1183     NaN     NaN    89    89   NaN   NaN


into four lines (with three indexes No  FetchTime  and MeasureNo):

                                   Moist  TDR
No           FetchTime  MeasureNo
 0 2012-11-07 11:38:17          1  1184   89 # from line 1  Moist1 and TDR1
 1                              2  1135   89 # from line 1  Moist2 and TDR2
 2                              3  1159   89 # from line 1  Moist3 and TDR3
 3                              4  1525   88 # from line 1  Moist4 and TDR4
 4 2012-11-07 11:38:18          1  1208   89 # from line 2  Moist1 and TDR1
 5                              2  1183   89 # from line 2  Moist2 and TDR2
 6                              3    NaN  NaN # from line 2  Moist3 and TDR3
 7                              4    NaN  NaN # from line 2  Moist4 and TDR4


by preserving the other columns and MOST important  preserving the order of the entries I
know I can iterate through each line with for row in dfiterrows():  but I read this is 
not very fast My first approach was this:

In [54]: data = []
In [55]: for d in range(1 5):
:     temp = dfix[:  ['FetchTime'  'MoistAve'  'MatTemp'  'TDRConduct'  'TDRAve'  'DeltaCount'  'tpAve'  'Moist%d' % d  'TDR%d' % d]]
:     tempcolumns = ['FetchTime'  'MoistAve'  'MatTemp'  'TDRConduct'  'TDRAve'  'DeltaCount'  'tpAve'  'RawMoist'  'RawTDR']
:     temp['MeasureNo'] = d
:     dataappend(temp)
:      
In [56]: test = pdconcat(data  ignore_index=True)
In [62]: testhead()
Out[62]: 
             FetchTime  MoistAve  MatTemp  TDRConduct  TDRAve  DeltaCount   tpAve  RawMoist  RawTDR  MeasureNo
0  2012-11-07 11:38:17     1137     482        515   8887          15  34450     1184      89          1
1  2012-11-07 11:38:18     1144     482        513   8888           2  34622     1208      89          1
2  2012-11-07 11:38:19     1110     482        496   8900           3  33784     1183      89          1
3  2012-11-07 11:38:19     1182     482        554   8860           3  35592     1110      89          1
4  2012-11-07 11:38:20     1261     482        587   8838           3  37572     1280      89          1


But I don't see a way to influence the concatenation to get the order I need 
Is there another way to get the resulting DataFrame I need?",1746940,,1746940.0,2012-10-15 14:59:53,2012-12-09 23:58:38,Efficiently expand lines from pandas DataFrame,<python><pandas>,1.0,2.0,0.0,
12907231,1,,2012-10-16 03:43:24,2,97,"I have two time series I need to join:

The first one is a time series of prices
like this 

ticker date       price 
SBUX   01/01/2012 55


The second time series are adjustment factors which are represented as

ticker start_date end_date    adjustment_factor
SBUX   01/01/1993 01/01/1994  0015


How do I join these time series  together in pandas as to express adjusted prices in expression 

adjusted_price = historical_prices * adjustment_factor


I understand I need to expand adjustment_factor interval time series into daily series by using date_range function 
The issue though is that each row of adjustment time series is going to have a different date range - is there a way to to batch the conversion from interval date type for entire adjustment factor time series rather then doing it for every row 

i had figured out that i need to pivot the first point-in-time timeseries for tickers go into the columns and date into rows and for the second timeseries expand the interval into daily granularity and also pivot it (through dataframepivot function by combining the two dataframes one can write function i need ",1748834,,1301710.0,2012-10-16 17:07:26,2012-10-16 18:44:55,joining point and interval data,<python><pandas>,1.0,,,
12939093,1,12940387.0,2012-10-17 16:37:42,1,45,"I would like to write a subclass of pandascoreindexIndex I'm following the guide to subclassing ndarrays which can be found in the numpy documentation Here's my code:

import numpy as np
import pandas as pd

class InfoIndex(pdcoreindexIndex):

    def __new__(subtype  data  info=None):
        # Create the ndarray instance of our type  given the usual
        # ndarray input arguments  This will call the standard
        # ndarray constructor  but return an object of our type
        # It also triggers a call to InfoArray__array_finalize__
        obj = pdcoreindexIndex__new__(subtype  data)
        # set the new 'info' attribute to the value passed
        objinfo = info
        # Finally  we must return the newly created object:
        return obj


However  it doesn't work; I only get a Index object:

In [2]: I = InfoIndex((3 ))

In [3]: I
Out[3]: Int64Index([3])


What am I doing wrong?",152439,,,,2012-10-20 16:17:24,Subclassing pandas.Index,<subclass><pandas>,2.0,,,
12910187,1,,2012-10-16 08:11:17,4,146,"I have pandas Dataframe with datetime index like 'YYYY-MM-DD HH:MM:SS'

Index               Parameter
2007-05-02 14:14:08     1348
2007-05-02 14:14:32     1348 
2007-05-02 14:14:41     1348 
2007-05-02 14:14:53     1348 
2007-05-02 14:15:01     1348 
2007-05-02 14:15:09     1348 

2007-05-30 23:08:02     1059 
2007-05-30 23:18:02     1059 
2007-05-30 23:28:02     1059 
2007-05-30 23:38:03     1058 


It is possible to get slice a DataFrame by year df['2007'] or by month df['2007-05']? 

But when I've tried to slice DataFrame by day  for example df['2007-05-02']  I've got the error: 

KeyError: ",1627810,,1301710.0,2012-10-16 13:18:29,2012-10-16 13:18:29,Pandas DataFrame slicing by day/hour/minute,<python><pandas><slicing>,1.0,1.0,1.0,
12947521,1,,2012-10-18 05:10:08,1,190,"I am doing a health science course where R or Stata are recommended  I'm trying to use Python / Numpy / Pandas instead as I wish to use it in future for financial time series analysis 

The data was Stata format so I copied the fields and saved them as a CSV
All fields imports are fine except that there are a number of columns of Yes/No some of which have blank fields

Import command is 

fhs = pdioparsersread_csv('F:\\BioStatistics\\fds\\fhs_c2csv'  header=0  index_col=0)


If there is a blank field the dtype is object (makes sense)

If there are no blanks some columns convert to TRUE/FALSE  others leave as Yes/No but dtype is bool  Any idea why?

I want all to by one dtype and expressed one way for viewing + stat analysis

I have achieve this by adding a row at the beginning with blank cells for the Boolean columns that had no spaces - so everything becomes an object  Then I use 
fhs = fhsdrop([1002]) to drop that row and data types are still good

I'd love to save it without this row and just be able to load the data each time with ""correct"" types but don't know if it possible when some of the columns will have all yes or no  and some will have blank cells  Is it possible?

Thanks  Sorry about the newbie question

Example:

Importing

      C1    C2    C3

R1   Yes   Yes    No

R2    No    No    No

R3   Yes         Yes

R4   Yes   Yes   Yes


first column comes into df as Yes  No  Yes  Yes  type bool xxxx below

2nd   column comes into df as Yes  No  NaN  Yes  type object

3rd   column comes into df as FALSE  FALSE  TRUE  TRUE  type bool

Damn  Just checked  I was wrong  If its yes and no then the column type is object

I'd like to tell it when importing to make them all object and stick with yes and no because:
1  I think the 2nd column must be object (as its mixed otherwise i think)
2  The data set is in yes / no and other class members will be looking at yes and no

What happened when I tried the solution

Here's my data:  link  

Here's the code:
    from pandas import *
    import numpy as np
    import pandas as pd  

def convert_bool(col):
    if str(col)title() ==  ""True"": #check for nan
        return ""Yes""
    elif str(col)title() == ""False"":
        return ""No""
    else:
        return col

fhs = pdread_csv('F:\\BioStatistics\\fds\\StatExportcsv'  converters={""death"": lambda x:convert_bool(x)}  header=0  index_col=0)  


and output link  ",1755173,,1755173.0,2012-10-18 21:56:40,2012-11-17 21:30:06,Python Pandas Mixed Boolean Yes/True and NaN Columns,<python><import><pandas>,2.0,2.0,,
12950024,1,12989920.0,2012-10-18 08:14:42,7,291,"I have a dataframe structured like this:

First     A                             B                         
Second  bar       baz       foo       bar       baz       foo     
Third   cat  dog  cat  dog  cat  dog  cat  dog  cat  dog  cat  dog
0         3    8    7    7    4    7    5    3    2    2    6    2
1         8    6    5    7    8    7    1    8    6    0    3    9
2         9    2    2    9    7    3    1    8    4    1    0    8
3         3    6    0    6    3    2    2    6    2    4    6    9
4         7    6    4    3    1    5    0    4    8    4    8    1


So there are three column levels I want to add a new column on the second level where for each of the third levels a computation is performed  for example 'new' = 'foo' + 'bar' So the resulting dataframe would look like:

First     A                                       B                                   
Second  bar       baz       foo       new       bar       baz       foo       new     
Third   cat  dog  cat  dog  cat  dog  cat  dog  cat  dog  cat  dog  cat  dog  cat  dog
0         3    8    7    7    4    7    7   15    5    3    2    2    6    2   11    5
1         8    6    5    7    8    7   16   13    1    8    6    0    3    9    4   17
2         9    2    2    9    7    3   16    5    1    8    4    1    0    8    1   16
3         3    6    0    6    3    2    6    8    2    6    2    4    6    9    8   15
4         7    6    4    3    1    5    8   11    0    4    8    4    8    1    8    5


I have found a workaround which is listed at the end of this post  but its not at all 'panda-style' and prone to errors The apply or transform function on a group seems like the right way to go but after hours of trying i still do not succeed I figured the correct way should be something like:

def func(data):

    fi = datacolumns[0][0]
    th = datacolumns[0][2]

    data[(fi 'new' th)] = data[(fi 'foo' th)] + data[(fi 'bar' th)]

    print data
    return data

print groupedapply(func)


The new column is properly added within the function  but is not returned Using the same function with transform would work if the 'new' column already exists in the df  but how do you add a new column at a specific level 'on the fly' or before grouping? 

The code to generate the sample df is:

import pandas  itertools

first = ['A' 'B']
second = ['foo' 'bar' 'baz']
third = ['dog'  'cat']

tuples = []
for tup in itertoolsproduct(first  second  third):
    tuplesappend(tup)

columns = pandasMultiIndexfrom_tuples(tuples  names=['First' 'Second' 'Third'])

data = nprandomrandint(0 10 (5  12))
df = pandasDataFrame(data  columns=columns)


And my workaround:

dfnew = None
grouped = dfgroupby(by=None  level=[0 2]  axis=1)

for name  group in grouped:
    newparam = groupxs('foo'  axis=1  level=1) + groupxs('bar'  axis=1  level=1)

    dftmp = groupjoin(pandasDataFrame(nparray(newparam)  columns=pandasMultiIndexfrom_tuples([(groupcolumns[0][0]  'new'  groupcolumns[0][2])]  names=['First' 'Second'  'Third'])))

    if dfnew is None:
        dfnew = dftmp
    else:
        dfnew = pandasconcat([dfnew  dftmp]  axis=1)

print dfnewsort_index(axis=1)


Wich works  but creating a new dataframe for each group and 'manually' assigning the levels is a really bad practice

So what is the proper way to do this? I found several posts dealing with similar questions  but all of these had only 1 level of columns  and thats exactly what im struggling with",1755432,,1755432.0,2012-10-18 08:31:58,2012-10-20 15:31:46,Add a column with a groupby on a hierarchical dataframe,<python><group-by><pandas>,1.0,2.0,5.0,
12840847,1,12841042.0,2012-10-11 13:37:12,0,81,"I am converting low-frequency data to a higher frequency with pandas (for instance monthly to daily) When making this conversion  I would like the resulting higher-frequency index to span the entire low-frequency window For example  suppose I have a monthly series  like so:

import numpy as np
from pandas import *
data = nprandomrandn(2)
s = Series(data  index=date_range('2012-01-01'  periods=len(data)  freq='M'))

s
2012-01-31    0
2012-02-29    1


Now  I convert it to daily frequency:

sresample('D')
2012-01-31     0
2012-02-01   NaN
2012-02-02   NaN
2012-02-03   NaN

2012-02-27   NaN
2012-02-28   NaN
2012-02-29     1


Notice how the resulting output goes from 2012-01-31 to 2012-02-29 But what I really want is days from 2011-01-01 to 2012-02-29  so that the daily index ""fills"" the entire January month  even if 2012-01-31 is still the only non-NaN observation in that month 

I'm also curious if there are built-in methods that give more control over how the higher-frequency period is filled with the lower frequency values In the monthly to daily example  the default is to fill in just the last day of each month; if I use a PeriodIndex to index my series I can also sresample('D'  convention='start') to have only the first observation filled in However  I also would like options to fill every day in the month with the monthly value  and to fill every day with the daily average (the monthly value divided by the number of days in the month)

Note that basic backfill and forward fill would not be sufficient to fill every daily observation in the month with the monthly value For example  if the monthly series runs from January to March but the February value is NaN  then a forward fill would carry the January values into February  which is not desired ",233446,,,,2012-10-11 13:45:40,Filling higher-frequency windows when upsampling with pandas,<python><pandas>,1.0,,,
12906450,1,12906708.0,2012-10-16 01:49:16,0,58,"I've performed some simple z-transforms on some variables I have in a pandas DataFrame There was a total of 216 columns in the dataframe  I transformed 196 of them and then concatenated the 197 onto the original 216 for a total of 412 total columns I then used the to_csv function to write the new dataframe to a csv The original data is about 300mb  while the new dataset is 12gb It seems odd that adding less than double the columns leads to around a 4x increase in size for the final csv The code I used is below Am I missing something? Or is there a more efficient way to write DataFrames to csv files? Everything looks fine when I take a look at the first row of the data Also  the number of rows are the same between all three of the DataFrames created in the code below 

import pandas as pd


full_data = pdread_csv('datacsv')

names = full_datacolumnstolist()
names = names[16:-2]
len(names) #197 as expected
transform = (full_data[names] - full_data[names]mean())/full_data[names]std() #Transform has 197 columns as expected 

column_names = transformcolumnstolist()

new_names = {}
for name in column_names:
    new_names[name] = name + '_standardized'

transform = transformrename(columns=new_names)


to_concat = [full_data  transform]

final_data = pdconcat(to_concat  axis=1)

final_datato_csv('transformed_datacsv'  index = False)
",1074057,,,,2012-10-16 02:28:57,Transformed Data Takes up 4x more space for a doubling of variables in pandas for python,<python><pandas>,1.0,,,
12945971,1,13674286.0,2012-10-18 01:51:08,23,980,"I want to be able to set the major and minor xticks and their labels for a time series graph plotted from a Pandas time series object  

The Pandas 09 ""what's new"" page says: 


  ""you can either use to_pydatetime or register a converter for the
  Timestamp type""


but I can't work out how to do that so that I can use the matplotlib axxaxisset_major_locator and axxaxisset_major_formatter (and minor) commands

If I use them without converting the pandas times  the x-axis ticks and labels end up wrong

By using the 'xticks' parameter I can pass the major ticks to pandasplot  and then set the major tick labels I can't work out how to do the minor ticks using this approach (I can set the labels on the default minor ticks set by pandasplot)

Here is my test code:

import pandas
print 'pandas__version__ is '  pandas__version__
print 'matplotlib__version__ is '  matplotlib__version__    

dStart = datetimedatetime(2011 5 1) # 1 May
dEnd = datetimedatetime(2011 7 1) # 1 July    

dateIndex = pandasdate_range(start=dStart  end=dEnd  freq='D')
print ""1 May to 1 July 2011""  dateIndex      

testSeries = pandasSeries(data=nprandomrandn(len(dateIndex)) 
                           index=dateIndex)    

ax = pltfigure(figsize=(7 4)  dpi=300)add_subplot(111)
testSeriesplot(ax=ax  style='v-'  label='first line')    

# using MatPlotLib date time locators and formatters doesn't work with new
# pandas datetime index
axxaxisset_minor_locator(matplotlibdatesWeekdayLocator(byweekday=(1) 
                                                           interval=1))
axxaxisset_minor_formatter(matplotlibdatesDateFormatter('%d\n%a'))
axxaxisgrid(True  which=""minor"")
axxaxisgrid(False  which=""major"")
axxaxisset_major_formatter(matplotlibdatesDateFormatter('\n\n\n%b%Y'))
pltshow()    

# set the major xticks and labels through pandas
ax2 = pltfigure(figsize=(7 4)  dpi=300)add_subplot(111)
xticks = pandasdate_range(start=dStart  end=dEnd  freq='W-Tue')
print ""xticks: ""  xticks
testSeriesplot(ax=ax2  style='-v'  label='second line' 
                xticks=xticksto_pydatetime())
ax2set_xticklabels([xstrftime('%a\n%d\n%h\n%Y') for x in xticks]);
# set the text of the first few minor ticks created by pandasplot
#    ax2set_xticklabels(['a' 'b' 'c' 'd' 'e']  minor=True)
# remove the minor xtick labels set by pandasplot 
ax2set_xticklabels([]  minor=True)
# turn the minor ticks created by pandasplot off 
# pltminorticks_off()
pltshow()
print testSeries['6/4/2011':'6/7/2011']


and it's output:

pandas__version__ is  091dev-3de54ae
matplotlib__version__ is  111
1 May to 1 July 2011 
[2011-05-01 00:00:00    2011-07-01 00:00:00]
Length: 62  Freq: D  Timezone: None




xticks:  
[2011-05-03 00:00:00    2011-06-28 00:00:00]
Length: 9  Freq: W-TUE  Timezone: None




2011-06-04   -0199393
2011-06-05   -0043118
2011-06-06    0477771
2011-06-07   -0033207
Freq: D


Update: I've been able to get closer to the layout I wanted by using a loop to build the major xtick labels:

# only show month for first label in month
month = dStartmonth - 1
xticklabels = []
for x in xticks:
    if  month != xmonth :
        xticklabelsappend(xstrftime('%d\n%a\n%h'))
        month = xmonth
    else:
        xticklabelsappend(xstrftime('%d\n%a'))


But this is a bit like doing the x-axis using axannotate  possible but not ideal",1629518,,1301710.0,2012-12-01 13:06:37,2012-12-02 21:53:14,Pandas timeseries plot setting x-axis major and minor ticks and labels,<python><matplotlib><pandas>,1.0,3.0,5.0,
12962705,1,12962864.0,2012-10-18 19:56:34,3,264,"Is there a function to enforce that the index is unique or is it only possibly to handle this in python 'itself' by converting to dict and back or something like that?

As noted in the comments below: python pandas is a project built on numpy/scipy 

to_dict and back works  but I bet this gets slow when you get BIG 

In [24]: a = pandasSeries([1 2 3]  index=[1 1 2])

In [25]: a
Out[25]: 
1    1
1    2
2    3

In [26]: a = ato_dict()

In [27]: a
Out[27]: {1: 2  2: 3}

In [28]: a = pandasSeries(a)

In [29]: a
Out[29]: 
1    2
2    3
",287238,,287238.0,2012-10-19 03:31:26,2012-10-20 15:20:44,python pandas remove duplicates in series,<python><pandas>,2.0,,,
12960574,1,,2012-10-18 17:44:44,3,237,"I am going through the 'Python for Data Analysis' book and having trouble in the 'Example: 2012 Federal Election Commision Database' section reading the data to a DataFrame The trouble is that one of the columns of data is always being set as the index column  even when the index_col argument is set to None 

Here is the link to the data : http://wwwfecgov/disclosurep/PDownloaddo

Here is the loading code (to save time in the checking  I set the nrows=10):

import pandas as pd
fec = pdread_csv('P00000001-ALLcsv' nrows=10 index_col=None)


To keep it short I am excluding the data column outputs  but here is my output (please not the Index values):

In [20]: fec

Out[20]:

Index: 10 entries  C00410118 to C00410118
Data columns:

dtypes: float64(4)  int64(3)  object(11)


And here is the book's output (again with data columns excluded):

In [13]: fec = read_csv('P00000001-ALLcsv')
In [14]: fec
Out[14]:

Int64Index: 1001731 entries  0 to 1001730

dtypes: float64(1)  int64(1)  object(14)


The Index values in my output are actually the first column of data in the file  which is then moving all the rest of the data to the left by one Would anyone know how to prevent this column of data to be listed as an index? I would like to have the index just +1 increasing integers

I am fairly new to python and pandas  so I apologize for any inconvenience Thanks",1757088,,,,2012-10-18 18:19:30,pandas read_csv index_col=None not working,<python><pandas>,1.0,1.0,,
12962765,1,12962979.0,2012-10-18 20:01:18,1,270,"I've been trying to figure this out for the last couple of hours

I have a list that I want to use as columns for DataFrames:

totalColumns = [a  b  c  d  e  fz]


I have several data frames that look like this:

DataFrameOne:

    b   f   j
1   12  5   6
2   4   99  2
3   10  77  16


DataFrameTwo:

    a   k   y
1   2   25  46
2   7   54  76
3   34  67  101
4   45  24  54


and many more

I want to reindex all the data frames according to totalColumns For example  after reindexing  DataFrameOne would look like this:

DataFrameOne:

    a    b   cfjz
1   NaN  5   NaN56NaN
2   NaN  99  NaN992NaN
3   NaN  77  NaN7716NaN


So I used the reindex method:

DataFrameOnereindex(columns=totalColumns)


It worked for some of the data frames  but I would get this exception with some data frames:

raise Exception('Reindexing only valid with uniquely valued Index '
Exception: Reindexing only valid with uniquely valued Index objects


Anyone can help me get passed this error that happens on some of the data frames?",1712282,,,,2012-10-18 22:00:53,Pandas DataFrame reindex column issue,<python><pandas>,1.0,0.0,,
12974404,1,12976590.0,2012-10-19 12:34:46,1,126,"I encountered this behaviour when doing basic data munging  like in this example:

In [55]: import pandas as pd
In [56]: import numpy as np
In [57]: rng = pddate_range('1/1/2000'  periods=10  freq='4h')
In [58]: lvls = ['A' 'A' 'A' 'B' 'B' 'B' 'C' 'C' 'C' 'C']
In [59]: df = pdDataFrame({'TS': rng  'V' : nprandomrandn(len(rng))  'L' : lvls})

In [60]: df
Out[60]: 
   L                  TS         V
0  A 2000-01-01 00:00:00 -1152371
1  A 2000-01-01 04:00:00 -2035737
2  A 2000-01-01 08:00:00 -0493008
3  B 2000-01-01 12:00:00 -0279055
4  B 2000-01-01 16:00:00 -0132386
5  B 2000-01-01 20:00:00  0584091
6  C 2000-01-02 00:00:00 -0297270
7  C 2000-01-02 04:00:00 -0949525
8  C 2000-01-02 08:00:00  0517305
9  C 2000-01-02 12:00:00 -1142195


the problem:

In [61]: df['TS']min()
Out[61]: 31969-04-01 00:00:00

In [62]: df['TS']max()
Out[62]: 31973-05-10 00:00:00


while this looks ok:

In [63]: df['V']max()
Out[63]: 058409076701429163

In [64]: min(df['TS'])
Out[64]: 


when aggregating after groupby:

In [65]: dfgroupby('L')min()
Out[65]: 
             TS         V
L                        
A  9466848e+17 -2035737
B  9467280e+17 -0279055
C  9467712e+17 -1142195

In [81]: val = dfgroupby('L')agg('min')['TS']['A']
In [82]: type(val)
Out[82]: numpyfloat64


Apparently in this particular case it has something to do with using frequency datetime index as argument of pdSeries function:

In [76]: rngmin()
Out[76]: 

In [77]: ts = pdSeries(rng)
In [78]: tsmin()
Out[78]: 31969-04-01 00:00:00

In [79]: type(tsmin())
Out[79]: numpydatetime64


However  my initial problem was with min/max of Timestamp series parsed from strings via pdread_csv()

What am I doing wrong?",1707495,,,,2012-10-19 14:39:59,Unexpected results of min() and max() methods of Pandas series made of Timestamp objects,<python><timestamp><pandas>,1.0,3.0,,
12844529,1,12846154.0,2012-10-11 16:45:14,0,111,"I have a problem with some groupy code which I'm quite sure once ran (on an older pandas version) On 09  I get No numeric types to aggregate errors Any ideas?

In [31]: data
Out[31]: 

DatetimeIndex: 2557 entries  2004-01-01 00:00:00 to 2010-12-31 00:00:00
Freq: 
Columns: 360 entries  -8975 to 8975
dtypes: object(360)

In [32]: latedges = linspace(-90  90  73)

In [33]: lats_new = linspace(-875  875  72)

In [34]: def _get_gridbox_label(x  bins  labels):
   :             return labels[searchsorted(bins  x) - 1]
   : 

In [35]: lat_bucket = lambda x: _get_gridbox_label(x  latedges  lats_new)

In [36]: dataTgroupby(lat_bucket)mean()
---------------------------------------------------------------------------
DataError                                 Traceback (most recent call last)
 in ()
----> 1 dataTgroupby(lat_bucket)mean()

/usr/lib/python27/site-packages/pandas/core/groupbypy in mean(self)
    295         """"""
    296         try:
--> 297             return self_cython_agg_general('mean')
    298         except DataError:
    299             raise

/usr/lib/python27/site-packages/pandas/core/groupbypy in _cython_agg_general(self  how  numeric_only)
   1415 
   1416     def _cython_agg_general(self  how  numeric_only=True):
-> 1417         new_blocks = self_cython_agg_blocks(how  numeric_only=numeric_only)
   1418         return self_wrap_agged_blocks(new_blocks)
   1419 

/usr/lib/python27/site-packages/pandas/core/groupbypy in _cython_agg_blocks(self  how  numeric_only)
   1455 
   1456         if len(new_blocks) == 0:
-> 1457             raise DataError('No numeric types to aggregate')
   1458 
   1459         return new_blocks

DataError: No numeric types to aggregate
",152439,,1252759.0,2012-10-16 10:52:06,2012-10-16 10:52:06,No numeric types to aggregate - change in groupby() behaviour?,<python><pandas>,1.0,,,
12924264,1,,2012-10-16 22:13:19,0,222,"lots of info on how to store my pandas dataframe to pytables   then read it back at that point it will have ""kind = v_v_attrspandas_type""  

Lots of information on how to read a csv into a pandas dataframe

but what I have is a pyTable table and want a pandas comparable one

Must be an example of this but all search attempts are clouded by other use cases

I could write it out as csv and re read it in but that seems silly   It is what I am doing for now",1615371,,,,2012-10-17 08:53:09,Say you had a pyTables Table and wanted a pandas table,<pandas><pytables>,1.0,,1.0,
12926660,1,12927871.0,2012-10-17 03:27:29,0,102,"I get the Fama-French factors from Ken French's data library using pandasiodata  but I can't figure out how to convert the integer year-month date index (eg  200105) to a datetime index so that I can take advantage of more pandas features

The following code runs  but my index attempt in the last un-commented line drops all data in DataFrame ff I also tried reindex()  but this doesn't change the index to range What is the pandas way? Thanks!

import pandas as pd
from pandasiodata import DataReader
import datetime as dt

ff = pdDataFrame(DataReader(""F-F_Research_Data_Factors""  ""famafrench"")[0])
ffcolumns = ['Mkt_rf'  'SMB'  'HML'  'rf']

start = ffindex[0]
start = dtdatetime(year=start//100  month=start%100  day=1)
end = ffindex[-1]
end = dtdatetime(year=end//100  month=end%100  day=1)
range = pdDateRange(start  end  offset=pddatetoolsMonthEnd())
ff = pdDataFrame(ff  index=range)
#ffreindex(range)
",334755,,,,2012-10-17 17:50:51,Convert integer index from Fama-French factors to datetime index in pandas,<python><datetime><pandas>,1.0,,,
12979568,1,12980313.0,2012-10-19 17:41:14,0,91,"I have a basic (almost naive) question for plotting on top of a pandas df Given the df below I am trying to do a stack bar plot for 'stats_value' and 'read1_length'  v/s 'lib_name'

    temp1=

               parent_library_name lib_name stats_value  read1_length
    58                  None     CXYY         106           150
    311                 CXYY     CSGW         128           150
    432                 CXYY     CSNS         109           150
    552                 CXYY     CXPS         125           150
    671                 CXYY     CXOA         123           150
    1113                CXYY     CXOC         108           150
    1394                CXYY     CXOO         129           150
    1675                CXYY     CXOP         101           150
    1794                CXYY     CXSP         132           150
    1914                CXYY     CXOY         116           150
    2356                CXYY     CXSO          69           150
    2635                CXYY     CSHT          77           150
    2914                CXYY     CXSU          76           150


Tried the following things:

c=temp1set_index('lib_name')
c[['stats_value' 'read1_length']]plot(kind='bar' stacked=True)

Error:
TypeError: unsupported operand type(s) for +: 'numpyfloat64' and 'str'


Tried something simple just to test:

c=temp1set_index('lib_name')
c[['stats_value']]plot()

Error:
AttributeError: 'numpyndarray' object has no attribute 'find'


So I think I am missing some trick here

Best 
-Abhi",369541,,369541.0,2012-10-19 18:06:09,2012-10-19 18:30:15,basic plotting with pandas,<python><plot><pandas>,1.0,3.0,,
12860421,1,12862196.0,2012-10-12 13:43:47,1,302,"df2 = pdDataFrame({'X' : ['X1'  'X1'  'X1'  'X1']  'Y' : ['Y2' 'Y1' 'Y1' 'Y1']  'Z' : ['Z3' 'Z1' 'Z1' 'Z2']})

    X   Y   Z
0  X1  Y2  Z3
1  X1  Y1  Z1
2  X1  Y1  Z1
3  X1  Y1  Z2

g=df2groupby('X')

pdpivot_table(g  values='X'  rows='Y'  cols='Z'  margins=False  aggfunc='count')



  Traceback (most recent call last):  AttributeError: 'Index' object
  has no attribute 'index'


How do I get a Pivot Table with counts of unique values of one DataFrame column for two other columns?
Is there aggfunc for count unique? Should I be using npbincount()?

NB I am aware of 'Series' values_counts() however I need a pivot table

EDIT: The output should be:

Z   Z1  Z2  Z3
Y             
Y1   1   1 NaN
Y2 NaN NaN   1
",59797,,59797.0,2012-10-12 15:06:52,2012-10-12 15:21:39,Python Pandas : pivot table with aggfunc = count unique distinct,<python><pandas><pivot-table>,2.0,1.0,,
12867178,1,12874054.0,2012-10-12 21:12:33,6,549,"In the following  male_trips is a big pandas data frame and stations is a small pandas data frame For each station id I'd like to know how many male trips took place The following does the job  but takes a long time:

mc = [ sum( male_trips['start_station_id'] == id ) for id in stations['id'] ]


how should I go about this instead?

Update! So there were two main approaches: groupby() followed by size()  and the simpler value_counts() I did a quick timeit  and the groupby approach wins by quite a large margin! Here is the code:

from timeit import Timer
setup = ""import pandas; male_trips=pandasload('maletrips')""
a  = ""male_tripsstart_station_idvalue_counts()""
b = ""male_tripsgroupby('start_station_id')size()""
Timer(a setup)timeit(100)
Timer(b setup)timeit(100)


and here is the result:

In [4]: Timer(a setup)timeit(100) # ",270572,,270572.0,2012-10-14 11:50:43,2012-10-14 14:18:27,pandas: count things,<python><pandas>,5.0,4.0,2.0,
12886240,1,12890620.0,2012-10-14 20:29:03,0,85,"I want to iterate over a Dataframe like this:

for i in yitertuples(): print i


result:

(datetimedate(2012  9  10)  63930000305175781  64589996337890625  63880001068115234  64099998474121094  5077000  64099998474121094)
(datetimedate(2012  9  11)  63490001678466797  63790000915527344  62509998321533203  63759998321533203  8966000  63759998321533203)


How can I create a new Dataframe of each iterated tuple with indexing the date object?",439693,,,,2012-10-15 10:45:13,iter over dataframe,<python><pandas>,1.0,0.0,,
12984205,1,,2012-10-20 00:38:18,0,194,"I am trying to stream a matplotlib generated through pandas dfplot() method  but having trouble to get the image rendered on the matplotlib canvas

Here is what I am doing

import cStringIO
import matplotlibpyplot as plt
import pandas
from matplotlibbackendsbackend_agg import FigureCanvasAgg as FigureCanvas


labels = ['a'  'b'  'c'  'd'  'e']
s = pandasSeries(nprandomrandn(5)  index=labels)


fig = pltfigure()
splot()
canvas = FigureCanvas(fig)
output = cStringIOStringIO()
canvasprint_png(output)

fh=open('checkpng' 'w')
fhwrite(outputgetvalue())


the checkpng turns out to be blank I think I am not able to render the plot generated by pandas plot function to canvas

Second Try: see the axes but no data

import cStringIO
import matplotlibpyplot as plt
import pandas
import numpy as np
from matplotlibbackendsbackend_agg import FigureCanvasAgg as FigureCanvas

labels = ['a'  'b'  'c'  'd'  'e']
s = pandasSeries(nprandomrandn(5)  index=labels)


fig = pltfigure()
ax = figadd_subplot(111)
splot(ax=ax)
canvas = FigureCanvas(fig)
output = cStringIOStringIO()
canvasprint_png(output)


fh=open('checkpng' 'w')
fhwrite(outputgetvalue())


Thanks!
-Abhi",369541,,369541.0,2012-10-20 00:51:10,2012-10-20 02:19:33,render pandas generated plot to matplotlib canvas for streaming,<python><matplotlib><pandas>,1.0,,,
12974774,1,12975201.0,2012-10-19 12:57:21,3,170,"I am trying to find a way to group data daily 

This is an example of my data set

Dates                              Price1                                 Price 2

2002-10-15  11:17:03pm              06                                     50

2002-10-15  11:20:04pm              14                                     24

2002-10-15  11:22:12pm              41                                     91

2002-10-16  12:21:03pm              16                                     14

2002-10-16  12:22:03pm              77                                     37
",1759369,,1759369.0,2012-11-11 11:40:26,2012-12-30 18:25:14,How to group data by time,<python><matplotlib><pandas>,3.0,1.0,,
13003769,1,13008876.0,2012-10-22 00:59:03,2,155,"I have multiple (more than 2) dataframes I would like to merge They all share the same value column:

In [431]: [xhead() for x in data]
Out[431]: 
[                     AvgStatisticData
DateTime                             
2012-10-14 14:00:00         39335996
2012-10-14 15:00:00         40210110
2012-10-14 16:00:00         48282816
2012-10-14 17:00:00         40593039
2012-10-14 18:00:00         40952014 
                      AvgStatisticData
DateTime                             
2012-10-14 14:00:00         47854712
2012-10-14 15:00:00         55041512
2012-10-14 16:00:00         55488026
2012-10-14 17:00:00         51688483
2012-10-14 18:00:00         57916672 
                      AvgStatisticData
DateTime                             
2012-10-14 14:00:00         54171233
2012-10-14 15:00:00         48718387
2012-10-14 16:00:00         59978616
2012-10-14 17:00:00         50984514
2012-10-14 18:00:00         54924745 
                      AvgStatisticData
DateTime                             
2012-10-14 14:00:00         65813114
2012-10-14 15:00:00         71397868
2012-10-14 16:00:00         76213973
2012-10-14 17:00:00         72729002
2012-10-14 18:00:00         73196415 
etc


I read that join can handle multiple dataframes  however I get:

In [432]: data[0]join(data[1:])

Exception: Indexes have overlapping values: ['AvgStatisticData']


I have tried passing rsuffix=[""%i"" % (i) for i in range(len(data))] to join and still get the same error I can workaround this by building my data list in a way where the column names don't overlap  but maybe there is a better way?  ",107156,,,,2012-10-22 09:54:30,Joining Multiple Dataframes with Pandas with overlapping Column Names?,<join><merge><pandas>,2.0,,,
13013351,1,13013548.0,2012-10-22 14:17:47,0,168,"I'm working with data like this:

Sample  Detector        Cq
P_1   106    2353152
P_1   106    23152458
P_1   106    23685083
P_1   135        24465698
P_1   135        2386892
P_1   135        23723469
P_1   17  22524242
P_1   17  20658733
P_1   17  21146122


As suggested in this post  I'm handling that with a MultiIndex However  I'm wondering how  with such a structure  do some additional checks Let's explain better: each ""Sample"" column has a fixed number of repeated ""Detector"" elements  from 1 (no duplication) to several duplicated elements I want to ensure that for each sample element  the number of detectors is always the same (ie  if P_1 has 3 ""106"" detectors  P_2 should have 3 ""106"" detectors as well)

Currently I'm doing this rather crudely:

def replicate_counter(dataframe  name):
    subset = dataframeix[name]
    num_replicates = subsetindexsize / subsetindexunique()size
    return num_replicates

# Further down
# dataframe is a MultiIndex DataFrame like above
counts = pandasSeries([replicate_counter(dataframe  item[0]) for item
                        in dataframeindex])unique()

if countssize != 1:
    raise ValueError(""Detectors not equal for all samples"")


It seems very hacky to me and probably there are better ways to do this in pandas How could this be accomplished?",241515,,,,2012-10-22 14:27:45,Pandas DataFrame with MultiIndex: efficient way of checking duplicate elements in one of the indices,<python><pandas><data-analysis>,1.0,,,
12946578,1,12948922.0,2012-10-18 03:21:26,0,103,"When I create a DataFrame  then sort by a column it appears to be sorted in the iteractive display (ie whatever repr gives)  but when I call the DataFrameplot() function it plots the unsorted array Calling matplotlibpylabplot works fine though I suspect it's something to do with clever pointer rearrangement not being passed to whatever the plot function is calling to access the data  or maybe I'm just doing something dumb I've tried this on pandas 081 (osx and linux with python27something) and pandas 090 (osx with python3something)

import pandas
import numpy
from matplotlibpylab import *
a = numpyrandomrandn(100 10)
df = pandasDataFrame(a)
dfshape
dfsort(column=0)
dfcolumns
dfsort(column=0  inplace=True)
df[0]
df[0]plot()
",287238,,,,2012-10-18 07:06:36,Is this a python pandas DataFrame post sort plot bug?,<python><plot><pandas>,1.0,1.0,,
12957593,1,12959490.0,2012-10-18 14:55:31,3,115,"I have a data set that's sort of like this (first lines shown):

Sample  Detector        Cq
P_1   106    2353152
P_1   106    23152458
P_1   106    23685083
P_1   135        24465698
P_1   135        2386892
P_1   135        23723469
P_1   17  22524242
P_1   17  20658733
P_1   17  21146122


Both ""Sample"" and ""Detector"" columns contain duplicated values (""Cq"" is unique): to be precise  each ""Detector"" appears 3 times for each sample  because it's a replicate in the data

What I need to do is to:

Reshape the table so that the columns contain Samples and rows Detectors
Rename the duplicate columns so that I know which replicate is it
I thought that DataFramepivot would do the trick  but it fails because of the duplicate data What would be the best approach? Rename the duplicates  then reshape  or is there a better option?

EDIT: I thought over it and I think it's better to state the purpose I need to store for each ""Sample"" the mean and standard deviation of their ""Detector"" ",241515,,241515.0,2012-10-19 07:43:33,2012-10-19 07:43:33,Pandas: reshape data with duplicate row names to columns,<python><pandas>,1.0,,,
13020346,1,,2012-10-22 21:42:20,1,147,"With 

df = read_csv('data\querycsv')


I'am getting:

    TIMESTAMP   MILLISECONDS    PRICE
0   15102012 08:00:06     350     246
1   15102012 08:00:06     630     247
2   15102012 08:00:06     640     249
3   15102012 08:00:06     650     245
4   15102012 08:00:06     710     243


I figured out that this one 

df = read_csv('data\querycsv'  parse_dates=[[0  1]]  index_col=0)


is concatenating the first two columns to a string but still not recognizing the index as a DatetimeIndex

Additionally this one

Import datetime
datetimedatetimestrptime(""15102012 15:30:00 890""  ""%d%m%Y %H:%M:%S %f"")


is doing the conversion job

QUESTION: how to do the conversion and DatetimeIndex in one rush at read_csv?",1766682,,,,2012-10-31 20:02:44,Pandas: Converting Timestamp and Milliseconds to DatetimeIndex,<csv><pandas><date-parsing>,2.0,2.0,,
13022392,1,13023268.0,2012-10-23 01:34:30,0,85,"I am working with an array created from a list of geographical coordinates describing a GPS trajectory The data is like this:

[[-51203018 -29996149]
 [-51203018 -2999625 ]
 [-5120266  -29996229]
   
 [-5164315  -29717896]
 [-51643112 -29717737]
 [-51642937 -29717709]]


What I want to do is to calculate the geographic distances between rows (with the special condition that the first element is always zero  at the starting point) This would give me either a list of distances with len(distances) == coord_arrayshape[1]  or maybe a third column in the same array

It is important to say that I already have a function that returns a distance between two points (two coordinate pairs)  but I don't know how to apply it with a single array operation instead of looping through row pairs

Currently I do the following to calculate segment distances in one new column  and cumulative distances in another new column (latlonarray is already shown above and distance(p1  p2) is already defined):

    dists = [00]
    for n in xrange(len(lonlat)-1):
        distsappend(distance(lonlat[n+1]  lonlat[n]))

    lonlatarray = numpyarray(lonlat)reshape((-1 2))
    distsarray = numpyarray(dists)reshape((-1 1))
    cumdistsarray = numpycumsum(distsarray)reshape((-1 1))

    print numpyhstack((lonlatarray  distsarray  cumdistsarray))

[[   -51203018      -29996149        0              0        ]
 [   -51203018      -2999625         704461338      704461338]
 [   -5120266       -29996229       3987928578     4692389917]
   
 [   -5164315       -29717896       1111669769  9252972742791]
 [   -51643112      -29717737       1177016407  9254149759198]
 [   -51642937      -29717709       1957670066  9256107429263]]


My main question is: ""How could I perform the distance function (which takes a pair of rows as argument) like an array operation instead of a loop?"" (that is  how could I properly vectorize it)

Other on-topic questions would be:

If I decide to use Pandas  is ther some clever trick to accomplish this?
Is there a way to put scipyspatialdistance to ""work for me"" using geographic distance (haversine  great-circle distance)?
Also  I would appreciate some tip if I am doing anything unnecessarily complicated

Thank you all  very much  for your interest",401828,,,,2012-10-23 03:47:22,Calculate diff of a numpy array using custom function instead of subtraction,<multidimensional-array><numpy><pandas><vectorization>,1.0,,,
12992958,1,,2012-10-20 21:40:19,1,94,"I'm trying to work with some stock market data I have the following DataFrame:

>>> ticker

DatetimeIndex: 707 entries  2010-01-04 00:00:00 to 2012-10-19 00:00:00
Data columns:
Open         707  non-null values
High         707  non-null values
Low          707  non-null values
Close        707  non-null values
Volume       707  non-null values
Adj Close    707  non-null values
dtypes: float64(5)  int64(1)


I'll reference a random closing price:

>>> ticker ['Close'] [704]
21789999999999999


What's the syntax to get the date of that 704th item? 

Similarly  how do I get the position in the array of the following item?:

>>> tickerClosemin ()
17670000000000002


I know this seems pretty basic  but I've spent a lot of time scouring the documentation If it's there  I'm absolutely missing it",1762007,,1301710.0,2012-10-21 14:47:18,2012-10-21 14:47:18,"Pandas, How to reference Timeseries Items?",<python><time-series><pandas>,1.0,2.0,,
12995222,1,,2012-10-21 05:20:41,1,119,"I am just getting started in Python and using pandas to write a little stock portfolio app The problem I am having is in my position class which creates pandas Series to represent the number of shares owned of each stock over time based on the trades So If I bought 50 shares in IBM on 10/10/2012 and sold 10 shares on 10/14/2012  I want the positionshares series for IBM to be:

10/10/2012: 50 
10/11/2012: 50 
10/12/2012: 50 
10/13/2012: 50
10/14/2012: 40
10/15/2012: 40
I plan on doing this by adding series that go from the trade date through the current date and then summing each of these series into one I am trying to use the Seriesadd function since I need a fill value to make sure the new shares series representing a new transaction to not NaN the old positions The problem is I don't know how to initialize a workable Series that can be added to properly I am trying Series(0) in the below code by that just returns:

Series([]  dtype=float64)


I also tried initializing it with just some random dates with a 0 value but I just kept getting that series back even after adding different series to it

Here is my code:

class Position: 
    def __init__(self  trades):
        selfticker = trades[0]ticker #grab the ticker we are dealing with
        selfshares = Series()
        for trade in trades:
            if tradetranType == 0 or tradetranType == 2:
                direction = 1 #a buy increases share amount
            else:
                direction = -1 # a sell decreases share amount
            dateRangeInFolio = pdDateRange(tradedate  datetimedatetimetoday())     #from the event date through today
            shareSer = Series(tradeshares * direction  index=dateRangeInFolio)
            selfsharesadd(shareSer  fill_value=0)
        print selfshares        


Thanks for any help I apologize if I am missing something very basic",1513413,,,,2012-10-21 14:46:13,Pandas adding Series to keep a running sum,<python><pandas>,1.0,1.0,,
13030402,1,13034577.0,2012-10-23 12:27:11,1,80,"When plotting a timeseries with the built-in plot function of pandas  it seems to ignore the timezone of my index: it always uses the UTC time for the x-axis An example:

import numpy as np
import matplotlibpyplot as plt
from pandas import rolling_mean  DataFrame  date_range

rng = date_range('1/1/2011'  periods=200  freq='S'  tz=""UTC"")
data = DataFrame(nprandomrandn(len(rng)  3)  index=rng  columns=['A'  'B'  'C'])
data_cet = datatz_convert(""CET"")

# plot with data in UTC timezone
fig  ax = pltsubplots()
data[[""A""  ""B""]]plot(ax=ax  grid=True)
pltshow()

# plot with data in CET timezone  but the x-axis remains the same as above
fig  ax = pltsubplots()
data_cet[[""A""  ""B""]]plot(ax=ax  grid=True)
pltshow()


The plot does not change  although the index has:

In [11]: dataindex[0]
Out[11]: 
In [12]: data_cetindex[0]
Out[12]: 


Should I file a bug  or do I miss something?",653364,,,,2012-10-23 15:58:16,Pandas plot function ignores timezone of timeseries,<python><pandas>,1.0,,,
12850345,1,12850453.0,2012-10-11 23:53:37,2,189,"I'm using python pandas data frame   I have a initial data frame say D I extract two data frames from it like this:

A = D[Dlabel == k]

B = D[Dlabel != k]

then I change the label in A and B:

Alabel = 1


Blabel = -1

I want to combine A and B so I can have them as one data frame something like union The order of the data not important   however when we sample A and B from D they retain their indexes from D",445491,,,,2012-10-12 00:07:38,how to combine two data frames in python pandas,<python><pandas>,1.0,1.0,1.0,
12996021,1,,2012-10-21 08:00:11,5,189,"Are there appropriate packages for timeseries (analysis and forecast) in python? 

And what methods they include?

Example: there are in R the forecast package with includes many functions Is there any same package like this in Python?",1757163,,1719510.0,2012-10-21 08:57:58,2012-10-21 08:57:58,packages for time series in python,<python><time-series><pandas><forecasting><statsmodels>,2.0,,1.0,
13021654,1,13021797.0,2012-10-22 23:48:58,1,152,"In R when you need to retrieve a column index based on the name of the column you could do

idx ",55299,,487339.0,2012-10-23 00:07:03,2012-10-23 18:27:34,Retrieving column index from column name in python pandas,<python><data.frame><pandas>,2.0,,,
13030245,1,13031591.0,2012-10-23 12:17:56,1,155,"In a regular time series you can shift it back or forward in time

eg for the following time series:

start = datetime(2012 1 1)
end = datetime(2012 4 1)
rng = pddate_range(start end)
ts = pdSeries(nprandomrandn(len(rng))  index = rng)


We can shift it with:

tsshift(2  freq=""D"")


How can I do this for a MultiIndex time series on just one level?

eg for the following MultiIndex time series:

mi = [(dt i) for dt in rng for i in range(5)]
ts_mi = pdSeries(nprandomrandn(len(mi))  index = pdMultiIndexfrom_tuples(mi))


Which might look something like:

2012-01-01  0   -0805353
            1    1467167
            2   -1207204
            3    1658394
            4    1497559
2012-01-02  0   -0742510
            1    0764594
            2    0558660
            3   -0479370
            4    0653849



Shifting it using ts_mishift(2  freq=""D"") gives:

None   -0805353
None    1467167
None   -1207204
None    1658394
None    1497559
None   -0742510
None    0764594
None    0558660
None   -0479370
None    0653849
None   -0138347
None    0357479
None   -0919202
None    1300977
None   -0360398

",544059,,,,2012-10-23 16:02:11,How to shift a pandas MultiIndex Series?,<python><time-series><pandas>,1.0,,,
13032834,1,13032992.0,2012-10-23 14:32:34,1,65,"I have a tab delimited file with four columns I need to combine 'col3' and 'col4' for each unique value pairs in 'col1' and 'col2' An example and output is shown as under 

One way I am thinking of is to use nested loops: outer loop read the lines sequentially and the inner loop reads all lines from the begining and look for map However this process seems to be computational intensive 

Is there an alternative way to do this 

col1    col2    col3    col4
a   c   1 2 physical
a   c   2 3 genetic
b   c   22  physical 
b   d   33 44   genetic
c   e   1 2 genetic
c   e   2   physical
c   f   33 44   physical
c   f   3   genetic
a   a   4   genetic
e   c   1 2 xxxxx


col1    col2    col3    col4
a   c   1 2 3   genetic physical
a   a   4   genetic
b   c   22  physical 
b   d   33 44   genetic
c   e   1 2 genetic physical xxxxx
c   f   3 33 44 genetic physical


It combines the values if 'col1' and 'col2' are switched as in the last line above with value 'xxxxx'",1649335,,1649335.0,2012-10-23 14:40:04,2012-10-23 16:09:21,Based on the values in two columns merge values in other columns,<python><merge><pandas>,2.0,,,
13000427,1,13003683.0,2012-10-21 17:42:17,0,131,"I would like to turn:

DateTime                     ColumnName        Min      Avg      Max                                                                                      
2012-10-14 11:29:23810000   Percent_Used       24       24       24
2012-10-14 11:29:23810000   Current_Count  254503   254503   254503
2012-10-14 11:29:23810000   Max           1048576  1048576  1048576
2012-10-14 11:34:23813000   Percent_Used       24       24       24
2012-10-14 11:34:23813000   Current_Count  254116   254116   254116
2012-10-14 11:34:23813000   Max           1048576  1048576  1048576


Into a dataframe where the the DateTimes are unique (an index) and the columns are:

DataTime  Percent_Used_Min  Percent_Used_Avg  Percent_Used_Max  Current_Count_Min  Current_Count_Avg  Current_Count_Max  Max_Min  Max_Avg  Max_Max

Basically  I want to mimic R's melt/cast without getting into hierarchical indexing or stacked dataframes I can't seem to to get exactly the above playing with stack/unstack  melt  or pivot/pivot_table -- Is there a good way to do this?

As An example  in R it would be something like:

dynamic_melt = melt(dynamic  id = c(""DateTime""  ""ColumnName""))
recast = dataframe(cast(dynamic_melt  DateTime ~ ))


The above data will be variable (ie the values of ColumnName won't always be the same thing  there might be more or less of them  and different names) ",107156,,107156.0,2012-10-21 17:49:25,2012-10-22 00:42:23,Reshape Long Format Multivalue Dataframes with Pandas,<python><pivot><pandas><reshape>,1.0,1.0,1.0,
13002850,1,,2012-10-21 22:32:02,1,81,"Date series looks something like this

In [89]:
dbclose[:5]

Out[89]:
datetime
2012-06-28 23:58:00    1243925
2012-06-28 23:59:00    1244125
2012-06-29 00:00:00    1244065
2012-06-29 00:01:00    1243875
2012-06-29 00:02:00    1243865
Name: close


I would like to subtract previous element from each element

In [93]:
dbclose[1:5] - dbclose[:4]

Out[93]:
datetime
2012-06-28 23:58:00   NaN
2012-06-28 23:59:00     0
2012-06-29 00:00:00     0
2012-06-29 00:01:00     0
2012-06-29 00:02:00   NaN
Name: close


Arrays was subtract without offset

But when I compare the array elements

n [94]:

dbclose[1:5] == dbclose[:4]
Out[94]:
datetime
2012-06-28 23:59:00    False
2012-06-29 00:00:00    False
2012-06-29 00:01:00    False
2012-06-29 00:02:00    False
Name: close
",1763885,,,,2012-10-23 20:16:36,Python Pandas: Vectorized operation bug?,<python><numpy><python-3.x><pandas>,2.0,,,
13028796,1,13029042.0,2012-10-23 10:49:18,-4,196,"I am using pandas to get the count of the Text type data and to find out the top 5 among the given data

Input file is as follows:

Gears of war 3
Gears of war
Assassin creed


Crysis 2
Gears of war3
Sims


My Output is as follows:

{
    'Gears of War 3': 6 
    'Batman': 5 
    'gears of war 3': 4 
    'Rocksmith': 5 
    'nan': 32870
}


I want my code to skip counting nan values in my csv file

My code is as follows:

data = pandasread_csv('D:\my_filecsv')

for colname  dtype in datadtypesto_dict()iteritems():
    if dtype == 'object':
        print colname
        count = Counter(data[colname])
        d = dict((str(k)  v) for k  v in countiteritems())
        f = dict(sorted(diteritems()  key=lambda item: item[1]  reverse = True)[:5])
",1667967,,63011.0,2012-10-23 11:00:45,2012-10-23 11:29:11,Skip NaN values while counting data in python Pandas,<python><pandas>,2.0,,,
13035764,1,13036848.0,2012-10-23 17:11:04,3,255,"I'm reading some automated weather data from the web The observations occur every 5 minutes and are compiled into monthly files for each weather station Once I'm done parsing a file  the DataFrame looks something like this:

                      Sta  Precip1hr  Precip5min  Temp  DewPnt  WindSpd  WindDir  AtmPress
Date                                                                                      
2001-01-01 00:00:00  KPDX          0           0     4       3        0        0     3031
2001-01-01 00:05:00  KPDX          0           0     4       3        0        0     3030
2001-01-01 00:10:00  KPDX          0           0     4       3        4       80     3030
2001-01-01 00:15:00  KPDX          0           0     3       2        5       90     3030
2001-01-01 00:20:00  KPDX          0           0     3       2       10      110     3028


The problem I'm having is that sometimes a scientist goes back and corrects observations -- not by editing the erroneous rows  but by appending a duplicate row to the end of a file Simple example of such a case is illustrated below:

import pandas 
import datetime
startdate = datetimedatetime(2001  1  1  0  0)
enddate = datetimedatetime(2001  1  1  5  0)
index = pandasDatetimeIndex(start=startdate  end=enddate  freq='H')
data = {'A' : range(6)  'B' : range(6)}
data1 = {'A' : [20  -30  40]  'B' : [-50  60  -70]}
df1 = pandasDataFrame(data=data  index=index)
df2 = pandasDataFrame(data=data1  index=index[:3])
df3 = df1append(df2)
df3
                       A   B
2001-01-01 00:00:00    0   0
2001-01-01 01:00:00    1   1
2001-01-01 02:00:00    2   2
2001-01-01 03:00:00    3   3
2001-01-01 04:00:00    4   4
2001-01-01 05:00:00    5   5
2001-01-01 00:00:00   20 -50
2001-01-01 01:00:00  -30  60
2001-01-01 02:00:00   40 -70


And so I need df3 to evenutally become:

                       A   B
2001-01-01 00:00:00   20 -50
2001-01-01 01:00:00  -30  60
2001-01-01 02:00:00   40 -70
2001-01-01 03:00:00    3   3
2001-01-01 04:00:00    4   4
2001-01-01 05:00:00    5   5


I thought that adding a column of row numbers (df3['rownum'] = range(df3shape[0])) would help me select out the bottom-most row for any value of the DatetimeIndex  but I am stuck on figuring out the group_by or pivot (or ???) statements to make that work

Any help would be most appreciated

Thanks!",1552748,,1552748.0,2012-10-23 23:22:38,2012-10-23 23:22:38,Remove rows with duplicate indices (Pandas DataFrame and TimeSeries),<python><pandas>,1.0,,1.0,
13003051,1,,2012-10-21 23:01:04,1,71,I have a DataFrame with about 25 columns  several of which hold data unsuitable for plotting  DataFramehist() throws errors on those  How can I specify that those columns should be excluded from the plotting?,63401,,,,2012-10-22 00:15:38,How do I exclude a few columns from a DataFrame plot?,<pandas>,1.0,,1.0,
13018134,1,13018496.0,2012-10-22 19:12:19,1,96,"Suppose we have a monthly time series  possibly with missing months  and upon loading the data into a pandas Series object with DatetimeIndex we wish to make sure each date observation is labeled as an end-of-month date However  the raw input dates may fall anywhere in the month  so we need to force them to end-of-month observations

My first thought was to do something like this:

import pandas as pd
pdDatetimeIndex([datetime(2012 1 20)  datetime(2012 7 31)]  freq='M')


However  this just leaves the dates as is [2012-01-20 2012-07-31] and does not force them to end-of-month values [2012-01-31 2012-07-31]

My second attempt was:

ix = pdDatetimeIndex([datetime(2012 1 20)  datetime(2012 7 31)]  freq='M')
s = pdSeries(nprandomrandn(len(ix))  index=ix)
sasfreq('M')


But this gives:

2012-01-31        NaN
2012-02-29        NaN
2012-03-31        NaN
2012-04-30        NaN
2012-05-31        NaN
2012-06-30        NaN
2012-07-31    079173
Freq: M


as under the hood the asfreq function is calling date_range for a DatetimeIndex

This problem is easily solved if I'm using PeriodIndex instead of DatetimeIndex; however  I need to support some frequencies that are not currently supported by PeriodIndex and as far as I know there is no way to extend pandas with my own Period frequencies",233446,,,,2012-10-22 20:25:10,Forcing dates to conform to a given frequency in pandas,<python><pandas>,1.0,2.0,,
13027147,1,13027878.0,2012-10-23 09:17:44,1,532,"I have data as those ones

        a       b       c       d       e
alpha   551    060    -012   2690   7628453
beta    339    094    -017   -020   -020
gamma   798    334    -141   774    2839493
delta   229    124    040    029    028


I want to do a nice publishable histogram as this one 

but with a break in the y axis so we can figure out the variation of a   b   c   d and e so that data will not be squashed by extreme values in e column as this one but using interlaced colorbar histogram:


I would like to do that in python (matplotlib  panda  numpy/scipy) or in mathematica or any other open and free high-level language (R  scilab  ) Thanks for your help 

edit: using matplotlib through pandas allows to adjust the space between the two subgraph using option button at bottom left ""hspace""",839119,,839119.0,2012-10-23 12:23:24,2012-10-23 12:23:24,Histogram with breaking axis and interlaced colorbar,<python><mathematica><histogram><pandas>,2.0,1.0,1.0,
13041180,1,,2012-10-24 00:10:21,3,141,"I'm new to Python and Pandas  and am having some trouble indexing by a date series  I am trying to pull data into a DataFrame from a SQLite db that consists of a date in format 'mm/dd/yyyy' and an equity price  I then create a new DataFrame using set_index to index the prices by the dates  How can I set the new index as a dateseries using the dates from my dataset? Does this require a datetime conversion or does DataFrame have the ability to convert from an object to a dateseries?  

Below is the code I am using:

import sqlite3 as db
import pandas as p

dbcon = dbconnect(ETF_DATA_TESTdb)
c = dbconcursor()
cexecute("""""" QUERY """""")
rs =pDataFramefrom_records(cfetchall() columns =['Date' 'Price'])
data = rsset_index('Date')


Thanks",1769851,,395287.0,2012-10-24 00:32:30,2012-12-05 19:40:03,DataFrame Indexing with a date series,<python><date><pandas>,1.0,4.0,,
13052844,1,13053381.0,2012-10-24 15:50:51,1,139,"I'm looking to decrease density of tick labels on differing subplot

import pandas as pd
import matplotlibpyplot as plt
import matplotlibgridspec as gridspec
from StringIO import StringIO
data = """"""\
    a   b   c   d
z   5465   627    1953   454
w   -127   441    1174   306
d   551    339    2298   229
t   7628453    -020   2839493    028
""""""
df = pdread_csv(StringIO(data)  sep='\s+')
gs = gridspecGridSpec(3  1 height_ratios=[1 1 4] )
ax0 = pltsubplot(gs[0])
ax1 = pltsubplot(gs[1])
ax2 = pltsubplot(gs[2])
dfplot(kind='bar'  ax=ax0 color=('Blue' 'DeepSkyBlue' 'Red' 'DarkOrange'))
dfplot(kind='bar'  ax=ax1 color=('Blue' 'DeepSkyBlue' 'Red' 'DarkOrange'))
dfplot(kind='bar'  ax=ax2 color=('Blue' 'DeepSkyBlue' 'Red' 'DarkOrange') rot=45)
ax0set_ylim(69998  78000)
ax1set_ylim(19998  29998)
ax2set_ylim(-2  28)
ax0legend()set_visible(False)
ax1legend()set_visible(False)
ax2legend()set_visible(False)
ax0spines['bottom']set_visible(False)
ax1spines['bottom']set_visible(False)
ax1spines['top']set_visible(False)
ax2spines['top']set_visible(False)
ax0xaxisset_ticks_position('none')
ax1xaxisset_ticks_position('none')
ax0xaxisset_label_position('top')
ax1xaxisset_label_position('top')
ax0tick_params(labeltop='off')
ax1tick_params(labeltop='off'  pad=15)
ax2tick_params(pad=15)
ax2xaxistick_bottom()
d = 015
kwargs = dict(transform=ax0transAxes  color='k'  clip_on=False)
ax0plot((-d +d) (-d +d)  **kwargs)
ax0plot((1-d 1+d) (-d +d)  **kwargs)
kwargsupdate(transform=ax1transAxes)
ax1plot((-d +d) (1-d 1+d)  **kwargs)
ax1plot((1-d 1+d) (1-d 1+d)  **kwargs)
ax1plot((-d +d) (-d +d)  **kwargs)
ax1plot((1-d 1+d) (-d +d)  **kwargs)
kwargsupdate(transform=ax2transAxes)
ax1plot((-d +d) (1-d/4 1+d/4)  **kwargs)
ax1plot((1-d 1+d) (1-d/4 1+d/4)  **kwargs)
pltshow()


which results in 


I would like to decrease tick labels in the two upper subplots How to do that ? Thanks

Bonus: 1) how to get rid of the dotted line on y=0 at the basis of the bars?
2) how to get rid of x-trick label between subplot 0 and 1?
3) how to set the back of the plot to transparency? (see the right-bottom broken y-axis line that disappears behind the back of the plot)",839119,,,,2012-10-24 16:55:56,matplotlib: how to decrease density of tick labels in subplots?,<python><plot><matplotlib><pandas>,2.0,,,
13072259,1,13072686.0,2012-10-25 15:41:32,3,92,"I'm having a little trouble with this maybe someone could direct me in the right direction here 

Suppose I have a data frame that looks as follows (actual dataset has many more entries and idents):

                         open ident
2011-01-01 00:00:00 -1252090   df1
2011-01-01 01:00:00 -1427444   df1
2011-01-01 02:00:00 -0415251   df1
2011-01-01 03:00:00 -0797411   df1
2011-01-01 04:00:00 -0515046   df1
2011-01-01 00:00:00  1107162   df2
2011-01-01 01:00:00  0073243   df2
2011-01-01 02:00:00  0224991   df2
2011-01-01 03:00:00 -1269277   df2
2011-01-01 04:00:00  0468960   df2


Is there any quick way to reformat the data frame to look as such?

                         df1        df2
2011-01-01 00:00:00 -1252090   1107162   
2011-01-01 01:00:00 -1427444   0073243
2011-01-01 02:00:00 -0415251   0224991
2011-01-01 03:00:00 -0797411  -1269277
2011-01-01 04:00:00 -0515046   0468960


I've played around with groupby and transpose to no avail  any tips would be great appreciated ",287950,,,,2012-10-25 16:06:06,Transforming Pandas dataframe,<python><pandas>,1.0,1.0,1.0,
13081030,1,13100681.0,2012-10-26 05:05:32,2,144,"I would like to slice the column which contains datetime type in the csv file using pandas  

thanks in advance

for ex: datacsv

Country Player Runs ScoreRate MatchDate Weekday
Afghanistan Mohammad Shahzad 118 9752 16-02-2010 Tue
india schin 112 9802 16-03-2010 wed


I want to slice the column containing datetime format ",1749059,,1199589.0,2012-10-26 10:13:01,2012-10-27 13:41:51,Indexing the datetime column in the csv file using pandas,<python><pandas>,2.0,3.0,,
13031803,1,13032528.0,2012-10-23 13:41:52,3,98,"I've currently switched my focus from R to Python I work with datatable in R a lot  and I find it sometimes quite difficult to find an equivalent for some functions in Python  

I have a pandas data frame that looks like this:


  df = pdDataFrame({'A':['abc' 'def'  'def'  'abc'  'def'  'def' 'abc'] 'B':[13123 45 1231 463 142131 4839  4341]})  
  
  
     A       B  
0  abc   13123    
1  def      45  
2  def    1231  
3  abc     463  
4  def  142131  
5  def    4839
6  abc    4341

  


I need to create a column that increments from 1 based on A and B  so that it indicates the increasing order of B So I first create the sorted data frame  and the column I'm interested in creating is C as below:


    A       B   C
1  abc     463  1
6  abc    4341  2
0  abc   13123  3
3  def      45  1
2  def    1231  2
5  def    4839  3
4  def  142131  4



In R  using the library(datatable)  this can be easily done in one line and creates a column within the original data table:


  df[  C := 1:N  by=A]


I've looked around and I think I might be able to make use of something like this:


  dfgroupby('A')size()
  or
  df['B']argsort()


but not sure how to proceed from here  and how to join the new column back to the original data frame It would be very helpful if anyone could give me any pointer  

Many thanks!",1768115,,1768115.0,2012-10-23 14:45:55,2012-10-23 14:57:02,Create a column which increments based on another column in Python,<python><r><pandas><data.table>,3.0,7.0,1.0,
13062300,1,13062357.0,2012-10-25 05:45:02,0,148,"I want to convert a dict into sorted dict in python

data = pandasread_csv('D:\myfilecsv')
for colname  dtype in datadtypesto_dict()iteritems():
    if dtype == 'object':
        print colname
        count = data[colname]value_counts()
        d = dict((str(k)  int(v)) for k  v in countiteritems())
        f = dict(sorted(diteritems()  key=lambda item: item[1]  reverse = True)[:5])
        print f

        m ={}
        m[""count""]= int(sum(count))    
        m[""Top 5""]= f    
        print m    
        k = jsondumps(m)
        print k    
f = {'Gears of war 3': 6  'Batman': 5  'gears of war 3': 4  'Rocksmith': 5  'Madden': 3}


My desired Output is :

f = {'Gears of war 3': 6  'Batman': 5  'Rocksmith': 5  'gears of war 3': 4  'Madden': 3}
k = {'count':24  'top 5':{'Gears of war 3': 6  'Batman': 5  'Rocksmith': 5  'gears of war 3': 4  'Madden': 3}}


(in the descending order of values and the result should be a dict)",1667967,,1240268.0,2012-10-25 10:43:15,2012-10-25 10:43:15,convert a dict to sorted dict in python,<python><pandas><sorteddictionary>,1.0,1.0,,
13079852,1,13082062.0,2012-10-26 02:08:13,3,74,"I have two sets of stock data in DataFrames:

> GOOGhead()
           Open   High    Low 
Date                                                                            
2011-01-03  2101  2105  2078 
2011-01-04  2112  2120  2105 
2011-01-05  2119  2121  2090 
2011-01-06  2067  2082  2055 
2011-01-07  2071  2077  2027

AAPLhead()
              Open    High     Low
Date                              
2011-01-03  59648  60559  59648
2011-01-04  60562  60618  60012
2011-01-05  60007  61033  60005
2011-01-06  61068  61843  61005
2011-01-07  61591  61825  61013


and I would like to stack them next two each other in a single DataFrame so I can access and compare columns (eg High) across stocks (GOOG vs AAPL)? What is the best way to do this in Pandas and access the subsequent columns (eg GOOG's High column and AAPL's High column) Thanks!",1185242,,,,2012-10-26 06:43:54,How do I stack two DataFrames next to each other in Pandas?,<python><pandas>,2.0,,,
13084342,1,,2012-10-26 09:33:09,0,129,"I have a df :

                  sales  net_pft
STK_ID RPT_Date                 
600141 20101231  46780    1833
       20110331  13725    0384
       20110630  32733    1132
       20110930  50386    1923
       20111231  65685    2325
       20120331  21088    0656
       20120630  46952    1591
600809 20101231  30166    4945
       20110331  18724    5061
       20110630  28948    6586
       20110930  35637    7075
       20111231  44882    7805
       20120331  22140    4925
       20120630  38157    7868 


And I want to do a rolling average for all columns  after groupby STK_ID  the rule expressed by pseudocode like :

if RPT_Date[4:8] == '0331':
    all_column = rolling_mean(all_column 2)

if RPT_Date[4:8] == '0630':
    all_column = rolling_mean(all_column 3)

if RPT_Date[4:8] == '0930':
    all_column = rolling_mean(all_column 4)

if RPT_Date[4:8] == '1231':
    all_column = rolling_mean(all_column 5)

if is_the_first_row():
    keep_original_values()


all_column here stands for 'sales'  'net_pft'  The final result would like :

                  sales  net_pft
STK_ID RPT_Date                 
600141 20101231  46780    1833   # same as original value
       20110331  30253    1109   # average of row1&row2 
       20110630  31079    1116   # average of row1&row2&row3

600809 20101231  30166    4945   # same as original value
       20110331  24445    5003   # average of row1&row2 



How to write in neat Pandas expression?",1072888,,1072888.0,2012-10-26 09:43:31,2012-10-27 13:33:17,How to do Pandas dataframe rolling_mean()?,<python><pandas>,1.0,1.0,,
12979062,1,12979842.0,2012-10-19 17:05:29,1,83,"After implementing a custom frequency in pandas by subclassing DateOffset  is it possible to ""register"" an offset alias for that frequency so that the alias can be used in built-in pandas functions such as date_range and resample?

For example  suppose I implement a custom twice-monthly frequency:

from pandastseriesoffsets import DateOffset  CacheableOffset

class TwiceMonthly(DateOffset  CacheableOffset):
    def apply(self  other):
        # Some date logic here

    @property
    def rule_code(self):
        return 'TM'


Now  instead of using TwiceMonthly() everywhere  I want to use the offset alias TM

# Suppose s is a time series
sresample('TM'  how='sum')
",233446,,,,2012-10-19 17:58:07,Using a custom offset alias in pandas,<python><pandas>,1.0,,,
13019719,1,13020027.0,2012-10-22 20:56:13,0,116,"I'm using pandas and I'm wondering what's the easiest way to get the business days between a start and end date using pandas?

There are a lot of posts out there regarding doing this in Python (for example)  but I would be interested to use directly pandas as I think that pandas can probably handle this quite easy",237690,,,,2012-10-22 21:25:24,Get business days between start and end date using pandas,<python><pandas>,1.0,,,
13040312,1,13040754.0,2012-10-23 22:27:37,0,100,"This is a two-part question  with an immediate question and a more general one

I have a pandas TimeSeries  ts
To know the first value after a certain time I can do this 

tsix[ts[datetime(2012 1 1 15 0 0):]first_valid_index()]


a) Is there a better  less clunky way to do it?

b) Coming from C  I have a certain phobia when dealing with these somewhat opaque  possibly mutable but generally not  possibly lazy but not always types So to be clear  when I do

ts[datetime(2012 1 1 15 0 0):]first_valid_index()


ts[datetime(2012 1 1 15 0 0):] is a pandasTimeSeries object right? And I could possibly mutate it

Does it mean that whenever I take a slice  there's a copy of ts being allocated in memory? Does it mean that this innocuous line of code could actually trigger the copy of a gigabyte of TimeSeries just to get an index value? 

Or perhaps they magically share memory and a lazy copy is done if one of the object is mutated for instance? But then  how do you know which specific operations trigger a copy? Maybe not slicing but how about renaming columns? It doesn't seem to say so in the documentation Does that bother you? Should it bother me or should I just learn not to worry and catch problems with a profiler?",645212,,,,2012-10-23 23:16:18,Selecting the first index after a certain timestamp with a pandas TimeSeries,<python><pandas><lazy-evaluation>,2.0,,,
13085709,1,13086305.0,2012-10-26 11:02:58,3,114,"I'm a beginner in Python and the Pandas library  and I'm rather confused by some basic functionality of data frame I've got a pandas dataframe as below:  

>>>dfhead()  
              X  Y       unixtime
0  652f5e69fcb3  1  1346689910622
1        400292  1  1346614723542
2  1c9d02e4f14e  1  1346862070161
3        610449  1  1346806384518
4        207664  1  1346723370096


However  after I performed some function:  

def unixTodate(unix):
  day = dtdatetimeutcfromtimestamp(unix/1000)strftime('%Y-%m-%d')
return day

df['day'] = df['unixtime']apply(unixTodate)


I could no longer make use of the dfhead() function:  

>>>dfhead()  


Int64Index: 5 entries  190648 to 626582
Data columns:
X              5  non-null values
Y              5  non-null values
unixtime       5  non-null values
day            5  non-null values
dtypes: int64(3)  object(5)


I can't see why this is happening Am I doing something wrong here? Any pointer is welcome! Thanks",1768115,,,,2012-10-26 11:43:09,"df.head() sometimes doesn't work in Pandas, Python",<python><pandas>,1.0,,,
13033270,1,13102877.0,2012-10-23 14:54:07,0,144,"I have a pandas data frame  called df

I want to save this in a gzipped format One way to do this is the following:

import gzip
import pandas

dfsave('filenamepickle')
f_in = open('filenamepickle'  'rb')
f_out = gzipopen('filenamepicklegz'  'wb')
f_outwritelines(f_in)
f_inclose()
f_outclose()


However  this requires me to first create a file called filenamepickle 
Is there a way to do this more directly  ie  without creating the filenamepickle? 

When I want to load the dataframe that has been gzipped I have to go through the same
step of creating filenamepickle For example  to read a file 
filename2picklegzip  which is a gzipped pandas dataframe  I know of the following method:

f_in = gzipopen('filename2picklegz'  'rb')
f_out = gzipopen('filename2pickle'  'wb')
f_outwritelines(f_in)
f_inclose()
f_outclose()

df2 = pandasload('filename2pickle')


Can this be done without creating filename2pickle first?",316357,,316357.0,2012-10-23 14:59:55,2012-10-27 18:19:16,How to save a pandas dataframe in gzipped format directly?,<python><gzip><pandas>,2.0,2.0,,
13061478,1,13062410.0,2012-10-25 04:13:43,1,246,"I have a dataframe with two columns each of which represents an organism They are called ORG1  and ORG2
I want to move the values of ORG2 into ORG1 for the corresponding index value

So  if ORG1 is 'A' and ORG2 is 'B' I want ORG1 to take the value 'B' from ORG2

I have already started work to identify indexes of the ORG2 organisms that I want to move  as follows:

def move_org2(x):
    org2_matches = Series(xORG2strcount(""ESBL""))
    return xix[org2_matches == 1]

org2_DF = move_org2(DF)

org2_DFORG2index


What is the best way to use this to change ORG1 values with the values at corresponding ORG2 indices",390388,,,,2012-10-25 05:54:10,Change a pandas DataFrame column value based on another column value,<python><data.frame><pandas>,1.0,,1.0,
13073045,1,,2012-10-25 16:25:48,2,314,"I wrote a function that took a dataframe generated from Pandas and produce a heatmap:

def drawHeatMap(df  city  province  collector  classtype  color  titleposy):
    try:
        thePlot = plmatshow(dfvalues  cmap='PuBuGn')
        plcolorbar(thePlot  orientation='vertical')
        aTitle = classtype + ' Composition Changes Over Time in ' + city + '  ' + province + '\n' + collector + ' collector ' + 'rs100'
        pltitle(aTitle  x=05  y=titleposy  style='oblique'  weight='bold')
        plxlabel('Collection Time')
        plxticks(range(len(dfcolumns))  dfcolumns  rotation=90)
        plyticks(range(len(dfindex))  dfindex)
        fileName = classtype + '-' + city + '-' + province + '-' + collector + 'png'
        plsavefig(fileName)
    except ZeroDivisionError:
        errorMessage = 'No Data Avaiable for ' + city + '  ' + province + ' with ' + collector + ' collector'
        print errorMessage


The problem I am having is  savefig() would save figures with the axis and graphics trimmed I have to use show()  maximize the graph and manually save the figure with the GUI button myself 

How can I fix my function so savefig() would save the graphs properly? I tried to put a line like this before plsavefig() to control my figure:

       plfigure(figsize=) 


but I end up producing some empty graphs What is the proper way to write a matplotlib function that give me full control on saving the figure?

Updated with Example of a problem figure:
",1712282,,1712282.0,2012-10-25 20:33:16,2012-10-25 20:33:16,matplotlib savefig() size control,<python><matplotlib><pandas>,2.0,,,
13089359,1,,2012-10-26 14:57:26,4,92,"Here's something spooky with pandas and HDF for Halloween:

df = pandasDataFrame([['a' 'b'] for i in range(1 1000)])
store = pandasHDFStore('testh5')
store['x'] = df
storeclose()


then

ls -l testh5
-rw-r--r-- 1 arthur arthur 1072080 Oct 26 10:50 testh5


11M? A bit steep but why not Here's where things get really spooky

store = pandasHDFStore('testh5') #open it again
store['x'] = df #do the same thing as before!
storeclose()


then

ls -l testh5
-rw-r--r-- 1 arthur arthur 2122768 Oct 26 10:52 testh5


You've now entered the Twilight zone Needless to say  the store is indistinguishable after the operation  but each iteration makes the file a little fattier

It seems to only happen when there are strings involved
Before I file a bug report  I'd like to know if I'm missing something here",645212,,645212.0,2012-10-26 15:05:02,2012-10-27 13:37:15,mystery when storing a dataframe containing strings in HDF with pandas,<python><pandas><hdf5>,2.0,2.0,,
13068551,1,13068991.0,2012-10-25 12:26:14,2,102,"I would like to modify DataFrame entries for some rows at a specific level of a hierarchical index Here is a canonical example:

>>> index = pdMultiIndexfrom_arrays([['a' 'a'  'b'  'b']  [1 2 1 2]]  
                                   names=['first'  'second'])
>>> data = pdDataFrame(nprandomrand(len(index))  index=index  columns=['A'])
>>> print data
                     A
first second          
a     1       0587781
      2       0560407
b     1       0492996
      2       0267799


I would like to set rows for which second==2 to 0 (for example) I tried using DataFramexs method but it returns a copy and not a view:

>>> selected = dataxs(2  level='second')
>>> print selected
              A
first          
a      0560407
b      0267799

>>> selected['A']=0
>>> print data

                    A
first second          
a     1       0587781
      2       0560407
b     1       0492996
      2       0267799


The last assignment did not affect data (it changed values in selected of course)",74342,,,,2012-11-01 23:59:15,pandas: slice on hierarchical index without a copy,<python><pandas><data-analysis>,1.0,,,
13078751,1,13083900.0,2012-10-25 23:19:21,2,72,"If I have a dataframe that has columns that include the same name  is there a way to combine the columns that have the same name with some sort of function (ie sum)?

For instance with:

In [186]:

df[""NY-WEB01""]head()
Out[186]:
NY-WEB01    NY-WEB01
DateTime        
2012-10-18 16:00:00  56     28
2012-10-18 17:00:00  186    120
2012-10-18 18:00:00  184    120
2012-10-18 19:00:00  182    120
2012-10-18 20:00:00  192    120


How might I collapse the NY-WEB01 columns (there are a  bunch of duplicate columns  not just NY-WEB01) by summing each row where the column name is the same?",107156,,,,2012-10-26 15:50:19,Merge Columns within a DataFrame that have the Same Name,<python><pandas>,1.0,,,
13105505,1,13119196.0,2012-10-28 00:55:26,3,95,"I'm trying to import fantasy basketball data from yql into a pandas data frame  but I'm running into issues with the nested content

The data from yql (resultsrows) looks like this (when I use type(resultsrows) I get list)

{u'display_position': u'PF' 
u'editorial_player_key': u'nbap4175' 
u'editorial_team_abbr': u'Uta' 
u'editorial_team_full_name': u'Utah Jazz' 
u'editorial_team_key': u'nbat26' 
u'eligible_positions': {u'position': u'PF'} 
u'headshot': {u'size': u'small' 
  u'url': u'http://lyimgcom/iu/api/res/12/KjAPlP83IIrP9iReWfjyjw--/YXBwaWQ9eXZpZGVvO2NoPTIxNTtjcj0xO2N3PTE2NDtkeD0xO2R5PTE7Zmk9dWxjcm9wO2g9NjA7cT0xMDA7dz00Ng--/http://lyimgcom/a/i/us/sp/v/nba/players_l/20101116/4175jpg'} 
  u'image_url': u'http://lyimgcom/iu/api/res/12/KjAPlP83IIrP9iReWfjyjw--/YXBwaWQ9eXZpZGVvO2NoPTIxNTtjcj0xO2N3PTE2NDtkeD0xO2R5PTE7Zmk9dWxjcm9wO2g9NjA7cT0xMDA7dz00Ng--/http://lyimgcom/a/i/us/sp/v/nba/players_l/20101116/4175jpg' 
u'is_undroppable': u'0' 
u'name': {u'ascii_first': u'Paul' 
  u'ascii_last': u'Millsap' 
  u'first': u'Paul' 
  u'full': u'Paul Millsap' 
  u'last': u'Millsap'} 
u'player_id': u'4175' 
u'player_key': u'304p4175' 
u'position_type': u'P' 
u'uniform_number': u'24'}


When I perform 

DataFrame(resultsrows) 


it imports the data fine  however the data in both headshot and name are imported as columns with their nested lists

I can access the sublist from iPython  however when I try to import it into a dataframe I get an error:

results[0]['name']

{u'ascii_first': u'Pau' 
 u'ascii_last': u'Gasol' 
 u'first': u'Pau' 
 u'full': u'Pau Gasol' 
 u'last': u'Gasol'}

 DataFrame([results[0]['name'])

 ValueError: If use all scalar values  must pass index


The behaviour that I want is to import the nested lists as their own columns rather than as a column containing the nested list How can I do this?

The end result that I would like is for a DataFrame with the following layout:

+---------------------------------------------------------------------------------------+
|display_position | () | ascii_first | ascii_last | first | full | last | player_id  |
+---------------------------------------------------------------------------------------+
|    Data         |       |             |            |       |      |      |            |
+---------------------------------------------------------------------------------------+
",599526,,599526.0,2012-10-29 09:37:22,2012-10-30 08:14:41,Accessing python sub list to import into pandas DataFrame,<python><pandas>,1.0,2.0,1.0,
13050003,1,13052373.0,2012-10-24 13:14:11,1,217,"I am trying to transform DataFrame  such that some of the rows will be replicated a given number of times For example:

df = pdDataFrame({'class': ['A'  'B'  'C']  'count':[1 0 2]})

  class  count
0     A      1
1     B      0
2     C      2


should be transformed to:

  class 
0     A   
1     C   
2     C 


This is the reverse of aggregation with count function Is there an easy way to achieve it in pandas (without using for loops or list comprehensions)?   

One possibility might be to allow DataFrameapplymap function return multiple rows (akin apply method of GroupBy) However  I do not think it is possible in pandas now",74342,,,,2012-10-24 15:25:40,pandas: apply function to DataFrame that can return multiple rows,<python><pandas><data-analysis>,2.0,1.0,1.0,
13063259,1,,2012-10-25 07:01:26,0,88,"I have dataframe data which has 3 columns - Date  segment and metric I am doing the following:

data = pandasread_csv(""Filenamecsv"")
ave = datagroupby('Segment')mean() #works
ave = datagroupby('Segment')median() #gives error
ave['median'] = datagroupby('Segment')median()

Traceback (most recent call last):
  File """"  line 1  in 
  File ""/usr/lib/pymodules/python27/pandas/core/framepy""  line 1453  in __setitem__
    self_set_item(key  value)
  File ""/usr/lib/pymodules/python27/pandas/core/framepy""  line 1488  in _set_item
    NDFrame_set_item(self  key  value)
  File ""/usr/lib/pymodules/python27/pandas/core/genericpy""  line 301  in _set_item
    self_dataset(key  value)
  File ""/usr/lib/pymodules/python27/pandas/core/internalspy""  line 616  in set
    assert(valueshape[1:] == selfshape[1:])
AssertionError
",1773339,,696134.0,2012-10-25 07:21:25,2012-10-25 10:33:04,How I do find median using pandas on a dataset?,<python><pandas>,1.0,1.0,,
13116394,1,13315888.0,2012-10-29 05:38:51,1,110,"My goal is to load a dataframe into a DB using a stdin pipe to a load statement executed at the command line (eg cat {file_loc} | /path/to/sql --command ""COPY table FROM STDIN WITH DELIMITER ' ';"") I'm aware that this approach is suboptimal; it's a workaround due to pyodbc issues ;)

What's the most efficient way to condense a dataframe so that each row is a string that contains delimiter-separated values with line breaks at the end? My solution  below  seems inefficient 

from pandas import *
import numpy as np
df = DataFrame(nprandomrandint(low=0  high=100  size=(5 3)) columns=['A' 'B' 'C'])
df2 = dfapply(lambda d: ' 'join([`x` for x in d]))


Writing the dataframe using dfto_csv() or similar is too slow

import timeit
m1=""""""df2=dfapply(lambda d: ' 'join([`x` for x in d]))""""""
met1t = timeitTimer(stmt=m1 setup=""from pandas import *; import numpy as np; df = DataFrame(nprandomrandint(low=0  high=100  size=(5 3)) columns=['A' 'B' 'C'])"")
print ""Method 1: %2f usec/pass"" % (1000000 * met1ttimeit(number=100000)/100000)
# 38182 usec/pass

m2=""""""dfto_csv('testoutcsv'  index=False  header=False)""""""
met2t = timeitTimer(stmt=m2 setup=""from pandas import *; import numpy as np; df = DataFrame(nprandomrandint(low=0  high=100  size=(5 3)) columns=['A' 'B' 'C'])"")
print ""Method 2:%2f usec/pass"" % (1000000 * met2ttimeit(number=100000)/100000)
# 55130 usec/pass
",1037869,,1037869.0,2012-10-29 15:42:55,2012-11-09 20:56:19,pandas: flatten df with delimiter,<python><numpy><pandas>,1.0,2.0,,
13148429,1,,2012-10-30 22:22:59,3,103,"I have the following DataFrame (df):

import numpy as np
import pandas as pd

df = pdDataFrame(nprandomrand(10  5))


I add more column(s) by assignment:

df['mean'] = dfmean(1)


How can I move the column mean to the front  ie set it as first column leaving the order of the other columns untouched?",1772165,,1479269.0,2013-01-04 06:09:36,2013-01-04 06:09:36,How to change the order of DataFrame columns?,<python><pandas>,3.0,2.0,1.0,
13030488,1,13031471.0,2012-10-23 12:32:19,2,182,"I'm trying to generate bar plots from a DataFrame like this:

            Pre    Post
 Measure1   04    19


These values are median values I calculated from elsewhere  and I have also their variance and standard deviation (and standard error  too) I would like to plot the results as a bar plot with the proper error bars  but specifying more than one error value to yerr yields an exception:

# Data is a DataFrame instance
fig = dataplot(kind=""bar""  yerr=[01  03])

[]
ValueError: In safezip  len(args[0])=1 but len(args[1])=2


If I specify a single value (incorrect) all is fine How can I actually give each column its correct error bar?",241515,,,,2012-10-23 13:25:25,Using pandas to plot barplots with error bars,<python><matplotlib><pandas>,1.0,,,
13064400,1,,2012-10-25 08:17:14,0,92,"I created a DatetimeIndex and I want to resample the data with that index When I do that I get an exception:

Traceback (most recent call last):
  File """"  line 1  in 
  File ""/usr/local/lib/python27/dist-packages/pandas-081-py27-linux-i686egg/pandas/core/genericpy""  line 188  in resample
limit=limit  base=base)

  File ""/usr/local/lib/python27/dist-packages/pandas-081-py27-linux-i686egg/pandas/tseries/resamplepy""  line 41  in __init__
    selffreq = to_offset(freq)

  File ""/usr/local/lib/python27/dist-packages/pandas-081-py27-linux-i686egg/pandas/tseries/frequenciespy""  line 392  in to_offset
raise ValueError(""Could not evaluate %s"" % freqstr)
ValueError: Could not evaluate 

[2012-03-02 09:00:00    2012-03-02 15:00:00]
Length: 73  Freq: 5T  Timezone: None


It seems that TimeGrouperresample is supposed to handle DatetimeIndex but a call to to_offset in the TimeGrouper init does not But I could be missing something here

Any idea how to resample by an index? 
Or perhaps a workaround - All I'm trying to do is sample by some frequency (say  1Min) but with start / end time that do not necessarily have data points in the original time series (I'm trying to have a bunch of time series sampled from 8am to 4pm  but some only have values from 9:30am  some from 10am etc)",1571660,,,,2012-11-20 21:15:59,Resample to DatetimeIndex raises an error,<python><time-series><pandas>,2.0,,,
13156660,1,13158109.0,2012-10-31 11:16:57,2,105,"I have the following data frame  subsetted from my original data frame  with columns event  unixtime  and day  and I want to add another column arbday which is the nth day since the first event (with the first visit being day 1):  

import numpy as np  
import datetime as dt  

>>> testdf = pdDataFrame({'event': range(1 4)  'unixtime': [1346617885925  1346961625305 1347214217566]} index=[343352 343353 343354])
>>> testdf['day'] = testdf['unixtime']apply(lambda x: dtdatetimeutcfromtimestamp(x/1000)date())

        event       unixtime         day   arbday
343352      1  1346617885925  2012-09-02        1
343353      2  1346961625305  2012-09-06        5
343354      3  1347214217566  2012-09-09        8


After looking around  I tried to do this by:  

>>> testdf2['arbday'] = npwhere(testdf2['event']==1  1  testdf2dayapply(lambda x: x-x[:1]))  
        event       unixtime         day   arbday
343352      1  1346617885925  2012-09-02        1
343353      2  1346961625305  2012-09-06      NaN
343354      3  1347214217566  2012-09-09      NaN

or  

>>> testdf2['arbday'] = npwhere(testdf2['event']==1  1  testdf2dayapply(lambda x: dttimedelta(x-x[:1])))
TypeError: 'datetimedate' object is not subscriptable 


What is the correct way to do this? Any pointer is much appreciated!  

EDIT: A follow-up question regarding to applying this over groups is here",1768115,,1768115.0,2012-11-01 10:35:43,2012-11-01 10:35:43,Compute the nth day from the first event in Pandas,<python><datetime><pandas>,1.0,,,
13089789,1,,2012-10-26 15:24:01,3,157,"I have pandas timeseries with datetime column as index:

2010-09-20    182827
2010-09-21    184330
2010-09-22    189805
2010-09-23    189700
2010-09-24    190200
2010-09-27    182896
2010-09-28    182485
2010-09-29    180817
2010-09-30    183304


How do I animate this dataset using simple matplotlib example",1701030,,982257.0,2012-11-07 21:58:14,2012-11-26 23:13:18,How can I animate a pandas timeseries?,<python><matplotlib><time-series><pandas>,1.0,2.0,,
13129618,1,13130357.0,2012-10-29 21:01:02,-1,315,"I have some values in a Python Pandas Serie (type: pandascoreseriesSeries)

In [1]: serie = pdSeries([00 9500 -700 8120 00 -900 00 00 -900 00 -640 2080 00 -900 00 -800 00 00 -800 -480 8400 -1000 1900 1300 -1000 -1000 00 -500 00 -1000 -1000 00 -900 00 -900 -900 630 -900 00 00 -900 -800 00 ])

In [2]: seriemin()
Out[2]: -1000

In [3]: seriemax()
Out[3]: 9500


I would like to get values of histogram (not necessary plotting histogram) I just need to get the frequency for each interval

Let's say that my intervals are going from [-200; -150] to [950; 1000]

so lower bounds are

lwb = range(-200 1000 50)


and upper bounds are

upb = range(-150 1050 50)


I don't know how to get frequency (the number of values that are inside each interval) now
I'm sure that defining lwb and upb is not necessary but I don't know what
function I should use to perform this!
(after diving in Pandas doc  I think cut function can help me because it's a discretization problem but I'm don't understand how to use it)

After being able to do this  I will have a look at the way to display histogram (but that's an other problem)",1555275,,,,2012-10-29 22:07:25,Histogram values of a Pandas Series,<python><numpy><pandas>,1.0,,,
13151514,1,13155030.0,2012-10-31 04:59:10,2,155,"I'm using Python 273 in 64-bit I installed pandas as well as matplotlib 111  both for 64-bit Right now  none of my plots are showing After attempting to plot from several different dataframes  I gave up in frustration and tried the following first example from http://pandaspydataorg/pandas-docs/dev/visualizationhtml:

INPUT:

import matplotlibpyplot as plt
ts = Series(randn(1000)  index=date_range ('1/1/2000'  periods=1000))
ts = tscumsum()
tsplot()
pylabshow()


OUTPUT: 

Axes(0125 01;0775x08)


And no plot window appeared Other StackOverflow threads I've read suggested I might be missing DLLs Any suggestions?",1518837,,750613.0,2012-10-31 05:19:32,2012-10-31 09:40:42,matplotlib plot window won't appear,<python><plot><matplotlib><64bit><pandas>,1.0,1.0,,
13169723,1,13180215.0,2012-11-01 01:19:51,4,135,"I have the following file (df_SOF1csv)  it is 1 million records long

Location Transport Transport1 DateOccurred CostCentre D_Time count
0 Lorry Car 07/09/2012 0 0:00:00 2
1 Lorry Car 11/09/2012 0 0:00:00 5
2 Lorry Car 14/09/2012 0 0:00:00 30
3 Lorry Car 14/09/2012 0 0:07:00 2
4 Lorry Car 14/09/2012 0 0:29:00 1
5 Lorry Car 14/09/2012 0 3:27:00 3
6 Lorry Car 14/09/2012 0 3:28:00 4
7 Lorry Car 21/09/2012 0 0:00:00 13
8 Lorry Car 27/09/2012 0 0:00:00 8
9 Lorry Car 28/09/2012 0 0:02:00 1
10 Train Bus 03/09/2012 2073 7:49:00 1
11 Train Bus 05/09/2012 2073 7:50:00 1
12 Train Bus 06/09/2012 2073 7:52:00 1
13 Train Bus 07/09/2012 2073 7:48:00 1
14 Train Bus 08/09/2012 2073 7:55:00 1
15 Train Bus 11/09/2012 2073 7:49:00 1
16 Train Bus 12/09/2012 2073 7:52:00 1
17 Train Bus 13/09/2012 2073 7:50:00 1
18 Train Bus 14/09/2012 2073 7:54:00 1
19 Train Bus 18/09/2012 2073 7:51:00 1
20 Train Bus 19/09/2012 2073 7:50:00 1
21 Train Bus 20/09/2012 2073 7:51:00 1
22 Train Bus 21/09/2012 2073 7:52:00 1
23 Train Bus 22/09/2012 2073 7:53:00 1
24 Train Bus 23/09/2012 2073 7:49:00 1
25 Train Bus 24/09/2012 2073 7:54:00 1
26 Train Bus 25/09/2012 2073 7:55:00 1
27 Train Bus 26/09/2012 2073 7:53:00 1
28 Train Bus 27/09/2012 2073 7:55:00 1
29 Train Bus 28/09/2012 2073 7:53:00 1
30 Train Bus 29/09/2012 2073 7:56:00 1


I am using pandas to analyse it I have been been trying for at least 40 hours
to find a way to group the data in a way that I can aggregate the time column D_Time

I have loaded the required modules 
I create a dataframe see below using DateOccured as an index 

df_SOF1 = read_csv('/users/fabulous/documents/df_SOF1csv'  index_col=3  parse_dates=True) # read file from disk


I can group by any column or iterate through any row eg

df_SOF1groupby('Location')sum()


However I have not found a way to sum up and take the mean of the D_Time column using pandas I have read over 20 articles on timedeltas etc but am still not the wiser how I do this in pandas

Any solution that can allow me do arithmetic on the D_Time column would be appreciated (even if it has to be done outside of pandas)

I thought one possible solution would be to change the D_Time column into seconds
__________________________________2012/11/01
I ran the following command on the 30 items above

df_SOF1groupby('Transport')agg({'D_Time': sum})

D_Time


Transport
Lorry   0:00:000:00:000:00:000:07:000:29:003:27:003:28
Train   7:49:007:50:007:52:007:48:007:55:007:49:007:52

It seems to sum the values together physically rather than give a numerical sum (like adding strings)

Cheers",1733061,,1733061.0,2012-11-01 16:32:35,2012-11-01 16:32:35,timedelta csv pandas,<python><pandas>,1.0,4.0,1.0,
13143050,1,,2012-10-30 16:03:17,1,127,"I work with significantly sized (48K rows  up to tens of columns) DataFrames At a certain point in their manipulation  I need to do pair-wise subtractions of column values and I was wondering if there is a more efficient way to do so rather than the one I'm doing (see below)

My current code:

 # Matrix is the pandas DataFrame containing all the data
 comparison_df = pandasDataFrame(index=matrixindex)
 combinations = itertoolsproduct(group1  group2)

 for observed  reference in combinations:

     observed_data = matrix[observed]
     reference_data = matrix[reference]

     comparison = observed_data - reference_data
     name = observed + ""_"" + reference
     comparison_df[name] = comparison


Since the data can be large (I'm using this piece of code also during a permutation test)  I'm interested in knowing if it can be optimized a bit

EDIT: As requested  here's a sample of a typical data set

ID                    A1      A2       A3       B1       B2       B3
Ku8QhfS0n_hIOABXuE    6343   6304    6410    6287    6403    6279
fqPEquJRRlSVSfL8A    6752   6681    6680    6677    6525    6739
ckiehnugOno9d7vf1Q    6297   6248    6524    6382    6316    6453
x57Vw5B5Fbt5JUnQkI    6268   6451    6379    6371    6458    6333


And a typical result would be  if the ""A"" group is group1 and ""B"" group2   for each ID row  to have for each column a pair (eg  A1_B1  A2_B1  A3_B1) corresponding to the pairings generated above  containing the subtraction for each row ID",241515,,241515.0,2012-10-31 10:17:01,2012-11-09 21:05:15,Calculating subtractions of pairs of columns in pandas DataFrame,<python><pandas><data-analysis>,1.0,4.0,,
13149288,1,,2012-10-30 23:54:10,1,30,"It's been months now since I started to use Pandas DataFrame to deserialize GPS data and perform some data processing and analyses

Although I am very impressed with Pandas robustness  flexibility and power  I'm a bit lost about which features  and in which way  I should use to properly model the data  both for clarity  simplicity and computational speed

Basically  each DataFrame is primarily indexed by a datetime object  having at least one column for a latitude-longitude tuple  and one column for elevation

The first thing I do is to calculate a new column with the geodesic distance between coordinate pairs (first one being 00)  using a function that takes two coordinate pairs as arguments  and from that new column I can calculate the cumulative distance along the track  which I use as a Linear Referencing System

The questions I need to address would be:

Is there a way in which I can use  in the same dataframe  two different monotonically increasing columns (cumulative distance and timestamp)  choosing whatever is more convenient in each given context at runtime  and use these indexes to auto-align newly inserted rows?
In the specific case of applying a diff function that could be vectorized (applied like an array operation instead of an iterative pairwise loop)  is there a way to do that idiomatically in pandas? Should I create a ""coordinate"" class which support the diff (__sub__) operation so I could use dataframelatlngdiff directly?
I'm not sure these questions are well formulated  but that is due  at least a bit  by the overwhelming number of possibilities  and a somewhat fragmented documentation (yet)

Also  any tip about using Pandas for GPS data (tracklogs) or Geospatial data in general is very much welcome

Thanks for any help!",401828,,,,2012-10-30 23:54:10,Which features of Pandas DataFrame could be used to model GPS Tracklog data (read from GPX file),<gps><pandas>,,,,
13175251,1,,2012-11-01 10:34:50,3,93,"This is a follow-up question from my other question:  

I have the following data frame  subsetted from my original data frame  with columns ob  event  unixtime  and day  and I want to add another column arbday which is the nth day since the first event (with the first visit being day 1) grouped by ob:  

import numpy as np  
import datetime as dt  

>>> newdf = pdDataFrame({'ob': ['a' 'a' 'b' 'b' 'c'  'd'  'e'  'e'  'e'  'f'  'f'  'f'] 'event': [1  2  1  2  1  1  1  2  3  1  2  3]  'unixtime': [1346682124716  1346682188598  1346745432765  1347080641650  1346676710509  1346702995184  1346530405978  1346530421609  1346530570952  1346617885925  1346961625305 1347214217566]} index=[343340  343341  343342  343343  343344  343345  343349  343350  343351  343352 343353 343354])
>>> newdf['day'] = newdf['unixtime']apply(lambda x: dtdatetimeutcfromtimestamp(x/1000)date())

        ob  event        unixtime          day  arbday
343340   a      1   1346682124716   2012-09-03       1
343341   a      2   1346682188598   2012-09-03       1
343342   b      1   1346745432765   2012-09-04       1
343343   b      2   1347080641650   2012-09-08       5
343344   c      1   1346676710509   2012-09-03       1
343345   d      1   1346702995184   2012-09-03       1
343349   e      1   1346530405978   2012-09-01       1
343350   e      2   1346530421609   2012-09-01       1
343351   e      3   1346530570952   2012-09-01       1
343352   f      1   1346617885925   2012-09-02       1
343353   f      2   1346961625305   2012-09-06       5
343354   f      3   1347214217566   2012-09-09       8


Within one ob  this will work:  

newdf['arbday'] = newdf['day']map(lambda x: (x-testdfget_value(newdf[newdfevent == 1]first_valid_index()  'day'))days+1)


or

newdf['arbday'] = newdf['day']map(lambda x: (x-newdfget_value(int(newdf[newdfevent == 1]index)  'day'))days+1)


I tried the following code and it worked:  

>>> newdf['arbday'] = newdfgroupby('ob')['day']transform(lambda x: (x-xmin())apply(lambda y: ydays)+1)

        event ob       unixtime         day arbday
343340      1  a  1346682124716  2012-09-03      1
343341      2  a  1346682188598  2012-09-03      1
343342      1  b  1346745432765  2012-09-04      1
343343      2  b  1347080641650  2012-09-08      5
343344      1  c  1346676710509  2012-09-03      1
343345      1  d  1346702995184  2012-09-03      1
343349      1  e  1346530405978  2012-09-01      1
343350      2  e  1346530421609  2012-09-01      1
343351      3  e  1346530570952  2012-09-01      1
343352      1  f  1346617885925  2012-09-02      1
343353      2  f  1346961625305  2012-09-06      5
343354      3  f  1347214217566  2012-09-09      8


But this is clearly not the most elegant way to do it Also  why has the order of event and ob changed?  

Any pointer will be much appreciated Thanks!  ",1768115,,1768115.0,2012-11-01 11:19:40,2013-01-05 05:45:19,Compute the nth day from the first event within each group in Pandas,<python><pandas>,1.0,,,
13165461,1,,2012-10-31 18:37:10,1,52,"I have a DataFrame that has a MultiIndex index It can be regenerated as follows:

import pandas as pd
import numpy as np
from numpyrandom import randn as randn
from numpyrandom import randint as randint
from datetime import datetime
# setup data
obs1 = [ob if ob > 0 else ob *-1 for ob in randn(10)*100]
obs2 = [randint(1000) for i in range(10)]
labels = ['A12'  'B12'  'A12'  'A12'  'A12' 'B12'  'A12' 'B12'  'A13'  'B13']
dates = [datetime(2012  11  i) for i in range(1 11)]
dates[0] = dates[1]
dates[5] = dates[6]
# setup index and dataframe
m_idx = pdMultiIndexfrom_tuples(zip(dates  labels)  names=['date'  'label'])
data_dict = {'observation1':obs1  'observation2':obs2}
df = pdDataFrame(data_dict  index=m_idx)


OUTPUT:

In [17]: df
Out[17]: 
                  observation1  observation2
date       label                            
2012-11-02 A12       79373668           224
           B12      130841316           477
2012-11-03 A12       45312814           835
2012-11-04 A12      163776946           623
2012-11-05 A12      115449437           722
2012-11-07 B12       38537737           842
           A12       84807516           396
2012-11-08 B12       35186265           707
2012-11-09 A13       60171620           336
2012-11-10 B13      123750614           540


Dates of Interest:

dates_of_interest = [datetime(2012 11 1)  datetime(2012 11 6)]


I am interested in creating a dataframe with a subset of the following criteria:

date is nearest to one of the dates of interest
label has 'A' in the string
So the result of my subindex would look like the following:

                  observation1  observation2
date       label                            
2012-11-02 A12       79373668           224
2012-11-07 A12       84807516           396


Ideally  I would be able to get data for all observations ""near"" the criteria  so that the return dataset might look like:

                  observation1  observation2
date       label                            
2012-11-02 A12       79373668           224
2012-11-05 A12      115449437           722
2012-11-07 A12       84807516           396


But for a start I would just be happy to get the first result I suspect that I need to use searchsort and asof  but I am not quite sure how to do that with A MultiIndex

Does anyone know how to get there from here?

Regards",1202834,,,,2012-10-31 18:37:10,Is it possible to use searchsorted with a MultiIndex index in Pandas?,<pandas>,,,1.0,
13180499,1,13197558.0,2012-11-01 15:39:55,1,94,"I have a DataFrame that consists of many stacked time series The index is (poolId  month) where both are integers  the ""month"" being the number of months since 2000 What's the best way to calculate one-month lagged versions of multiple variables?

Right now  I do something like:

cols_to_shift = [""bal""  5 more columns]
df_shift = df[cols_to_shift]groupby(level=0)transform(lambda x: xshift(-1))


For my data  this took me a full 60 s to run (I have 48k different pools and a total of 718k rows)

I'm converting this from R code and the equivalent datatable call:

dtshift ",250839,,250839.0,2012-11-02 14:59:42,2012-11-02 15:48:02,Most efficient way to shift MultiIndex time series,<pandas>,1.0,1.0,,
13119515,1,13123189.0,2012-10-29 10:10:58,1,99,"My data looks like this: (ch = channel  det = detector)

ch det time counts 
1   1    0    123
    2    0    121
    3    0    125 
2   1    0    212
    2    0    210
    3    0    210 
1   1    1    124
    2    1    125
    3    1    123 
2   1    1    210
    2    1    209
    3    1    213


Note  in reality the time column is a float with 12 or so significant digits  still constant for all detectors of 1 measurement  but it's value is not predictable  nor in a sequence

What I need to create is a dataframe that looks like this:

c  time  mean_counts_over_detectors
1   0       xxx
2   0       yyy
1   1       zzz
1   1       www


Ie  I would like to apply npmean over all counts of the detectors of 1 channel at each time separately
I could write kludgy loops  but I feel that pandas must have something built-in for this? I am  still a beginner at pandas  and especially with MultiIndex there are so many concepts  I am not sure what I should be looking for in the docs?

The title contains 'condition' because I thought that maybe the fact that I want the mean over all detectors of one channel for the counts where the time is the same can be expressed as a slicing condition?",680232,,,,2012-10-29 14:05:50,How to apply condition on level of pandas.multiindex?,<python><pandas>,2.0,,1.0,
13183271,1,13184361.0,2012-11-01 18:24:52,0,116,"Simple setup    

import pandas
from datetime import datetime

v = [100  200]
i = [datetime(2012  1  31)  datetime(2012  4  30)]
s = pandasSeries(data=v  index=i)  


Here's the original timeseries:

In [11]: s
Out[11]: 
2012-01-31    100
2012-04-30    200


And after resampling:

In [12]: sresample('Q')
Out[12]: 
2012-03-31    100
2012-06-30    200
Freq: Q-DEC


So Jan/Apr got aligned to Mar/Jun  

Why didn't pandas choose Dec/Mar instead?

Also  is there any way I can get it to align to Dec/Mar (which seem to be closer to the original dates) with the timeseries functions?  ",931775,,,,2012-11-01 19:44:30,How do I resample/align a pandas timeseries to the closest calendar quarters?,<python><time-series><pandas><resampling>,1.0,,,
13185454,1,13187017.0,2012-11-01 21:04:54,1,178,"Here's the setup code:

import pandas
from datetime import datetime

a_values = [1728  1635  1733]
a_index = [datetime(2011  10  31)  datetime(2012  1  31)  datetime(2012  4  30)]
a = pandasSeries(data=a_values  index=a_index)

aa_values = [6419  5989  6006]
aa_index = [datetime(2011  9  30)  datetime(2011  12  31)  datetime(2012  3  31)]
aa = pandasSeries(data=aa_values  index=aa_index)

apol_values = [1100  1179  969]
apol_index = [datetime(2011  8  31)  datetime(2011  11  30)  datetime(2012  2  29)]
apol = pandasSeries(data=apol_values  index=apol_index)


Here's what the data looks like in a table (3rd value for APOL isn't shown):



The goal is to align the data to calendar quarter markers so the 3 data sets can be compared  Just glancing at the below dates  Mar 2012  Dec 2011  and Sep 2011 seem like reasonable markers for alignment

Here's the output with fill_method='ffill':

In [6]: aresample('Q'  fill_method='ffill')
Out[6]: 
2011-12-31    1728
2012-03-31    1635
2012-06-30    1733
Freq: Q-DEC

In [7]: aaresample('Q'  fill_method='ffill')
Out[7]: 
2011-09-30    6419
2011-12-31    5989
2012-03-31    6006
Freq: Q-DEC

In [8]: apolresample('Q'  fill_method='ffill')
Out[8]: 
2011-09-30    1100
2011-12-31    1179
2012-03-31     969
Freq: Q-DEC


Which looks like this:



Notice how the most recent numbers in each series don't line up

And here's the output with fill_method='bfill':

In [9]: aresample('Q'  fill_method='bfill')
Out[9]: 
2011-12-31    1635
2012-03-31    1733
2012-06-30     NaN
Freq: Q-DEC

In [10]: aaresample('Q'  fill_method='bfill')
Out[10]: 
2011-09-30    6419
2011-12-31    5989
2012-03-31    6006
Freq: Q-DEC

In [11]: apolresample('Q'  fill_method='bfill')
Out[11]: 
2011-09-30    1179
2011-12-31     969
2012-03-31     NaN
Freq: Q-DEC


Which looks like this:



Again  the most recent numbers in the series don't line up

Is this the expected output of resample() in this scenario?  

What can I do to get results where the most recent 3 numbers above are aligned and everything else follows appropriately?

EDIT:  Here's what the desired output looks like:

",931775,,931775.0,2012-11-01 21:21:44,2012-11-01 23:21:40,Using resample to align multiple timeseries in pandas,<python><time-series><pandas><resampling>,1.0,4.0,0.0,2012-11-02 03:00:12
13187778,1,,2012-11-02 00:57:33,4,498,"I am interested in knowing how to convert a pandas dataframe into a numpy array  including the index  and set the dtypes

dataframe:

label   A    B    C
ID                                 
1   NaN  02  NaN
2   NaN  NaN  05
3   NaN  02  05
4   01  02  NaN
5   01  02  05
6   01  NaN  05
7   01  NaN  NaN


convert df to array returns:

array([[ nan   02   nan] 
       [ nan   nan   05] 
       [ nan   02   05] 
       [ 01   02   nan] 
       [ 01   02   05] 
       [ 01   nan   05] 
       [ 01   nan   nan]])


However  I would like:

array([[ 1  nan   02   nan] 
       [ 2  nan   nan   05] 
       [ 3  nan   02   05] 
       [ 4  01   02   nan] 
       [ 5  01   02   05] 
       [ 6  01   nan   05] 
       [ 7  01   nan   nan]] 
     dtype=[('ID'  '",1792954,,1792954.0,2012-11-02 02:54:45,2012-11-02 10:16:00,pandas dataframe to numpy array - include index,<python><numpy><pandas>,1.0,,,
13221218,1,13221579.0,2012-11-04 17:50:34,1,102,"I have a dataframe that looks like this:


DatetimeIndex: 2016910 entries  2009-01-02 04:51:00 to 2012-11-02 20:00:00
Freq: T
Data columns:
X1    2016910  non-null values
X2    2016910  non-null values
X3    2016910  non-null values
X4    2016910  non-null values
X5    2016910  non-null values
dtypes: float64(5)


and I would like to ""filter"" it by accessing only certain times across the whole range of dates  For example  I'd like to return a dataframe that contains all rows where the time is between 13:00:00 and 14:00:00  but for all of the dates  I am reading the data from a CSV file and the datetime is one column  but I could just as easily make the input CSV file contain a separate date and time  I tried the separate date and time route  and created a multiindex  but when I did  I ended up with two index columns -- one of them containing the proper date with an incorrect time instead of just a date  and the second one containing an incorrect date  and then a correct time  instead of just a time  The input data for my multiindex attempt looked like this:

 20090102 04:51:00 899900 899900 899900 899900 100
 20090102 05:36:00 900100 900100 900100 900100 200
 20090102 05:44:00 901400 901400 901400 901400 100
 20090102 05:50:00 900500 900500 900500 900500 500
 20090102 05:56:00 901000 901000 901000 901000 300
 20090102 05:57:00 901000 901000 901000 901000 200


which I tried to read using this code:

 singledf = pdDataFramefrom_csv(""inputfile"" header=None index_col=[0 1] parse_dates=True)


which resulted in a dataframe that looks like this:

singledfsort()
singledf


MultiIndex: 716244 entries  () to ()
Data columns:
X2    716244  non-null values
X3    716244  non-null values
X4    716244  non-null values
X5    716244  non-null values
X6    716244  non-null values
dtypes: float64(4)  int64(1)


Maybe the multiindex approach is totally wrong  but it's one thing I tried  It seems like it is stuck on using a datetime object  and wants to force the index columns to have a datetime instead of just a date or a time  My source CSV files for the my non-multiindex attempt looks like this:

20090102 04:51:00 899900 899900 899900 899900 100
20090102 05:36:00 900100 900100 900100 900100 200
20090102 05:44:00 901400 901400 901400 901400 100
20090102 05:50:00 900500 900500 900500 900500 500
20090102 05:56:00 901000 901000 901000 901000 300


I am using pandas 9  Any suggestions are appreciated!",1487345,,,,2012-11-04 18:35:32,How to select rows within a pandas dataframe based on time only when index is date and time,<data.frame><pandas>,1.0,,,
13227865,1,13319626.0,2012-11-05 07:38:20,1,174,"How to convert from pandasDatetimeIndex to numpydatetime64?

I get:

>>> type(dfindexto_datetime())
Out[56]: pandastseriesindexDatetimeIndex


Is it safe to do numpyarray(datetimeindex dtype=numpydatetime64)?",1579844,,1579844.0,2012-11-05 07:43:25,2012-11-10 05:42:40,How to convert from pandas.DatetimeIndex to numpy.datetime64?,<python><datetime><numpy><pandas>,2.0,,,
13239297,1,,2012-11-05 19:51:07,4,198,"New to Pandas  looking for the most efficient way to do this

I have a Series of DataFrames  Each DataFrame has the same columns but different indexes  and they are indexed by date  The Series is indexed by ticker symbol  So each item in the Sequence represents a single time series of each individual stock's performance

I need to randomly generate a list of n data frames  where each dataframe is a subset of some random assortment of the available stocks' histories  It's ok if there is overlap  so long as start end end dates are different

This following code does it  but it's really slow  and I'm wondering if there's a better way to go about it:

Code

def random_sample(data=None  timesteps=100  batch_size=100  subset='train'):
    if type(data) != pdSeries:
        return None

    if subset=='validate':
        offset = 0
    elif subset=='test':
        offset = 200
    elif subset=='train':
        offset = 400

    tickers = nprandomrandint(0  len(data)  size=len(data))

    ret_data = []
    while len(ret_data) != batch_size:
        for t in tickers:
            data_t = data[t]
            max_len = len(data_t)-timesteps-1
            if len(ret_data)==batch_size: break
            if max_len-offset ",544178,,,,2012-11-05 21:18:51,Python Pandas -- Random sampling of time series,<python><pandas>,1.0,2.0,,
13133458,1,,2012-10-30 05:57:48,0,96,"I'm looking to quickly cast about ~10-20M ISO date-time strings with microsecond precision to datetime64 for use as a DataFrame index in pandas 

I'm on pandas 09  and have tried the solutions suggested over on git  but I'm finding it taking 20 to 30 minutes  or never finishing 

I think I've found the problem Compare the speed of these two:

rng = date_range('1/1/2000'  periods=2000000  freq='ms')
strings = [xstrftime('%Y-%m-%d %H:%M:%S%f') for x in rng]
timeit to_datetime(strings)


On my laptop  ~300ms 

rng = date_range('1/1/2000'  periods=2000000  freq='ms')
strings = [xstrftime('%Y%m%dT%H%M%S%f') for x in rng]
timeit to_datetime(strings)


On my laptop  forever and a day

I'm probably going to just change the c++ code that generates the timestamps to put them in the more verbose ISO form for now  as looping through and fixing the format on tens of millions of stamps is probably pretty slow",1784599,,1784599.0,2012-10-30 16:10:24,2012-11-09 21:12:37,Pandas Casting ISO String to datetime64,<python><datetime><pandas>,1.0,,,
13167391,1,,2012-10-31 21:03:56,2,124,"PS: cross posted on pydata mailing listsorry I am in need of quick help

Hi Guys

I am creating a groupby object from a pandas df and want to select out all the groups with > 1 size The following doesnt seem to work

grouped[groupedsize > 1 ]

also how can one filter out certain values from a grouped df

For example

remove all the rows from grouped where the colmun 'name' has a value 'foo' or 'bar'

Contrived Example

df = pandasDataFrame({'A': ['foo' 'bar' 'foo' 'foo'] 
                        'B': range(4)})
grouped = dfgroupby('A')


Need the groupby object after removing the groups that have a group size  1]


Expected:

A
foo 0
    2
    3


Thanks!
-Abhi",369541,,369541.0,2012-10-31 21:42:02,2012-11-01 17:00:54,filtering grouped df in pandas,<python><pandas>,1.0,5.0,1.0,
13188114,1,13197520.0,2012-11-02 01:41:04,4,67,"I am looking for a clean way to reorder the index in a group
Example code:

import numpy as np
import pandas as pd

mydates = pddate_range('1/1/2012'  periods=1000  freq='D')
myts = pdSeries(nprandomrandn(len(mydates))  index=mydates)
grouped = mytsgroupby(lambda x: xtimetuple()[7])
mymin = groupedmin()
mymax = groupedmax()


The above gives me what I want  aggregate stats on julian day of the year BUT I would then like to reorder the group so the last half (183 days) is placed in front of the 1st half
With a normal numpy array:

myindex = nparange(1 367)
myindex = npconcatenate((myindex[183:] myindex[:183]))


But I can't do this with the groupby it raises a not implement error  

Note: this is a cross post from google-groups Also I have been reading on complangpython  unfortunately people tend to ignore some posts eg from google groups

Thanks in advance 
Bevan",1790114,,1240268.0,2012-11-02 11:37:03,2012-11-02 14:46:13,Reindex or reorder group,<pandas>,2.0,1.0,2.0,
13223360,1,13243951.0,2012-11-04 21:56:14,0,202,"DateOccurred    CostCentre  TimeDifference
03/09/2012  2073    28138
03/09/2012  6078    34844
03/09/2012  8273    31215
03/09/2012  8367    28160
03/09/2012  8959    32037
03/09/2012  9292    30118
03/09/2012  9532    34200
03/09/2012  9705    27240
03/09/2012  10085   31431
03/09/2012  10220   22555
04/09/2012  6078    41126
04/09/2012  7569    31101
04/09/2012  8273    30994
04/09/2012  8959    30064
04/09/2012  9532    34655
04/09/2012  9705    26475
04/09/2012  10085   31443
04/09/2012  10220   33970
05/09/2012  2073    28221
05/09/2012  6078    27894
05/09/2012  7569    29012
05/09/2012  8239    42208
05/09/2012  8273    31128
05/09/2012  8367    27993
05/09/2012  8959    20669
05/09/2012  9292    33070
05/09/2012  9532    8189
05/09/2012  9705    27540
05/09/2012  10085   28798
05/09/2012  10220   23164
06/09/2012  2073    28350
06/09/2012  6078    35648
06/09/2012  7042    27129
06/09/2012  7569    31546
06/09/2012  8239    39945
06/09/2012  8273    31107
06/09/2012  8367    27795
06/09/2012  9292    32974
06/09/2012  9532    30320
06/09/2012  9705    37462
06/09/2012  10085   31703
06/09/2012  10220   7807
06/09/2012  14573   186
07/09/2012  0   0
07/09/2012  0   0
07/09/2012  2073    28036
07/09/2012  6078    31969
07/09/2012  7569    32941
07/09/2012  8273    30073
07/09/2012  8367    29391
07/09/2012  9292    31927
07/09/2012  9532    30127
07/09/2012  9705    27604
07/09/2012  10085   28108
08/09/2012  2073    28463
10/09/2012  6078    31266
10/09/2012  8239    16390
10/09/2012  8273    31140
10/09/2012  8959    30858
10/09/2012  9532    30794
10/09/2012  9705    28752
11/09/2012  0   0
11/09/2012  0   0
11/09/2012  0   0
11/09/2012  0   0
11/09/2012  0   0
11/09/2012  2073    28159
11/09/2012  6078    36835
11/09/2012  8239    45354
11/09/2012  8273    30922
11/09/2012  8367    31382
11/09/2012  8959    29670
11/09/2012  9292    33582
11/09/2012  9705    29394
11/09/2012  10085   17140
12/09/2012  2073    28283
12/09/2012  6078    31139
12/09/2012  7042    35063
12/09/2012  8273    31075
12/09/2012  8367    29795
12/09/2012  9292    33496
12/09/2012  9532    31669
12/09/2012  9705    26166
12/09/2012  10085   29889
12/09/2012  10220   35656
13/09/2012  2073    28144
13/09/2012  6078    30544
13/09/2012  7097    30866
13/09/2012  8273    30772
13/09/2012  8367    32387
13/09/2012  8959    29307
13/09/2012  9292    32348
13/09/2012  9532    28137
13/09/2012  9705    28823
13/09/2012  10085   31543
13/09/2012  10220   28293
14/09/2012  0   12433
14/09/2012  0   12434
14/09/2012  0   12434
14/09/2012  0   12434
14/09/2012  0   12434
14/09/2012  0   0
14/09/2012  0   0
14/09/2012  0   0
14/09/2012  0   12433
14/09/2012  0   0
14/09/2012  0   12433
14/09/2012  0   0
14/09/2012  0   0
14/09/2012  0   0
14/09/2012  0   0
14/09/2012  0   0
14/09/2012  0   0
14/09/2012  0   0
14/09/2012  0   0
14/09/2012  0   0
14/09/2012  0   0
14/09/2012  0   0
14/09/2012  0   0
14/09/2012  0   0
14/09/2012  0   0
14/09/2012  0   0
14/09/2012  0   0
14/09/2012  0   1720
14/09/2012  0   0
14/09/2012  0   0
14/09/2012  0   0
14/09/2012  0   0
14/09/2012  0   0
14/09/2012  0   0
14/09/2012  0   0
14/09/2012  0   384
14/09/2012  0   0
14/09/2012  0   0
14/09/2012  0   0
14/09/2012  0   383
14/09/2012  2073    28438
14/09/2012  6078    27255
14/09/2012  8273    29989
14/09/2012  8959    26892
14/09/2012  9292    33202
14/09/2012  9532    30862
14/09/2012  9705    26857
14/09/2012  10085   32657
14/09/2012  10220   27296
15/09/2012  6078    3832
17/09/2012  6078    30004
17/09/2012  7569    30390
17/09/2012  8239    41421
17/09/2012  8273    26337
17/09/2012  8367    31631
17/09/2012  8959    17989
17/09/2012  9292    35703
17/09/2012  9532    36542
17/09/2012  9705    27488
17/09/2012  10085   30849
17/09/2012  10220   32575
18/09/2012  2073    28293
18/09/2012  6078    27450
18/09/2012  7569    30323
18/09/2012  8239    38481
18/09/2012  8273    31154
18/09/2012  8367    27944
18/09/2012  8959    28196
18/09/2012  9292    30844
18/09/2012  9532    33128
18/09/2012  9705    32100
19/09/2012  2073    28227
19/09/2012  6078    32243
19/09/2012  7569    29041
19/09/2012  8239    42791
19/09/2012  8273    30966
19/09/2012  8367    26420
19/09/2012  8959    29394
19/09/2012  9292    14865
19/09/2012  9532    23618
19/09/2012  10085   31614
19/09/2012  10220   8686
20/09/2012  2073    28260
20/09/2012  6078    30446
20/09/2012  7097    34909
20/09/2012  7569    30869
20/09/2012  8273    31079
20/09/2012  8367    30162
20/09/2012  9292    13104
20/09/2012  9532    36614
20/09/2012  9705    35617
20/09/2012  10085   31821
20/09/2012  10220   30055
20/09/2012  14573   468
21/09/2012  0   0
21/09/2012  0   0
21/09/2012  0   0
21/09/2012  0   0
21/09/2012  0   0
21/09/2012  0   0
21/09/2012  0   0
21/09/2012  0   0
21/09/2012  0   0
21/09/2012  0   3
21/09/2012  0   0
21/09/2012  0   0
21/09/2012  0   3
21/09/2012  2073    28308
21/09/2012  6078    33833
21/09/2012  7569    32335
21/09/2012  9292    33824
21/09/2012  9532    33376
21/09/2012  10220   21002
22/09/2012  2073    28402
23/09/2012  2073    28109
24/09/2012  2073    28431
24/09/2012  6078    30027
24/09/2012  7097    31914
24/09/2012  8239    35617
24/09/2012  8273    30670
24/09/2012  8367    29084
24/09/2012  8959    31023
24/09/2012  9292    34394
24/09/2012  9532    31255
24/09/2012  9705    18758
24/09/2012  10085   29290
24/09/2012  10220   33230
25/09/2012  2073    28506
25/09/2012  6078    32043
25/09/2012  7042    34953
25/09/2012  7569    30898
25/09/2012  8239    41297
25/09/2012  8273    31012
25/09/2012  8367    29645
25/09/2012  8959    29904
25/09/2012  9532    37875
25/09/2012  9705    13280
25/09/2012  10085   35023
25/09/2012  10220   31359
26/09/2012  2073    28388
26/09/2012  6078    29765
26/09/2012  7097    31561
26/09/2012  7569    29151
26/09/2012  8239    40369
26/09/2012  8367    28174
26/09/2012  8959    26554
26/09/2012  9292    32104
26/09/2012  9532    33194
26/09/2012  9705    30377
26/09/2012  10085   31503
26/09/2012  10220   28310
27/09/2012  0   0
27/09/2012  0   0
27/09/2012  0   0
27/09/2012  0   0
27/09/2012  0   0
27/09/2012  0   0
27/09/2012  0   0
27/09/2012  0   0
27/09/2012  2073    28491
27/09/2012  6078    31137
27/09/2012  8239    38403
27/09/2012  8273    31117
27/09/2012  8367    28462
27/09/2012  9292    32387
27/09/2012  9532    23023
27/09/2012  9705    32790
27/09/2012  10085   33460
27/09/2012  10220   31782
28/09/2012  0   161
28/09/2012  2073    28381
28/09/2012  7569    32322
28/09/2012  8239    38362
28/09/2012  8273    30533
28/09/2012  8959    17128
28/09/2012  9292    32484
28/09/2012  9532    18586
28/09/2012  9705    27902
29/09/2012  2073    28583


Above is a sample of a dataframe which has a million records
How can I slice or group it by Week or  Month and sum seconds column by cost centre?*
I have read/tried 30 of the articles on this site which appear by doing a search for
List item pandas  python  groupby  split  dataframe  week with out success 
I am using python 27 and pandas 09
I've read the Time Series / Date functionality section in the pandas 09 tutorial but couldn't
 make anything work with a dataframe I would like to use the features in there such as Business week
",1733061,,100297.0,2012-11-04 21:56:48,2012-11-06 16:18:33,python split a pandas data frame by week or month and group the data based on these sp,<python><datetime><group-by><data.frame><pandas>,2.0,1.0,,
13261691,1,13261865.0,2012-11-07 00:46:05,1,58,"So I have a sample data set like this in csv:-

name    team    date       score
John    A   3/9/12      100
John    B   3/9/12      99
Jane    B   4/9/12      102
Peter   A   9/9/12      103
Josie   C   11/9/12     111
Rachel  A   30/10/12    98
Kate    B   31/10/12    103
David   C   1/11/12     104


Executing the following:-

from pandasioparsers import read_csv

df = read_csv(""data/Workbook1csv""  index_col=[""team""  ""name""])

df

                 date  score
team name                   
A    John      3/9/12    100
B    John      3/9/12     99
     Jane      4/9/12    102
A    Peter     9/9/12    103
C    Josie    11/9/12    111
A    Rachel  30/10/12     98
B    Kate    31/10/12    103
C    David    1/11/12    104


How do I compress the first index (""team"") further so that I don't have duplicate values?  To become:-

                 date  score
team name                   
A    John      3/9/12    100
     Peter     9/9/12    103
     Rachel  30/10/12     98
B    John      3/9/12     99
     Jane      4/9/12    102
     Kate    31/10/12    103
C    Josie    11/9/12    111
     David    1/11/12    104
",482506,,,,2012-11-07 01:08:22,understanding MultiIndex,<pandas>,1.0,,1.0,
13261737,1,13261966.0,2012-11-07 00:51:10,3,95,"I am trying this simple setup of variables:

In [94]: cc
Out[94]: 
                 d0         d1
class sample                    
5     66      0128320  0970817
      66      0160488  0969077
      77      0919263  0008597
6     77      0811914  0123960
      88      0639887  0262943
      88      0312303  0660786

In [101]: bb
Out[101]: 
                     d0         d1
class sample                    
2     22      0730631  0656266
      33      0871292  0942768
3     44      0081831  0714360
      55      0600095  0770108

In [102]: aa
Out[102]: 
                     d0         d1
class sample                    
0     00      0190409  0789750
      11      0588001  0250663
1     22      0888343  0428968
      33      0185525  0450020


I can perform the following command

In [103]: aaappend(bb)
Out[103]: 
                     d0         d1
class sample                    
0     00      0190409  0789750
      11      0588001  0250663
1     22      0888343  0428968
      33      0185525  0450020
2     22      0730631  0656266
      33      0871292  0942768
3     44      0081831  0714360
      55      0600095  0770108


Why I cant perform the following command in the same manner?

aaappend(cc)


[I get the following exception]

ValueError: all arrays must be same length


UPDATE:

It works fine if I did not provide column names  but if for example I have 4 columns  with names ['d0' 'd0' 'd1' 'd1'] for 4X4 and 8X4  it does not work anymore

here is the code for reproducing the error

import pandas
y1 = [['0' '0' '1' '1'] ['00' '11' '22' '33']]
y2 = [['2' '2' '3' '3' '4' '4'] ['44' '55' '66' '77' '88' '99']]
x1  = nprandomrand(4 4)
x2 = nprandomrand(6 4)
cols = ['d1']*2 + ['d2']*2
names = ['class' 'idx']
aa = pandasDataFrame(x1 index=y1 columns = cols)
aaindexnames = names
print aa
bb = pandasDataFrame(x2 index=y2 columns = cols)
bbindexnames = names
print bb

aaappend(bb)


What should I do to get this running?

Thanks",308849,,308849.0,2012-11-07 08:47:57,2012-11-07 14:59:29,Append two multi indexed data frames in pandas,<python><pandas>,1.0,2.0,,
13261855,1,13319544.0,2012-11-07 01:07:03,1,151,"I would like to return a dataFrame with each row sorted (let's say descending)  So if I have the pandasDataFrame named data:

In [38]: data
Out[38]: 
                  c1        c2        c3        c4        c5        c6
Date                                                                  
2012-10-22  0973371  0226342  0968282  0872330  0273880  0746156
2012-10-19  0497048  0351332  0310025  0726669  0344202  0878755
2012-10-18  0315764  0178584  0838223  0749962  0850462  0400253
2012-10-17  0162879  0068409  0704094  0712860  0537545  0009789


I would like the following returned:

In [39]: sorted_frame
Out[39]: 
                   0         1         2         3         4         5
Date                                                                  
2012-10-22  0973371  0968282  0872332  0746156  0273880  0226342
2012-10-19  0878755  0726669  0497048  0351332  0344202  0310025
2012-10-18  0850462  0838223  0749962  0400253  0315764  0178584
2012-10-17  0712860  0704094  0537545  0162879  0068409  0009789


I've tried DataFramesort(axis = 1) however  that doesn't achieve the desired result:

In [40]: datasort(axis = 1)
Out[43]: 
                  c1        c2        c3        c4        c5        c6
Date                                                                  
2012-10-22  0973371  0226342  0968282  0872330  0273880  0746156
2012-10-19  0497048  0351332  0310025  0726669  0344202  0878755
2012-10-18  0315764  0178584  0838223  0749962  0850462  0400253
2012-10-17  0162879  0068409  0704094  0712860  0537545  0009789


I've created the following function that accomplishes what I'm looking for (using the pandasTimeSeriesorder()):

import numpy

def sorted_by_row(frame  ascending = False):
    vals = numpytile(numpynan frameshape)
    for row in numpyarange(frameshape[0]):
        vals[row  :] = frameix[row  :]order(ascending = ascending)
    return pandasDataFrame(vals  index = frameindex)


However  my goal is to be able to use a row-wise function in the DataFrameapply() method (so I can apply the desired functionality to other functions I build)  I've tried:

 #TimeSeriesorder() sorts a pandasTimeSeries object
 dataapply(lambda x: xorder()  axis = 1)


But again  I'm not getting the desired DataFrame above (I've outputted enough DataFrame's so I'll spare the page the real estate)

Your help is greatly appreciated 

-B",963989,,963989.0,2012-11-07 01:56:32,2012-11-10 05:26:09,`pandas.DataFrame.apply` in a row by row operation,<python><pandas>,2.0,,,
13202326,1,13203497.0,2012-11-02 20:07:37,0,81,"I'm seeing a strange behaviour in the pandasto_datetime function If I put in a string  I get the correct date:

In [100]: pandasto_datetime(' 2012-10-19 16:32:35')
Out[100]: datetimedatetime(2012  10  19  16  32  35)


However  I've got a data set that has a datetime column with strings that have the same format as the string in line 100 above:

In [101]: data_frame = pandasread_csv('my_datacsv'  header=None  names=['bid'  'datetime'])
In [102]: data_frameix[0]

Out[102]:
bid                                    428916
datetime                  2012-10-19 16:32:35  # NOTE: THIS IS A STRING
Name: 0


When I try to set the datetime column to a timestamp  I get a very strange datetime object:

In [102]: data_frame['datetime'] = pandasto_datetime(data_frame['datetime'])
In [103]: data_frameix[0]
Out [103]: 
bid                                    428916
datetime                  1970-01-16 80:32:35  # SEE THIS
Name: 0


So either I'm misunderstanding the way that to_datetime works (very possible) or this is unexpected behavior (less possible) Which is it?",313588,,,,2012-11-02 21:48:04,Pandas to_datetime produces confusing result,<pandas>,1.0,,,
13226029,1,13226352.0,2012-11-05 04:43:47,1,202,"So I learned that I can use DataFramegroupby without having a MultiIndex to do subsampling/cross-sections

On the other hand  when I have a MultiIndex on a DataFrame  I still need to use DataFramegroupby to do sub-sampling/cross-sections

So what is a MultiIndex good for apart from the quite helpful and pretty display of the hierarchies when printing?",680232,,,,2012-11-05 05:50:24,Benefits of panda's multiindex?,<python><pandas><multi-index>,1.0,,1.0,
13236098,1,13236277.0,2012-11-05 16:20:58,1,90,"Suppose I have a csv file with 400 columns  I cannot load the entire file into a DataFrame (won't fit in memory)  However  I only really want 50 columns  and this will fit in memory  I don't see any built in Pandas way to do this  What do you suggest?  I'm open to using the PyTables interface  or pandasiosql  

The best-case scenario would be a function like:  pandasread_csv(  columns=['name'  'age'  'income'])  Ie we pass a list of column names (or numbers) that will be loaded",993872,,,,2012-11-10 05:41:17,How to load only specific columns from csv file into a DataFrame,<pandas>,2.0,,,
13258974,1,13259057.0,2012-11-06 20:36:45,1,75,"I would like to apply a function to a dataframe and receive a single dictionary as a result pandasapply gives me a Series of dicts  and so currently I have to combine keys from each I'll use an example to illustrate 

I have a pandas dataframe like so 

In [20]: df
Out[20]:
          0  1
0  2025745  a
1 -1840914  b
2 -0428811  c
3  0718237  d
4  0079593  e


I have some function that returns a dictionary For this example I'm using a toy lambda function lambda x: {x: ord(x)} that returns a dictionary 

In [22]: what_i_get = df[1]apply(lambda x: {x: ord(x)})
In [23]: what_i_get
Out[23]:
0     {'a': 97}
1     {'b': 98}
2     {'c': 99}
3    {'d': 100}
4    {'e': 101}
Name: 1


apply() gives me a series of dictionaries  but what I want is a single dictionary 

I could create it with something like this: 

In [41]: what_i_want = {}
In [42]: for elem in what_i_get:
   :    for k v in elemiteritems():
   :        what_i_want[k] = v
   :

In [43]: what_i_want
Out[43]: {'a': 97  'b': 98  'c': 99  'd': 100  'e': 101}


But it seems I should be able to get what I want more directly ",484596,,,,2012-11-06 20:54:09,Get a Dictionary by applying function to pandas Series,<python><pandas>,2.0,,,
13269890,1,13270110.0,2012-11-07 12:33:12,2,96,"I have two pandas dataframes:

from pandas import DataFrame
df1 = DataFrame({'col1':[1 2] 'col2':[3 4]})
df2 = DataFrame({'col3':[5 6]})     


What is the best practice to get their cartesian product (of course without writing it explicitly like me)?

#df1  df2 cartesian product
df_cartesian = DataFrame({'col1':[1 2 1 2] 'col2':[3 4 3 4] 'col3':[5 5 6 6]})
",1087310,,308903.0,2012-11-07 12:54:09,2012-11-07 12:54:09,cartesian product in pandas,<python><pandas>,1.0,3.0,,
13216087,1,13216413.0,2012-11-04 04:28:56,3,92,"I've got some radar data that's in a bit of an odd format  and I can't figure out how to correctly pivot it using the pandas library

My data:

    speed   time
loc     
A    63  0000
B    61  0000
C    63  0000
D    65  0000
A    73  0005
B    71  0005
C    73  0005
D    75  0005


I'd like to turn that into a DataFrame that looks like this:

    0000    0005
loc     
A    63     73
B    61     71
C    63     73
D    65     75


I've done a lot of fiddling around but can't seem to get the syntax correct  Can anyone please help?

Thanks!",40707,,,,2012-11-04 05:53:01,pandas DataFrame pivoting issue,<python><pandas>,2.0,,2.0,
13218461,1,13218891.0,2012-11-04 12:21:07,3,151,"I calculated a model using OLS (multiple linear regression) I divided my data to train and test (half each)  and then I would like to predict values for the 2nd half of the labels

model = OLS(labels[:half]  data[:half])
predictions = modelpredict(data[half:])


The problem is that I get and error:
    File ""/usr/local/lib/python27/dist-packages/statsmodels-050-py27-linux-i686egg/statsmodels/regression/linear_modelpy""  line 281  in predict
    return npdot(exog  params)
    ValueError: matrices are not aligned

I have the following array shapes:
datashape: (426  215)
labelsshape: (426 )

If I transpose the input to modelpredict  I do get a result but with a shape of (426 213)  so I suppose its wrong as well (I expect one vector of 213 numbers as label predictions):

modelpredict(data[half:]T)


Any idea how to get it to work?",1571660,,,,2012-11-04 13:23:20,Predicting values using an OLS model with statsmodels,<python><pandas><linear-regression><statsmodels>,1.0,,,
13250499,1,13251056.0,2012-11-06 11:55:09,1,59,"I am trying to add attributes to a subclass of pandasDataFrame and they disappear after pickling and unpickling:

import cPickle
import pandas as pd

class MyClass(pdDataFrame):
    def __init__(self):
        super(MyClass  self)__init__()
        selfbar = 1

myc = MyClass()
with open('mycpickle'  'wb')as myfile:
    cPickledump(myc myfile)
with open('mycpickle'  'rb')as myfile:
    b = cPickleload(myfile)
print bbar


Output:

Traceback (most recent call last):
File ""test_dfpy""  line 14  in 
print bbar
File ""C:\Python27\lib\site-packages\pandas\core\framepy""  line 1771  in __getattr__
(type(self)__name__  name))
AttributeError: 'MyClass' object has no attribute 'bar'


Any idea how I can add attributes safely?",1579844,,,,2012-11-06 12:28:06,Attributes to a subclass of pandas.DataFrame disappear after pickle,<python><class><inheritance><subclass><pandas>,1.0,,,
13289368,1,13313756.0,2012-11-08 12:46:13,1,88,"I am attempting to install the python package pandas  

All my existing python gear has been installed using home-brew / easy_install / pip  however pip and easy_install both fail on pandas -- claiming that i do not have numpy > 16 (though when in python numpy__version__ returns 162) 

Despite this pip install numpy --upgrade reports that I am up-to-date 

To hack around this  I git-cloned the source code down  and ran python setuppy install in my /Library/Python/ directory  It seemed to build okay  however when i import pandas  i get an error and i'm not sure what to do about it 

Can anyone help me link the compiled library to my existing install? 

The error follows: 

dlopen(/usr/local/Cellar/python/273/Frameworks/Pythonframework/Versions/27/lib/python27/site-packages/pandas-091dev_5a152bd-py27-macosx-107-x86_64egg/pandas/libso  2): Symbol not found: _floatify
  Referenced from: /usr/local/Cellar/python/273/Frameworks/Pythonframework/Versions/27/lib/python27/site-packages/pandas-091dev_5a152bd-py27-macosx-107-x86_64egg/pandas/libso
Expected in: flat namespace
in /usr/local/Cellar/python/273/Frameworks/Pythonframework/Versions/27/lib/python27/site-packages/pandas-091dev_5a152bd-py27-macosx-107-x86_64egg/pandas/libso
Traceback (most recent call last):
File """"  line 1  in 
File ""/usr/local/Cellar/python/273/Frameworks/Pythonframework/Versions/27/lib/python27/site-packages/pandas-091dev_5a152bd-py27-macosx-107-x86_64egg/pandas/__init__py""  line 10  in 
import pandaslib as lib
ImportError: dlopen(/usr/local/Cellar/python/273/Frameworks/Pythonframework/Versions/27/lib/python27/site-packages/pandas-091dev_5a152bd-py27-macosx-107-x86_64egg/pandas/libso  2): Symbol not found: _floatify
Referenced from: /usr/local/Cellar/python/273/Frameworks/Pythonframework/Versions/27/lib/python27/site-packages/pandas-091dev_5a152bd-py27-macosx-107-x86_64egg/pandas/libso
Expected in: flat namespace
in /usr/local/Cellar/python/273/Frameworks/Pythonframework/Versions/27/lib/python27/site-packages/pandas-091dev_5a152bd-py27-macosx-107-x86_64egg/pandas/libso
",1453172,,,,2012-12-01 23:57:20,Linking source installed pandas to homebrew'd python,<python><pandas>,2.0,2.0,1.0,
13249135,1,,2012-11-06 10:33:34,1,326,"I'm having trouble installing the Python Pandas library on my Mac OSX computer

I type the following in Terminal:

$ sudo easy_install pandas


But then I get the following:

Searching for pandas
Reading http://pypipythonorg/simple/pandas/
Reading http://pandaspydataorg
Reading http://pandassourceforgenet
Best match: pandas 090
Downloading http://pypipythonorg/packages/source/p/pandas/pandas-
090zip#md5=04b1d8e11cc0fc30ae777499d89003ec
Processing pandas-090zip
Writing /tmp/easy_install-ixjbQO/pandas-090/setupcfg
Running pandas-090/setuppy -q bdist_egg --dist-dir /tmp/easy_install-ixjbQO/pandas-
090/egg-dist-tmp-EGREoT
warning: no files found matching 'setupeggpy'
no previously-included directories found matching 'doc/build'
warning: no previously-included files matching '*so' found anywhere in distribution
warning: no previously-included files matching '*pyd' found anywhere in distribution
warning: no previously-included files matching '*pyc' found anywhere in distribution
warning: no previously-included files matching 'git*' found anywhere in distribution
warning: no previously-included files matching 'DS_Store' found anywhere in distribution
warning: no previously-included files matching '*png' found anywhere in distribution
unable to execute gcc: No such file or directory
error: Setup script exited with error: command 'gcc' failed with exit status 1


I do have Xcode and gcc installed  however  gcc is only found when I type:

$ gcc
-bash: gcc: command not found

$ gcc-42
i686-apple-darwin11-gcc-421: no input files


What should I do?",1715271,,1240268.0,2012-12-10 13:08:31,2012-12-10 13:08:31,Installing Pandas on Mac OSX,<python><pandas>,2.0,,,
13279690,1,13288184.0,2012-11-07 22:55:24,3,102,"This may be a bug  but it may also be a subtlety of pandas that I'm missing I'm combining two dataframes and the result's index isn't sorted What's weird is that I've never seen a single instance of combine_first that failed to maintain the index sorted before



>>> a1
                            X  Y
DateTime                                    
2012-11-06 16:00:11477563      8        80
2012-11-06 16:00:11477563      8        63
>>> a2
                        X  Y
DateTime                                   
2012-11-06 15:11:09006507      1        37
2012-11-06 15:11:09006507      1        36
>>> a1combine_first(a2)
                            X  Y
DateTime                                   
2012-11-06 16:00:11477563      8        80
2012-11-06 16:00:11477563      8        63
2012-11-06 15:11:09006507      1        37
2012-11-06 15:11:09006507      1        36
>>> a2combine_first(a1)
                            X  Y
DateTime                                    
2012-11-06 16:00:11477563      8        80
2012-11-06 16:00:11477563      8        63
2012-11-06 15:11:09006507      1        37
2012-11-06 15:11:09006507      1        36


I can reproduce  so I'm happy to take suggestions Guesses as to what's going on are most welcome",645212,,,,2012-11-09 18:26:21,unexpected behavior when combining two dataframes in pandas,<python><pandas>,2.0,1.0,,
13295735,1,13295801.0,2012-11-08 18:50:39,4,182,"I have a dataframe as below

      itm Date                  Amount 
67    420 2012-09-30 00:00:00   65211
68    421 2012-09-09 00:00:00   29424
69    421 2012-09-16 00:00:00   29877
70    421 2012-09-23 00:00:00   30990
71    421 2012-09-30 00:00:00   61303
72    485 2012-09-09 00:00:00   71781
73    485 2012-09-16 00:00:00     NaN
74    485 2012-09-23 00:00:00   11072
75    485 2012-09-30 00:00:00  113702
76    489 2012-09-09 00:00:00   64731
77    489 2012-09-16 00:00:00     NaN


when I try to apply a function to the Amount column I get the following error
ValueError: cannot convert float NaN to integer

I have tried applying a function using isnan from the Math Module
I have tried the pandas replace attribute
I tried the sparse data attribute from pandas 09
I have also tried if NaN == NaN statement in a function
I have also looked at this article How do I replace NA values with zeros in R? whilst looking at some other articles 
All the methods I have tried have not worked or do not recognise NaN
Any Hints or solutions would be appreciated ",1733061,,1679863.0,2012-11-08 18:51:10,2012-11-08 19:11:12,How can I replace all the NaN values with Zero's in a column of a pandas dataframe,<python><pandas>,1.0,2.0,1.0,
13107598,1,13112913.0,2012-10-28 09:10:25,2,63,"Suppose I have a Pandas dataframe df has columns a b c dz  And I want to: dfgroupby('a')apply(my_func()) for columns d-z  while leave column 'b' & 'c' unchanged  How to do that ?

I notice Pandas can apply different function to different column by passing a dict  But I have a long column list and just want parameters to set or tip to simply tell Pandas to bypass some columns and apply my_func() to rest of columns ? (Otherwise I have to build a long dict)",1072888,,,,2012-10-28 21:30:26,"How to df.groupby(cols).apply(my_func) for some columns, while leave a few columns not tackled?",<python><pandas>,1.0,,,
13114512,1,13115473.0,2012-10-29 00:28:10,2,298,"In python  how can I reference previous row and calculate something against it?  Specifically  I am working with dataframes in pandas - I have a data frame full of stock price information that looks like this:

           Date   Close  Adj Close
251  2011-01-03  14748     14325
250  2011-01-04  14764     14341
249  2011-01-05  14705     14283
248  2011-01-06  14866     14440
247  2011-01-07  14793     14369


Here is how I created this dataframe:

import pandas

url = 'http://ichartfinanceyahoocom/tablecsv?s=IBM&a=00&b=1&c=2011&d=11&e=31&f=2011&g=d&ignore=csv'
data = data = pandasread_csv(url)

## now I sorted the data frame ascending by date 
data = datasort(columns='Date')


Starting with row number 2  or in this case  I guess it's 250 (PS - is that the index?)  I want to calculate the difference between 2011-01-03 and 2011-01-04  for every entry in this dataframe  I believe the appropriate way is to write a function that takes the current row  then figures out the previous row  and calculates the difference between them  the use the pandas apply function to update the dataframe with the value  

Is that the right approach?  If so  should I be using the index to determine the difference?  (note - I'm still in python beginner mode  so index may not be the right term  nor even the correct way to implement this)",854739,,,,2012-10-29 03:17:30,Calculating difference between two rows in Python / Pandas,<python><pandas>,2.0,4.0,1.0,
13229750,1,13231460.0,2012-11-05 10:00:56,3,62,"I would like to add attributes to a subclass of DataFrame  but I get an error:

>>> import pandas as pd
>>>class Foo(pdDataFrame):
     def __init__(self):
         selfbar=None
         
>>> Foo()


RuntimeError: maximum recursion depth exceeded
",1579844,,,,2012-11-05 11:56:35,How to add attributes to a subclass of pandas.DataFrame?,<pandas>,2.0,1.0,2.0,
13250046,1,,2012-11-06 11:27:42,3,116,"I am importing study data into a Pandas data frame using read_csv 

My subject codes are 6 numbers coding  among others  the day of birth For some of my subjects this results in a code with a leading zero (eg ""010816"")

When I import into Pandas  the leading zero is stripped of and the column is formatted as int64

Is there a way to import this column unchanged maybe as a string? 

I tried using a custom converter for the column  but it does not work - it seems as if the custom conversion takes place before Pandas converts to int",1802883,,1462920.0,2012-11-06 11:46:11,2012-11-06 12:29:29,Pandas csv-import: Keep leading zeros in a column,<python><pandas>,1.0,,,
13331698,1,,2012-11-11 13:48:53,1,268,"Suppose I have a df which has columns of 'ID'  'col_1'  'col_2' And I define a function :

f = lambda x  y : my_function_expression

Now I want to apply the f to df's two columns 'col_1'  'col_2' to element-wise calculate a new column 'col_3'   somewhat like :

df['col_3'] = df[['col_1' 'col_2']]apply(f)  
# Pandas gives : TypeError: ('() takes exactly 2 arguments (1 given)'


How to do ?

** Add detail sample as below ***

import pandas as pd

df = pdDataFrame({'ID':['1' '2' '3']  'col_1': [0 2 3]  'col_2':[1 4 5]})
mylist = ['a' 'b' 'c' 'd' 'e' 'f']

def get_sublist(sta end):
    return mylist[sta:end+1]

#df['col_3'] = df[['col_1' 'col_2']]apply(get_sublist axis=1)
# expect above to output df as below 

  ID  col_1  col_2            col_3
0  1      0      1       ['a'  'b']
1  2      2      4  ['c'  'd'  'e']
2  3      3      5  ['d'  'e'  'f']
",1072888,,1072888.0,2012-11-13 12:59:35,2012-11-13 12:59:35,How to apply a function to two columns of Pandas dataframe,<python><pandas>,2.0,4.0,2.0,
13213039,1,13214246.0,2012-11-03 20:09:28,3,61,"I have several Pandas Series objects that look like this:

r = pdSeries({'a': [1 2 3 4]})
s = pdSeries({'b': [2 4 1]})
u = pdSeries({'c': [8 6]})
v = pdSeries({'d': [4 3 1]})


I'd like to convert these Series objects into a data fram with the dictionay keys as column names and the values as columns My desired output is:

     'r'    's'    'u'    'v'
0     1      2      8      4
1     2      4      6      3
2     3      1     Nan     1
3     4     Nan    Nan    Nan


How can I create a data frame object as depicted above? I'm aware of the fillna method  but I could not get this to work with my data The missing values should be Nan Thanks for the help",1255817,,,,2012-11-03 22:44:49,Reshape a group of Pandas Series into a DataFrame and fillin missing values,<scipy><pandas>,1.0,,,
13256917,1,13257677.0,2012-11-06 18:17:32,2,100,"With the DataFrame below as an example  

In [83]:
df = pdDataFrame({'A':[1 1 2 2] 'B':[1 2 1 2] 'values':nparange(10 30 5)})
df
Out[83]:
   A  B  values
0  1  1      10
1  1  2      15
2  2  1      20
3  2  2      25


What would be a simple way to generate a new column containing some aggregation of the data over one of the columns?

For example  if I sum values over items in A 

In [84]:
dfgroupby('A')sum()['values']
Out[84]:
A
1    25
2    45
Name: values


How can I get 

   A  B  values  sum_values_A
0  1  1      10            25
1  1  2      15            25
2  2  1      20            45
3  2  2      25            45
",189418,,,,2012-11-06 21:26:44,Pandas: Creating aggregated column in DataFrame,<python><pandas>,4.0,,2.0,
13261175,1,,2012-11-06 23:39:52,1,93,"There seem to be a lot of possibilities to pivot flat table data into a 3d array but I'm somehow not finding one that works: Suppose I have some data with columns=['name'  'type'  'date'  'value'] When I try to pivot via

pivot(index='name'  columns=['type'  'date']  values='value')


I get 

ValueError: Buffer has wrong number of dimensions (expected 1  got 2)


Am I reading docs from dev pandas maybe? It seems like this is the usage described there I am running 08 pandas

I guess  I'm wondering if I have a MultiIndex ['x'  'y'  'z'] Series  is there a pandas way to put that in a panel? I can use groupby and get the job done  but then this is almost like what I would do in numpy to assemble an n-d array Seems like a fairly generic operation so I would imagine it might be implemented already",287238,,287238.0,2012-11-07 14:37:40,2012-11-07 14:47:42,pandas pivot dataframe to 3d data,<python><pandas>,1.0,,,
13333159,1,13337623.0,2012-11-11 16:39:08,0,85,"I am trying to use resample method to fill the gaps in timeseries data But I also want to know which row was used to fill the missed data

This is my input series

In [28]: data
Out[28]: 
Date
2002-09-09    23325
2002-09-11    23305
2002-09-16    23025
2002-09-18    23010
2002-09-19    23005
Name: Price


With resample  I will get this

In [29]: dataresample(""D""  fill_method='bfill')
Out[29]: 
Date
2002-09-09    23325
2002-09-10    23305
2002-09-11    23305
2002-09-12    23025
2002-09-13    23025
2002-09-14    23025
2002-09-15    23025
2002-09-16    23025
2002-09-17    23010
2002-09-18    23010
2002-09-19    23005
Freq: D


I am looking for 

Out[29]: 
Date
2002-09-09    23325  2002-09-09
2002-09-10    23305  2012-09-11
2002-09-11    23305  2012-09-11
2002-09-12    23025  2012-09-16
2002-09-13    23025  2012-09-16
2002-09-14    23025  2012-09-16
2002-09-15    23025  2012-09-16
2002-09-16    23025  2012-09-16
2002-09-17    23010  2012-09-18  
2002-09-18    23010  2012-09-18
2002-09-19    23005  2012-09-19


Any help?",240828,,,,2012-11-12 02:25:17,pandas's resample with fill_method: Need to know data from which row was copied?,<python><pandas>,1.0,,,
13298633,1,,2012-11-08 22:04:48,1,64,"I have a loop in Python which sequentially imports CSV files  assigns them to a temporary DataFrame object and then attempts to merge/concact them to a 'master' DataFrame The code is below:

for csv_path in csv_paths:
    df = pdread_csv(''+csv_path+'')
    df = dfset_index('Player')
    if len(MLS_Stats) == 0:
        MLS_Stats = pdconcat([MLS_Stats df])
    else:
        MLS_Stats = pdmerge(MLS_Stats  df  how=""outer"" left_index=True right_index=True)


The MLS_Stats DF is initially empty  which is the reasoning for the if loop  since I don't think you can merge a DF with an empty DF 

For each merge  I want build the DataFrame by including any new uniquely indexed rows and new columns  but exclude overlapping columns The above code currently includes the overlapping columns with _x and _y suffixes

I know there must be something I'm not understanding  because this doesn't seem like an uncommon situation",1438637,,,,2012-11-09 19:03:23,Merge parameters for Pandas,<python><pandas>,1.0,6.0,,
13348640,1,,2012-11-12 17:40:02,1,143,"I am plotting a bar and line plot in one figure and having problems with correctly formatting the shared x-axis tick labels The point on the line is not in sync with the center of the bar where the tick label is drawn

PS: I am plotting through pandas plot function

Example:

A Single Bar plot (works fine)

libs_summary_pandas_df[['read_count']]plot(kind='bar' ax=axis color=['#E41A1C'])




B Overlaying with line plot on the secondary y-axis (x-axis labels are messed up)

libs_summary_pandas_dftotal_yieldmap(lambda x: x/10000000000)plot(kind='line' ax=axis)




Thanks!
-Abhi",369541,,369541.0,2012-11-12 18:44:17,2012-11-12 18:44:17,formatting x-axis labels with two y-axis in matplotlib (bar and line plot),<python><matplotlib><pandas>,,1.0,,
13353233,1,,2012-11-12 23:19:02,4,70,"Suppose I have the following DataFrame:

   a         b
0  A  1516733
1  A  0035646
2  A -0942834
3  B -0157334
4  A  2226809
5  A  0768516
6  B -0015162
7  A  0710356
8  A  0151429


And I need to group it given the ""edge B""; that means the groups will be:

   a         b
0  A  1516733
1  A  0035646
2  A -0942834
3  B -0157334

4  A  2226809
5  A  0768516
6  B -0015162

7  A  0710356
8  A  0151429


That is any time I find a 'B' in the column 'a' I want to split my DataFrame

My current solution is:

#create the dataframe
s = pdSeries(['A' 'A' 'A' 'B' 'A' 'A' 'B' 'A' 'A'])
ss = pdSeries(nprandomrandn(9))
dff = pdDataFrame({""a"":s ""b"":ss})

#my solution
count  = 0
ls = []
for i in s:
    if i==""A"":
        lsappend(count)
    else:
        lsappend(count)
        count+=1
dff['grpb']=ls


and I got the dataframe:

    a   b           grpb
0   A   1516733    0
1   A   0035646    0
2   A   -0942834   0
3   B   -0157334   0
4   A   2226809    1
5   A   0768516    1
6   B   -0015162   1
7   A   0710356    2
8   A   0151429    2


Which I can then split with dffgroupby('grpb')

Is there a more efficient way to do this using pandas functions?",218808,,1240268.0,2012-11-12 23:38:50,2012-11-13 09:21:03,Best way to split a DataFrame given an edge,<python><pandas>,1.0,,,
13369684,1,13370512.0,2012-11-13 22:06:53,4,90,Before  there was larry and structured/record arrays in NumPy  but I wonder if they are used any more with any frequency given the rapid development of the pandas package Coming from R  I would always get stuck having to unpack the record arrays to modify values from multiple columns and reassign them back into the structure but I'm so glad that pandas now allows this for its data frames I wonder if there are any uses for which record arrays are still superior (does it have some useful methods that pandas does not have)?,143476,,,,2012-11-13 23:13:27,Has the DataFrame object from pandas superceded the other alternatives for heterogeneous data types?,<python><numpy><scipy><pandas>,1.0,1.0,,
13166842,1,,2012-10-31 20:20:44,2,161,"What is the best way to multiply all the columns of a Pandas DataFrame by a column vector stored in a Series? I used to do this in Matlab with repmat()  which doesn't exist in Pandas I can use nptile()  but it looks ugly to convert the data structure back and forth each time 

Thanks",1789674,,391762.0,2012-10-31 20:53:10,2012-11-09 21:01:11,pandas dataframe multiply with a series,<data.frame><pandas><multiplying>,3.0,1.0,,
13244095,1,13244747.0,2012-11-06 03:51:14,4,103,"I came across this dataset:

http://archiveicsuciedu/ml/machine-learning-databases/auto-mpg/auto-mpgdata

and I couldn't find a simple way of getting this into a Pandas Dataframe I manually parsed this into a list of lists and then called the Dataframe constructor  but is there an easier way of doing this Thanks!",190894,,,,2012-11-06 10:55:00,What's the easiest way of getting this data into a Pandas Dataframe?,<pandas>,1.0,,1.0,
13326887,1,13326956.0,2012-11-10 22:58:50,0,84,"I have some percentages in a data frame column

pc
032
045
049
060
068
087


And i want to end up with something like this

pc     group
032    1
045    2
049    2
060    2
068    3
087    3


I've tried 

df[""group""]=3

if df[""pc""]",1365135,,,,2012-11-10 23:07:08,Data frame column with conditional data - Python,<python><numpy><data.frame><pandas>,1.0,,,
13331518,1,13332682.0,2012-11-11 13:26:27,1,114,"How Do I add a single item to a serialized panda series I know it's not the most efficient way memory wise  but i still need to do that

Something along:

>> x = Series()
>> N = 4
>> for i in xrange(N):
>>     xsome_appending_function(i**2)    
>> print x

0 | 0
1 | 1
2 | 4
3 | 9


also  how can i add a single row to a pandas DataFrame? ",1724926,,,,2012-11-19 15:03:18,How to add a single item to a Pandas Series,<python><pandas>,2.0,,,
13354725,1,,2012-11-13 02:18:10,1,151,"I do a lot of data analysis in perl and I am trying to replicate this work in python using pandas  numpy  matplotlib  etc

The general workflow goes as follows:

1) glob all the files in a directory

2) parse the files because they have metadata

3) use regex to isolate relevant lines in a given file (They usually begin with a tag such as 'LOOPS')

4) split the lines that match the tag and load data into hashes

5) do some data analysis

6) make some plots

Here is a sample of what I typically do in perl:

print""Reading File:\n"";                              # gets data
foreach my $vol ($SmallV  $LargeV) {
  my $base_name = ""${NF}flav_${vol}/BlockedWflow_low_${vol}_[0-9][0-9]_-025_$Mass{$vol}"";
  my @files = ;                         # globs for file names
  foreach my $f (@files) {                           # loops through matching files
    print"" $f\n"";
    my @split = split(/_/  $f);
    my $beta = $split[4];
    if (!grep{$_ eq $beta} @{$Beta{$vol}}) {         # constructs Beta hash
      push(@{$Beta{$vol}}  $split[4]);
    }
    open(IN  "");
    close IN;
    my @lines = grep{$_=~/^LOOPS/} @in;       # greps for lines with the header LOOPS
    foreach my $l (@lines) {                  # loops through matched lines
      my @split = split(/\s+/  $l);           # splits matched lines
      push(@{$val{$vol}{$beta}{$split[1]}{$split[2]}{$split[4]}}  $split[6]);# reads data into hash
      if (!grep{$_ eq $split[1]} @smearingt) {# fills the smearing time array
        push(@smearingt  $split[1]);
      }
      if (!grep{$_ eq $split[4]} @{$block{$vol}}) {# fills the number of blockings
        push(@{$block{$vol}}  $split[4]);
      }
    }
  }
  foreach my $beta (@{$Beta{$vol}}) {
    foreach my $loop (0 1 2 3 4) {         # loops over observables
      foreach my $b (@{$block{$vol}}) {    # beta values
        foreach my $t (@smearingt) {       # and smearing times
          $avg{$vol}{$beta}{$t}{$loop}{$b} = stat_mod::avg(@{$val{$vol}{$beta}{$t}{$loop}{$b}});     # to find statistics
          $err{$vol}{$beta}{$t}{$loop}{$b} = stat_mod::stdev(@{$val{$vol}{$beta}{$t}{$loop}{$b}});
        }
      }
    }
  }
}
print""File Read in Complete!\n"";


My hope is to load this data into a Hierarchical Indexed data structure with indices of the perl hash becoming indicies of my python data structure  Every example I have come across so far of pandas data structures has been highly contrived where the whole structure (indicies and values) was assigned manually in one command and then manipulated to demonstrate all the features of the data structure  Unfortunately I can not assign the data all at once because I don't know what mass  beta  sizes  etc are in the data that is going to be analyzed  Am I doing this the wrong way?  Does anyone know a better way of doing this?  The data files are immutable  I will have to parse through them using regex which I understand how to do  What I need help with is putting the data into an appropriate data structure so that I can take averages  standard deviations  perform mathematical operations  and plot the data

Typical data has a header that is an unknown number of lines long but the stuff I care about looks like this:

Alpha 05 05 04
Alpha 05 05 04
LOOPS 0 0 0 2 05 17800178
LOOPS 0 1 0 2 05 084488326
LOOPS 0 2 0 2 05 098365135  
LOOPS 0 3 0 2 05 11638834
LOOPS 0 4 0 2 05 10438407
LOOPS 0 5 0 2 05 019081102
POLYA NHYP 0 2 05 -00200002 0119196 -00788721 -0170488 
BLOCKING COMPLETED
Blocking time 1474 seconds
WFLOW 001 157689 230146 0000230146 0000230146 000170773 -00336667
WFLOW 002 166552 228275 0000913101 000136591 000640552 -00271222
WFLOW 003 175 225841 000203257 000335839 00135 -00205722
WFLOW 004 183017 222891 000356625 000613473 00224607 -00141664
WFLOW 005 190594 219478 000548695 000960351 00328218 -000803792
WFLOW 006 19773 215659 000776372 00136606 00441807 -000229793
WFLOW 007 20443 21149 0010363 0018195 00561953 000296648


What I (think) I want  I preface this with think because I am new to python and an expert may know a better data structure  is a Hierarchical Indexed Series that would look like this:

volume   mass   beta   observable   t   value

1224     00    56    0            0   1234
                                    1   1490
                                    2   1222
                       1            0   1234
                                    1   1234
2448     00    57    0            1   1234


and so on like this:  http://pandaspydataorg/pandas-docs/dev/indexinghtml#indexing-hierarchical

For those of you who don't understand the perl:

The meat and potatoes of what I need is this:

push(@{$val{$vol}{$beta}{$split[1]}{$split[2]}{$split[4]}}  $split[6]);# reads data into hash


What I have here is a hash called 'val'  This is a hash of arrays  I believe in python speak this would be a dict of lists  Here each thing that looks like this: '{$something}' is a key in the hash 'val' and I am appending the value stored in the variable $split[6] to the end of the array that is the hash element specified by all 5 keys  This is the fundamental issue with my data is there are a lot of keys for each quantity that I am interested in

==========

UPDATE

I have come up with the following code which results in this error:

Traceback (most recent call last):
  File ""wflow_2lattice_matchingpy""  line 39  in 
    index = MultiIndexfrom_tuples(zipped  names=['volume'  'beta'  'montecarlo_time  smearing_time'])
NameError: name 'MultiIndex' is not defined


Code:

#!/usr/bin/python

from pandas import Series  DataFrame
import pandas as pd
import glob
import re
import numpy

flavor = 4
mass = 00

vol = []
b = []
m_t = []
w_t = []
val = []

#tup_vol = (1224  1632  2448)
tup_vol = 1224  1632
for v in tup_vol:
  filelist = globglob(str(flavor)+'flav_'+str(v)+'/BlockedWflow_low_'+str(v)+'_*_00*')
  for filename in filelist:
    print 'Reading filename:  '+filename
    f = open(filename  'r')
    junk  start  vv  beta  junk  mass  mont_t = resplit('_'  filename)
    ftext = freadlines()
    for line in ftext:
      if rematch('^WFLOW*'  line):
        line=linestrip()
        junk  smear_t  junk  junk  wilson_flow  junk  junk  junk = resplit('\s+'  line)
        volappend(v)
        bappend(beta)
        m_tappend(mont_t)
        w_tappend(smear_t)
        valappend(wilson_flow)
zipped = zip(vol  beta  m_t  w_t)
index = MultiIndexfrom_tuples(zipped  names=['volume'  'beta'  'montecarlo_time  smearing_time'])
data = Series(val  index=index)
",1659087,,1659087.0,2012-11-13 21:09:26,2012-11-15 04:45:07,Trying to parse text files in python for data analysis,<python><parsing><numpy><scipy><pandas>,3.0,11.0,,
13385860,1,13386025.0,2012-11-14 19:25:28,2,87,"I have the following file named 'datacsv':

    1997 Ford E350
    1997  Ford   E350
    1997 Ford E350 ""Super  luxurious truck""
    1997 Ford E350 ""Super """"luxurious"""" truck""
    1997 Ford E350 "" Super luxurious truck ""
    ""1997"" Ford E350
    1997 Ford E350
    2000 Mercury Cougar


And I would like to parse it into a pandas DataFrame so that the DataFrame looks as follows:

       Year     Make   Model              Description
    0  1997     Ford    E350                     None
    1  1997     Ford    E350                     None
    2  1997     Ford    E350   Super  luxurious truck
    3  1997     Ford    E350  Super ""luxurious"" truck
    4  1997     Ford    E350    Super luxurious truck
    5  1997     Ford    E350                     None
    6  1997     Ford    E350                     None
    7  2000  Mercury  Cougar                     None


The best I could do was:

    pdread_table(""datacsv""  sep=r' '  names=[""Year""  ""Make""  ""Model""  ""Description""])


Which gets me:

    Year     Make   Model              Description
 0  1997     Ford    E350                     None
 1  1997    Ford     E350                     None
 2  1997     Ford    E350   Super  luxurious truck
 3  1997     Ford    E350  Super ""luxurious"" truck
 4  1997     Ford    E350   Super luxurious truck 
 5  1997     Ford    E350                     None
 6  1997     Ford    E350                     None
 7  2000  Mercury  Cougar                     None


How can I get the DataFrame without those whitespaces?",1715271,,982257.0,2012-11-14 19:38:36,2012-11-14 19:38:36,How can I remove extra whitespace from strings when parsing a csv file in Pandas?,<python><parsing><pandas>,2.0,,,
13281305,1,13291523.0,2012-11-08 01:51:17,0,92,"I have a space separated CSV file in following format:

2012-11-01 1 2012-12-01 4 2013-02-01 6
2012-12-01 2 2013-01-01 nan
2012-11-01 3 2012-12-01 5 2013-01-01 5 2013-04-01 7


basically dates followed by a value  but the dates are sparse Some of the values are nan  or also could be missing I would like to be able to read this into Pandas and line up the values based on the corresponding dates

Running Pandas:

import pandas as pd
pdread_csv('sparsecsv'  sep="" ""  parse_dates=True)


errors with:

ValueError: Expecting 6 columns  got 8 in row 1


What would be a way to read this file and align the date/values?

(Is there some ""pre-processing"" I could do maybe?)

Thanks",304690,,,,2012-11-08 15:00:01,reading sparse csv file into pandas,<python><csv><pandas>,1.0,,,
13299769,1,,2012-11-08 23:33:25,2,110,"I am dealing with a dataset where observations occur between opening and closing hours -- but the service closes on the day after it opens For example  opening occurs at 7am and closing at 1am  the following day 

This feels like a very common problem -- I've searched around for it and am open to the fact I might just not know the correct terms to search for 

For most of my uses it's enough to do something like: 

   open_close = pdDatetimeIndex(start='2012-01-01 05:00:00'  periods = 15  offset='D')


Then I can just do fun little groupbys on the df: dfgroupby(open_closeasof)agg(func)

But I've run into an instance where I need to grab multiple of these open-close periods What I really want to be able to do is just have an DatetimeIndex where I get to pick when an day starts So I could just redefine 'day' to be from 5AM to 5AM The nice thing about this is I can then use things like df[dfindexdayofweek == 6] and get back everything from 5AM on Sunday to 5AM on Monda 

It feels like Periodsor something inside of pandas anticipated this request Would love help figuring it out 

EDIT:

I've also figured this out via creating another column with the right day df['shift_day'] = df['datetime']apply(magicFunctionToFigureOutOpenClose)
 -- so this isn't blocking my progress Just feels like something that could be nicely integrated into the package (or datetimeor somewhere)",684543,,243434.0,2013-01-29 00:53:49,2013-01-29 00:53:49,Change starting and ending hour of pandas timestamp,<python><pandas>,1.0,,,
13384795,1,13386916.0,2012-11-14 18:16:18,0,56,"I'm using pandas time series indexed with a DatetimeIndex  and I need to have support for semiannual frequencies The basic semiannual frequency has 1H=Jan-Jun and 2H=Jul-Dec  though some series might have the last month be a month other than December  for instance 1H=Dec-May and 2H=Jun-Nov 

I imagine I could certainly achieve what I want by making a custom class that derives from pandas' DateOffset class However  before I go and do that  I'm curious if there is a way I can simply use a built-in frequency  for instance a 6-month frequency? I have tried to do this  but cannot get resampling to the way I want

For example:

import numpy as np
import pandas as pd
from datetime import datetime

data = nparange(12)
s = pdSeries(data  pddate_range(start=datetime(2007 1 31)  periods=len(data)  freq=""M""))
sresample(""6M"")

Out[11]:
2007-01-31    00
2007-07-31    35
2008-01-31    90
Freq: 6M


Notice how pandas is aggregating using windows from Aug-Jan and Feb-Jul In this base case I would want Jan-Jun and Jul-Dec ",233446,,,,2012-11-14 20:39:03,Using built-in pandas frequencies to simulate semiannual frequency,<python><pandas>,1.0,,,
13389203,1,13389808.0,2012-11-14 23:29:30,1,135,"I have a series with a MultiIndex like this:

import numpy as np
import pandas as pd

buckets = nprepeat(['a' 'b' 'c']  [3 5 1])
sequence = [0 1 5 0 1 2 4 50 0]

s = pdSeries(
    nprandomrandn(len(sequence))  
    index=pdMultiIndexfrom_tuples(zip(buckets  sequence))
)

# In [6]: s
# Out[6]: 
# a  0    -1106047
#    1     1665214
#    5     0279190
# b  0     0326364
#    1     0900439
#    2    -0653940
#    4     0082270
#    50   -0255482
# c  0    -0091730


I'd like to get the s['b'] values where the second index ('sequence') is between 2 and 10

Slicing on the first index works fine:

s['a':'b']
# Out[109]: 
# bucket  value
# a       0        1828176
#         1        0160496
#         5        0401985
# b       0       -1514268
#         1       -0973915
#         2        1285553
#         4       -0194625
#         5       -0144112


But not on the second  at least by what seems to be the two most obvious ways:

1) This returns elements 1 through 4  with nothing to do with the index values

s['b'][1:10]

# In [61]: s['b'][1:10]
# Out[61]: 
# 1     0900439
# 2    -0653940
# 4     0082270
# 50   -0255482


However  if I reverse the index and the first index is integer and the second index is a string  it works:

In [26]: s
Out[26]: 
0   a   -0126299
1   a    1810928
5   a    0571873
0   b   -0116108
1   b   -0712184
2   b   -1771264
4   b    0148961
50  b    0089683
0   c   -0582578

In [25]: s[0]['a':'b']
Out[25]: 
a   -0126299
b   -0116108
",529841,,,,2012-11-15 00:47:54,pandas: slice a MultiIndex by range of secondary index,<python><pandas>,3.0,,,
13347288,1,13349131.0,2012-11-12 16:16:43,2,75,"I have a Pandas Series sampled at irregular times (roughly 5 s  but always a couple of ms off due to latency)

Can I plot this data to have nice x-axis tick labels? So far I am getting only this:

>>> print(bare_dataindex)

[2012-11-08 12:00:05130309    2012-11-09 11:38:18997584]
Length: 16332  Freq: None  Timezone: None
>>> bare_dataplot()




After consulting the Pandas documentation I found that this can be made prettier by resampling and interpolating the data:

>>> bare_dataresample('5s')interpolate()plot()




But this introduces potential errors  though  depending on my original sample times  so I am wondering if there is a loseless way to get pretty tick labels  given that the index is consisting of Timestamps even in the first scenario",544059,,,,2012-11-12 18:12:34,How do I get nice tick labels for an irregularly sampled time series?,<python><matplotlib><pandas>,1.0,,,
13395725,1,,2012-11-15 10:37:58,0,146,"Currently I have a pandas DataFrame like this:

 ID                    A1      A2       A3       B1       B2       B3
 Ku8QhfS0n_hIOABXuE    6343   6304    6410    6287    6403    6279
 fqPEquJRRlSVSfL8A    6752   6681    6680    6677    6525    6739
 ckiehnugOno9d7vf1Q    6297   6248    6524    6382    6316    6453
 x57Vw5B5Fbt5JUnQkI    6268   6451    6379    6371    6458    6333


This DataFrame is used with a statistic which then requires a permutation test (EDIT: to be precise  random permutation) The indices of each column need to be shuffled (sampled) 100 times To give an idea of the size  the number of rows can be around 50 000

EDIT: The permutation is along the rows  ie shuffle the index for each column

The biggest issue here is one of performance I want to permute things in a fast way

An example I had in mind was:

import random
import joblib

def permutation(dataframe):
    return dataframeapply(randomsample  axis=1  k=len(dataframe))

permute = joblibdelayed(permutation)
pool = joblibParallel(n_jobs=-2) # all cores minus 1
result = pool(permute(dataframe) for item in range(100))


The issue here is that by doing this  the test is not stable: apparently the permutation works  but it is not as ""random"" as it would without being done in parallel  and thus there's a loss of stability in the results when I use the permuted data in follow-up calculations

So my only ""solution"" was to precalculate all indices for all columns prior to doing the paralel code  which slows things down considerably

My questions are:

Is there a more efficient way to do this permutation? (not necessarily parallel)
Is the parallel approach (using multiple processes  not threads) feasible?
EDIT: To make things clearer  here's what should happen for example to column A1 after one shuffling:

Ku8QhfS0n_hIOABXuE    6268   
fqPEquJRRlSVSfL8A    6343
ckiehnugOno9d7vf1Q    6752
x57Vw5B5Fbt5JUnQk     6297


(ie the row values were moving around) 

EDIT2: Here's what I'm using now:

def _generate_indices(indices  columns  nperm):

    randomseed(1234567890)
    num_genes = indicessize

    for item in range(nperm):

        permuted = pandasDataFrame(
            {column: randomsample(genes  num_genes) for column in columns} 
             index=range(genessize)
        )

        yield permuted


(in short  building a DataFrame of resampled indices for each column)

And later on (yes  I know it's pretty ugly): 

 # Data is the original DataFrame
 # Indices one of the results of that generator

 permuted = dict()

 for column in datacolumns:

    value = data[column]
    permuted[column] = value[indices[column]values]values

 permuted_table = pandasDataFrame(permuted  index=dataindex)
",241515,,241515.0,2012-11-20 13:24:38,2012-11-20 13:24:38,Efficient way of doing permutations with pandas over a large DataFrame,<python><pandas><data-analysis>,1.0,5.0,1.0,
13352369,1,13370603.0,2012-11-12 22:06:27,3,292,"Installed latest version of pandas 090 in case this was an error EDIT: forgot to mention this is Python 27
 Trying to read Excel file That part seems ok
 Originally  I was trying iteritems() for each row of the pandas dataframe  as the id_company had to be verified against a mysql database (code not included) Same/similar error message to putting it into a tuple (code is below) Error message follows

Note there is a reindex() but it didn't work before  either The reindex() was kind of a hail-mary

As a work-around  I'm probably going to simply import from my target sql and do a join I'm concerned because of the size of the datasets

 import pandas as pd
def runNow():
    #identify sheet
    source = 'C:\Users\jlalonde\Desktop\startup_geno\startupgenome_w_id_xl_20121109xlsx'
    xls_file = pdExcelFile(source)
    sd = xls_fileparse('Sheet1')
    source_u = sddrop_duplicates(cols = 'id_company'  take_last=False)
    source_r = source_u[['id_company' 'id_good' 'description'  'website' 'keyword'  'company_name' 'founded_month'  'founded_year'  'description']]
    source_i = source_rreindex() #hail mary
    tup_r = [tuple(x) for x in source_ivalues]


Here is the error:

Traceback (most recent call last):
  File """"  line 1  in 
    sg_sql_2runNow()
  File ""sg_sql_2py""  line 31  in runNow
    tup_r = [tuple(x) for x in source_rvalues]
  File ""C:\Python27\lib\site-packages\pandas\core\framepy""  line 1443  in as_matrix
    return self_dataas_matrix(columns)T
  File ""C:\Python27\lib\site-packages\pandas\core\internalspy""  line 723  in as_matrix
    mat = self_interleave(selfitems)
  File ""C:\Python27\lib\site-packages\pandas\core\internalspy""  line 743  in _interleave
    indexer = itemsget_indexer(blockitems)
  File ""C:\Python27\lib\site-packages\pandas\core\indexpy""  line 748  in get_indexer
    raise Exception('Reindexing only valid with uniquely valued Index '
Exception: Reindexing only valid with uniquely valued Index objects


So  after hammering my head against the wall on this for the better part of the day  can anyone tell me if this is a bug or if I am missing something really obvious?",1819380,,1819380.0,2012-11-13 01:52:15,2012-11-13 23:20:53,pandas Reindexing only valid with uniquely valued Index objects,<python><pandas>,1.0,4.0,,
13378490,1,,2012-11-14 12:04:13,0,87,"I have been porting a few things for a computational investing course  and I got all the other macports  but 

>""sudo port install py27-pandas @073"" 


is giving me a lot of trouble This is the return I get:

>John--MacBook-Pro:~ John$ sudo port install py27-pandas @073
Password:
--->  Computing dependencies for py27-pandas
--->  Dependencies to be installed: py27-scipy gcc45 swig-python bison gsed swig py27-tables hdf5-18 lzo2 py27-cython cython_select py27-numexpr py27-scientific netcdf
--->  Building gcc45
Error: orgmacportsbuild for port gcc45 returned: command execution failed
Error: Failed to install gcc45
Please see the log file for port gcc45 for details:
    /opt/local/var/macports/logs/_opt_local_var_macports_sources_rsyncmacportsorg_release_tarballs_ports_lang_gcc45/gcc45/mainlog
Error: The following dependencies were not installed: py27-scipy gcc45 swig-python bison gsed swig py27-tables hdf5-18 lzo2 py27-cython cython_select py27-numexpr py27-scientific netcdf
To report a bug  follow the instructions in the guide:
    http://guidemacportsorg/#projecttickets
Error: Processing of port py27-pandas failed


I do have Xcode  CLT for Xcode  but I must be missing something else Thanks for the help everyone!",1823595,,166749.0,2012-11-14 13:01:46,2012-11-21 04:37:55,Trouble Porting Pandas for Python on Mac,<python><osx><pandas>,1.0,5.0,,
13400938,1,13433631.0,2012-11-15 15:39:28,0,70,"I have a a dataframe with the following structure:


Int64Index: 1152 entries  0 to 143
Data columns:
cuepos             1152  non-null values
response           1152  non-null values
soa                1152  non-null values
targetpos          1152  non-null values
testorientation    1152  non-null values
dtypes: float64(3)  int64(2)


The cuepos column and the targetpos column both contain integer values of either 1 or 2

I would like to group this data by congruency between cuepos and targetpos  In other words  I would like to produce two groups  one for rows in which cuepos == targetpos and another group for which cuepos != targetpos

I can't seem to figure out how I might do this  I looked at using grouping functions  but these seem only to act on a single column or am I mistaken?  Can someone point me in the right direction?

Thanks in advance!
Blz",1156707,,,,2012-11-17 18:45:48,Is there a way to group by logical comparison of two columns in Pandas?,<python><pandas>,2.0,,,
13404468,1,,2012-11-15 19:11:57,3,140,"If i want to calculate the mean of two categories in Pandas  I can do like this:

data = {'Category': ['cat2' 'cat1' 'cat2' 'cat1' 'cat2' 'cat1' 'cat2' 'cat1' 'cat1' 'cat1' 'cat2'] 
        'values': [1 2 3 1 2 3 1 2 3 5 1]}
my_data = DataFrame(data)
my_datagroupby('Category')mean()

Category:     values:   
cat1     2666667
cat2     1600000


I have a lot of data formatted this way  and now I need to do a T-test to see if the mean of cat1 and cat2 are statistically different How can I do that?",1827631,,1301710.0,2012-11-15 21:26:07,2012-11-16 09:34:21,T-test in Pandas (Python),<python><statistics><pandas>,1.0,1.0,1.0,
13413590,1,13413845.0,2012-11-16 09:17:22,1,227,"I have a df :

>>> df
                 STK_ID  EPS  cash
STK_ID RPT_Date                   
601166 20111231  601166  NaN   NaN
600036 20111231  600036  NaN    12
600016 20111231  600016  43   NaN
601009 20111231  601009  NaN   NaN
601939 20111231  601939  25   NaN
000001 20111231  000001  NaN   NaN


Then I just want the records whose EPS is not NaN  that is  dfdrop() will return the dataframe as below:

                  STK_ID  EPS  cash
STK_ID RPT_Date                   
600016 20111231  600016  43   NaN
601939 20111231  601939  25   NaN


How to do that ?",1072888,,,,2012-11-17 20:27:33,How to drop rows of Pandas dataframe whose value of certain column is NaN,<python><pandas>,2.0,1.0,1.0,
13421929,1,13423323.0,2012-11-16 17:57:11,2,92,"It has been posted that slicing on the second index can be done on a multi-indexed pandas Series:

import numpy as np
import pandas as pd

buckets = nprepeat(range(3)  [3 5 7])
sequence = nphstack(map(range [3 5 7]))

s = pdSeries(nprandomrandn(len(sequence))  
              index=pdMultiIndexfrom_tuples(zip(buckets  sequence)))

print s

0  0    0021362
   1    0917947
   2   -0956313
1  0   -0242659
   1    0398657
   2    0455909
   3    0200061
   4   -1273537
2  0    0747849
   1   -0012899
   2    1026659
   3   -0256648
   4    0799381
   5    0064147
   6    0491336


Then to get the first three rows for the first index=1  you simply say:

s[1]ix[range(3)]

0   -0242659
1    0398657
2    0455909


This works fine for 1-dimensional Series  but not for DataFrames:

buckets = nprepeat(range(3)  [3 5 7])
sequence = nphstack(map(range [3 5 7]))

d = pdDataFrame(nprandomrandn(len(sequence) 2)  
                 index=pdMultiIndexfrom_tuples(zip(buckets  sequence)))

print d

            0         1
0 0  1217659  0312286
  1  0559782  0686448
  2 -0143116  1146196
1 0 -0195582  0298426
  1  1504944 -0205834
  2  0018644 -0979848
  3 -0387756  0739513
  4  0719952 -0996502
2 0  0065863  0481190
  1 -1309163  0881319
  2  0545382  2048734
  3  0506498  0451335
  4  0872743 -0070985
  5 -1160473  1082550
  6  0331796 -0366597

d[1]ix[range(3)]

0  0    0312286
   1    0686448
   2    1146196
Name: 1


It gives you the ""1th"" column of data  and the first three rows  irrespective of the first index level How can you get the first three rows for the first index=1 for a multi-indexed DataFrame?",1772265,,1772265.0,2012-11-16 18:29:58,2012-11-16 19:42:03,pandas: slice a MultiIndex DataFrame by range of secondary index,<python-2.7><pandas><multi-index>,1.0,,,
13421947,1,13423940.0,2012-11-16 17:58:23,1,65,"I'm trying to get the xlimits of a plot as a python datetime object from a time series plot created with pandas  Using axget_xlim() returns the axis limits as a numpyfloat64  and I can't figure out how to convert the numbers to a usable datetime

import pandas 
from matplotlib import dates
import matplotlibpyplot as plt
from datetime import datetime
from numpyrandom import randn

ts = pandasSeries(randn(10000)  index=pandasdate_range('1/1/2000' 
    periods=10000  freq='H')) 
tsplot()
ax = pltgca()

axset_xlim(datetime(2000 1 1))
d1  d2 = axget_xlim()
print ""%s(%s) to %s(%s)"" % (d1  type(d1)  d2  type(d2))

print ""Using matplotlib: %s"" % datesnum2date(d1)
print ""Using datetime: %s"" % datetimefromtimestamp(d1)


which returns:

2629680 () to 2729670 ()
Using matplotlib: 0720-12-25 00:00:00+00:00
Using datetime: 1970-01-03 19:02:48


According to the pandas timeseries docs  pandas uses the numpydatetime64 dtype  I'm using pandas version '090'

I am using get_xlim() instead directly accessing the pandas series because I am using the  xlim_changed callback to do other things when the user moves around in the plot area

Hack to get usable values 

For the above example  the limits are returned in hours since the Epoch  So I can convert to seconds since the Epoch and use timegmtime() to get somewhere usable  but this still doesn't feel right

In [66]: d1  d2 = axget_xlim()

In [67]: timegmtime(d1*60*60)
Out[67]: timestruct_time(tm_year=2000  tm_mon=1  tm_mday=1  tm_hour=0  tm_min=0  tm_sec=0  tm_wday=5  tm_yday=1  tm_isdst=0)  
",653689,,653689.0,2012-11-16 20:02:34,2012-11-16 20:28:22,Getting usable dates from Axes.get_xlim() in a pandas time series plot,<matplotlib><pandas>,1.0,,,
13354866,1,13358304.0,2012-11-13 02:41:12,0,100,"Considering the following DataFrames

In [136]:
df = pdDataFrame({'A':[1 1 2 2] 'B':[1 2 1 2] 'C':nparange(10 30 5)})set_index(['A' 'B'])
df
Out[136]:
      C
A B    
1 1  10
  2  15
2 1  20
  2  25

In [130]:
vals = pdDataFrame({'A':[1 2] 'values':[True False]})set_index('A')
vals
Out[130]:
  values
A       
1   True
2  False


How can I select only the rows of df with corresponding True values in vals? 

If I reset_index on both frames I can now merge/join them and slice however I want  but how can I do it using the (multi)indexes?",189418,,,,2012-11-13 09:32:59,Pandas: Selection with MultiIndex,<python><pandas>,1.0,1.0,,
13361326,1,13362096.0,2012-11-13 13:10:40,2,84,"Suppose I have the following DataFrame (timeseries  first column is a DateTimeIndex)

                           atn   file
datetime                             
2012-10-08 14:00:00  23007462      1
2012-10-08 14:30:00  27045666      1
2012-10-08 15:00:00  31483825      1
2012-10-08 15:30:00  37540651      2
2012-10-08 16:00:00  43564573      2
2012-10-08 16:00:00  48589852      2
2012-10-08 16:00:00  55289452      2


My goal is to to extract the rows with the first appearance of a certain number in the last column 'file'  so to obtain a table similar to this: 

       datetime             atn
file                             
1      2012-10-08 14:00:00  23007462
2      2012-10-08 15:30:00  37540651


My approach was to groupby 'file' and then aggregate on 'first':

dtgroupby(by=""file"")aggregate(""first"")


But the problem with this is that then the index is not used as a column which is grouped I solved this by first adding the index as a column by:

dt2 = dtreset_index()
dt2groupby(by=""file"")aggregate(""first"")


But now the problem is that the datetime column aren't dates anymore but floats:

          datetime        atn
file                         
1     1349705e+18  23007462
2     1349710e+18  37540651


Is there

a way to convert the floats back to a datetime?
OR a way to preserve the datetimes in the groupby/aggregate-operation?
OR a better way to achieve this the final tabel?
The example dataframe can be used as follows:

Copy this (to clipboard):

2012-10-08 14:00:00   23007462      1
2012-10-08 14:30:00   27045666      1
2012-10-08 15:00:00   31483825      1
2012-10-08 15:30:00   37540651      2
2012-10-08 16:00:00   43564573      2
2012-10-08 16:00:00   48589852      2
2012-10-08 16:00:00   55289452      2


And then:

dt = pandasread_clipboard(sep="" ""  parse_dates=True  index_col=0  
                           names=[""datetime""  ""atn""  ""file""])
",653364,,653364.0,2012-11-13 13:15:52,2012-11-14 00:11:06,Preserving datetime index in groupby operation,<python><pandas>,3.0,3.0,,
13432213,1,13443473.0,2012-11-17 15:59:38,2,238,"I have a Pandas Dataframe which contains dates which I converted to a pandas TimeSeries

From there  I wanted to add a column to the DF which would be the same as the date column  just in Period format with frequency set to months

The problem is  within the dataframe  the period column prints as numbers (2009-1 prints as 468  2009-2 prints as 469  etc)

When I create a separate PeriodIndex object outside of the DF  this is not an issue

What am I doing wrong?

Code I used to convert unformatted time column to DateTime:

subset['Created On'] = pdto_datetime(subset['Created On'])


Code for creating column with Periods:

subset['Month'] = pdPeriodIndex(subset['Created On'] freq='M')


Code that creates a separate PeriodIndex object and properly displays dates in month format:

months = pdPeriodIndex(subset['Created On'] freq='M')


EDIT:

As requested in the comments  subset[:1]to_dict() outputs:

#[Out]# {'Created On': {12822544: }  'City': {12822544: 'BROOKLYN'}  'Borough': {12822544: 'Unspecified'}  'Location': {12822544: '(4065662129596871  -7395806621423951)'}  'Closed Date': {12822544: '01/07/2009 12:00 AM'}}


Note that since my OP  I lost my session and had to re-upload the data to a DF At this point  I've only converted the column 'Created On' to a timestamp using the pdto_datetime method Since then  I've tried using:

subset['Created On']resample('M')


Which results in the error:

TypeError: Only valid with DatetimIndex or PeriodIndex


Maybe a part of the issue is that I'm not using the date column as the DF index? If so  that wouldn't work well since it contains a ton of non-unique values and I'm already using a Unique ID field which is more representative of an index",1438637,,1438637.0,2012-11-18 00:31:42,2012-11-18 19:01:30,Pandas time period data type prints as numbers?,<python><pandas>,1.0,2.0,1.0,
13405611,1,,2012-11-15 20:27:19,3,158,"I want to do a rolling computation on missing data

Sample Code: (For sake of simplicity I'm giving an example of a rolling sum but I want to do something more generic)

foo = lambda z: z[pandasnotnull(z)]sum() 
x = nparange(10  dtype=""float"")    
x[6] = npNaN
x2 = pandasSeries(x)    
pandasrolling_apply(x2  3  foo)


which produces:

0   NaN    
1   NaN
2     3    
3     6    
4     9    
5    12    
6   NaN    
7   NaN    
8   NaN    
9    24


I think that during the ""rolling""  window with missing data is being ignored for computation I'm looking to get a result along the lines of:

0   NaN    
1   NaN    
2     3    
3     6    
4     9    
5    12    
6     9    
7    12    
8    15    
9    24


Many thanks!",1827813,,1240268.0,2012-11-15 22:59:37,2012-11-15 23:09:07,Pandas rolling apply with missing data,<python><pandas><missing-data><rolling>,1.0,1.0,,
13439098,1,13453153.0,2012-11-18 09:56:00,0,175,"I have a sample Pandas dataframe df which has multi_level index:

>>> df
                STK_Name   ROIC   mg_r
STK_ID RPT_Date                       
002410 20111231      ???  0401  0956
300204 20111231      ???  0375  0881
300295 20111231     ????  2370  0867
300288 20111231     ????  1195  0861
600106 20111231     ????  1214  0857
300113 20111231     ????  0837  0852


and stk_list is defined as stk_list = ['600106' '300204' '300113']

I want to get the rows of df whose value of sub_level index STK_ID is within stk_list  The output is as blow:

                STK_Name   ROIC   mg_r
STK_ID RPT_Date                       
300204 20111231      ???  0375  0881
600106 20111231     ????  1214  0857
300113 20111231     ????  0837  0852


Basiclly  I can achive the target for this sample data by 

df = dfreset_index() ; df[dfSTK_IDisin(stk_list)]


But I already have columns 'STK_ID' & 'RPT_Date' in my application dataframe  so reset_index() will cause error Anyway  I want a directly filter against index instead of columns

Learn from this : How to filter by sub-level index in Pandas

I try df[dfindexmap(lambda x: x[0]isin(stk_list))]   and Pandas 081 gives AttributeError: 'unicode' object has no attribute 'isin' 

My question:  How should I filter rows of Pandas dataframe by checking whether sub-level index value within a list without using the reset_index() & set_index() methods?",1072888,,667301.0,2012-11-19 11:50:28,2012-11-19 12:28:33,How to filter rows of Pandas dataframe by checking whether sub-level index value within a list?,<python><pandas>,2.0,3.0,,
13411544,1,13485766.0,2012-11-16 06:26:40,1,346,"When deleting a column in a DataFrame I use del DF['column-name'] and all is well Why does del DFcolumn_name not work also?

I am using v091",390388,,,,2012-11-21 03:12:31,Delete column from pandas DataFrame,<data.frame><pandas>,2.0,,,
13416344,1,,2012-11-16 12:10:27,3,125,"I have a file with intraday prices every ten minutes [0:41] times in a day Each date is repeated 42 times The multi-index below should ""collapse"" the repeated dates into one for all times

There are 62 035 rows x 3 columns: [date  time  price]
I would like write a function to get the difference of the ten minute prices  restricting differences to each unique date
In other words   09:30 is the first time of each day and 16:20 is the last: I cannot overlap differences between days of price from 16:20 - 09:30 The differences should start as 09:40 - 09:30 and end as 16:20 - 16:10 for each unique date in the dataframe  

Here is my attempt Any suggestions would be greatly appreciated

def diffSeries(rounded data):

'''This function accepts a column called rounded from 'data'
 The 2nd input 'data' is a dataframe 
'''

df=roundedshift(1)
idf=dataset_index(['date'  'time'])  
data['diff']=['000']

  for i in range(0 length(rounded)):

    for day in idfindexlevels[0]:


      for time in idfindexlevels[1]:

        if idfindexlevels[1]!=1620:

          data['diff']=rounded[i]-df[i]

        else:
          day+=1
          time+=2

data[['date' 'time' 'price' 'II' 'diff']]to_csv('finalcsv')

return data['diff']


Then I call: 

data=read_csv('filecsv')

rounded=roundSeries(data['price'] 5) 

diffSeries(rounded data)


On the traceback - I get an Assertion Error",1829447,,1014938.0,2012-11-16 17:50:59,2012-12-10 23:07:33,A Multi-Index Construction for Intraday TimeSeries (10 min price data),<python><data.frame><time-series><pandas><multi-index>,2.0,,,
13423689,1,13849796.0,2012-11-16 20:07:37,1,103,"I am trying to run some model simulations on PiCloud and deal with the results with Pandas  (PiCloud is basically an interface to the Amazon cluster  through which I run things on an Ubuntu 1104 virtual environment)

The problematic command seems to be:

fplf_df = pdDataFrame(fpld  columns = var_name_list  index = sample_names_ordered)


fpld is a dict (61 keys that are strings  and values are one numpy array(length 1) and the rest floats)  and var_name_list and sample_names_ordered are lists or arrays of strings

When I run the script containing the command  all the PiCloud jobs terminate with this error:

Traceback (most recent call last):
 File ""/usr/local/picloud/employee/pimployee/job_utilpy""  line 119  in process_job
  result = func(*args  **kwargs)
 File ""/home/itchy/ecopetrol/ec-working/pecube_scripts/run_eceS1_cloudpy""  line 297  in  run_pecube_map
 File ""/usr/local/lib/python27/dist-packages/pandas/core/framepy""  line 125  in __init__
sdict  columns  index = self_init_dict(data  index  columns  dtype)
 File ""/usr/local/lib/python27/dist-packages/pandas/core/framepy""  line 176  in _init_dict
v = Series(v  index=index)
 File ""/usr/local/lib/python27/dist-packages/pandas/core/seriespy""  line 172  in __new__
subarrindex = index
File ""/usr/local/lib/python27/dist-packages/pandas/core/seriespy""  line 193  in _set_index
raise AssertionError('Lengths of index and values did not match!')
AssertionError: Lengths of index and values did not match!


The bugger is that it works fine when I run it on my machine  and it works when I ssh into the virtual environment on Amazon's servers and make the DataFrame manually with IPython  I have also tried to reproduce the error by giving index arguments that don't exist  or other arbitrary index arguments  and some are successful and others aren't  but none raise this particular error  It seems to me (confirmed by manual tests) that it shouldn't matter exactly what the length of the index is because the values only compose one row  so when the index is added it just fills down  which is exactly what I want

So what exactly causes this error?  I doubt the root of the problem lies with Pandas  but I think that if i have some idea what might be going wrong at the Pandas level  I can figure out what is going wrong with the communication from my machine to the virtual env (or whatever the real problem is)

For what it's worth:
My machine has pandas 091rc1 (latest bleeding-edge Ubuntu 1204 update  maybe this morning) and the Amazon env has 090 (I think) from an egg in mid-October  On Amazon  IPython and regular python calls from the terminal seem to be importing the same version of Pandas

Any help would be appreciated

Thanks!",1696130,,,,2012-12-12 22:28:09,pandas dataframe index error: AssertionError: Lengths of index and values did not match,<python><cloud><data.frame><pandas>,1.0,1.0,,
13445174,1,,2012-11-18 22:14:06,1,109,"After fighting with NumPy and dateutil for days  I recently discovered the amazing Pandas library I've been poring through the documentation and source code  but I can't figure out how to get date_range() to generate indices at the right breakpoints

from datetime import date
import pandas as pd

start = date('2012-01-15')
end = date('2012-09-20')
# 'M' is month-end  instead I need same-day-of-month
date_range(start  end  freq='M')


What I want:

2012-01-15
2012-02-15
2012-03-15

2012-09-15


What I get:

2012-01-31
2012-02-29
2012-03-31

2012-08-31


I need month-sized chunks that account for the variable number of days in a month This is possible with dateutilrrule:

rrule(freq=MONTHLY  dtstart=start  bymonthday=(startday  -1)  bysetpos=1)


Ugly and illegible  but it works How can do I this with pandas? I've played with both date_range() and period_range()  so far with no luck

My actual goal is to use groupby  crosstab and/or resample to calculate values for each period based on sums/means/etc of individual entries within the period In other words  I want to transform data from:

                total
2012-01-10 00:01    50
2012-01-15 01:01    55
2012-03-11 00:01    60
2012-04-28 00:01    80

#Hypothetical usage
dataframeresample('total'  how='sum'  freq='M'  start='2012-01-09'  end='2012-04-15') 


to

                total
2012-01-09          105 # Values summed
2012-02-09          0   # Missing from dataframe
2012-03-09          60
2012-04-09          0   # Data past end date  not counted


Given that Pandas originated as a financial analysis tool  I'm virtually certain that there's a simple and fast way to do this Help appreciated!",649167,,649167.0,2012-11-19 02:35:01,2012-11-19 14:16:57,Date ranges in Pandas,<datetime><time-series><pandas>,2.0,,,
13370525,1,13371090.0,2012-11-13 23:14:28,2,254,"I have a dictionary name date_dict keyed by datetime dates with values corresponding to integer counts of observations I convert this to a sparse series/dataframe with censored observations that I would like to join or convert to a series/dataframe with continuous dates The nasty list comprehension is my hack to get around the fact that pandas apparently won't automatically covert datetime date objects to an appropriate DateTime index

df1 = pdDataFrame(data=date_dictvalues() 
                   index=[datetimedatetimecombine(i  datetimetime()) 
                          for i in date_dictkeys()] 
                   columns=['Name'])
df1 = df1sort(axis=0)


This example has 1258 observations and the DateTime index runs from 2003-06-24 to 2012-11-07

df1head()
             Name
Date
2003-06-24   2
2003-08-13   1
2003-08-19   2
2003-08-22   1
2003-08-24   5


I can create an empty dataframe with a continuous DateTime index  but this introduces an unneeded column and seems clunky I feel as though I'm missing a more elegant solution involving a join

df2 = pdDataFrame(data=None columns=['Empty'] 
                   index=pdDateRange(min(date_dictkeys()) 
                                      max(date_dictkeys())))
df3 = df1join(df2 how='right')
df3head()
            Name    Empty
2003-06-24   2   NaN
2003-06-25  NaN  NaN
2003-06-26  NaN  NaN
2003-06-27  NaN  NaN
2003-06-30  NaN  NaN


Is there a simpler or more elegant way to fill a continuous dataframe from a sparse dataframe so that there is (1) a continuous index  (2) the NaNs are 0s  and (3) there is no left-over empty column in the dataframe?

            Name
2003-06-24   2
2003-06-25   0
2003-06-26   0
2003-06-27   0
2003-06-30   0
",1574687,,1306530.0,2012-11-14 00:34:27,2012-11-14 00:34:27,Filling continuous pandas dataframe from sparse dataframe,<python><python-2.7><pandas>,1.0,,3.0,
13419822,1,13420016.0,2012-11-16 15:43:15,0,96,"I noticed a bug in my program and the reason it is happening is because it seems that pandas is copying by reference a pandas dataframe instead of by value I know immutable objects will always be passed by reference but pandas dataframe is not immutable so I do not see why it is passing by reference Can anyone provide some information? 

Thanks!
Andrew",1449148,,,,2012-11-16 15:55:06,"pandas dataframe, copy by value",<python><pandas>,1.0,,,
13184553,1,13184793.0,2012-11-01 19:57:40,2,47,"I have a dataframe which includes a column that has a list When I write the dataframe to a file then re-open it  I end up converting the list to a string Is there a way to safely read/write dataframes that have lists as members?

df1 = DataFrame({'a':[['john quincy'  'tom jones'  'jerry rice'] ['bob smith' 'sally ride' 'little wayne'] ['seven' 'eight' 'nine'] ['ten' 'eleven' 'twelve']] 'b':[9 2 4 5]  'c': [7 3 0 9]})

df1to_csv('tempcsv')
df2 = read_csv('tempcsv')

#note how the list (df1) has been converted to a string (df2)
df1['a'][0]
['john quincy'  'tom jones'  'jerry rice']

df2['a'][0]
""['john quincy'  'tom jones'  'jerry rice']""
",983191,,983191.0,2012-11-01 20:53:09,2012-11-01 20:53:09,list to string in df.to-csv(),<python><pandas>,2.0,3.0,,
13293810,1,13301453.0,2012-11-08 16:54:59,0,109,"I would like to import the following csv as strings not as int64 Pandas read_csv automatically converts it to int64  but I need this column as string

ID
00013007854817840016671868
00013007854817840016749251
00013007854817840016754630
00013007854817840016781876
00013007854817840017028824
00013007854817840017963235
00013007854817840018860166


df = read_csv('samplecsv')

dfID
>>

0   -9223372036854775808
1   -9223372036854775808
2   -9223372036854775808
3   -9223372036854775808
4   -9223372036854775808
5   -9223372036854775808
6   -9223372036854775808
Name: ID


Unfortunately using converters gives the same result 

df = read_csv('samplecsv'  converters={'ID': str})
dfID
>>

0   -9223372036854775808
1   -9223372036854775808
2   -9223372036854775808
3   -9223372036854775808
4   -9223372036854775808
5   -9223372036854775808
6   -9223372036854775808
Name: ID
",387251,,,,2012-12-10 15:59:06,Import pandas dataframe column as string not int,<pandas>,2.0,1.0,,
13388330,1,,2012-11-14 22:14:50,0,40,"I have a Panel of data and I want to slice the cude of data to plot it

Eg A slice on the minor dimension The data is 


Dimensions: 71 (items) x 192 (major) x 19 (minor)
Items: gain to gain_delta
Major axis: AFG to ZWE
Minor axis: ISO3 to 2011


So If I want  to plot this slice:

scoresix[['gain'] ['ESP'] 2:21]


gives


Dimensions: 1 (items) x 1 (major) x 17 (minor)
Items: gain to gain
Major axis: ESP to ESP
Minor axis: 1995 to 2011>


but these all give errors:

scoresix[['gain'] ['ESP'] 2:21]plot()

scoresix[['gain'] ['ESP'] 2:21]to_frameplot()

a=scoresix[['gain'] ['ESP'] 2:21]
plot(a)


Thanks!",556641,,,,2012-11-15 13:15:20,Ploting pandas panel across one dimension,<python><pandas>,1.0,,,
13434077,1,,2012-11-17 19:34:36,0,74,"I just started using Pandas  and so far it's been great for what I'm doing I have just one problem though; the tables it generates won't align column names and rows Is there a solution to this? Basically  I don't see column names above the correct data  but all packed to the left The data shows up nicely formatted  though

PS I used the pdread_csv() function to output a csv file with a date series",1434094,,,,2012-11-17 19:54:12,Python Pandas Data Formatting Not Great,<python><python-2.7><pandas>,1.0,1.0,,2012-11-18 12:52:35
13385663,1,13389827.0,2012-11-14 19:13:39,1,91,"I have a dataframe with monthly financial data:

In [89]: vfiax_monthlyhead()
Out[89]: 
            year  month  day       d   open  close   high    low  volume  aclose
2003-01-31  2003      1   31  731246  6495  6495  6495  6495       0   6495
2003-02-28  2003      2   28  731274  6398  6398  6398  6398       0   6398
2003-03-31  2003      3   31  731305  6459  6459  6459  6459       0   6459
2003-04-30  2003      4   30  731335  6993  6993  6993  6993       0   6993
2003-05-30  2003      5   30  731365  7361  7361  7361  7361       0   7361


I'm trying to calculate the returns like that:

In [90]: returns = (vfiax_monthlyopen[1:] - vfiax_monthlyopen[:-1])/vfiax_monthlyopen[1:]


But I'm getting only zeroes:

In [91]: returnshead()
Out[91]: 
2003-01-31   NaN
2003-02-28     0
2003-03-31     0
2003-04-30     0
2003-05-30     0
Freq: BM  Name: open


I think that's because the arithmetic operations get aligned on the index and that makes the [1:] and [:-1] useless

My workaround is:

In [103]: returns = (vfiax_monthlyopen[1:]values - vfiax_monthlyopen[:-1]values)/vfiax_monthlyopen[1:]values

In [104]: returns = pdSeries(returns  index=vfiax_monthlyindex[1:])

In [105]: returnshead()
Out[105]: 
2003-02-28   -0015161
2003-03-31    0009444
2003-04-30    0076362
2003-05-30    0049993
2003-06-30    0012477
Freq: BM


Is there a better way to calculate the returns? I don't like the conversion to array and then back to Series",243238,,,,2012-11-15 00:45:03,Calculating returns from a dataframe with financial data,<pandas><finance>,2.0,,,
13445241,1,13445630.0,2012-11-18 22:22:39,3,154,"I want to find all values in a Pandas dataframe that contain whitespace (any arbitrary amount) and replace those values with NaNs

Any ideas how this can be improved?

Basically I want to turn this:

                   A    B    C
2000-01-01 -0532681  foo    0
2000-01-02  1490752  bar    1
2000-01-03 -1387326  foo    2
2000-01-04  0814772  baz     
2000-01-05 -0222552         4
2000-01-06 -1176781  qux     


Into this:

                   A     B     C
2000-01-01 -0532681   foo     0
2000-01-02  1490752   bar     1
2000-01-03 -1387326   foo     2
2000-01-04  0814772   baz   NaN
2000-01-05 -0222552   NaN     4
2000-01-06 -1176781   qux   NaN


I've managed to do it with the code below  but man is it ugly It's not Pythonic and I'm sure it's not the most efficient use of pandas either I loop through each column and do boolean replacement against a column mask generated by applying a function that does a regex search of each value  matching on whitespace

for i in dfcolumns:
    df[i][df[i]apply(lambda i: True if research('^\s*$'  str(i)) else False)]=None


It could be optimized a bit by only iterating through fields that could contain empty strings:

if df[i]dtype == npdtype('object')


But that's not much of an improvement

And finally  this code sets the target strings to None  which works with Pandas' functions like fillna()  but it would be nice for completeness if I could actually insert a NaN directly instead of None

Help!",221390,,,,2012-11-18 23:15:17,Replacing blank values (white space) with NaN in pandas,<python><pandas>,1.0,2.0,1.0,
